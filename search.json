[
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/about.html#educationwork-background",
    "href": "about/about.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "about/about.html#r-experience",
    "href": "about/about.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "about/about.html#research-interests",
    "href": "about/about.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "about/about.html#hometown",
    "href": "about/about.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "about/about.html#hobbies",
    "href": "about/about.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "about/about.html#fun-fact",
    "href": "about/about.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Your Name\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 601, Fundamentals of Data Science. The course provides students with an introduction to R and the tidyverse, scientific publishing, and collaboration through GitHub, building a foundation for future coursework. Students also are introduced to general data management and data wrangling skills, with an emphasis on best practice workflows and tidy data management.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 21, 2022\n\n\n601_finalpro\n\n\nSai Padma Pothula\n\n\n\n\nDec 18, 2022\n\n\nPopular Baby Name Analysis\n\n\nAleacia Messiah\n\n\n\n\nDec 18, 2022\n\n\nEmma Narkewicz Final Project\n\n\nEmma Narkewicz\n\n\n\n\nDec 18, 2022\n\n\nNaughton Final Project\n\n\nCourtney Naughton\n\n\n\n\nDec 17, 2022\n\n\nAnalysis of calories spent during exercise\n\n\nAlana Mazur\n\n\n\n\nDec 17, 2022\n\n\nDACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system\n\n\nDaniel Seriy\n\n\n\n\nDec 17, 2022\n\n\nFinal Project - Samplepalooza\n\n\nRyan O’Donnell\n\n\n\n\nDec 14, 2022\n\n\nFinal Project\n\n\nSiddharth Nammara Kalyana Raman\n\n\n\n\nInvalid Date\n\n\nDACSS 601 Final Project - Caitlin Rowley\n\n\nCaitlin Rowley\n\n\n\n\nNov 26, 2022\n\n\nTransportation Emissions from 2015-2022\n\n\nConnor Skowyra\n\n\n\n\nInvalid Date\n\n\nCitizenship Laws by Country\n\n\nKRISTIN ABIJAOUDE\n\n\n\n\nInvalid Date\n\n\nGlobal Military Spending - An Analysis\n\n\nJulian Castoro\n\n\n\n\nInvalid Date\n\n\nGlobal Military Spending - An Analysis\n\n\nJulian Castoro\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nJulian Castoro\n\n\n\n\nDec 22, 2022\n\n\nFinal Project: Flower Sales over the Fall Semester\n\n\nMichaela Bowen\n\n\n\n\nDec 22, 2022\n\n\nHow has Terrorism Grown Over The Years ? - A Visual Study\n\n\nVishnupriya Varadharaju\n\n\n\n\nDec 18, 2022\n\n\nFinal Project\n\n\nDarron Bunt\n\n\n\n\nDec 18, 2022\n\n\nLayoffs dataset\n\n\nManan Patel\n\n\n\n\nDec 15, 2022\n\n\nFinal Project - Abby Balint\n\n\nAbby Balint\n\n\n\n\nInvalid Date\n\n\nFinal Project - Layoffs since pandemic\n\n\nKavya Harlalka\n\n\n\n\nInvalid Date\n\n\nOlympic Analysis\n\n\nMatthew Norberg\n\n\n\n\nInvalid Date\n\n\nFinal Project\n\n\nSaid Arslan\n\n\n\n\nInvalid Date\n\n\nFinal Project Erika Nagai\n\n\nErika Nagai\n\n\n\n\nDec 21, 2022\n\n\nFinal Project\n\n\nNiyati Sharma\n\n\n\n\nDec 20, 2022\n\n\nSaisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data\n\n\nSaisrinivas Ambatipudi\n\n\n\n\nDec 18, 2022\n\n\nDACSS 601 Final Project\n\n\nJanhvi Joshi\n\n\n\n\nDec 18, 2022\n\n\nFinal Project\n\n\nMariia Dubyk\n\n\n\n\nDec 18, 2022\n\n\n601_Final_Project\n\n\nSahasra\n\n\n\n\nDec 18, 2022\n\n\nDACSS 601: Florida Homelessness\n\n\nDane Shelton\n\n\n\n\nDec 17, 2022\n\n\nFinal_Project\n\n\nTejaswini_Ketineni\n\n\n\n\nDec 17, 2022\n\n\n601 Fall Final Project- A study on the crime data of Massachusetts state\n\n\nJerin Jacob\n\n\n\n\nDec 17, 2022\n\n\nFinal Project\n\n\nKarla Barrett-Dexter\n\n\n\n\nDec 17, 2022\n\n\nSarah McAlpine - Final Project\n\n\nSarah McAlpine\n\n\n\n\nDec 15, 2022\n\n\nMovie Exploratoy Analysis: Final Project\n\n\nShriya Sehgal\n\n\n\n\nDec 14, 2022\n\n\nFinal Project\n\n\nJack Sniezek\n\n\n\n\nDec 14, 2022\n\n\nSan Francisco Crime Data Exploration\n\n\nNikita Masanagi\n\n\n\n\nDec 10, 2022\n\n\nDACSS 601: Final Paper\n\n\nVinitha Maheswaran\n\n\n\n\nDec 5, 2022\n\n\nFinal project: Calculating and visualizing texture profile analysis\n\n\nXiaoyan Hu\n\n\n\n\nNov 30, 2022\n\n\nDACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan\n\n\nGuanhua Tan\n\n\n\n\nInvalid Date\n\n\nJerinJacob_final_projectpdf\n\n\nJerin Jacob\n\n\n\n\nDec 17, 2022\n\n\nSF Library Usage Analysis\n\n\nPrajakti Kapade\n\n\n\n\nDec 1, 2022\n\n\nFinal Paper\n\n\nMatthew O'Neill\n\n\n\n\nNov 15, 2022\n\n\nFinal Project\n\n\nNeeharika Karanam\n\n\n\n\nInvalid Date\n\n\nFinal Project\n\n\nOwen Tibby\n\n\n\n\nInvalid Date\n\n\nFinal Paper\n\n\nSanjana Jhaveri\n\n\n\n\nDec 12, 2022\n\n\nFinal Project\n\n\nTheresa Szczepanski\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/601_final_project.html",
    "href": "posts/601_final_project.html",
    "title": "601_finalpro",
    "section": "",
    "text": "Introduction This data set describes about Bike buyers from Pacific, Europe and North America. Data can be collected from previous buyers records. Analysing and modelling these datasets gives us an idea of what kind of people are buying bikes.Based on this data, we can predict who would be likely to purchase a bike using a classification algorithm. Our potential target variable is “Purchased.Bike”, which is binary (Yes = 1, No = 0). It is not very easy to read this data because you should have a clear understanding on how certain variables are impacting some variables. This data will give us an insight on income, occupation, age, Marital status which I believe are major factors in purchasing a bike. I am interested to study this bike buyers dataset. However there might some difficulties to identify some patterns. I have never studied any data sets related to automobiles. I would like to have a hands on experience on automobile related things. I believe this will help me understand more about data. We will be analysing data like acquiring, examining, querying the data. Then, we will visualise the data and determine needs for cleaning that is the most important phase of any data project. After completion of data understanding phase, we will prepare the data. In the data preparation phase, we will determine how to use the data set. For example, correction, removing or replacing.\nData Description The data has been provided in the form of a CSV file, which contains the following information:\n\nID - An identifier column for each record\nMarital Status - Is the record for a person who is Married, or Single\nGender - Is the record for a person who is Male, Female, or NA (not given)\nIncome - Income level of the person. Values given in integer dollars\nChildren - Number of children for the person\nEducation - Education level of the person\nOccupation - Occupation that the person currently has\nHome Owner - Is the person a home owner (Yes) or not (No)? NA indicates no data available\nCars - Number of cars that the person owns\nCommute Distance - Distance to commute to ????\nRegion - Region the person is from\nAge - Age of the person\nPurchased Bike - Did the person purchase a bike (Yes) or not? (No)\n\nData Exploration\nFor the purposes of building a supervised classification algorithm, we set our target variable as Purchased Bike, which is 1 if the person did purchase a bike and 0 if the person did not.\nWe would now like to explore all the variables we have to understand their distributions, any outliers / missing values, and which are the best that can be used as feature variables.\nThese have been explored in the Jupyter notebook, with relevant observations noted in the markdown cells.\n\nlibrary('tidyverse')\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary('ggplot2')\nbike_buyers = read_csv('_data/bike_buyers.csv')\n\nRows: 1000 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Marital Status, Gender, Education, Occupation, Home Owner, Commute ...\ndbl (5): ID, Income, Children, Cars, Age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike_buyers\n\n# A tibble: 1,000 × 13\n      ID Marital S…¹ Gender Income Child…² Educa…³ Occup…⁴ Home …⁵  Cars Commu…⁶\n   <dbl> <chr>       <chr>   <dbl>   <dbl> <chr>   <chr>   <chr>   <dbl> <chr>  \n 1 12496 Married     Female  40000       1 Bachel… Skille… Yes         0 0-1 Mi…\n 2 24107 Married     Male    30000       3 Partia… Cleric… Yes         1 0-1 Mi…\n 3 14177 Married     Male    80000       5 Partia… Profes… No          2 2-5 Mi…\n 4 24381 Single      <NA>    70000       0 Bachel… Profes… Yes         1 5-10 M…\n 5 25597 Single      Male    30000       0 Bachel… Cleric… No          0 0-1 Mi…\n 6 13507 Married     Female  10000       2 Partia… Manual  Yes         0 1-2 Mi…\n 7 27974 Single      Male   160000       2 High S… Manage… <NA>        4 0-1 Mi…\n 8 19364 Married     Male    40000       1 Bachel… Skille… Yes         0 0-1 Mi…\n 9 22155 <NA>        Male    20000       2 Partia… Cleric… Yes         2 5-10 M…\n10 19280 Married     Male       NA       2 Partia… Manual  Yes         1 0-1 Mi…\n# … with 990 more rows, 3 more variables: Region <chr>, Age <dbl>,\n#   `Purchased Bike` <chr>, and abbreviated variable names ¹​`Marital Status`,\n#   ²​Children, ³​Education, ⁴​Occupation, ⁵​`Home Owner`, ⁶​`Commute Distance`\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/601_final_project.html#including-plots",
    "href": "posts/601_final_project.html#including-plots",
    "title": "601_finalpro",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html",
    "href": "posts/601_Final_Project_snammarakaly.html",
    "title": "Final Project",
    "section": "",
    "text": "According to the Police Foundation’s the crime analysis is defined as the qualitative and quantitative study of crime and law enforcement information in combination with socio-demographic and spatial factors to apprehend criminals, prevent crime, reduce disorder, and evaluate organizational procedures.\nThe primary purpose of crime analysis is to assist or support a police department’s operations. These activities include patrolling, patrolling operations, crime prevention and reduction methods, problem-solving, evaluation and accountability of police actions, criminal investigation, arrest, and prosecution. Crime analysis would not be possible without police forces.\nSo in this project we have taken a small sample of Philadelphia crime data to perform some statistical analysis and understand their trends. The dataset was taken from OpenDataPhilly. The OpenDataPhilly is a source for the open data in the Philadelphia region.\nSome of the questions to which I want to find out the answers are :\nWhat are the different categories of crime happening in Philadelphia and what are the most common crimes?\nHow is the trend of crime as the years progress, whether the crimes are increasing or decreasing? This will help us to determine whether the strategies implemented by the police force to reduce the crime rate is working or not.\nThe month with the most number of crimes?\nThe hour with the most number of crimes?\nThe district in Philadelphia with most number of crimes?\nThe answers to the above three questions will help us to determine when and where do we need to increase the security?\n\n\nCode\n#Loading libraries\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(summarytools)\n\n\nWarning: package 'summarytools' was built under R version 4.2.2\n\n\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\nCode\nload(\"snkraman_final.RData\")\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html#import-the-data",
    "href": "posts/601_Final_Project_snammarakaly.html#import-the-data",
    "title": "Final Project",
    "section": "Import the Data",
    "text": "Import the Data\nImporting the Philadelphia crime data into R.\n\n\nCode\n#dataset<-read_csv(\"snkraman_final.RData\")\nhead(dataset)\n\n\n# A tibble: 6 × 14\n  Dc_Dist Psa   Dispatch_Date_Time  Year  Month Dispatch…¹  Hour  Dc_Key Locat…²\n  <chr>   <chr> <dttm>              <chr> <chr> <time>     <dbl>   <dbl> <chr>  \n1 35      D     2009-07-19 01:09:00 2009  07    01:09          1 2.01e11 5500 B…\n2 09      R     2009-06-25 00:14:00 2009  06    00:14          0 2.01e11 1800 B…\n3 17      1     2015-04-25 12:50:00 2015  04    12:50         12 2.02e11 800 BL…\n4 23      K     2009-02-10 14:33:00 2009  02    14:33         14 2.01e11 2200 B…\n5 22      3     2015-10-06 18:18:00 2015  10    18:18         18 2.02e11 1500 B…\n6 22      3     2015-10-09 00:49:00 2015  10    00:49          0 2.02e11 1500 B…\n# … with 5 more variables: UCR_General <dbl>, Text_General_Code <chr>,\n#   Police_Districts <dbl>, Lon <dbl>, Lat <dbl>, and abbreviated variable\n#   names ¹​Dispatch_Time, ²​Location_Block"
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html#dataset-summary",
    "href": "posts/601_Final_Project_snammarakaly.html#dataset-summary",
    "title": "Final Project",
    "section": "Dataset Summary",
    "text": "Dataset Summary\nThe columns and their descriptions are as follows :\n\nDc_Dist - A two character field that names the District boundary.\nPsa - It is a single character field that names the Police Service Area boundary.\nDC_Key - The unique identifier of the crime that consists of Year+District+Unique ID.\nDispatch_Date_Time - The date and time that the officer was dispatched to the scene.\nDispatch_Date - It is the dispatch date formatted as character.\nDispatch_Time - It is the dispatach time formatted as character.\nHour - It is the generalized hour of the dispatched time.\nLocation_Block - The location of crime generalized by the street block.\nUCR_General - Universal Crime Reporting, it is used to compare crimes in other areas.\nText_General_Code - It defines the crime category.\nPolice_Districts - It defines the police district where the crime happened.\nMonth - It defines the month and year on which the crime happened.\nLon - Longitude of the crime location.\nLat - Latitude of the crime location.\n\n\n\nCode\nprint(dfSummary(dataset, \n                varnumbers= FALSE, \n                plain.ascii= FALSE, \n                style= \"grid\", \n                graph.magnif= 0.80, \n                valid.col= TRUE),\n      method= 'render', \n      table.classes= 'table-condensed')\n\n\n\n\nData Frame Summary\ndataset\nDimensions: 2220256 x 14\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Valid\n      Missing\n    \n  \n  \n    \n      Dc_Dist\n[character]\n      1. 152. 243. 254. 195. 126. 357. 228. 149. 0210. 18[ 15 others ]\n      183647(8.3%)161381(7.3%)150376(6.8%)138399(6.2%)131146(5.9%)130351(5.9%)126499(5.7%)120496(5.4%)116000(5.2%)108927(4.9%)853034(38.4%)\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Psa\n[character]\n      1. 22. 13. 34. 45. E6. D7. K8. J9. H10. F[ 20 others ]\n      487711(22.0%)447161(20.1%)400769(18.1%)60130(2.7%)52898(2.4%)51656(2.3%)51178(2.3%)50883(2.3%)50549(2.3%)48318(2.2%)519003(23.4%)\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Dispatch_Date_Time\n[POSIXct, POSIXt]\n      min : 2006-01-01med : 2011-03-09 03:11:00max : 2017-03-23 01:29:00range : 11y 2m 22d 1H 29M 0S\n      1729275 distinct values\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Year\n[character]\n      1. 20062. 20083. 20074. 20095. 20106. 20127. 20118. 20139. 201410. 2015[ 2 others ]\n      232577(10.5%)222118(10.0%)222021(10.0%)203659(9.2%)198048(8.9%)195544(8.8%)194264(8.7%)185308(8.3%)185132(8.3%)182349(8.2%)199236(9.0%)\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Month\n[character]\n      1. 082. 073. 054. 065. 036. 107. 048. 099. 0110. 11[ 2 others ]\n      202943(9.1%)200388(9.0%)196653(8.9%)193874(8.7%)188858(8.5%)188751(8.5%)187082(8.4%)185960(8.4%)179756(8.1%)170828(7.7%)325163(14.6%)\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Dispatch_Time\n[hms, difftime]\n      min : 0med : 52260max : 86340units : secs\n      1440 distinct values\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Hour\n[numeric]\n      Mean (sd) : 13.2 (6.8)min ≤ med ≤ max:0 ≤ 14 ≤ 23IQR (CV) : 10 (0.5)\n      24 distinct values\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Dc_Key\n[numeric]\n      Mean (sd) : 201097291687 (323181332)min ≤ med ≤ max:199812085407 ≤ 2.01106e+11 ≤ 2.01777e+11IQR (CV) : 586952056 (0)\n      2220256 distinct values\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Location_Block\n[character]\n      1. 4600 BLOCK E ROOSEVELT BL2. 1000 BLOCK MARKET ST3. 5200 BLOCK FRANKFORD AVE4. 0 BLOCK N 52ND ST5. 1300 BLOCK MARKET ST6. 1600 BLOCK S CHRISTOPHER 7. 1500 BLOCK MARKET ST8. 2300 BLOCK COTTMAN AVE9. 2800 BLOCK KENSINGTON AVE10. 2700 BLOCK KENSINGTON AVE[ 106072 others ]\n      4450(0.2%)3970(0.2%)3769(0.2%)2779(0.1%)2736(0.1%)2351(0.1%)2331(0.1%)2088(0.1%)2042(0.1%)2028(0.1%)2191712(98.7%)\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      UCR_General\n[numeric]\n      Mean (sd) : 1272.5 (814.7)min ≤ med ≤ max:100 ≤ 800 ≤ 2600IQR (CV) : 1200 (0.6)\n      26 distinct values\n      \n      2219602\n(100.0%)\n      654\n(0.0%)\n    \n    \n      Text_General_Code\n[character]\n      1. All Other Offenses2. Other Assaults3. Thefts4. Vandalism/Criminal Mischi5. Theft from Vehicle6. Narcotic / Drug Law Viola7. Fraud8. Recovered Stolen Motor Ve9. Burglary Residential10. Aggravated Assault No Fir[ 23 others ]\n      435476(19.6%)275523(12.4%)254714(11.5%)199335(9.0%)169539(7.6%)136599(6.2%)113555(5.1%)94186(4.2%)93979(4.2%)68421(3.1%)378275(17.0%)\n      \n      2219602\n(100.0%)\n      654\n(0.0%)\n    \n    \n      Police_Districts\n[numeric]\n      Mean (sd) : 12.1 (5.8)min ≤ med ≤ max:1 ≤ 12 ≤ 22IQR (CV) : 9 (0.5)\n      22 distinct values\n      \n      2217675\n(99.9%)\n      2581\n(0.1%)\n    \n    \n      Lon\n[numeric]\n      Mean (sd) : -75.1 (0.1)min ≤ med ≤ max:-75.3 ≤ -75.2 ≤ -75IQR (CV) : 0.1 (0)\n      197531 distinct values\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n    \n      Lat\n[numeric]\n      Mean (sd) : 40 (0)min ≤ med ≤ max:39.9 ≤ 40 ≤ 40.1IQR (CV) : 0.1 (0)\n      169409 distinct values\n      \n      2220256\n(100.0%)\n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\n##Tidy Data\n\n\nCode\nhead(dataset)\n\n\n# A tibble: 6 × 14\n  Dc_Dist Psa   Dispatch_Date_Time  Year  Month Dispatch…¹  Hour  Dc_Key Locat…²\n  <chr>   <chr> <dttm>              <chr> <chr> <time>     <dbl>   <dbl> <chr>  \n1 35      D     2009-07-19 01:09:00 2009  07    01:09          1 2.01e11 5500 B…\n2 09      R     2009-06-25 00:14:00 2009  06    00:14          0 2.01e11 1800 B…\n3 17      1     2015-04-25 12:50:00 2015  04    12:50         12 2.02e11 800 BL…\n4 23      K     2009-02-10 14:33:00 2009  02    14:33         14 2.01e11 2200 B…\n5 22      3     2015-10-06 18:18:00 2015  10    18:18         18 2.02e11 1500 B…\n6 22      3     2015-10-09 00:49:00 2015  10    00:49          0 2.02e11 1500 B…\n# … with 5 more variables: UCR_General <dbl>, Text_General_Code <chr>,\n#   Police_Districts <dbl>, Lon <dbl>, Lat <dbl>, and abbreviated variable\n#   names ¹​Dispatch_Time, ²​Location_Block\n\n\n\n\nCode\ntail(dataset)\n\n\n# A tibble: 6 × 14\n  Dc_Dist Psa   Dispatch_Date_Time  Year  Month Dispatch…¹  Hour  Dc_Key Locat…²\n  <chr>   <chr> <dttm>              <chr> <chr> <time>     <dbl>   <dbl> <chr>  \n1 06      3     2017-01-17 08:33:00 2017  01    08:33          8 2.02e11 300 BL…\n2 01      1     2017-01-17 09:13:00 2017  01    09:13          9 2.02e11 2100 B…\n3 16      1     2017-01-17 22:35:00 2017  01    22:35         22 2.02e11 N 38TH…\n4 16      1     2017-01-17 22:35:00 2017  01    22:35         22 2.02e11 N 38TH…\n5 19      2     2017-01-18 01:23:00 2017  01    01:23          1 2.02e11 6000 B…\n6 16      1     2017-01-17 16:20:00 2017  01    16:20         16 2.02e11 N 34TH…\n# … with 5 more variables: UCR_General <dbl>, Text_General_Code <chr>,\n#   Police_Districts <dbl>, Lon <dbl>, Lat <dbl>, and abbreviated variable\n#   names ¹​Dispatch_Time, ²​Location_Block\n\n\nCheck the number of rows that has null data in the dataset.\n\n\nCode\nsum(is.na(dataset))\n\n\n[1] 3889\n\n\nChecking the attributes that has null data.\n\n\nCode\ncols_null_data<-colSums(is.na(dataset))\ncolnames(dataset)[cols_null_data>0]\n\n\n[1] \"UCR_General\"       \"Text_General_Code\" \"Police_Districts\" \n\n\nChecking the number of rows in each column that has null data.\n\n\nCode\nsum(is.na(dataset$UCR_General))\n\n\n[1] 654\n\n\nCode\nsum(is.na(dataset$Police_Districts))\n\n\n[1] 2581\n\n\nCode\nsum(is.na(dataset$Lon))\n\n\n[1] 0\n\n\nCode\nsum(is.na(dataset$Lat))\n\n\n[1] 0\n\n\nRemoving the rows from the dataset that has latitude and longitude as a null value.\n\n\nCode\ndataset<-subset(dataset,dataset$Lat!=\"NA\" & dataset$Lon!=\"NA\")\nhead(dataset)\n\n\n# A tibble: 6 × 14\n  Dc_Dist Psa   Dispatch_Date_Time  Year  Month Dispatch…¹  Hour  Dc_Key Locat…²\n  <chr>   <chr> <dttm>              <chr> <chr> <time>     <dbl>   <dbl> <chr>  \n1 35      D     2009-07-19 01:09:00 2009  07    01:09          1 2.01e11 5500 B…\n2 09      R     2009-06-25 00:14:00 2009  06    00:14          0 2.01e11 1800 B…\n3 17      1     2015-04-25 12:50:00 2015  04    12:50         12 2.02e11 800 BL…\n4 23      K     2009-02-10 14:33:00 2009  02    14:33         14 2.01e11 2200 B…\n5 22      3     2015-10-06 18:18:00 2015  10    18:18         18 2.02e11 1500 B…\n6 22      3     2015-10-09 00:49:00 2015  10    00:49          0 2.02e11 1500 B…\n# … with 5 more variables: UCR_General <dbl>, Text_General_Code <chr>,\n#   Police_Districts <dbl>, Lon <dbl>, Lat <dbl>, and abbreviated variable\n#   names ¹​Dispatch_Time, ²​Location_Block\n\n\nChecking whether there are any rows that has latitude and longitude as the null values after filtering the dataset.\n\n\nCode\nsum(is.na(dataset$Lon))\n\n\n[1] 0\n\n\nCode\nsum(is.na(dataset$Lat))\n\n\n[1] 0"
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html#processing-and-visualization",
    "href": "posts/601_Final_Project_snammarakaly.html#processing-and-visualization",
    "title": "Final Project",
    "section": "Processing and Visualization",
    "text": "Processing and Visualization\nThe Text_General_Code represents the crime category.\n\n\nCode\nsum(is.na(dataset$Text_General_Code))\n\n\n[1] 654\n\n\nWe are calculating the number of occurrences of each crime type in the dataset.\n\n\nCode\ncountData<- dataset%>%count(Text_General_Code)\ncountData<-countData[-c(1),]\ncountData\n\n\n# A tibble: 33 × 2\n   Text_General_Code                  n\n   <chr>                          <int>\n 1 Aggravated Assault No Firearm  68421\n 2 All Other Offenses            435476\n 3 Arson                           5643\n 4 Burglary Non-Residential       23182\n 5 Burglary Residential           93979\n 6 Disorderly Conduct             39798\n 7 DRIVING UNDER THE INFLUENCE    52750\n 8 Embezzlement                    4642\n 9 Forgery and Counterfeiting      4816\n10 Fraud                         113555\n# … with 23 more rows\n\n\nNow we are going to see the visualized representation of each occurrence of crime category in Philadelphia.\n\n\nCode\nlibrary(ggplot2)\nggplot(data = countData, mapping = aes(x= n, y= reorder(Text_General_Code, n)))+\n  geom_col(aes(fill = Text_General_Code))+\n  geom_text(data = countData[c(1,33),],mapping = aes(label = n))+\n   theme_minimal()+\n  labs(title = \"Crime Category and their Frequency in Philadelphia\",\n       y = NULL,\n       x = \"Frequency\")+\n theme(legend.position = \"none\")\n\n\n\n\n\nRearranging the data in decreasing order so that it would be helpful for us to know the major crimes happening in the city.\n\n\nCode\ncountData<-countData[order(countData$n,decreasing = T),]\ncountData\n\n\n# A tibble: 33 × 2\n   Text_General_Code                   n\n   <chr>                           <int>\n 1 All Other Offenses             435476\n 2 Other Assaults                 275523\n 3 Thefts                         254714\n 4 Vandalism/Criminal Mischief    199335\n 5 Theft from Vehicle             169539\n 6 Narcotic / Drug Law Violations 136599\n 7 Fraud                          113555\n 8 Recovered Stolen Motor Vehicle  94186\n 9 Burglary Residential            93979\n10 Aggravated Assault No Firearm   68421\n# … with 23 more rows\n\n\nExtracting the data of the top 10 crimes happening in Philadelphia.\n\n\nCode\ntop_crime_data<-countData[1:10,]\ntop_crime_data\n\n\n# A tibble: 10 × 2\n   Text_General_Code                   n\n   <chr>                           <int>\n 1 All Other Offenses             435476\n 2 Other Assaults                 275523\n 3 Thefts                         254714\n 4 Vandalism/Criminal Mischief    199335\n 5 Theft from Vehicle             169539\n 6 Narcotic / Drug Law Violations 136599\n 7 Fraud                          113555\n 8 Recovered Stolen Motor Vehicle  94186\n 9 Burglary Residential            93979\n10 Aggravated Assault No Firearm   68421\n\n\n\n\nCode\nggplot(data = top_crime_data, mapping = aes(x= n, y= reorder(Text_General_Code, n)))+\n  geom_col(aes(fill = Text_General_Code))+\n  geom_text(data = top_crime_data[c(1,10),],mapping = aes(label = n))+\n   theme_minimal()+\n  labs(title = \"Common Crime Category in San Francisco\",\n       y = NULL,\n       x = \"Frequency\")+\n theme(legend.position = \"none\")\n\n\n\n\n\nFrom the above graph we can see that “All Other Offenses” crime category is the most frequently occurring crime. All the other crimes are similar in range to their neighbors but the frequency of “All Other Offenses” is quite high compared to the other crime categories.\nNow we are going to perform crime analysis per month. In the below code we are extracting the month and year on which the crime has happened from Dispatch_Date and making them as a separate attribute so that we can perform analysis on that.\n\n\nCode\nhead(dataset)\n\n\n# A tibble: 6 × 14\n  Dc_Dist Psa   Dispatch_Date_Time  Year  Month Dispatch…¹  Hour  Dc_Key Locat…²\n  <chr>   <chr> <dttm>              <chr> <chr> <time>     <dbl>   <dbl> <chr>  \n1 35      D     2009-07-19 01:09:00 2009  07    01:09          1 2.01e11 5500 B…\n2 09      R     2009-06-25 00:14:00 2009  06    00:14          0 2.01e11 1800 B…\n3 17      1     2015-04-25 12:50:00 2015  04    12:50         12 2.02e11 800 BL…\n4 23      K     2009-02-10 14:33:00 2009  02    14:33         14 2.01e11 2200 B…\n5 22      3     2015-10-06 18:18:00 2015  10    18:18         18 2.02e11 1500 B…\n6 22      3     2015-10-09 00:49:00 2015  10    00:49          0 2.02e11 1500 B…\n# … with 5 more variables: UCR_General <dbl>, Text_General_Code <chr>,\n#   Police_Districts <dbl>, Lon <dbl>, Lat <dbl>, and abbreviated variable\n#   names ¹​Dispatch_Time, ²​Location_Block\n\n\n\n\nCode\n#this frame was used to separate Year and Month from the dataset.\n#But as we took the image of the dataset there is no need to run this block as the Dispatch_Date column is overridden in the new frame\n\n#dataset<- dataset %>%\n # separate(`Dispatch_Date`,c('Year','Month'),sep = \"-\")\n\n#head(dataset)\n\n\nCount the number of crimes happened on each year from 2006 to 2017.\n\n\nCode\ncountCrimeByYear <- dataset %>% \n  group_by(Year) %>% \n  summarise(total = n())\ncountCrimeByYear\n\n\n# A tibble: 12 × 2\n   Year   total\n   <chr>  <int>\n 1 2006  232577\n 2 2007  222021\n 3 2008  222118\n 4 2009  203659\n 5 2010  198048\n 6 2011  194264\n 7 2012  195544\n 8 2013  185308\n 9 2014  185132\n10 2015  182349\n11 2016  166051\n12 2017   33185\n\n\n\n\nCode\nggplot(countCrimeByYear, aes(x=Year, y=total)) + \n  geom_point(size=3) + \n  geom_segment(aes(x=Year, \n                   xend=Year, \n                   y=0, \n                   yend=total)) + \n  labs(title=\"Average Crimes per Year in Philadelphia\", \n       caption=\"source: mpg\") + \n  theme(axis.text.x = element_text(angle=65, vjust=0.6))\n\n\n\n\n\nThe above plot shows the trend of the crime as the year progresses. We can see that on an average the crime has decreased by a great factor as the years progressed.\nCount the number of crimes happened on each month for all the years from 2006 to 2017.\n\n\nCode\ncountCrimeByMonth <- dataset %>% \n  group_by(Month) %>% \n  summarise(total = n())\nhead(countCrimeByMonth)\n\n\n# A tibble: 6 × 2\n  Month  total\n  <chr>  <int>\n1 01    179756\n2 02    161409\n3 03    188858\n4 04    187082\n5 05    196653\n6 06    193874\n\n\n\n\nCode\nhead(countCrimeByMonth)\n\n\n# A tibble: 6 × 2\n  Month  total\n  <chr>  <int>\n1 01    179756\n2 02    161409\n3 03    188858\n4 04    187082\n5 05    196653\n6 06    193874\n\n\nCode\ntheme_set(theme_classic())\n\nggplot(countCrimeByMonth, aes(x = Month, y = total))+\n  geom_col(fill = \"firebrick3\")+\n  theme_minimal()+\n  labs(\n    title = \"Crime per Month in Philadelphia\",\n    subtitle = \"From 2006 to 2017\",\n    x = \"Month\",\n    y = \"Total Crime\"\n  )\n\n\n\n\n\nCounting the number of crimes happened at each hour.\n\n\nCode\ncountCrimeByHour <- dataset %>% \n  group_by(Hour) %>% \n  summarise(total = n())\nhead(countCrimeByHour)\n\n\n# A tibble: 6 × 2\n   Hour  total\n  <dbl>  <int>\n1     0 119042\n2     1  94198\n3     2  66566\n4     3  46011\n5     4  29887\n6     5  22603\n\n\n\n\nCode\nlibrary(scales)\ntheme_set(theme_classic())\n\n# Plot\nggplot(countCrimeByHour, aes(x=countCrimeByHour$Hour, y=countCrimeByHour$total)) + \n  geom_point(col=\"tomato2\", size=3) +   # Draw points\n  geom_segment(aes(x=countCrimeByHour$Hour, \n                   xend=countCrimeByHour$Hour, \n                   y=min(countCrimeByHour$total), \n                   yend=max(countCrimeByHour$total)), \n               linetype=\"dashed\", \n               size=0.1) +   # Draw dashed lines\n  labs(title=\"Dot Plot for the number of crimes per hour\", \n       caption=\"source: mpg\") +  \n  coord_flip()\n\n\n\n\n\n\n\nCode\ncountCrimeByHour <- dataset %>% \n  group_by(Hour) %>% \n  summarise(total = n())\ncountCrimeByHour<-countCrimeByHour[order(countCrimeByHour$total,decreasing = T),]\ncountCrimeByHour<-head(countCrimeByHour)\ncountCrimeByHour\n\n\n# A tibble: 6 × 2\n   Hour  total\n  <dbl>  <int>\n1    16 133738\n2    17 125895\n3    19 121618\n4    22 119620\n5     0 119042\n6    18 118810\n\n\n\n\nCode\nlibrary(scales)\ntheme_set(theme_classic())\n\n# Plot\nggplot(countCrimeByHour, aes(x=countCrimeByHour$Hour, y=countCrimeByHour$total)) + \n  geom_point(col=\"tomato2\", size=3) +   # Draw points\n  geom_segment(aes(x=countCrimeByHour$Hour, \n                   xend=countCrimeByHour$Hour, \n                   y=min(countCrimeByHour$total), \n                   yend=max(countCrimeByHour$total)), \n               linetype=\"dashed\", \n               size=0.1) +   # Draw dashed lines\n  labs(title=\"Dot Plot for the number of crimes per hour\", \n       caption=\"source: mpg\") +  \n  coord_flip()\n\n\n\n\n\nThough from the above graph we can see that the max number of crimes happened at 16:00 hours, in order to infer the time range we need gather information from the first graph and see the collective time range in which maximum number of crimes are happening.\nNow we will analyse the number of crimes per district. Below we are counting the number of crimes happened in each district.\n\n\nCode\ncountCrimeByPoliceDistrict<- dataset %>% \n  group_by(Police_Districts) %>% \n  summarise(total = n())\nhead(countCrimeByPoliceDistrict)\n\n\n# A tibble: 6 × 2\n  Police_Districts  total\n             <dbl>  <int>\n1                1  48008\n2                2 116180\n3                3 114689\n4                4  31113\n5                5  96025\n6                6  44444\n\n\nIn order to know the top 6 districts where the most crimes are happening, we’ll first rearrange the data in descending order and take the top 6 rows from the dataframe. You can see the top 6 districts and the number of crimes happening in each district clearly below.\n\n\nCode\ncountTopCrimeByPoliceDistrict<-countCrimeByPoliceDistrict[order(countCrimeByPoliceDistrict$total,decreasing = T),]\ncountTopCrimeByPoliceDistrict<-head(countTopCrimeByPoliceDistrict)\ncountTopCrimeByPoliceDistrict\n\n\n# A tibble: 6 × 2\n  Police_Districts  total\n             <dbl>  <int>\n1               11 183196\n2               17 161245\n3               16 153103\n4               18 150186\n5               15 135628\n6                9 132875\n\n\nWe’ll plot a pie chart for the above data. The below pie chart shows labels of each district and also a color. The label that has the lightest color is the district where most number of crimes are happening and the label with the darkest color is the district where the least number of crimes are happening. You can also see their value range in the scale shown beside the pie chart.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(countTopCrimeByPoliceDistrict, aes(x = \"\", y = \"\", fill = countTopCrimeByPoliceDistrict$total)) +\n  geom_col() +\n  geom_label(aes(label = countTopCrimeByPoliceDistrict$Police_Districts),\n             position = position_stack(vjust = 0.5),\n             show.legend = FALSE) +\n  coord_polar(theta = \"y\")\n\n\n\n\n\nFrom the above pie chart we can clearly see that “11” is the district where the most number of crimes are happening in Philadelphia.\nIn the given dataset we have latitude and longitude values. So let’s try to plot the crime location in the map.\n\n\nCode\nmap_drug <- dataset %>% \n  filter(Year == 2006) %>% \n  select(Location_Block, Lon, Lat)\nmap_drug<-head(map_drug,50)\nmap_drug\n\n\n# A tibble: 50 × 3\n   Location_Block            Lon   Lat\n   <chr>                   <dbl> <dbl>\n 1 7200 BLOCK SAUL ST      -75.1  40.0\n 2 2200 BLOCK COTTMAN AVE  -75.1  40.0\n 3 1900 BLOCK S MOLE ST    -75.2  39.9\n 4 2000 BLOCK S HEMBERGER  -75.2  39.9\n 5 6600 BLOCK LYNFORD ST   -75.1  40.0\n 6 1700 BLOCK BORBECK AV   -75.1  40.1\n 7 1800 BLOCK S HICKS ST   -75.2  39.9\n 8 2400 BLOCK S 24TH ST    -75.2  39.9\n 9 5900 BLOCK REACH ST     -75.1  40.0\n10 7900 BLOCK BURHOLME AVE -75.1  40.1\n# … with 40 more rows\n\n\n\n\nCode\nlibrary(leaflet)\n\n\nico <- makeIcon(iconUrl = \"https://cdn.iconscout.com/icon/free/png-256/drugs-26-129384.png\",iconWidth=47/2, iconHeight=41/2)\nmap2 <- leaflet()\nmap2 <- addTiles(map2)\nmap2 <- addMarkers(map2, data = map_drug, icon = ico, popup = map_drug[,\"Location_Block\"])\nmap2\n\n\n\n\n\n\nThe above map shows the locations of 50 crime scenes happened around Philadelphia in 2006.\n\n\nCode\nmap_drug <- dataset %>% \n  filter(Text_General_Code=='Thefts',Year=='2006') %>% \n  select(Location_Block, Lon, Lat)\nmap_drug<-head(map_drug,50)\nmap_drug\n\n\n# A tibble: 50 × 3\n   Location_Block           Lon   Lat\n   <chr>                  <dbl> <dbl>\n 1 6600 BLOCK LYNFORD ST  -75.1  40.0\n 2 1700 BLOCK BORBECK AV  -75.1  40.1\n 3 2400 BLOCK S 24TH ST   -75.2  39.9\n 4 2200 BLOCK OREGON AVE  -75.2  39.9\n 5 300 BLOCK GERRITT ST   -75.2  39.9\n 6 100 BLOCK CARPENTER ST -75.1  39.9\n 7 500 BLOCK S 2ND ST     -75.1  39.9\n 8 4700 BLOCK UMBRIA ST   -75.2  40.0\n 9 0 BLOCK MIFFLIN ST     -75.1  39.9\n10 6400 BLOCK RIDGE AV    -75.2  40.0\n# … with 40 more rows\n\n\n\n\nCode\nlibrary(leaflet)\n\n\nico <- makeIcon(iconUrl = \"https://cdn.iconscout.com/icon/free/png-256/drugs-26-129384.png\",iconWidth=47/2, iconHeight=41/2)\nmap2 <- leaflet()\nmap2 <- addTiles(map2)\nmap2 <- addMarkers(map2, data = map_drug, icon = ico, popup = map_drug[,\"Location_Block\"])\nmap2\n\n\n\n\n\n\nThe above map shows the locations of 50 theft crimes happened around Philadelphia in the year 2006."
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html#reflection",
    "href": "posts/601_Final_Project_snammarakaly.html#reflection",
    "title": "Final Project",
    "section": "Reflection",
    "text": "Reflection\nI’ve learned a lot from working on this project. Before taking this course I did not have any experience in R. We see a lot of analytics used in stock exchange. Initially, I thought of choosing a stock exchange dataset and work on trends. But when I came across crime analysis dataset, it had the latitude and longitude values and I want to experiment with plotting the values on the map. So I went with crime analytics dataset. After selecting the dataset I did not understand what kind of inferences can I draw from the dataset. Then I got a question in my mind why do we actually need to analyse the crime data and who will be using this. The answer to this question helped me to start my process of analysis, frame different research questions and draw inferences from it.\nMy thought process was to understand how each column in the dataset are related. When we find a relation between the columns we can deep dive and narrow down our research further. For example, Initially I found the relation between the crime type and their frequency. Later I went down to find what are most frequent crime categories in Philadelhia.\nLater, when I was exploring about the different graphs we can plot with R, I found many interesting plots. But I did not understand how to manipulate the data in order to draw few of the graphs. I need to research and explore new techniques that will help me to manipulate data according to the needs. I tried some new ways to extract and create new column apart from the ones thought in class.\nThere were many challenges I faced while I was working on this project. I was using the tutorials and techniques learnt in class and also went through different websites in order to know how to manipulate data and draw plots. R is really a powerful tool and there is a lot for me to explore and learn so that I can draw better inferences and plot better visualizations."
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html#conclusion",
    "href": "posts/601_Final_Project_snammarakaly.html#conclusion",
    "title": "Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nNow we have answers to all our questions. The common crime categories in Philadelphia are “All Other Offenses”, “Other Assaults”, “Thefts”, “Vandalism”, “Theft from Vehicle”. Though all the crime category’s frequency are in similar range to their neighbors “All Other Offenses” frequency is way greater than other crime categories. From the aboce Lollipop chart we can see that the crime rate has decreased significantly over the years from 2006 to 2017.\nFrom the monthly crime plot we can infer that though there are differences in the number of crimes happened in each month there isn’t any month that is significantly different from other months. So we can’t develop any strategy according to the monthly analysis.\nFrom hourly analysis we can infer that the crimes have happened at all the times but if you see collectively as a group most of the crimes have happened between 6pm to 12am. From the pie plot we also know the districts where the most number of crimes happened.\nFrom all the above inferences the police need to take strategies like increasing the police force or security during night times, crime prone districts and develop technologies to prevent the crimes. Finally, when we see the yearly plot we can understand that the strategies taken by the Police Force are working as the crime rate has decreased significantly over the years."
  },
  {
    "objectID": "posts/601_Final_Project_snammarakaly.html#bibliography",
    "href": "posts/601_Final_Project_snammarakaly.html#bibliography",
    "title": "Final Project",
    "section": "Bibliography",
    "text": "Bibliography\nDataset from Kaggle- https://www.kaggle.com/datasets/mchirico/philadelphiacrimedata\nReferred crime analysis from - https://cops.usdoj.gov/ric/Publications/cops-w0273-pub.pdf\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media.\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC.\nWickham, H. (2010). A layered grammar of graphics. Journal of Computational I and Graphical Statistics, 19(1), 3-28."
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html",
    "href": "posts/AlanaZMazur_FinalProject.html",
    "title": "Analysis of calories spent during exercise",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary('dplyr')\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#introduction",
    "href": "posts/AlanaZMazur_FinalProject.html#introduction",
    "title": "Analysis of calories spent during exercise",
    "section": "Introduction",
    "text": "Introduction\nAssessing vital signs data such as heart rate and body temperature is much easier than performing direct measurements of caloric expenditure during exercise. Therefore, it is interesting to determine which vital signs and body characteristics better serve as a proxy for the caloric expenditure during an activity session. In this study, I will use a dataset with 15000 samples to offer insights as to different ways in which caloric expenditure can be measured during an activity session. Not considered in this dataset are other characteristics such as type of activity, diet, hydration, circadian rhythm, and environmental factors, including altitude and temperature.\nAnalysis of this dataset raises some questions that this study seeks to explore. The main question asks the following: which variables in the dataset have the greatest impact on caloric expenditure? Along with this prompt, this study will be guided by the following questions: Is there a correlation between calories spent per minute and different variables such as age, gender, height, weight, heart rate and body temperature? Therefore, one of the main goals of this study is finding variables more closely tied to calorie expenditure. To do this, I will use the join function to assess the calories dataframe. Here, I have decided to not implement a linear regression model and instead compute correlations between individual columns.\nBefore delving into these questions, however, I will perform a series of checks and visualizations of this dataset. Important questions to be considered are as follows: Are the values collected in the dataset representative of different demographics? Is the dataset balanced across age and gender? What information can be gathered by disaggregating the dataset by gender? This analysis will require computing basic statistics as well as plotting histograms and scatterplots of various columns in the dataset."
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#read-the-data",
    "href": "posts/AlanaZMazur_FinalProject.html#read-the-data",
    "title": "Analysis of calories spent during exercise",
    "section": "Read the data",
    "text": "Read the data\nThe data is contained in two CSV files, namely calories.csv and exercise.csv. I read them into dataframes as follows.\n\ncalories <- read.csv('_data/fmendesdat263xdemos/calories.csv')\nhead(calories)\n\n   User_ID Calories\n1 14733363      231\n2 14861698       66\n3 11179863       26\n4 16180408       71\n5 17771927       35\n6 15130815      123\n\n\n\nexercise <- read.csv('_data/fmendesdat263xdemos/exercise.csv')\nhead(exercise)\n\n   User_ID Gender Age Height Weight Duration Heart_Rate Body_Temp\n1 14733363   male  68    190     94       29        105      40.8\n2 14861698 female  20    166     60       14         94      40.3\n3 11179863   male  69    179     79        5         88      38.7\n4 16180408 female  34    179     71       13        100      40.5\n5 17771927 female  27    154     58       10         81      39.8\n6 15130815 female  36    151     50       23         96      40.7\n\n\nAs can be seen, both dataframes share the User_ID column. The calories dataframe has only one additional column, “Calories.” The exercise dataframe has various details about each participant:\n\ncolnames(exercise)\n\n[1] \"User_ID\"    \"Gender\"     \"Age\"        \"Height\"     \"Weight\"    \n[6] \"Duration\"   \"Heart_Rate\" \"Body_Temp\""
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#describe-and-visualize-the-data",
    "href": "posts/AlanaZMazur_FinalProject.html#describe-and-visualize-the-data",
    "title": "Analysis of calories spent during exercise",
    "section": "Describe and visualize the data",
    "text": "Describe and visualize the data\nLet’s analyse the data distribution of each column of the exercise dataframe. The only categorical variable in this dataset is gender.\n\ntable(exercise$Gender)\n\n\nfemale   male \n  7553   7447 \n\n\nIn the dataset, I observe only two gender categories, evenly distributed and with no missing entries. As to continuous variables, I am focusing on the Age, Height, and Weight variables in this section. In doing so, I will plot histograms to visualize the distributions of each variable. Firstly, I start with Age, as shown below.\n\nhist(exercise$Age)\n\n\n\n\nFrom my observation, this dataset is skewed towards young adults, yet it also includes a substantive amount of middle-aged and elderly individuals. It is also interesting to notice the ages of the youngest and oldest individuals as well as the median age.\n\nmin(exercise$Age)\n\n[1] 20\n\nmax(exercise$Age)\n\n[1] 79\n\nmedian(exercise$Age)\n\n[1] 39\n\n\nIn the following graphic, I would like to perform a disaggregation of the age distribution according to gender. If this dataset is well constructed, it is expected to see a similar age distribution for each gender. To implement this, I use the function filter to collect all data for each gender in a different dataframe. Then, I plot histograms and overlay them for easy comparison. The color code is orange for women participants and purple for men participants.\n\nex_female <- filter(exercise, Gender == \"female\")\nex_male <- filter(exercise, Gender == \"male\") \nhist(ex_female$Age, col = \"orange\", density = 10, main = \"Gender-disaggregated age histogram\")\nhist(ex_male$Age, col = \"purple\", density = 10, angle = 135, add = TRUE)\n\n\n\n\nAs expected, the female and male histograms line up very closely, as shown above.\nNow, in the graphic below, I plot the distributions of the variables Height and Weight.\n\nhist(exercise$Height, col = \"gray\", density = 10, angle = 0, main = \"Gender-disaggregated height histogram\")\nhist(ex_female$Height, col = \"orange\", density = 10, add = TRUE)\nhist(ex_male$Height, col = \"purple\", density = 10, angle = 135, add = TRUE)\n\n\n\n\nHere, three histograms are superimposed. The gray lines, represent the height distribution of the entire dataset. From this information, I gather that the gray line yields a fairly distorted bell-shaped curve, which reflects how the dataset combines the two gender distributions, female and male. When we make the gender-disaggregated histograms, female in orange and male in purple, respectively, the individual gender lines yield normal bell-shaped curves.\nThis demonstrates why the height dataset showing aggregated gender doesn’t yield the familiar normal distribution.\n\nhist(exercise$Weight, col = \"gray\", density = 10, angle = 0, main = \"Gender-disaggregated weight histogram\")\nhist(ex_female$Weight, col = \"orange\", density = 10, add = TRUE)\nhist(ex_male$Weight, col = \"purple\", density = 10, angle = 135, add = TRUE)\n\n\n\n\nFor the weight distributions, an even stronger effect is observed. The aggregated histogram shows two peaks. This suggests that there are two underlying distributions. By plotting the disaggregated histograms based on gender, the normal bell-shaped curves are shown as expected.\nNext step in this study is to assess how weight and height correlate. Some of the questions this dataset produce include, for instance, to what extent it showcases not only variation across gender (female and male) but also across other demographics, such body build, able-bodiedness, non-binary gender, race, ethnicity, nationality, class, etc. For the purposes of this study, I focus on the body-mass index (BMI), cognizant of the fact that the demographic categories mentioned above are not captured in the data. I will create a scatterplot of height x weight so a better visualization of each individual data point might be provided. The points are color-coded by gender, as outlined above. Finally, I will also plot the curves for BMI 25 and 30, which are the thresholds for overweight and obese. Therefore, any points above or to the left of both curves (graph 2) are considered obese. Any points to the right and below the curves show normative body weight, according to the BMI standardization. The points between both curves are considered less overweight, according to the BMI standard.\n\nlibrary(\"scales\")\nplot(ex_male$Height, ex_male$Weight, pch = 20, cex=0.5, col = alpha(\"purple\", 0.2)) \npoints(ex_female$Height, ex_female$Weight, pch = 20, cex=0.5, col = alpha(\"orange\", 0.2)) \n\n\n\ncurve(25*(x/100)^2, from = 120, to = 220, col = \"gray\")\ncurve(30*(x/100)^2, from = 120, to = 220, add = TRUE, col = \"gray\")\n\n\n\n\nIt is relevant to mention the process through which I have been able to assemble the data yielded in the graph above. The issues that I have come across while building this plot include the following. Despite testing different commands (e.g., adding the add = TRUE option, reordering the commands, etc.), I haven’t been able to merge the two graphs into one plot. Eventually, I have decided to showcase and analyse both graphs separately."
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#data-join-and-data-preparation",
    "href": "posts/AlanaZMazur_FinalProject.html#data-join-and-data-preparation",
    "title": "Analysis of calories spent during exercise",
    "section": "Data join and Data preparation",
    "text": "Data join and Data preparation\nSince the calories data are in a separate file, I will need to join the dataframes to further analyse the calorie expenditure. To show the relational data between these multiple datasets, I will use a visual representation in the following sections.\n\njoined <- inner_join(calories, exercise, by = \"User_ID\")\ndim(joined)\n\n[1] 15000     9\n\n\nThe new dataset yields 9 columns as expected and maintains the 15.000 original columns. This shows a perfect alignment between the User_ID column from both dataframes.\nI will now create a new column in the dataframe above showing calorie expenditure per unit of time.\n\njoined$Calories_per_min <- joined$Calories/joined$Duration\nhead(joined)\n\n   User_ID Calories Gender Age Height Weight Duration Heart_Rate Body_Temp\n1 14733363      231   male  68    190     94       29        105      40.8\n2 14861698       66 female  20    166     60       14         94      40.3\n3 11179863       26   male  69    179     79        5         88      38.7\n4 16180408       71 female  34    179     71       13        100      40.5\n5 17771927       35 female  27    154     58       10         81      39.8\n6 15130815      123 female  36    151     50       23         96      40.7\n  Calories_per_min\n1         7.965517\n2         4.714286\n3         5.200000\n4         5.461538\n5         3.500000\n6         5.347826\n\n\nWith this operation the new dataframe contains one more column, which I have named Calories_per_min. In the following section, I will use the Calories_per_min to calculate multiple correlations."
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#correlations",
    "href": "posts/AlanaZMazur_FinalProject.html#correlations",
    "title": "Analysis of calories spent during exercise",
    "section": "Correlations",
    "text": "Correlations\nIn the operations below, I will calculate the correlations between Calories_per_minand multiple numeric and not numeric variables. I will start with gender. This calculation answers in a quantitative manner the question whether the caloric expenditure of an individual can be predicted by gender.\n\ncor(joined$Gender == \"female\", joined$Calories_per_min)\n\n[1] 0.006035416\n\ncor(joined$Gender == \"male\", joined$Calories_per_min)\n\n[1] -0.006035416\n\n\nAmong the variables classified within Gender, as seen above, there is a very small correlation of 0.6 percent. This shows a very small difference between women and men participants.\n\ncor(joined$User_ID, joined$Calories_per_min)\n\n[1] 0.0002792329\n\n\nAs to User_ID, there is a negligible correlation, which is expected since the User_IDis an arbitrary numeric identifier.\n\ncor(joined$Heart_Rate, joined$Calories_per_min)\n\n[1] 0.8725305\n\n\nConversely, the Heart_Rate variable shows high correlation with Calorie_per_min expenditure at about 87 percent. This means this variable offers a high predicting value for calorie expenditure, contrary to the variables calculated in the operations above.\n\ncor(joined$Age, joined$Calories_per_min)\n\n[1] 0.4085408\n\n\nAs seen above, Age can also be considered a substantial predicting factor influencing calorie expenditure with rates ranging at 40 percent.\n\ncor(joined$Height, joined$Calories_per_min)\n\n[1] 0.01561341\n\ncor(joined$Weight, joined$Calories_per_min)\n\n[1] 0.05739519\n\n\nSurprisingly, in taking account of Height and Weight, results show discrepant values between these two variables with rates of 1.5 and 5.7 percent, respectively. This means that an individual’s weight offers a higher predicting value for calorie usage than one’s height.\n\ncor(joined$Body_Temp, joined$Calories_per_min)\n\n[1] 0.6719657\n\n\nLastly, accounting for the variable Body_Temp yields an even more striking result at 67 percent. This demonstrates that body temperature is a highly influential factor in determining calorie usage per unit of time, after Heart_Rate predicting values."
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#conclusion",
    "href": "posts/AlanaZMazur_FinalProject.html#conclusion",
    "title": "Analysis of calories spent during exercise",
    "section": "Conclusion",
    "text": "Conclusion\nThis study has been my first endeavor in Data Science. I have chosen a very clean dataset to be able to delve directly into my analysis without needing to perfmor extensive data clenaing, as other datasets I have considered would require. Here, I have been able to apply skills I have learned throughout the semester, including data visualization, cleaning, filtering and mutation as well as the statistic concept of correlation.\nA next step in this study could be to implement a linear regression model, which would provide a way to predict the calories spent by anyone in the general population.\nThe idea to perform gender disaggregation was inspired by a book I read last summer called “Invisible Women” by Caroline Criado Perez. Despite representing a low predicting factor, providing disagreggated gender data is relevant however small predicting values might be across gender variables. This is important antidote to scientific predictions flattening different determinants across binary and non-binary gender, age, able-bodiedness, race, ethnicity, social status, and other intersecting categories. It also opens up the floor for necessary gender-equal analysis to be developed across STEM fields and beyond."
  },
  {
    "objectID": "posts/AlanaZMazur_FinalProject.html#bibliography",
    "href": "posts/AlanaZMazur_FinalProject.html#bibliography",
    "title": "Analysis of calories spent during exercise",
    "section": "Bibliography",
    "text": "Bibliography\nCriado-Perez, C. (2019). Invisible women: Exposing data bias in a world designed for men. Chatto & Windus\nFernandez, F. (n.d.). Fmendes-DAT263x-demos. https://www.kaggle.com/datasets/fmendes/fmendesdat263xdemos\nRCore Team. (2021). R: The R Project for Statistical Computing. https://www.r-project.org/\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data (First edition). O’Reilly. https://r4ds.had.co.nz/index.html"
  },
  {
    "objectID": "posts/AleaciaMessiah_FinalProject.html",
    "href": "posts/AleaciaMessiah_FinalProject.html",
    "title": "Popular Baby Name Analysis",
    "section": "",
    "text": "Introduction and DataVisualizationReflection and ConclusionBibliography and Appendix\n\n\n\nIntroduction\nIt’s not hard to imagine a brand new mother and father leaving the hospital with their newborn child, cradling their bundle of joy as they eagerly drive home to spend time playing with the newest member of their family. Diaper changes, frequent feedings, and sleepless nights are among the multitude of things they will need to be prepared for during the baby’s first years of life. However, the one thing that excites them at the arrival of the baby is choosing his or her name. Over the years, many parents have named their children various popular and unique names like Sarah, Gertrude, Michael, and Samuel based on lists of baby girl and boy names. These lists have become so popular that the Social Security Administration (SSA) compiled a dataset that includes not just the top 1,000 popular names but the majority of names that are assigned to a Social Security Number (SSN). In this final project, I will analyze the Social Security Administration dataset containing almost all names associated with an SSN to answer the below research questions.\n1. What are the top five baby names for each year from 1910 to 2021?\n2. Which names have the largest difference in occurrences per region (i.e. which names have undergone the most change in usage)?\n3. Are there consistently more popular girl names or popular boy names?\n\n\nData\nThis dataset from the SSA contains fifty-one text files containing the two-digit state code, sex, year of birth ranging from 1910 to 2021, name, and number of occurrences for each state and the District of Columbia where the individual has an SSN. The data is restricted to only include names with at least five occurrences to preserve privacy. All of these files combined has 6,311,504 observations corresponding to a name for each state and year and 5 variables consisting of State, Gender, Year, Name, and Count.\nReviewing the summary of this baby_orig dataset, I find there are no missing values which is helpful when I do my analysis and visualizations. The states that make up the majority of the data are California with 400,762 observations (6.3% of the data), Texas with 368,987 observations (5.8% of the data), and New York with 309,532 observations (4.9% of the data). There are more female names than male names in this dataset with 3,510,324 female names (55.6% of the data) and 2,801,180 male names (44.4% of the data), meaning some states had a larger concentration of those with feminine names than masculine names in some years, which I find interesting. The names that appear the most within this dataset (not counting occurrences) is James with 7,409 observations, Leslie with 7,407 observations, and Lee with 7,313 observations. The highest number of occurrences for a name is 10,026 which happens to be with the male name Robert in New York the year 1947.\n\n\nCode\n# import baby names data \nbaby_orig <- read_delim(c(\"_data/namesbystate/AK.TXT\", \"_data/namesbystate/AL.TXT\", \"_data/namesbystate/AR.TXT\", \"_data/namesbystate/AZ.TXT\", \"_data/namesbystate/CA.TXT\", \"_data/namesbystate/CO.TXT\", \"_data/namesbystate/CT.TXT\", \"_data/namesbystate/DC.TXT\", \"_data/namesbystate/DE.TXT\", \"_data/namesbystate/FL.TXT\", \"_data/namesbystate/GA.TXT\", \"_data/namesbystate/HI.TXT\", \"_data/namesbystate/IA.TXT\", \"_data/namesbystate/ID.TXT\", \"_data/namesbystate/IL.TXT\", \"_data/namesbystate/IN.TXT\", \"_data/namesbystate/KS.TXT\", \"_data/namesbystate/KY.TXT\", \"_data/namesbystate/LA.TXT\", \"_data/namesbystate/MA.TXT\", \"_data/namesbystate/MD.TXT\", \"_data/namesbystate/ME.TXT\", \"_data/namesbystate/MI.TXT\", \"_data/namesbystate/MN.TXT\", \"_data/namesbystate/MO.TXT\", \"_data/namesbystate/MS.TXT\", \"_data/namesbystate/MT.TXT\", \"_data/namesbystate/NC.TXT\", \"_data/namesbystate/ND.TXT\", \"_data/namesbystate/NE.TXT\", \"_data/namesbystate/NH.TXT\", \"_data/namesbystate/NJ.TXT\", \"_data/namesbystate/NM.TXT\", \"_data/namesbystate/NV.TXT\", \"_data/namesbystate/NY.TXT\", \"_data/namesbystate/OH.TXT\", \"_data/namesbystate/OK.TXT\", \"_data/namesbystate/OR.TXT\", \"_data/namesbystate/PA.TXT\", \"_data/namesbystate/RI.TXT\", \"_data/namesbystate/SC.TXT\", \"_data/namesbystate/SD.TXT\", \"_data/namesbystate/TN.TXT\", \"_data/namesbystate/TX.TXT\", \"_data/namesbystate/UT.TXT\", \"_data/namesbystate/VA.TXT\", \"_data/namesbystate/VT.TXT\", \"_data/namesbystate/WA.TXT\", \"_data/namesbystate/WI.TXT\", \"_data/namesbystate/WV.TXT\", \"_data/namesbystate/WY.TXT\"), delim = \",\", col_names = c(\"State\", \"Gender\", \"Year\", \"Name\", \"Count\"))\n\n\nRows: 6311504 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): State, Gender, Name\ndbl (2): Year, Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# view baby_orig data frame\nbaby_orig\n\n\n# A tibble: 6,311,504 × 5\n   State Gender  Year Name     Count\n   <chr> <chr>  <dbl> <chr>    <dbl>\n 1 AK    F       1910 Mary        14\n 2 AK    F       1910 Annie       12\n 3 AK    F       1910 Anna        10\n 4 AK    F       1910 Margaret     8\n 5 AK    F       1910 Helen        7\n 6 AK    F       1910 Elsie        6\n 7 AK    F       1910 Lucy         6\n 8 AK    F       1910 Dorothy      5\n 9 AK    F       1911 Mary        12\n10 AK    F       1911 Margaret     7\n# … with 6,311,494 more rows\n\n\nCode\n# view summary of baby_orig\ndfSummary(baby_orig)\n\n\nData Frame Summary  \nbaby_orig  \nDimensions: 6311504 x 5  \nDuplicates: 0  \n\n----------------------------------------------------------------------------------------------------------------\nNo   Variable      Stats / Values              Freqs (% of Valid)     Graph                 Valid      Missing  \n---- ------------- --------------------------- ---------------------- --------------------- ---------- ---------\n1    State         1. CA                        400762 ( 6.3%)        I                     6311504    0        \n     [character]   2. TX                        368987 ( 5.8%)        I                     (100.0%)   (0.0%)   \n                   3. NY                        309532 ( 4.9%)                                                  \n                   4. IL                        237839 ( 3.8%)                                                  \n                   5. FL                        218192 ( 3.5%)                                                  \n                   6. PA                        206944 ( 3.3%)                                                  \n                   7. OH                        204165 ( 3.2%)                                                  \n                   8. GA                        191367 ( 3.0%)                                                  \n                   9. MI                        190023 ( 3.0%)                                                  \n                   10. NC                       181647 ( 2.9%)                                                  \n                   [ 41 others ]               3802046 (60.2%)        IIIIIIIIIIII                              \n\n2    Gender        1. F                        3510324 (55.6%)        IIIIIIIIIII           6311504    0        \n     [character]   2. M                        2801180 (44.4%)        IIIIIIII              (100.0%)   (0.0%)   \n\n3    Year          Mean (sd) : 1977.2 (31.3)   112 distinct values                      :   6311504    0        \n     [numeric]     min < med < max:                                                 . : :   (100.0%)   (0.0%)   \n                   1910 < 1983 < 2021                                         . . : : : :                       \n                   IQR (CV) : 52 (0)                                  . : . : : : : : : :                       \n                                                                      : : : : : : : : : :                       \n\n4    Name          1. James                       7409 ( 0.1%)                              6311504    0        \n     [character]   2. Leslie                      7407 ( 0.1%)                              (100.0%)   (0.0%)   \n                   3. Lee                         7313 ( 0.1%)                                                  \n                   4. John                        7221 ( 0.1%)                                                  \n                   5. Robert                      7174 ( 0.1%)                                                  \n                   6. Jessie                      6922 ( 0.1%)                                                  \n                   7. William                     6880 ( 0.1%)                                                  \n                   8. Michael                     6756 ( 0.1%)                                                  \n                   9. Mary                        6699 ( 0.1%)                                                  \n                   10. Charles                    6624 ( 0.1%)                                                  \n                   [ 32393 others ]            6241099 (98.9%)        IIIIIIIIIIIIIIIIIII                       \n\n5    Count         Mean (sd) : 50.7 (173.1)    4938 distinct values   :                     6311504    0        \n     [numeric]     min < med < max:                                   :                     (100.0%)   (0.0%)   \n                   5 < 12 < 10026                                     :                                         \n                   IQR (CV) : 26 (3.4)                                :                                         \n                                                                      :                                         \n----------------------------------------------------------------------------------------------------------------\n\n\nCode\n# find the name with the largest number of occurrences\nfilter(baby_orig, Count == 10026)\n\n\n# A tibble: 1 × 5\n  State Gender  Year Name   Count\n  <chr> <chr>  <dbl> <chr>  <dbl>\n1 NY    M       1947 Robert 10026\n\n\nAfter importing the data and looking at its summary statistics, I will refine some of the values to make it clearer what they represent. I will rename the values in the State and Gender columns to their appropriate state names and male and female as the State_Full and Gender_Full columns. I will also add a column called Region to compare names between different regions of the United States since it would be difficult to compare all fifty states and the District of Columbia. Note that the District of Columbia is included in the Middle Atlantic region despite not being a state. Additionally, Gender_Full will be converted into a factor for ease of visualization.\n\n\nCode\nbaby <- baby_orig %>% \n  # create a Region column\n  mutate(Region = case_when(\n    State == c(\"CT\", \"ME\", \"MA\", \"NH\", \"RI\", \"VT\") ~ \"New England\",\n    State == c(\"DC\", \"DE\", \"MD\", \"NJ\", \"NY\", \"PA\") ~ \"Middle Atlantic\",\n    State == c(\"AL\", \"AR\", \"FL\", \"GA\", \"KY\", \"LA\", \"MS\", \"MO\", \"NC\", \"SC\", \"TN\", \"VA\", \"WV\") ~ \"South\",\n    State == c(\"IL\", \"IN\", \"IA\", \"KS\", \"MI\", \"MN\", \"NE\", \"ND\", \"OH\", \"SD\", \"WI\") ~ \"Midwest\",\n    State == c(\"AZ\", \"NM\", \"OK\", \"TX\") ~ \"Southwest\",\n    State == c(\"AK\", \"CA\", \"CO\", \"HI\", \"ID\", \"MT\", \"NV\", \"OR\", \"UT\", \"WA\", \"WY\") ~ \"West\",\n    TRUE ~ NA_character_\n    )) %>% \n  # replace NAs in Region column\n  fill(Region, .direction = \"down\") %>% \n  # recode State values\n  mutate(State_Full = recode(State, \"AL\" = \"Alabama\", \"AK\" = \"Alaska\", \"AZ\" = \"Arizona\", \"AR\" = \"Arkansas\", \"CA\" = \"California\", \"CO\" = \"Colorado\", \"CT\" = \"Connecticut\", \"DE\" = \"Delaware\", \"DC\" = \"District of Columbia\", \"FL\" = \"Florida\", \"GA\" = \"Georgia\", \"HI\" = \"Hawaii\", \"ID\" = \"Idaho\", \"IL\" = \"Illinois\", \"IN\" = \"Indiana\", \"IA\" = \"Iowa\", \"KS\" = \"Kansas\", \"KY\" = \"Kentucky\", \"LA\" = \"Louisiana\", \"ME\" = \"Maine\", \"MD\" = \"Maryland\", \"MA\" = \"Massachussetts\", \"MI\" = \"Michigan\", \"MN\" = \"Minnesota\", \"MS\" = \"Mississippi\", \"MO\" = \"Missouri\", \"MT\" = \"Montana\", \"NE\" = \"Nebraska\", \"NV\" = \"Nevada\", \"NH\" = \"New Hampshire\", \"NJ\" = \"New Jersey\", \"NM\" = \"New Mexico\", \"NY\" = \"New York\", \"NC\" = \"North Carolina\", \"ND\" = \"North Dakota\", \"OH\" = \"Ohio\", \"OK\" = \"Oklahoma\", \"OR\" = \"Oregon\", \"PA\" = \"Pennsylvania\", \"RI\" = \"Rhode Island\", \"SC\" = \"South Carolina\", \"SD\" = \"South Dakota\", \"TN\" = \"Tennessee\", \"TX\" = \"Texas\", \"UT\" = \"Utah\", \"VT\" = \"Vermont\", \"VA\" = \"Virginia\", \"WA\" = \"Washington\", \"WV\" = \"West Virginia\", \"WI\" = \"Wisconsin\", \"WY\" = \"Wyoming\")) %>% \n  # recode Gender values\n  mutate(Gender_Full = recode(Gender, \"M\" = \"Male\", \"F\" = \"Female\")) %>% \n  # delete State and Gender columns\n  select(!c(State, Gender))\n\n\nWarning in State == c(\"CT\", \"ME\", \"MA\", \"NH\", \"RI\", \"VT\"): longer object length\nis not a multiple of shorter object length\n\n\nWarning in State == c(\"DC\", \"DE\", \"MD\", \"NJ\", \"NY\", \"PA\"): longer object length\nis not a multiple of shorter object length\n\n\nWarning in State == c(\"AL\", \"AR\", \"FL\", \"GA\", \"KY\", \"LA\", \"MS\", \"MO\", \"NC\", :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in State == c(\"IL\", \"IN\", \"IA\", \"KS\", \"MI\", \"MN\", \"NE\", \"ND\", \"OH\", :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in State == c(\"AK\", \"CA\", \"CO\", \"HI\", \"ID\", \"MT\", \"NV\", \"OR\", \"UT\", :\nlonger object length is not a multiple of shorter object length\n\n\nCode\n# convert Gender_Full into a factor\nbaby$Gender_Full <- factor(baby$Gender_Full)\n# view baby dataset\nbaby\n\n\n# A tibble: 6,311,504 × 6\n    Year Name     Count Region State_Full Gender_Full\n   <dbl> <chr>    <dbl> <chr>  <chr>      <fct>      \n 1  1910 Mary        14 West   Alaska     Female     \n 2  1910 Annie       12 West   Alaska     Female     \n 3  1910 Anna        10 West   Alaska     Female     \n 4  1910 Margaret     8 West   Alaska     Female     \n 5  1910 Helen        7 West   Alaska     Female     \n 6  1910 Elsie        6 West   Alaska     Female     \n 7  1910 Lucy         6 West   Alaska     Female     \n 8  1910 Dorothy      5 West   Alaska     Female     \n 9  1911 Mary        12 West   Alaska     Female     \n10  1911 Margaret     7 West   Alaska     Female     \n# … with 6,311,494 more rows\n\n\nCode\n# check that all states are in their appropriate regions\ntable(baby$Region)\n\n\n\nMiddle Atlantic         Midwest     New England           South       Southwest \n         883857         1384692          371959         1876994          690296 \n           West \n        1103706 \n\n\nCode\ntable(baby$State_Full)\n\n\n\n             Alabama               Alaska              Arizona \n              139293                29755               122085 \n            Arkansas           California             Colorado \n              105373               400762               112986 \n         Connecticut             Delaware District of Columbia \n               84948                33706                57265 \n             Florida              Georgia               Hawaii \n              218192               191367                56069 \n               Idaho             Illinois              Indiana \n               60445               237839               145357 \n                Iowa               Kansas             Kentucky \n               97515                97839               122608 \n           Louisiana                Maine             Maryland \n              153712                51852               116699 \n      Massachussetts             Michigan            Minnesota \n              123558               190023               119310 \n         Mississippi             Missouri              Montana \n              117316               144136                47144 \n            Nebraska               Nevada        New Hampshire \n               74699                50829                40557 \n          New Jersey           New Mexico             New York \n              159695                77755               309532 \n      North Carolina         North Dakota                 Ohio \n              181647                47666               204165 \n            Oklahoma               Oregon         Pennsylvania \n              121465                92410               206944 \n        Rhode Island       South Carolina         South Dakota \n               41540               122527                48865 \n           Tennessee                Texas                 Utah \n              147212               368987                93764 \n             Vermont             Virginia           Washington \n               29483               154400               130809 \n       West Virginia            Wisconsin              Wyoming \n               79250               121398                28751 \n\n\nThis dataset is now pretty tidy, so I will prepare for my visualizations by generating some summary data frames that will make it easier to analyze and answer my research questions.\nThe first data frame consists of the top five baby names for each year by grouping the baby data frame by Year, arranging in descending order by Count, and then taking only the top five observations for each year.\nThe second data frame will help answer the second research question by calculating the difference between the minimum and maximum occurrences for each name in each region. This is done by grouping the baby data frame by Region and Name and then summarizing by the minimum Count (min_count), maximum Count (max_count), and the difference between the two (difference). For ease of visualization, I will only extract the top five names with the largest difference in each region.\nThe third data frame finds the number of popular names for each gender in each region in order to determine whether there are more popular girl names or popular boy names. This data frame filters the baby data frame with only names that have a Count of 2,000 or more, grouped by Gender_Full and Region, and summarized by the number of occurrences. The definition of “popular” is arbitrary but I chose a limit of 2,000 occurrences or more for each name to be considered popular because the mean Count is 50.7, so 2,000 seems like a good value determining whether the name is deemed popular or not.\n\n\nCode\n# get the top 5 baby names for each year\ntop_5_baby <- baby %>% \n  group_by(Year) %>% \n  arrange(desc(Count)) %>% \n  slice_head(n = 5)\n# view top_5_baby data frame\ntop_5_baby\n\n\n# A tibble: 560 × 6\n# Groups:   Year [112]\n    Year Name  Count Region          State_Full   Gender_Full\n   <dbl> <chr> <dbl> <chr>           <chr>        <fct>      \n 1  1910 Mary   2913 West            Pennsylvania Female     \n 2  1910 Mary   1923 Middle Atlantic New York     Female     \n 3  1910 Helen  1604 Middle Atlantic Pennsylvania Female     \n 4  1910 Anna   1534 Middle Atlantic Pennsylvania Female     \n 5  1910 John   1326 Middle Atlantic Pennsylvania Male       \n 6  1911 Mary   3188 Middle Atlantic Pennsylvania Female     \n 7  1911 Mary   2322 Middle Atlantic New York     Female     \n 8  1911 Helen  1733 Middle Atlantic Pennsylvania Female     \n 9  1911 John   1672 Middle Atlantic Pennsylvania Male       \n10  1911 John   1624 Middle Atlantic New York     Male       \n# … with 550 more rows\n\n\nCode\n# get the top 5 names with the largest difference between occurrences in each region\nchange_baby <- baby %>% \n  group_by(Region, Name) %>% \n  summarize(min_count = min(Count),\n            max_count = max(Count),\n            difference = max_count - min_count) %>% \n  arrange(desc(difference)) %>% \n  slice_head(n = 5)\n\n\n`summarise()` has grouped output by 'Region'. You can override using the\n`.groups` argument.\n\n\nCode\n# view change_baby data frame\nchange_baby\n\n\n# A tibble: 30 × 5\n# Groups:   Region [6]\n   Region          Name    min_count max_count difference\n   <chr>           <chr>       <dbl>     <dbl>      <dbl>\n 1 Middle Atlantic Robert          5     10026      10021\n 2 Middle Atlantic John            5      9639       9634\n 3 Middle Atlantic Michael         5      9241       9236\n 4 Middle Atlantic Mary            5      8184       8179\n 5 Middle Atlantic Linda           5      7542       7537\n 6 Midwest         Michael         5      6221       6216\n 7 Midwest         Linda           5      5885       5880\n 8 Midwest         David           5      5495       5490\n 9 Midwest         Robert          5      5427       5422\n10 Midwest         James           5      5349       5344\n# … with 20 more rows\n\n\nCode\n# get the most popular gender of names for each region\ngender_baby <- baby %>% \n  filter(Count >= 2000) %>% \n  group_by(Gender_Full, Region) %>% \n  summarize(n = n())\n\n\n`summarise()` has grouped output by 'Gender_Full'. You can override using the\n`.groups` argument.\n\n\nCode\n# view gender_baby data frame\ngender_baby\n\n\n# A tibble: 12 × 3\n# Groups:   Gender_Full [2]\n   Gender_Full Region              n\n   <fct>       <chr>           <int>\n 1 Female      Middle Atlantic   931\n 2 Female      Midwest           631\n 3 Female      New England        31\n 4 Female      South             191\n 5 Female      Southwest         213\n 6 Female      West              611\n 7 Male        Middle Atlantic  2010\n 8 Male        Midwest          1425\n 9 Male        New England       217\n10 Male        South             395\n11 Male        Southwest         563\n12 Male        West             1419\n\n\n\n\n\nFor my research question regarding the top five baby names for each year, I will use the top_5_baby data frame I created to aid in making this visualization. I created a scatterplot below with Year as the independent variable and Count as the dependent variable. I also added Gender_Full as a color aesthetic and Name as a fill aesthetic in order to see the gender and names for each year. Moreover, Name text labels were included to display the names for each point on this plot.\n\n\nCode\n# create a scatterplot of Count vs. Year with Name labels and Gender colors\nggplot(top_5_baby, aes(Year, Count)) +\n  geom_point(aes(color = Gender_Full, fill = Name)) +\n  geom_text(\n    label = top_5_baby$Name,\n    nudge_x = 0.25, nudge_y = 0.25,\n    check_overlap = T\n  ) +\n  labs(\n    x = \"Year of Birth\",\n    y = \"Number of Occurrences\",\n    title = \"Scatterplot of Occurrences vs. Year of Birth\",\n    subtitle = \"Range of years is 1910 to 2021\",\n    caption = \"Data: https://www.ssa.gov/oact/babynames/limits.html\"\n  )\n\n\n\n\n\nThis scatterplot illustrates well the top names and their occurrences over time. We can see from the early years around 1925 that the name “Mary” was very popular and shows up several times during those years. The name “John” was also a popular name at the same time. As we get to the 1950s, we see less female names and more male names such as “Robert”, “John”, and “David”. This may be in correlation to the “baby boom” that occurred from the late 1940s to the early 1960s as World War II ended and families were settling down and having more children. Moving toward the 1970s through 1990s, girl names such as “Jennifer” and “Jessica” are rising in popularity while boy names like “Robert” dwindle in popularity in favor for “Michael” and “Daniel”. Lastly, from the 2000s on, there’s a good mix of popular male and female names like “Jayden”, “Mia”, and “Isabella”, which could be related to celebrities and other well-known people naming their kids very unique names and the general public wanting to keep the trend with their children.\nAlthough this plot effectively portrays popular names over time, it is not clear which points correspond to each year. To resolve this, I created an interactive version of my scatterplot above to make it easier to view which points are with which year, gender, and name. I removed the text labels to reduce clutter on the graph. Hovering over each point shows the Year, Count, Gender_Full, and Name.\n\n\nCode\n# create an interactive version of the scatterplot above\nggplotly(ggplot(top_5_baby, aes(Year, Count)) +\n  geom_point(aes(color = Gender_Full, fill = Name)) +\n  labs(\n    x = \"Year of Birth\",\n    y = \"Number of Occurrences\",\n    title = \"Scatterplot of Occurrences vs. Year of Birth\",\n    subtitle = \"Range of years is 1910 to 2021\",\n    caption = \"Data: https://www.ssa.gov/oact/babynames/limits.html\"\n  ))\n\n\n\n\n\n\nThis scatterplot looks a lot cleaner and clearer as to which names are in the top five for each year. As we saw previously, “Mary” consistently ranks in the top five from 1910 to around 1937, with its highest number of occurrences in 1918 at 8,184. Also aforementioned in the data import paragraph, we see “Robert” with the highest number of occurrences overall at 10,026 in 1947. “John” has the lowest number of occurrences overall at 1,326 in 1910. It’s interesting to observe how the number of occurrences changed over time from under 2,500 in 1910 to reaching over 10,000 to slowly coming back to 2,500 by 2021. This demonstrates that even with the American population increasing over the last century, not all parents are naming their children after the latest trending names.\nThe below bar graph will answer my second research question about the names that have the largest differences in occurrences per region. I used the change_baby data frame I made earlier with Name as the independent variable and difference as the dependent variable. I added color by Region and faceting by Region as well to make it easier to see which region has the names that went through the most change in usage over time. Additionally, text labels were added to show the difference for each name.\n\n\nCode\n# create a bar graph of difference vs. Name colored and faceted by Region\nggplot(change_baby, aes(Name, difference)) +\n  geom_col(aes(fill = Region)) +\n  facet_wrap(vars(Region), scales = \"free_x\") +\n  guides(x = guide_axis(angle = 90)) +\n  geom_text(\n    label = change_baby$difference,\n    check_overlap = T\n  ) +\n  labs(\n    x = \"Name\", \n    y = \"Difference\",\n    title = \"Bar Graph of Difference vs. Name\",\n    subtitle = \"Note: This only includes names with the largest differences \\n between minimum and maximum occurrences per region\",\n    caption = \"Data: https://www.ssa.gov/oact/babynames/limits.html\"\n  )\n\n\n\n\n\nLooking at this chart, it’s clear that the differences in minimum and maximum occurrences between names from different regions vary immensely. We see the Middle Atlantic region went through the most changes, along with the West, Midwest, Southwest, New England, and South in that order. “Robert” went through the greatest change in usage overall with a difference of 10,021 occurrences in the Middle Atlantic region. Furthermore, “Robert” also appeared in the Midwest, New England, and West graphs with large differences. The names that went through the greatest change in usage for the Midwest, New England, South, Southwest, and West are “Michael” with a difference of 6,216, “Robert” with a difference of 3,890, “James” with a difference of 3,877, “Linda” with a difference of 5,054, and “Michael” with a difference of 8,257 respectively.\nWhat’s intriguing about this graph is that there are more male names that went through a larger change in usage than female names. There are only one to two female names in each region except for New England. I wonder if girl names were generally not as popular as boy names or if there is a different reason why this phenomenon occurred?\nFinally, my last visualization is a scatterplot that answers my last research question of whether there are more popular girl names or popular boy names, “popular” meaning the name has a number of occurrences of 2,000 or more. We’ve already seen part of the answer from the last visualization above but the scatterplot below uses the gender_baby data frame created earlier with Region as the independent variable and n as the dependent variable. A color aesthetic is added to portray Gender_Full for each point as well as a text label showing n for each point.\n\n\nCode\n# create a scatterplot of n vs. Region with color by Gender_Full\nggplot(gender_baby, aes(Region, n)) +\n  geom_point(aes(color = Gender_Full)) +\n  geom_text(\n    label = gender_baby$n,\n    nudge_x = 0.25, nudge_y = 0.25,\n    check_overlap = T\n  ) +\n  labs(\n    x = \"Region of the United States\",\n    y = \"Number of Popular Names\",\n    title = \"Scatterplot of Popular Names vs. Region\",\n    subtitle = \"Color by Gender\",\n    caption = \"Data: https://www.ssa.gov/oact/babynames/limits.html\"\n  )\n\n\n\n\n\nThis plot illustrates there are more popular boy names for each region than popular girl names. The Middle Atlantic region has 2,010 popular boy names which is the highest overall, with the Midwest with 1,425 names, the West with 1,419 names, the Southwest with 563 names, the South with 395 names, and New England with 217 names following after. The Middle Atlantic also has the most popular girl names with 931 names, with the Midwest with 631 names, the West with 611 names, the Southwest with 213 names, the South with 191 names, and New England with 31 names following shortly after.\nThe reason why the Middle Atlantic consistently has the most popular baby names and the largest changes in usage for each name is most likely because New York and Pennsylvania are part of the Middle Atlantic and the population in both places grew exponentially over time, especially with immigration. There was a variety of names given to immigrants when they entered the United States so it makes sense that some who had foreign names that were difficult to pronounce in English were given more “American” names as American citizens. The other plausible reason may be due to the “baby boom” as aforementioned previously and the huge increase in newborn babies led to popular names changing rapidly.\n\n\n\nReflection\nOverall, I feel excited and proud of this project. I have always been interested in learning the origins of baby names (especially mine since the spelling of my name is not common) and looking at their popularity. The process with this project seemed relatively straightforward but had some complexity to it with the addition of the `Region` variable and re-coding the `State` and `Gender` variables. It was easier to compare the frequency of baby names across `Region` versus `State` but if I had more time, I would do a comparison with `State` to see how the frequency of names vary within each `State` in each region.\nMoreover, I had to change the analysis for my second research question in order to show the baby names that went through the most change in usage over time. Limiting only to the top five names in each region that went through the most change was the best option for visualization purposes; otherwise, it would’ve been difficult to clearly view the graph with every name and its `difference` in each region.\nI would love to know the reasons behind why boy names are more popular than girl names and why boy names changed more often in usage than girl names. The answer to these observations would be a wonderful starting point for the next steps to this project in the future. I would also like to go more in depth with the analysis by looking into the names that had the least number of occurrences (in this dataset it would be five) and researching why these names were not as popular as the others.\n\n\nConclusion\nFrom this Social Security Administration dataset on baby names, we can conclude that the top five baby names vary each year but in several years we see “Mary”, “John”, “Robert”, “Jennifer”, and “Michael” consistently appear towards the top. The range of occurrences for each name had gone from 2,500 to 10,000 between 1910 and 2021. The Middle Atlantic region has the greatest change in usage for names with “Robert” having the largest change overall. In general, male names had a larger difference in occurrences than female names. Finally, in regards to popularity, boy names tend to be more popular than girl names, with the Middle Atlantic region again ranking the highest with popular names overall (2,010 boy and 931 girl names). All of the research questions have been answered but in the future it would be beneficial to research more about the reasoning behind these phenomenons such as the popularity and change in usage of boy names.\n\n\n\n\nBibliography\n“International Programs - Two- Letter State Abbreviations.” Accessed December 18, 2022. https://www.ssa.gov/international/coc-docs/states.html.\n“Popular Baby Names.” Accessed December 17, 2022. https://www.ssa.gov/oact/babynames/limits.html.\n“State Regions.” Accessed December 18, 2022. https://www.infoplease.com/us/states/regions-of-the-us.\nC. Sievert. Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC Florida, 2020.\nComtois D (2022). summarytools: Tools to Quickly and Neatly Summarize Data. R package version 1.0.1, https://CRAN.R-project.org/package=summarytools.\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686.\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media.\nWickham H, Hester J, Chang W, Bryan J (2022). devtools: Tools to Make Developing R Packages Easier. R package version 2.4.5, https://CRAN.R-project.org/package=devtools.\n\n\nAppendix\n\n\nCode\n# view session info\nsession_info()\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 19044)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_India.utf8\n ctype    English_India.utf8\n tz       America/New_York\n date     2022-12-23\n pandoc   2.18 @ C:/Program Files/RStudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version date (UTC) lib source\n assertthat      0.2.1   2019-03-21 [1] CRAN (R 4.2.1)\n backports       1.4.1   2021-12-13 [1] CRAN (R 4.2.0)\n base64enc       0.1-3   2015-07-28 [1] CRAN (R 4.2.0)\n bit             4.0.5   2022-11-15 [1] CRAN (R 4.2.2)\n bit64           4.0.5   2020-08-30 [1] CRAN (R 4.2.1)\n broom           1.0.2   2022-12-15 [1] CRAN (R 4.2.2)\n cachem          1.0.6   2021-08-19 [1] CRAN (R 4.2.1)\n callr           3.7.3   2022-11-02 [1] CRAN (R 4.2.2)\n cellranger      1.1.0   2016-07-27 [1] CRAN (R 4.2.1)\n checkmate       2.1.0   2022-04-21 [1] CRAN (R 4.2.1)\n cli             3.4.1   2022-09-23 [1] CRAN (R 4.2.2)\n codetools       0.2-18  2020-11-04 [2] CRAN (R 4.2.1)\n colorspace      2.0-3   2022-02-21 [1] CRAN (R 4.2.1)\n crayon          1.5.2   2022-09-29 [1] CRAN (R 4.2.1)\n crosstalk       1.2.0   2021-11-04 [1] CRAN (R 4.2.1)\n data.table      1.14.6  2022-11-16 [1] CRAN (R 4.2.2)\n DBI             1.1.3   2022-06-18 [1] CRAN (R 4.2.1)\n dbplyr          2.2.1   2022-06-27 [1] CRAN (R 4.2.1)\n devtools      * 2.4.5   2022-10-11 [1] CRAN (R 4.2.2)\n digest          0.6.31  2022-12-11 [1] CRAN (R 4.2.2)\n dplyr         * 1.0.10  2022-09-01 [1] CRAN (R 4.2.1)\n ellipsis        0.3.2   2021-04-29 [1] CRAN (R 4.2.1)\n evaluate        0.19    2022-12-13 [1] CRAN (R 4.2.2)\n fansi           1.0.3   2022-03-24 [1] CRAN (R 4.2.1)\n farver          2.1.1   2022-07-06 [1] CRAN (R 4.2.1)\n fastmap         1.1.0   2021-01-25 [1] CRAN (R 4.2.1)\n forcats       * 0.5.2   2022-08-19 [1] CRAN (R 4.2.1)\n fs              1.5.2   2021-12-08 [1] CRAN (R 4.2.1)\n gargle          1.2.1   2022-09-08 [1] CRAN (R 4.2.1)\n generics        0.1.3   2022-07-05 [1] CRAN (R 4.2.1)\n ggplot2       * 3.4.0   2022-11-04 [1] CRAN (R 4.2.2)\n glue            1.6.2   2022-02-24 [1] CRAN (R 4.2.1)\n googledrive     2.0.0   2021-07-08 [1] CRAN (R 4.2.1)\n googlesheets4   1.0.1   2022-08-13 [1] CRAN (R 4.2.1)\n gtable          0.3.1   2022-09-01 [1] CRAN (R 4.2.1)\n haven           2.5.1   2022-08-22 [1] CRAN (R 4.2.1)\n hms             1.1.2   2022-08-19 [1] CRAN (R 4.2.1)\n htmltools       0.5.4   2022-12-07 [1] CRAN (R 4.2.2)\n htmlwidgets     1.6.0   2022-12-15 [1] CRAN (R 4.2.2)\n httpuv          1.6.7   2022-12-14 [1] CRAN (R 4.2.2)\n httr            1.4.4   2022-08-17 [1] CRAN (R 4.2.1)\n jsonlite        1.8.4   2022-12-06 [1] CRAN (R 4.2.2)\n knitr           1.41    2022-11-18 [1] CRAN (R 4.2.2)\n labeling        0.4.2   2020-10-20 [1] CRAN (R 4.2.0)\n later           1.3.0   2021-08-18 [1] CRAN (R 4.2.1)\n lazyeval        0.2.2   2019-03-15 [1] CRAN (R 4.2.1)\n lifecycle       1.0.3   2022-10-07 [1] CRAN (R 4.2.1)\n lubridate       1.9.0   2022-11-06 [1] CRAN (R 4.2.2)\n magick          2.7.3   2021-08-18 [1] CRAN (R 4.2.1)\n magrittr        2.0.3   2022-03-30 [1] CRAN (R 4.2.1)\n MASS            7.3-57  2022-04-22 [2] CRAN (R 4.2.1)\n matrixStats     0.63.0  2022-11-18 [1] CRAN (R 4.2.2)\n memoise         2.0.1   2021-11-26 [1] CRAN (R 4.2.1)\n mime            0.12    2021-09-28 [1] CRAN (R 4.2.0)\n miniUI          0.1.1.1 2018-05-18 [1] CRAN (R 4.2.1)\n modelr          0.1.10  2022-11-11 [1] CRAN (R 4.2.2)\n munsell         0.5.0   2018-06-12 [1] CRAN (R 4.2.1)\n pander          0.6.5   2022-03-18 [1] CRAN (R 4.2.1)\n pillar          1.8.1   2022-08-19 [1] CRAN (R 4.2.1)\n pkgbuild        1.4.0   2022-11-27 [1] CRAN (R 4.2.2)\n pkgconfig       2.0.3   2019-09-22 [1] CRAN (R 4.2.1)\n pkgload         1.3.2   2022-11-16 [1] CRAN (R 4.2.2)\n plotly        * 4.10.1  2022-11-07 [1] CRAN (R 4.2.2)\n plyr            1.8.8   2022-11-11 [1] CRAN (R 4.2.2)\n prettyunits     1.1.1   2020-01-24 [1] CRAN (R 4.2.1)\n processx        3.8.0   2022-10-26 [1] CRAN (R 4.2.1)\n profvis         0.3.7   2020-11-02 [1] CRAN (R 4.2.1)\n promises        1.2.0.1 2021-02-11 [1] CRAN (R 4.2.1)\n pryr            0.1.5   2021-07-26 [1] CRAN (R 4.2.1)\n ps              1.7.2   2022-10-26 [1] CRAN (R 4.2.1)\n purrr         * 0.3.5   2022-10-06 [1] CRAN (R 4.2.1)\n R6              2.5.1   2021-08-19 [1] CRAN (R 4.2.1)\n rapportools     1.1     2022-03-22 [1] CRAN (R 4.2.1)\n Rcpp            1.0.9   2022-07-08 [1] CRAN (R 4.2.1)\n readr         * 2.1.3   2022-10-01 [1] CRAN (R 4.2.1)\n readxl          1.4.1   2022-08-17 [1] CRAN (R 4.2.2)\n remotes         2.4.2   2021-11-30 [1] CRAN (R 4.2.1)\n reprex          2.0.2   2022-08-17 [1] CRAN (R 4.2.1)\n reshape2        1.4.4   2020-04-09 [1] CRAN (R 4.2.1)\n rlang           1.0.6   2022-09-24 [1] CRAN (R 4.2.1)\n rmarkdown       2.19    2022-12-15 [1] CRAN (R 4.2.2)\n rstudioapi      0.14    2022-08-22 [1] CRAN (R 4.2.1)\n rvest           1.0.3   2022-08-19 [1] CRAN (R 4.2.2)\n scales          1.2.1   2022-08-20 [1] CRAN (R 4.2.1)\n sessioninfo     1.2.2   2021-12-06 [1] CRAN (R 4.2.1)\n shiny           1.7.4   2022-12-15 [1] CRAN (R 4.2.2)\n stringi         1.7.8   2022-07-11 [1] CRAN (R 4.2.1)\n stringr       * 1.5.0   2022-12-02 [1] CRAN (R 4.2.2)\n summarytools  * 1.0.1   2022-05-20 [1] CRAN (R 4.2.2)\n tibble        * 3.1.8   2022-07-22 [1] CRAN (R 4.2.2)\n tidyr         * 1.2.1   2022-09-08 [1] CRAN (R 4.2.1)\n tidyselect      1.2.0   2022-10-10 [1] CRAN (R 4.2.1)\n tidyverse     * 1.3.2   2022-07-18 [1] CRAN (R 4.2.1)\n timechange      0.1.1   2022-11-04 [1] CRAN (R 4.2.2)\n tzdb            0.3.0   2022-03-28 [1] CRAN (R 4.2.1)\n urlchecker      1.0.1   2021-11-30 [1] CRAN (R 4.2.1)\n usethis       * 2.1.6   2022-05-25 [1] CRAN (R 4.2.1)\n utf8            1.2.2   2021-07-24 [1] CRAN (R 4.2.1)\n vctrs           0.5.1   2022-11-16 [1] CRAN (R 4.2.2)\n viridisLite     0.4.1   2022-08-22 [1] CRAN (R 4.2.2)\n vroom           1.6.0   2022-09-30 [1] CRAN (R 4.2.1)\n withr           2.5.0   2022-03-03 [1] CRAN (R 4.2.1)\n xfun            0.35    2022-11-16 [1] CRAN (R 4.2.2)\n xml2            1.3.3   2021-11-30 [1] CRAN (R 4.2.1)\n xtable          1.8-4   2019-04-21 [1] CRAN (R 4.2.1)\n yaml            2.3.6   2022-10-18 [1] CRAN (R 4.2.1)\n\n [1] C:/Users/srika/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html",
    "href": "posts/Caitlin Rowley - Final Project.html",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#introduction",
    "href": "posts/Caitlin Rowley - Final Project.html#introduction",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Introduction",
    "text": "Introduction\nFor my final project, I selected data from the Inside AirBnB website, which allows the public to access information regarding listings by geographic area. My data set captures host- and room-specific metrics related to all Airbnb rental listings in Boston, MA from September 2021 through September 2022. Information for each listing included in the data set is presented using the following variables: (1) ID number for the particular listing, (2) name of the listing, (3) listing host ID number, (4) listing host name, (5) listing neighborhood group, (6) listing neighborhood, (7) listing latitude, (8) listing longitude, (9) type of listing (i.e., private room, entire home/apartment, shared room, or hotel room), (10) listing price, (11) minimum number of nights per stay, (12) number of listing reviews, (13) most recent listing review, (14) number of listing reviews per month, (15) number listing-specific host listings (i.e., the number of unique listings by host), (16) listing availability over the next year, (17) number of reviews for listing over the past 12 months, and (18) listing licensure status. The combination of each of these variables makes up a case, which in this instance equates to a unique Airbnb listing.\nIn reviewing these data, I thought it would be interesting to evaluate the following research question: Are there are particular listing features—specifically, listing neighborhood and listing type—that may contribute to listing price? As part of this task, I will also be running analyses on additional summary statistics to gain a better understanding of patterns that may appear within the data."
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#data",
    "href": "posts/Caitlin Rowley - Final Project.html#data",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Data",
    "text": "Data\nTo begin this analysis, I will read in my data set.\n\n# install packages and load libraries:\n\ninstall.packages(\"readr\")\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\ninstall.packages(\"readxl\")\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggtext)\n\n# read in dataset:\n\nBoston <- read_csv(\"_data/Boston AirBnB Data.csv\")\nhead(Boston)\n\n# A tibble: 6 × 18\n     id name       host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n  <dbl> <chr>        <dbl> <chr>   <lgl>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n1  3168 TudorStud…    3697 Mark    NA      Bright…    42.4   -71.2 Privat…    99\n2  3781 HARBORSID…    4804 Frank   NA      East B…    42.4   -71.0 Entire…   132\n3  5506 ** Fort H…    8229 Terry   NA      Roxbury    42.3   -71.1 Entire…   149\n4  6695 Fort Hill…    8229 Terry   NA      Roxbury    42.3   -71.1 Entire…   179\n5  7903 Colorful,…   14169 Stacy   NA      Charle…    42.4   -71.1 Privat…   116\n6  8521 Sunsplash…  306681 Janet   NA      Allston    42.4   -71.1 Entire…   300\n# … with 8 more variables: minimum_nights <dbl>, number_of_reviews <dbl>,\n#   last_review <chr>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>,\n#   number_of_reviews_ltm <dbl>, license <chr>, and abbreviated variable names\n#   ¹​host_name, ²​neighbourhood_group, ³​neighbourhood, ⁴​latitude, ⁵​longitude,\n#   ⁶​room_type"
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#tidy-data",
    "href": "posts/Caitlin Rowley - Final Project.html#tidy-data",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Tidy Data",
    "text": "Tidy Data\nI will now tidy the data to look for missing values and duplicates. I will also rename columns as needed.\n\n# look for duplicates\n# look for missing values\n# remember na.rm=TRUE for calculations\n\n# At first glance, it seems as though there are no values in the column titled \"neighbourhood_group.\" So, I will find all unique values within that column to determine whether it can be removed from my tidy data set.\n\nunique(Boston[c(\"neighbourhood_group\")])\n\n# A tibble: 1 × 1\n  neighbourhood_group\n  <lgl>              \n1 NA                 \n\n# I now know that there is no data within this column. I will remove it from my data set.\n\nBoston_tidy <- subset(Boston, select = -c(neighbourhood_group))\n\n# I can see from viewing this data frame that there are no other columns that are absent any values, so I will move on to other tidying tasks.\n\n# rename columns:\n\nnames(Boston_tidy) <- c('room_id', 'room_name', 'host_id', 'host_name', 'neighborhood', 'room_latitude', 'room_longitude', 'room_type', 'room_price', 'min_nights', 'number_reviews', 'last_review', 'reviews_per_month', 'host_listings', 'availability_next_365', 'number_reviews_LTM', 'room_license')\n\n# find duplicates:\n\nduplicates <- duplicated(Boston_tidy)\n\n# reached \"max.print\", so I will increase the limit and identify if any values within the vector = TRUE:\n\noptions(max.print=999999)\nduplicates[\"TRUE\"]\n\n[1] NA\n\nhead(Boston_tidy)\n\n# A tibble: 6 × 17\n  room_id room_name      host_id host_…¹ neigh…² room_…³ room_…⁴ room_…⁵ room_…⁶\n    <dbl> <chr>            <dbl> <chr>   <chr>     <dbl>   <dbl> <chr>     <dbl>\n1    3168 TudorStudio       3697 Mark    Bright…    42.4   -71.2 Privat…      99\n2    3781 HARBORSIDE-Wa…    4804 Frank   East B…    42.4   -71.0 Entire…     132\n3    5506 ** Fort Hill …    8229 Terry   Roxbury    42.3   -71.1 Entire…     149\n4    6695 Fort Hill Inn…    8229 Terry   Roxbury    42.3   -71.1 Entire…     179\n5    7903 Colorful, mod…   14169 Stacy   Charle…    42.4   -71.1 Privat…     116\n6    8521 SunsplashedSe…  306681 Janet   Allston    42.4   -71.1 Entire…     300\n# … with 8 more variables: min_nights <dbl>, number_reviews <dbl>,\n#   last_review <chr>, reviews_per_month <dbl>, host_listings <dbl>,\n#   availability_next_365 <dbl>, number_reviews_LTM <dbl>, room_license <chr>,\n#   and abbreviated variable names ¹​host_name, ²​neighborhood, ³​room_latitude,\n#   ⁴​room_longitude, ⁵​room_type, ⁶​room_price\n\n\nThe “Boston_tidy” data frame now has 17 variables and 5,185 rows of data. After removing the “neighborhood group” column (blank values across each case) and all additional duplicate and blank observations, each row now represents one unique case—or in this instance, a unique rental listing.\nNext, I will mutate variables. I will start off by adding the variable “room_coordinates” to my overall data set. I think this may come in handy if I choose to use a map for visualization, as I may need to match coordinates between my data set and those included in mapping packages such as ‘map_data().’\n\n# mutate lat and lon to create \"room_coordinates\"\n# keep lat and lon columns for now\n\nBoston_mutate <- Boston_tidy %>%\nmutate(\"room_coordinates\" = paste(room_latitude, room_longitude))\ncolnames(Boston_mutate)\n\n [1] \"room_id\"               \"room_name\"             \"host_id\"              \n [4] \"host_name\"             \"neighborhood\"          \"room_latitude\"        \n [7] \"room_longitude\"        \"room_type\"             \"room_price\"           \n[10] \"min_nights\"            \"number_reviews\"        \"last_review\"          \n[13] \"reviews_per_month\"     \"host_listings\"         \"availability_next_365\"\n[16] \"number_reviews_LTM\"    \"room_license\"          \"room_coordinates\""
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#exploratory-analysis",
    "href": "posts/Caitlin Rowley - Final Project.html#exploratory-analysis",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nI will next conduct some exploratory analysis to glean more insight related to my data set. First, I will generate statistics using raw data (not excluding outliers or values equal to 0) related to room price, minimum number of nights per stay, number of reviews per room, and number of listings by host.\n\n# summary statistics for entire data set: \n\nsummary.data.frame(Boston_mutate)\n\n    room_id           room_name            host_id           host_name        \n Min.   :3.168e+03   Length:5185        Min.   :     3697   Length:5185       \n 1st Qu.:2.083e+07   Class :character   1st Qu.: 18517776   Class :character  \n Median :4.322e+07   Mode  :character   Median : 87330733   Mode  :character  \n Mean   :1.440e+17                      Mean   :125494858                     \n 3rd Qu.:5.358e+07                      3rd Qu.:212359760                     \n Max.   :7.160e+17                      Max.   :479130189                     \n                                                                              \n neighborhood       room_latitude   room_longitude    room_type        \n Length:5185        Min.   :42.23   Min.   :-71.20   Length:5185       \n Class :character   1st Qu.:42.33   1st Qu.:-71.11   Class :character  \n Mode  :character   Median :42.35   Median :-71.08   Mode  :character  \n                    Mean   :42.35   Mean   :-71.09                     \n                    3rd Qu.:42.37   3rd Qu.:-71.06                     \n                    Max.   :42.41   Max.   :-70.91                     \n                                                                       \n   room_price        min_nights     number_reviews    last_review       \n Min.   :    0.0   Min.   :  1.00   Min.   :   0.00   Length:5185       \n 1st Qu.:  100.0   1st Qu.:  2.00   1st Qu.:   1.00   Class :character  \n Median :  179.0   Median : 10.00   Median :   9.00   Mode  :character  \n Mean   :  230.6   Mean   : 27.41   Mean   :  47.31                     \n 3rd Qu.:  275.0   3rd Qu.: 32.00   3rd Qu.:  54.00                     \n Max.   :10000.0   Max.   :730.00   Max.   :1100.00                     \n                                                                        \n reviews_per_month host_listings    availability_next_365 number_reviews_LTM\n Min.   : 0.010    Min.   :  1.00   Min.   :  0.0         Min.   :  0.00    \n 1st Qu.: 0.310    1st Qu.:  2.00   1st Qu.: 77.0         1st Qu.:  0.00    \n Median : 1.060    Median :  6.00   Median :187.0         Median :  2.00    \n Mean   : 1.806    Mean   : 62.29   Mean   :189.7         Mean   : 13.19    \n 3rd Qu.: 2.660    3rd Qu.: 45.00   3rd Qu.:315.0         3rd Qu.: 17.00    \n Max.   :18.110    Max.   :477.00   Max.   :365.0         Max.   :256.00    \n NA's   :1225                                                               \n room_license       room_coordinates  \n Length:5185        Length:5185       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n# summary statistics for particular group of variables:\n\nBoston_mutate %>% \n  select(room_price, min_nights, number_reviews, host_listings, availability_next_365) %>% \n  summary()\n\n   room_price        min_nights     number_reviews    host_listings   \n Min.   :    0.0   Min.   :  1.00   Min.   :   0.00   Min.   :  1.00  \n 1st Qu.:  100.0   1st Qu.:  2.00   1st Qu.:   1.00   1st Qu.:  2.00  \n Median :  179.0   Median : 10.00   Median :   9.00   Median :  6.00  \n Mean   :  230.6   Mean   : 27.41   Mean   :  47.31   Mean   : 62.29  \n 3rd Qu.:  275.0   3rd Qu.: 32.00   3rd Qu.:  54.00   3rd Qu.: 45.00  \n Max.   :10000.0   Max.   :730.00   Max.   :1100.00   Max.   :477.00  \n availability_next_365\n Min.   :  0.0        \n 1st Qu.: 77.0        \n Median :187.0        \n Mean   :189.7        \n 3rd Qu.:315.0        \n Max.   :365.0        \n\n\nThis raw data indicates that room prices for Boston Airbnbs range from $0-$10,000 per night—both of which I would assume are outliers—with the median price equaling $179 per night and the average price equaling $231 per night. Regarding the minimum number of nights per stay, values ranged from 1-730 nights. At first, I assumed the maximum value was an outlier, but Airbnb does offer long-term stays (“long-term” being defined as more than 28 days), so it is possible that this particular listing is for long-terms stays only. I also included the number of reviews per listing in this analysis to see if this may be an indicator of the popularity of certain rooms and, by extension, certain hosts. In the same vein, I included number of room-specific host listings in this summary, with the values ranging from 1-477 listings. The median number of room-specific listings per host is 6, while the average is 62. The final component of this analysis is listing availability over the next 365 days. The values range from 0-365 days, with the median value being 187 days and the average being 190 days.\nAs a precursor to a deeper analysis on number of host listings, I filtered the data set to include only values greater than one in the “host_listings” column, which tells us the number of rooms listed by the same host.\n\n# filter by hosts with more than one listing:\n\nBoston_id <- Boston_mutate%>%\n  filter(host_listings>1)%>%\n  group_by(host_id, host_listings)\nhead(Boston_id)\n\n# A tibble: 6 × 18\n# Groups:   host_id, host_listings [4]\n  room_id room_name      host_id host_…¹ neigh…² room_…³ room_…⁴ room_…⁵ room_…⁶\n    <dbl> <chr>            <dbl> <chr>   <chr>     <dbl>   <dbl> <chr>     <dbl>\n1    5506 ** Fort Hill …    8229 Terry   Roxbury    42.3   -71.1 Entire…     149\n2    6695 Fort Hill Inn…    8229 Terry   Roxbury    42.3   -71.1 Entire…     179\n3    8521 SunsplashedSe…  306681 Janet   Allston    42.4   -71.1 Entire…     300\n4    8789 Curved Glass …   26988 Anne    Beacon…    42.4   -71.1 Entire…     110\n5   10813 Back Bay Apt-…   38997 Michel… Back B…    42.4   -71.1 Entire…     135\n6   10986 North End (Wa…   38997 Michel… North …    42.4   -71.1 Entire…     135\n# … with 9 more variables: min_nights <dbl>, number_reviews <dbl>,\n#   last_review <chr>, reviews_per_month <dbl>, host_listings <dbl>,\n#   availability_next_365 <dbl>, number_reviews_LTM <dbl>, room_license <chr>,\n#   room_coordinates <chr>, and abbreviated variable names ¹​host_name,\n#   ²​neighborhood, ³​room_latitude, ⁴​room_longitude, ⁵​room_type, ⁶​room_price\n\n\nThis output indicates that that there are 3,918 room-specfic listings whose hosts have more than one unique listing in Boston’s Airbnb database.\nWe can also dig a little deeper into the number of room-specific listings by host for some additional context.\n\nmax_listings <- Boston_mutate%>%\n  select(host_id, host_name, room_name, neighborhood, host_listings, room_price, room_type)\nmax_477 <- max_listings[max_listings$host_listings == '477',]\nunique(max_477$host_name)\n\n[1] \"Blueground\"\n\n# check listing prices:\n\nsummary(max_477$room_price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  142.0   205.0   231.0   245.1   268.0   487.0 \n\n# check listing room types:\n\nunique(max_477$room_type)\n\n[1] \"Entire home/apt\"\n\n# check listing neighborhoods:\n\nunique(max_477$neighborhood)\n\n [1] \"Bay Village\"             \"South Boston\"           \n [3] \"Back Bay\"                \"South End\"              \n [5] \"West End\"                \"Allston\"                \n [7] \"Brighton\"                \"Beacon Hill\"            \n [9] \"Downtown\"                \"Charlestown\"            \n[11] \"Chinatown\"               \"Fenway\"                 \n[13] \"South Boston Waterfront\" \"Dorchester\"             \n[15] \"Mission Hill\"            \"Roxbury\"                \n[17] \"Longwood Medical Area\"   \"North End\"              \n[19] \"Jamaica Plain\"           \"West Roxbury\"           \n\n\nWe now know that there is one host, “Blueground,” who has 477 unique listings. This is the highest number of listings for a unique host within the entire data set. We do not gain much valuable insight from the remaining analyses, which indicate (1) summary statistics for the room prices across Blueground’s listings (prices range from $142-$487 per night), (2) room types included across listings (entire home/apartment), and (3) neighborhoods across listings (listings in 20 Boston neighborhoods); I just thought it would be interesting to see if there were any patterns. However, aside from room type, the data appear to be widely spread.\nI will next generate some summary statistics for the categorical variable indicating listing neighborhood. Specifically, I would like to know which neighborhoods have the most and least number of listings.\n\n# max listing by neighborhood:\n\nunique_neighbor <- unique(Boston_mutate$neighborhood)\nnames(which.max(table(Boston_mutate$neighborhood)))\n\n[1] \"Allston\"\n\n# min listing by neighborhood:\n\nunique_neighbor <- unique(Boston_mutate$neighborhood)\nnames(which.min(table(Boston_mutate$neighborhood)))\n\n[1] \"Leather District\"\n\n\nWe can see from this tabulation that the most frequently listed neighborhood is Allston, and the neighborhood with the least amount of listings is the Leather District I will keep this in mind as I continue with my exploratory analysis; it would be interesting to see if the number of listings in either of these two neighborhoods provides insight into room price.\nNext, I will create a subset of data that includes the new variable “median_price,” which will only include room prices greater than $0. I will be using the median as the unit of measure to account for outliers.\n\n# find median room price by neighborhood:\n\nBoston_median_neighbor <- Boston_mutate%>%\n  filter(room_price>0) %>%\n  group_by(neighborhood)%>%\n    summarize(median_price = median(room_price))\nprint(head(Boston_median_neighbor))\n\n# A tibble: 6 × 2\n  neighborhood median_price\n  <chr>               <dbl>\n1 Allston               161\n2 Back Bay              287\n3 Bay Village           160\n4 Beacon Hill           199\n5 Brighton              125\n6 Charlestown           187\n\nsummary(Boston_median_neighbor$median_price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   82.5   142.9   174.0   186.8   235.2   388.0 \n\n\nThinking back to my question regarding the relationship between number of listings, neighborhood, and room price, there does not seem to be a correlation. We know that Allston has the highest number of listings, but we can also see from this most recent analysis that it ranks 14th out of the 26 Boston neighborhoods in terms of median room price ($161 per night). The Leather District, despite having the lowest number of room listings, is ranked 16th out of the 26 neighborhoods ($159 per night).\nI will now see if we can glean any interesting insight from data related to median room price by room type.\n\n# find median room price by neighborhood:\n\nBoston_median_type <- Boston_mutate%>%\n  filter(room_price>0) %>%\n  group_by(room_type)%>%\n    summarize(median_price = median(room_price))\nprint(head(Boston_median_type))\n\n# A tibble: 4 × 2\n  room_type       median_price\n  <chr>                  <dbl>\n1 Entire home/apt        229  \n2 Hotel room             396  \n3 Private room            84  \n4 Shared room             41.5\n\nsummary(Boston_median_type$median_price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  41.50   73.38  156.50  187.62  270.75  396.00 \n\n\nThis analysis tells us that the median room price for a hotel across the city of Boston is $396 per night, the median price of an entire home/apartment is $229 per night, the median price of a private room is $84 per night, and the median price for a shared room is $42 per night. I will consider this as I move forward with additional analyses.\nI will continue working with this data set, but I will now group by the original variables “room_type” and “neighborhood.” This will be useful in terms of both summary statistics and visualization.\n\n# find median room prices by neighborhood and room type:\n\nBoston_median <- Boston_mutate%>%\n  filter(room_price>0) %>%\n  group_by(room_type, neighborhood)%>%\n    summarize(median_price = median(room_price))\nprint(head(Boston_median))\n\n# A tibble: 6 × 3\n# Groups:   room_type [1]\n  room_type       neighborhood median_price\n  <chr>           <chr>               <dbl>\n1 Entire home/apt Allston              216 \n2 Entire home/apt Back Bay             278 \n3 Entire home/apt Bay Village          200 \n4 Entire home/apt Beacon Hill          207 \n5 Entire home/apt Brighton             191 \n6 Entire home/apt Charlestown          240.\n\nsummary(Boston_median$median_price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   85.25  150.50  176.11  228.50  750.00 \n\n\nWe can see from this data set that the highest median room price is $750/night for a shared room in the Fenway neighborhood. The lowest median room price is $10/night for a shared room in Charlestown. Additionally, while there is spread across room types when viewing the most expensive listings, many of the lower-cost listings are shared rooms, which aligns with our previous analysis.\nI would like to dig a bit more into this by running additional analyses on listings in the Fenway and Charlestown neighborhoods.\n\n# summary statistics for Fenway: \n\nFenway <- Boston_mutate%>%\nfilter(neighborhood==\"Fenway\")%>%\n  select(neighborhood, room_type, room_price)\nsummary(Fenway$room_price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   35.0   135.0   242.5   276.9   354.5  1277.0 \n\n# group by room_type:\n\nFenway_group <- Fenway%>%\n  group_by(room_type)%>%\n  summarize(mean_price = mean(room_price, na.rm = TRUE))\nFenway_group\n\n# A tibble: 3 × 2\n  room_type       mean_price\n  <chr>                <dbl>\n1 Entire home/apt       320.\n2 Private room          148.\n3 Shared room           750 \n\n# 'table()' function not working, so filter by room type:\n\nShared <- Fenway%>%\n  filter(room_type==\"Shared room\")\nhead(Shared)\n\n# A tibble: 1 × 3\n  neighborhood room_type   room_price\n  <chr>        <chr>            <dbl>\n1 Fenway       Shared room        750\n\nPrivate <- Fenway%>%\n  filter(room_type==\"Private room\")\nhead(Private)\n\n# A tibble: 6 × 3\n  neighborhood room_type    room_price\n  <chr>        <chr>             <dbl>\n1 Fenway       Private room        199\n2 Fenway       Private room        265\n3 Fenway       Private room        295\n4 Fenway       Private room        130\n5 Fenway       Private room        109\n6 Fenway       Private room         84\n\nEntire <- Fenway%>%\n  filter(room_type==\"Entire home/apt\")\nhead(Entire)\n\n# A tibble: 6 × 3\n  neighborhood room_type       room_price\n  <chr>        <chr>                <dbl>\n1 Fenway       Entire home/apt        415\n2 Fenway       Entire home/apt        275\n3 Fenway       Entire home/apt        299\n4 Fenway       Entire home/apt        499\n5 Fenway       Entire home/apt        250\n6 Fenway       Entire home/apt        250\n\n\nWe now know that room prices in the Fenway neighborhood ranges from $35 per night to $1,277 per night. This is a substantial range, so I don’t think it provides much useful insight. However, we can see that average room prices vary significantly based on the type of room listing: $750 for a shared room; $320 for an entire home/apartment; and $148 for a private room. This does not exactly align with the trends we saw related to price per night by room type across the city. We also know that, specifically for the Fenway neighborhood, the number of listings by room type varies; there are 179 listings for an entire home/apt, 64 listings for private rooms, but only one listing for a shared room. So, this finding lessens the relevance of our average prices, as there is not a comparable distribution of data across metrics. It also creates contrast between overall trends citywide and what we see within neighborhoods."
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#visualization",
    "href": "posts/Caitlin Rowley - Final Project.html#visualization",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Visualization",
    "text": "Visualization\nI will next focus on data visualization. I will first generate a bar chart portraying median room price by neighborhood, as was described during exploratory analysis. This visual does not exclude outliers, though it will exclude room prices that equal zero.\n\nlibrary(RColorBrewer)\nlibrary(ggtext)\nlibrary(ggplot2)\n\n# group median price by neighborhood:\n\nBoston_median_price <- Boston_mutate%>%\n  filter(room_price>0) %>%\n  group_by(neighborhood)%>%\n    summarize(median_price = median(room_price))\n\n# generate bar chart:\n\nggplot(Boston_median_price, aes(x=neighborhood, y=median_price, fill=neighborhood)) +\ngeom_bar(stat=\"identity\") +\nscale_fill_hue() +\n  theme_classic() +\n  labs(x=\"Neighborhood\",y=\"Median Price per Night\", title = \"Boston Airbnb Median Rental Prices \\nby Neighborhood\")+\n  theme(axis.text.x = element_markdown(angle=90, hjust=1))\n\n\n\n\nThis bar chart provides us with a visual of the neighborhoods with the highest median room price per night. These are (1) Chinatown at $388 per night , (2) Back Bay at $287 per night, and (3) Downtown at about $261 per night. The neighborhoods with the lowest median prices are Roxbury at about $83 per night, (2) Dorchester at $97 per night, and (3) Hyde Park at about $99 per night.\nI will also adjust this visual to display median room price by room type, as was also described during exploratory analysis.\n\n# group median price by room_type:\n\nBoston_median_type <- Boston_mutate%>%\n  filter(room_price>0) %>%\n  group_by(room_type)%>%\n    summarize(median_price = median(room_price))\n\n# generate bar chart:\n\nggplot(Boston_median_type, aes(x=room_type, y=median_price, fill=room_type)) +\ngeom_bar(stat=\"identity\") +\nscale_fill_hue() +\n  theme_classic() +\n  labs(x=\"Room Type\",y=\"Median Price per Night\", title = \"Boston Airbnb Median Rental Prices \\nby Room Type\")+\n  theme(axis.text.x = element_markdown(angle=90, hjust=1))\n\n\n\n\nAs we know, hotel rooms in the city of Boston are the most expensive when compared to other listing types at $396 per night. Next are entire homes/apartments at $229 per night, followed by private rooms ($84 per night) and shared rooms (about $42 per night).\nI will next generate a geom_point chart to visualize room price by neighborhood. I will also use facet wrapping to separate the values by room type. I will also apply a boxplot overlay to capture both the interquartile range and outliers. I will first need to exclude strong outliers\n\n# remove outliers:\n\nis_outlier <- function(x) {\n  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))\n}\n\nBoston_outlier <- Boston_mutate %>%\n  filter(!is_outlier(room_price))\n\n# create dataframe:\n\nBoston_outlier%>%\n  filter(room_price>0, room_price<800) %>%\n  group_by(room_type, neighborhood)\n\n# A tibble: 4,918 × 18\n# Groups:   room_type, neighborhood [65]\n   room_id room_name     host_id host_…¹ neigh…² room_…³ room_…⁴ room_…⁵ room_…⁶\n     <dbl> <chr>           <dbl> <chr>   <chr>     <dbl>   <dbl> <chr>     <dbl>\n 1    3168 TudorStudio      3697 Mark    Bright…    42.4   -71.2 Privat…      99\n 2    3781 HARBORSIDE-W…    4804 Frank   East B…    42.4   -71.0 Entire…     132\n 3    5506 ** Fort Hill…    8229 Terry   Roxbury    42.3   -71.1 Entire…     149\n 4    6695 Fort Hill In…    8229 Terry   Roxbury    42.3   -71.1 Entire…     179\n 5    7903 Colorful, mo…   14169 Stacy   Charle…    42.4   -71.1 Privat…     116\n 6    8521 SunsplashedS…  306681 Janet   Allston    42.4   -71.1 Entire…     300\n 7    8789 Curved Glass…   26988 Anne    Beacon…    42.4   -71.1 Entire…     110\n 8   10813 Back Bay Apt…   38997 Michel… Back B…    42.4   -71.1 Entire…     135\n 9   10986 North End (W…   38997 Michel… North …    42.4   -71.1 Entire…     135\n10   18711 230B1 · The …   71783 Lance   Dorche…    42.3   -71.1 Entire…     133\n# … with 4,908 more rows, 9 more variables: min_nights <dbl>,\n#   number_reviews <dbl>, last_review <chr>, reviews_per_month <dbl>,\n#   host_listings <dbl>, availability_next_365 <dbl>, number_reviews_LTM <dbl>,\n#   room_license <chr>, room_coordinates <chr>, and abbreviated variable names\n#   ¹​host_name, ²​neighborhood, ³​room_latitude, ⁴​room_longitude, ⁵​room_type,\n#   ⁶​room_price\n\n# generate geom_point chart\n# facet wrap\n# boxplot overlay\n\nBoston_outlier%>%\n group_by(room_type, neighborhood)%>%\n  ggplot(aes(x=neighborhood, y=room_price)) +\n  geom_point(alpha=.08, size=3, color = \"light pink\")+\n  facet_wrap(\"room_type\")+\n  labs(x=\"Neighborhood\",y=\"Price per Night\", title = \"Boston Airbnb Rental Prices by Neighborhood and Room Type\")+\n  theme_light()+\n  geom_boxplot()+\n  theme(axis.text.x = element_markdown(angle = 90, hjust=1))\n\n\n\n\nThis visual is bit tricky to read, but at a quick glance, we can at the very least see that there are the most listings for entire homes/apartments across the city. We can also see the shared rooms have the lowest prices overall. Though I think this provides useful information, it is still too difficult to read due to the number of neighborhoods, so I am going to apply the three variables (room price, room type, and neighborhood) to another visual.\nI will instead display a simpler version of this geom_point chart without the facet wrap so that the visual only captures neighborhood and room price per night.\n\n# generate geom_point chart with boxplot:\n\nBoston_outlier%>%\n group_by(room_type, neighborhood)%>%\n  ggplot(aes(x=neighborhood, y=room_price)) +\n  geom_point(alpha=.08, size=5, color = \"light pink\")+\n  labs(x=\"Neighborhood\",y=\"Price per Night\", title = \"Boston Airbnb Rental Prices by Neighborhood\")+\n  theme_light()+\n  geom_boxplot()+\n  theme(axis.text.x = element_markdown(angle = 90, hjust=1))\n\n\n\n# want to add values: text(x = Boston_outlier$room_price, y = Boston_outlier$room_price, labels = Boston_outlier$room_price)\n\nHere, we can see the distribution of prices across neighborhoods using individual data points. We can also see the spread of data points with the boxplot overlay, which includes the minimum value, the values within the 25th quartile, the median value, the values within the 75th quartile, and the maximum value. The boxplot also indicates outliers. With this visualization, we can see that neighborhoods with the narrowest distribution of data points—or, in this case, room prices—are Chinatown and the Leather District, while the neighborhoods with the broadest distribution of data points seem to be Charlestown, Harbor Islands and Mattapan.\nI will next visualize the data using a choropleth map. I will generate a map of the Boston area and apply data related to neighborhood, room price, and room type.\n\nlibrary(maps)\nlibrary(viridisLite)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# generate map\n\nstates_map <- map_data(\"state\")\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      <NA>\n2 -87.48493 30.37249     1     2 alabama      <NA>\n3 -87.52503 30.37249     1     3 alabama      <NA>\n4 -87.53076 30.33239     1     4 alabama      <NA>\n5 -87.57087 30.32665     1     5 alabama      <NA>\n6 -87.58806 30.32665     1     6 alabama      <NA>\n\nma_map <- filter(states_map, region==\"massachusetts\") %>%\nggplot(., aes(x=long, y=lat, group=group)) +\n  geom_polygon(fill=\"light pink\", color=\"white\")\nprint(ma_map)\n\n\n\n\nI’ve generated the map of Massachusetts, so now I will work on merging my data sets to apply as an overlay to the map.\n\n# merge 'ma_map' and 'Boston_tidy' by coordinates\n\n# mutate and rename columns\n\nBoston_coord <- Boston_mutate %>%\n  rename(\"coordinates\" = \"room_coordinates\")\nhead(Boston_coord)\n\n# A tibble: 6 × 18\n  room_id room_name      host_id host_…¹ neigh…² room_…³ room_…⁴ room_…⁵ room_…⁶\n    <dbl> <chr>            <dbl> <chr>   <chr>     <dbl>   <dbl> <chr>     <dbl>\n1    3168 TudorStudio       3697 Mark    Bright…    42.4   -71.2 Privat…      99\n2    3781 HARBORSIDE-Wa…    4804 Frank   East B…    42.4   -71.0 Entire…     132\n3    5506 ** Fort Hill …    8229 Terry   Roxbury    42.3   -71.1 Entire…     149\n4    6695 Fort Hill Inn…    8229 Terry   Roxbury    42.3   -71.1 Entire…     179\n5    7903 Colorful, mod…   14169 Stacy   Charle…    42.4   -71.1 Privat…     116\n6    8521 SunsplashedSe…  306681 Janet   Allston    42.4   -71.1 Entire…     300\n# … with 9 more variables: min_nights <dbl>, number_reviews <dbl>,\n#   last_review <chr>, reviews_per_month <dbl>, host_listings <dbl>,\n#   availability_next_365 <dbl>, number_reviews_LTM <dbl>, room_license <chr>,\n#   coordinates <chr>, and abbreviated variable names ¹​host_name,\n#   ²​neighborhood, ³​room_latitude, ⁴​room_longitude, ⁵​room_type, ⁶​room_price\n\nma_map_df <- filter(states_map, region==\"massachusetts\")\nma_mutate <- ma_map_df %>%\n  mutate(\"coordinates\" = paste(lat, long))\nhead(ma_mutate)\n\n       long      lat group order        region         subregion\n1 -70.45089 41.40193    20  5926 massachusetts martha's vineyard\n2 -70.45662 41.39047    20  5927 massachusetts martha's vineyard\n3 -70.45662 41.37328    20  5928 massachusetts martha's vineyard\n4 -70.46808 41.35609    20  5929 massachusetts martha's vineyard\n5 -70.50819 41.35609    20  5930 massachusetts martha's vineyard\n6 -70.56548 41.34464    20  5931 massachusetts martha's vineyard\n                         coordinates\n1  41.401927947998 -70.4508895874023\n2 41.3904724121094 -70.4566192626953\n3 41.3732833862305 -70.4566192626953\n4 41.3560943603516 -70.4680786132812\n5  41.3560943603516 -70.508186340332\n6 41.3446350097656 -70.5654830932617\n\n# merge data \n\nmap_merge <- merge(ma_mutate, Boston_coord, by = \"coordinates\", all=T)\nhead(map_merge)\n\n                         coordinates      long      lat group order\n1 41.2415008544922 -70.0211715698242 -70.02117 41.24150    22  6202\n2 41.2415008544922 -70.0498199462891 -70.04982 41.24150    22  6203\n3 41.2529640197754 -69.9753341674805 -69.97533 41.25296    22  6201\n4 41.2529640197754 -70.0956573486328 -70.09566 41.25296    22  6204\n5 41.2586898803711 -69.9524230957031 -69.95242 41.25869    22  6200\n6 41.2586898803711 -70.1529541015625 -70.15295 41.25869    22  6205\n         region subregion room_id room_name host_id host_name neighborhood\n1 massachusetts nantucket      NA      <NA>      NA      <NA>         <NA>\n2 massachusetts nantucket      NA      <NA>      NA      <NA>         <NA>\n3 massachusetts nantucket      NA      <NA>      NA      <NA>         <NA>\n4 massachusetts nantucket      NA      <NA>      NA      <NA>         <NA>\n5 massachusetts nantucket      NA      <NA>      NA      <NA>         <NA>\n6 massachusetts nantucket      NA      <NA>      NA      <NA>         <NA>\n  room_latitude room_longitude room_type room_price min_nights number_reviews\n1            NA             NA      <NA>         NA         NA             NA\n2            NA             NA      <NA>         NA         NA             NA\n3            NA             NA      <NA>         NA         NA             NA\n4            NA             NA      <NA>         NA         NA             NA\n5            NA             NA      <NA>         NA         NA             NA\n6            NA             NA      <NA>         NA         NA             NA\n  last_review reviews_per_month host_listings availability_next_365\n1        <NA>                NA            NA                    NA\n2        <NA>                NA            NA                    NA\n3        <NA>                NA            NA                    NA\n4        <NA>                NA            NA                    NA\n5        <NA>                NA            NA                    NA\n6        <NA>                NA            NA                    NA\n  number_reviews_LTM room_license\n1                 NA         <NA>\n2                 NA         <NA>\n3                 NA         <NA>\n4                 NA         <NA>\n5                 NA         <NA>\n6                 NA         <NA>\n\n# remove if room_id is NA when merged with map data\n\nmap_merged <- map_merge %>% filter(!is.na(map_merge$room_id))\nhead(map_merged)\n\n         coordinates long lat group order region subregion  room_id\n1 42.23117 -71.15312   NA  NA    NA    NA   <NA>      <NA> 13997739\n2  42.2353 -71.12879   NA  NA    NA    NA   <NA>      <NA> 18305618\n3 42.23533 -71.13256   NA  NA    NA    NA   <NA>      <NA>  3164650\n4 42.23543 -71.13076   NA  NA    NA    NA   <NA>      <NA> 13697676\n5   42.23547 -71.131   NA  NA    NA    NA   <NA>      <NA> 15665935\n6  42.2355 -71.13134   NA  NA    NA    NA   <NA>      <NA> 16379917\n                                           room_name   host_id host_name\n1 Hidden Gem Near Boston w/ 2 bedrooms HighSpeedWifi  29538655 Hu And Lu\n2     Boston’s Luxury Cellar Apartment, Comfy & Cozy 126495858    Silvia\n3                 2bd APT 20min from Downtown Boston  16052189  Jonathan\n4                    Calm and cozy room in clean Apt  16052189  Jonathan\n5   Boston private room - Spacious, tidy, close to T  16052189  Jonathan\n6    Beautiful 2bd Boston open concept apt w/parking  16052189  Jonathan\n  neighborhood room_latitude room_longitude       room_type room_price\n1    Hyde Park      42.23117      -71.15312 Entire home/apt        119\n2    Hyde Park      42.23530      -71.12879 Entire home/apt        135\n3    Hyde Park      42.23533      -71.13256 Entire home/apt         89\n4    Hyde Park      42.23543      -71.13076    Private room         28\n5    Hyde Park      42.23547      -71.13100    Private room         30\n6    Hyde Park      42.23550      -71.13134 Entire home/apt        149\n  min_nights number_reviews last_review reviews_per_month host_listings\n1          1            677   9/13/2022              9.04             1\n2          2             43    9/4/2022              7.17             1\n3         29             33    3/2/2021              0.33             5\n4         91             11  11/25/2020              0.17             5\n5         91              6  12/31/2019              0.12             5\n6         29              7    7/2/2019              0.10             5\n  availability_next_365 number_reviews_LTM room_license\n1                   279                125         <NA>\n2                   313                 43   STR-467769\n3                   267                  0         <NA>\n4                   291                  0         <NA>\n5                   308                  0         <NA>\n6                   274                  0         <NA>\n\n\nI now have my map and my merged data, but the data would be illegible if it the map is kept at its current scale. I’d like the data points to be mapped according to each listing’s coordinates, so I will try an iteration of the above code to generate this visual. For this graph, I will include all listings by neighborhood and by price.\n\n# load map, plot data\n\nMA_map <- map_data(\"state\")\nBoston_map <- filter(states_map, region == \"massachusetts\")%>%\n  ggplot() + geom_polygon(data = map_merged, aes(x = long, y = lat, group = group), colour = \"black\", fill = NA) + geom_point(data = Boston_mutate, aes(x = room_latitude, y = room_longitude, size = room_price, color = neighborhood)) + scale_y_reverse() + scale_x_reverse() + labs(x=\"Latitude\",y=\"Longitude\", title = \"Boston Airbnb \\nRental Prices \\nby Neighborhood\") + coord_map() + theme(axis.text.x = element_markdown(angle = 90, hjust=1))\nBoston_map + theme(legend.position=\"left\")\n\n\n\n\nWhile this is helpful in terms of understanding the geographic spread of Boston neighborhoods (though because there are so many, some colors appear to be very similar), there are too many data points included in this visual to accurately depict the spread of room price. I will instead return to measuring median room prices in a way that captures both neighborhood and room type.\nThis visual captures median room price by both neighborhood and room type. I think this is the clearest and most informative visual.\n\nBoston_med_neigh_room <- Boston_median%>%\n  ggplot(aes(median_price, neighborhood))+\n  geom_point(aes(color=room_type, shape=room_type))+  \n  labs(x=\"Median Room Price\",y=\"Neighborhood\", title = \"Boston Airbnb Median Room Prices by \\nNeighborhood and Room Type\")\nprint(Boston_med_neigh_room)\n\n\n\n\nThis visual clearly depicts median room prices by neighborhood according to room type (as denoted by different shapes). We can observe general trends here: compared to private and shared rooms, entire homes/apartments tend to be priced higher; neighborhoods nearer Downtown or the water (e.g., Chinatown, Back Bay, Downtown) tend to have higher median prices. However, more notable is that not every neighborhood has listings for each room type, so it is quite difficult to determine trends in pricing across the entire city. Additionally, this visual does not account for the number of listings, so it is possible that there are instances—similar to that we saw in the Fenway neighborhood—where there may be significant variation in the number of listings for a particular room type, which ultimately skews the data. So, while this provides information that sets the groundwork for the foundation of my research question—we now know, for example, the spread of data across neighborhood, room price, and room type—I have very limited insight related to whether correlations exist between these variables."
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#reflection",
    "href": "posts/Caitlin Rowley - Final Project.html#reflection",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Reflection",
    "text": "Reflection\nI found this project to be extremely interesting, specifically in terms of selecting criteria to evaluate. The process of finalizing my research question was a combination of considering ways in which I could slice the data to gain insight into different patterns within the data, and also running various analyses to see which provided interesting outputs. Initially, I conducted analyses to identify trends across variables outside the scope of my research question to see if any links could be drawn to room price, but I did not find those analyses to be very informative; I eventually settled on investigating the correlation between room price and neighborhood/room type because I thought the latter two would prove to be the clearest indicators of pricing trends. So, I ran a plethora of code chunks that I hoped would determine patterns across these three variables, and provided visuals for outputs that I thought were most informative.\nHowever, upon deeper analysis, I realized that there are more factors that need to be considered if I were to determine any true patterns within the data. For example, as noted in my final visual, I now realize that it is important to understand the spread of listings across neighborhoods; in other words, in order to gauge the effect of room type on room price by neighborhood, I need to understand not only how many listings there are per neighborhood, but also the distribution of those listings across room types. This would then need to be incorporated into my analyses, which would, of course, require a more in-depth evaluation of the data.\nIn terms of challenges, I would say that I found adjusting certain aesthetics in my visuals to be difficult at times; I am, however, confident that this will improve as I gain more experience."
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#conclusion",
    "href": "posts/Caitlin Rowley - Final Project.html#conclusion",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Conclusion",
    "text": "Conclusion\nAs someone who lives close to Boston, it is unsurprising to see certain trends across neighborhood rental prices; as previously noted, it would generally be expected that neighborhoods closer to the water and downtown Boston (e.g., Chinatown, Back Bay, and Downtown) would be more expensive per night compared to neighborhoods in lower Boston (e.g., Roxbury, Dorchester, and Hyde Park). However, these trends were not necessarily conclusive, as variation across room types indicated outliers, and I did not account for the spread of listings—by either sheer quantity nor by room type—across each neighborhood. So, aside from gathering insight from bivariate analyses—such as median room prices by neighborhood and room type separately, as displayed in the colored bar charts—it is difficult to summarize meaningful conclusions based on my research question.\nTo better assess the combined effects of neighborhood, I’d like to do more in-depth analysis considering number of listings per neighborhood within the context of room price and room type. Fenway is an example of a neighborhood for which the room type with the highest median price is a shared room, and while this contradicts overall pricing trends, there is only one shared room listing. So, I think this would be an interesting path to consider additional factors that may contribute to room prices, specifically in terms of median, average, and range values. I’d also like to do more analysis to determine outliers so that we can more accurately gauge average room prices. As an additional metric, it may also be interesting to dive deeper into the minimum number of nights per stay—as mentioned during exploratory analysis—to see if there are any implications of long-terms stays across the data set in terms of room price."
  },
  {
    "objectID": "posts/Caitlin Rowley - Final Project.html#citations",
    "href": "posts/Caitlin Rowley - Final Project.html#citations",
    "title": "DACSS 601 Final Project - Caitlin Rowley",
    "section": "Citations",
    "text": "Citations\n\nGet the Data: Boston, Massachusetts, United States (2022). Inside Airbnb. http://insideairbnb.com/get-the-data/\nGrolemund, Hadley Wickham and Garrett. “R For Data Science.” R For Data Science, O’Reilly Media, Inc., Dec. 2016, https://r4ds.had.co.nz/index.html#welcome.\nRStudio Team (2020). RStudio: Integrated Development for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html",
    "href": "posts/Connor Skowyra's Final Project.html",
    "title": "Transportation Emissions from 2015-2022",
    "section": "",
    "text": "During this research project, the goal is to find a relationship in the transportation industry regarding emissions. Using a summary and visualization, I would like to provide a detailed conclusion on emissions by understanding the types of gas being emitted over the set time horizon. I want to see if more harmful pollutants are being gradually removed from our atmosphere or not, and which sector of the transportation industry is most responsible. The data was obtained by a report from Climate TRACE, a nonprofit organization uses imagery and created data sets from the website “Data Is Plural”."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#research-question",
    "href": "posts/Connor Skowyra's Final Project.html#research-question",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Research Question",
    "text": "Research Question\nI am interested in the connection of the trajectory of emissions in the transportation industry to see if new legislation created in the government and clean energy investments have been effective at reducing emissions in the United States. In the United States, the issue of climate change has been hotly contested for years as people argue over the best path moving forward to improve conditions for our society. Citizens have protested new pipelines, demanded politicians to create policies to move away from fossil fuels, and increasing are becoming frustrated with stagnation on action. One of the most highest emitters is the transportation industry. When individuals think of mass polluters, we tend to think of companies like electric, gas, and waste industries. However, we need to think deeper to come to a realization, how do the majority of people get to work, get groceries, mail and or packages being delivered? The transportation industry is the answer, but this leads to another question, “How much emissions per ton are emitted annually and how this dilemma is changing since the push for clean energy beginning in 2015?”"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#data-selection",
    "href": "posts/Connor Skowyra's Final Project.html#data-selection",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Data Selection",
    "text": "Data Selection\nThe analysis of transportation emissions is focused in a few different business areas like environmental, transportation and utility industries. Analysts for these types of companies look at the data to determine next steps in their projects whether it is wanting to do their part in reducing emissions, complying with government regulations or showing a positive public message. the analysis of transportation emissions and understanding the trajectory can help us create a cleaner future for the next generation. This information was obtained by Climate Trace by using satellite imagery, Database of Road Transportation Emissions (DARTE), and Average Annual Daily Traffic(AADT) data from the U.S. Highway Performance Monitoring System. Using this type of technology, databases, and machine learning, the model will provide answers on amount of emissions annually, sector of transportation, type of gas within the United States. The goal of the data set is to understand if emissions are decreasing but it is important to understand the severity of gases’ emitted.\nIn the previous section, I mentioned the most important variables in the data are annual emissions, sector of transportation, and type of gas. I want to see if annual emissions are decreasing or not since 2015 and see if government and corporate actions have been working. Then, the next stage is understanding which gas and or sector of transportation produces most annual emissions within the United States.\nLooking at the data set, I realize there are a couple of different problems as most columns are undefined, confusing, or not necessary to this research. Another issue is we need to understand our variables in particular is the type of gases being emitted. The most common sources of gas are Carbon Dioxide (CO2), Methane (CH4), and Nitrous Oxide (N2O). The final type is Carbon Dioxide Equivalent (CO2e) which were other forms of gas converted into a greenhouse gas. I am expecting a significant amount of Carbon Dioxide transportation emissions as the gas is created from burning fossil fuels like oil which our nation relies on almost entirely with a smaller proportion of the remaining gases to fill out our data."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#data-manipulation",
    "href": "posts/Connor Skowyra's Final Project.html#data-manipulation",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Data Manipulation",
    "text": "Data Manipulation\nTo clean the data we will need to install two packages called tidyverse and dplyr."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#installing-packages-and-reading-data",
    "href": "posts/Connor Skowyra's Final Project.html#installing-packages-and-reading-data",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Installing Packages and Reading Data",
    "text": "Installing Packages and Reading Data\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.2.2\n\n\nLoading required package: timechange\n\n\nWarning: package 'timechange' was built under R version 4.2.2\n\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\noptions(scipen=999)\n\nUsing Tidyverse and Dplyr will give us access to edit rows and columns in R by renaming columns as an example. Another important aspect is finding how much rows and columns are in the data set, which we can find using code dim().\n\ndim(covid.data)\n\nError in eval(expr, envir, enclos): object 'covid.data' not found"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#rows-190-columns-10",
    "href": "posts/Connor Skowyra's Final Project.html#rows-190-columns-10",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Rows = 190, Columns = 10",
    "text": "Rows = 190, Columns = 10\nThe result shows 190 10 when submitted, this expression states there are 190 rows and 10 columns of data. In R, one row equals one observation of data. This statement is almost identical to be used to review the columns as one column equals one variable in the data set. Therefore, since there are 10 columns, 10 variables are identified. Now, I want to read my data set.\n\nlibrary(readr)\nTransportationRoad <- read_csv(\"_data/transportation/country_road-transportation_emissions.csv\")\n\nRows: 35 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): iso3_country, original_inventory_sector, gas, emissions_quantity_u...\ndbl  (1): emissions_quantity\nlgl  (1): temporal_granularity\ndttm (4): start_time, end_time, created_date, modified_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncountry_other_transport_emissions <- read_csv(\"_data/transportation/country_other-transport_emissions.csv\")\n\nRows: 35 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): iso3_country, original_inventory_sector, gas, emissions_quantity_u...\ndbl  (1): emissions_quantity\nlgl  (1): temporal_granularity\ndttm (4): start_time, end_time, created_date, modified_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncountry_railways_emissions <- read_csv(\"_data/transportation/country_railways_emissions.csv\")\n\nRows: 35 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): iso3_country, original_inventory_sector, gas, emissions_quantity_u...\ndbl  (1): emissions_quantity\nlgl  (1): temporal_granularity\ndttm (4): start_time, end_time, created_date, modified_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncountry_shipping_emissions <- read_csv(\"_data/transportation/country_shipping_emissions.csv\")\n\nRows: 40 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): iso3_country, original_inventory_sector, gas, emissions_quantity_u...\ndbl  (1): emissions_quantity\ndttm (4): start_time, end_time, created_date, modified_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncountry_international_aviation_emissions <- read_csv(\"_data/transportation/country_international-aviation_emissions.csv\")\n\nRows: 40 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): iso3_country, original_inventory_sector, gas, emissions_quantity_u...\ndbl  (1): emissions_quantity\ndttm (4): start_time, end_time, created_date, modified_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncountry_domestic_aviation_emissions <- read_csv(\"_data/transportation/country_domestic-aviation_emissions.csv\")\n\nRows: 40 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): iso3_country, original_inventory_sector, gas, emissions_quantity_u...\ndbl  (1): emissions_quantity\ndttm (4): start_time, end_time, created_date, modified_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#joining-data",
    "href": "posts/Connor Skowyra's Final Project.html#joining-data",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Joining Data",
    "text": "Joining Data\nOpening the downloaded data reveals that I am missing some important information like other sectors of transportation. I would like to create a bivariate graph to include two variables so we need to complete further steps to effectively organize the data and prepare it for visualization. The first step is combining these data sets together using function full_join()\n\nTransportationRoadFullJoin <- full_join(TransportationRoad,country_other_transport_emissions)\n\nJoining, by = c(\"iso3_country\", \"start_time\", \"end_time\",\n\"original_inventory_sector\", \"gas\", \"emissions_quantity\",\n\"emissions_quantity_units\", \"temporal_granularity\", \"created_date\",\n\"modified_date\")\n\nView(country_railways_emissions)\nTransportationRoadFullJoin1 <- full_join(TransportationRoadFullJoin,country_railways_emissions)\n\nJoining, by = c(\"iso3_country\", \"start_time\", \"end_time\",\n\"original_inventory_sector\", \"gas\", \"emissions_quantity\",\n\"emissions_quantity_units\", \"temporal_granularity\", \"created_date\",\n\"modified_date\")\n\nView(country_shipping_emissions)\nTransportationRoadFullJoin2 <- full_join(TransportationRoadFullJoin,country_shipping_emissions)\n\nJoining, by = c(\"iso3_country\", \"start_time\", \"end_time\",\n\"original_inventory_sector\", \"gas\", \"emissions_quantity\",\n\"emissions_quantity_units\", \"temporal_granularity\", \"created_date\",\n\"modified_date\")\n\nView(country_international_aviation_emissions)\nTransportationRoadFullJoin3 <- full_join(TransportationRoadFullJoin2,country_international_aviation_emissions)\n\nJoining, by = c(\"iso3_country\", \"start_time\", \"end_time\",\n\"original_inventory_sector\", \"gas\", \"emissions_quantity\",\n\"emissions_quantity_units\", \"temporal_granularity\", \"created_date\",\n\"modified_date\")\n\nTransportationEmissions_2015_2021 <- full_join(TransportationRoadFullJoin3,country_domestic_aviation_emissions)\n\nJoining, by = c(\"iso3_country\", \"start_time\", \"end_time\",\n\"original_inventory_sector\", \"gas\", \"emissions_quantity\",\n\"emissions_quantity_units\", \"temporal_granularity\", \"created_date\",\n\"modified_date\")"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#removing-unnecessary-columns",
    "href": "posts/Connor Skowyra's Final Project.html#removing-unnecessary-columns",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Removing Unnecessary Columns",
    "text": "Removing Unnecessary Columns\nI chose to combine these data sets one at a time in order to view them and make sure all of the information is successfully entered. On the Master File, you will see all of the inventory sectors in transportation together with the values. As this data was calculated with technology, some information not necessary is present, needing to be removed by using function select()\n\nTransportationEmissions_2015_2021 <- TransportationEmissions_2015_2021 %>% select(-c(end_time, temporal_granularity:modified_date))"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#mutating-the-year",
    "href": "posts/Connor Skowyra's Final Project.html#mutating-the-year",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Mutating the Year",
    "text": "Mutating the Year\nOur main data set now contains all relevant columns to answer our research question. One error on the data set that has not been resolved is that I do not want a date, I only want the year so we will need to mutate the data, using function mutate()\n\nTransportationEmissions_2015_2021 <- TransportationEmissions_2015_2021 %>% \n  mutate(start_time = year(start_time))"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#renaming-columns",
    "href": "posts/Connor Skowyra's Final Project.html#renaming-columns",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nIn our data set, the column start_time will only show the year instead of the entire date, making the process of creating a visualization easier. The column names can be updated to make our data set look more clean so we will need to rename the columns. Using the rename() function, we can rename multiple columns in one command.\n\nTransportationEmissions_2015_2021 = TransportationEmissions_2015_2021 %>% rename(Country = iso3_country, Year = start_time, Section_of_Transportation = original_inventory_sector, Type_of_Gas = gas, Annual_Total_Emissions = emissions_quantity, Unit_Of_Measurement = emissions_quantity_units)"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#filter-variables",
    "href": "posts/Connor Skowyra's Final Project.html#filter-variables",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Filter Variables",
    "text": "Filter Variables\nIn our data sets, some of our joined data had statistics for 2022 while some did not. We can not include 2022 as it would give us unrealistic results and would be not accurate due to having insufficient information. We need to filter out all data with the year 2022. I will achieve this by using function filter()\n\nTransportationEmissions_2015_2021 <- filter(TransportationEmissions_2015_2021, Year < '2022')"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#data-frame-for-total-2015-2021-transportation-emissions",
    "href": "posts/Connor Skowyra's Final Project.html#data-frame-for-total-2015-2021-transportation-emissions",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Data Frame for Total 2015-2021 Transportation Emissions",
    "text": "Data Frame for Total 2015-2021 Transportation Emissions\nAfter improving our data set and being able to easily understand our variables and observations, I am confident that a summary of the data can be created. To summarize the data, we will need to isolate and arrange some variables to a new data frame.\n\nnet_emissions <- TransportationEmissions_2015_2021 %>% \n  select(Year, Annual_Total_Emissions) %>% \n  group_by(Year) %>% \n  summarise(Sum_of_Annual_Total_Emissions = sum(Annual_Total_Emissions))\n\n\nsummary(net_emissions)\n\n      Year      Sum_of_Annual_Total_Emissions\n Min.   :2015   Min.   :4480136779           \n 1st Qu.:2016   1st Qu.:5038651667           \n Median :2018   Median :5191797816           \n Mean   :2018   Mean   :5086377127           \n 3rd Qu.:2020   3rd Qu.:5267393335           \n Max.   :2021   Max.   :5320615286           \n\n\nOur new data frame shows the total amount of emissions on a yearly basis since 2015. I observed an increase of emissions in 2015 to 2019 due to our economy growing at a rapid pace with the introduction of globalization and more transportation needed to move things. 2020 is the lowest year of emissions with multiple events happened causing a decrease of emissions, a major reason is COVID-19, limiting travel from countries with less people commuting to work and leisure. Another reason for the decrease is the threat of climate change and individuals wanting to be ecologically friendly by demanding clean energy investment to the government. 2018 is the highest year of emissions likely linked with our growing economy and need of cheap transportation."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#transportation-emissions",
    "href": "posts/Connor Skowyra's Final Project.html#transportation-emissions",
    "title": "Transportation Emissions from 2015-2022",
    "section": "2021 Transportation Emissions",
    "text": "2021 Transportation Emissions\nI need to separate 2021 emissions from all of the other years to determine highest amount of emissions in each sector of transportation to make an effective visualization of one of the deliverables. The next task is creating a new data frame to understand sector of transportation with the highest amount of emissions so I will need to find a sum for each sector.\n\nEmissions2021 <- TransportationEmissions_2015_2021 %>% filter(Year == '2021') %>% group_by( Section_of_Transportation) %>% summarise(Sum_of_Annual_Total_Emissions = sum(Annual_Total_Emissions))"
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#gas-type",
    "href": "posts/Connor Skowyra's Final Project.html#gas-type",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Gas Type",
    "text": "Gas Type\nIn our previous graph, we will find that road transportation as the majority of emissions. All other sectors combined would be smaller, this tells us that road transportation is the area, where our focus of reducing emissions should be a priority. Our final data frame required will inform us of the type and amount of gases emitted from 2015 to 2021.\n\nGasTypeEmissionTotal <- TransportationEmissions_2015_2021 %>% \n  select(Year, Type_of_Gas, Annual_Total_Emissions) %>% \n  group_by(Type_of_Gas) %>% \n summarise(Sum_of_Annual_Total_Emissions = sum(Annual_Total_Emissions))\n\nOur data sets states that the majority of gases produced within the Transportation industry are Carbon Dioxide (Co2) or Carbon Dioxide Equivalent gases (Co2E) with a fraction being Methane and Nitrous Oxide.\nNow, that all of our data sets have been successfully created, we can begin to create our visualizations."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#data-visualization",
    "href": "posts/Connor Skowyra's Final Project.html#data-visualization",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo make the correct visualizations, we need to understand the variables that need to be graphed. A discrete variable are values that can be obtained by counting. An example of a discrete is our years column with a value like 2015, which can be counted. A continuous variable measures random values that mean something such as the sector of industry. We are going to use two variables for all of the plots so we will be using bi-variate graphs.\nUsing this information, I managed to separate my variables into either discrete or continuous in order to find the best visualization. I tried a few different plots to graph the data like a jitter and bar graph but I decided the count graph with a combination of a line graph was best. In the net emissions, both variables are discrete as the year and annual emissions were counted. The count graph shows the values in a precise, clear way and the line graph shows the change of data over time.\nAfter I decided on my two graphs, I used my two discrete variables “Year and Annual Total Emissions” from the net_emissions dataset and created the visualization.\n\nggplot(net_emissions, aes(x = Year, y = Sum_of_Annual_Total_Emissions)) + geom_line (aes(group=1)) + geom_count(colour = \"darkblue\", fill = 4) + labs(title = \"Net Emissions By Year\",y = \"Total Emissions\",x = \"Year\") + theme(plot.title = element_text(size = 26, face = \"bold\", color = \"black\"),axis.title.x = element_blank(),\naxis.title.y = element_blank(), legend.position = \"none\")\n\n\n\n\nNow the plot show changes of emissions by the transportation industry in 2015 to 2021. Looking at the visualization, there is a significant drop off in emissions in 2020 but increased for the majority of years.\nOur next step is creating a plot for the year of 2021 by using our Emissions2021 data frame to determine most emissions per sector. I will be using a column graph as we are using one discrete and continuous variable for analysis.\n\nggplot(Emissions2021, aes(x = Section_of_Transportation, y = Sum_of_Annual_Total_Emissions)) + geom_col(colour = \"darkblue\", fill = 4) + labs(title = \"Transportation Sector Emissions in 2021\",y = \"Total Emissions\",x = \"Transportation Sector\") + theme(plot.title = element_text(size = 22, face = \"bold\", color = \"black\"),axis.title.x = element_blank(),\naxis.title.y = element_blank(), legend.position = \"none\")\n\n\n\n\nThe visualization for the transportation sector states that road transportation industry is most significant emitter with little contributions from the remaining sectors.\nThe final visualization needed to answer our research question is the amount of emissions by gas from 2015-2021 to understand most prominent gases by transportation industry. I will be using a column graph once again.\n\nggplot(GasTypeEmissionTotal, aes(x = Type_of_Gas, y = Sum_of_Annual_Total_Emissions)) + geom_col(colour = \"darkblue\", fill = 4) + labs(title = \"Gas Emission Total\",y = \"Total Emissions\",x = \"Type of Gas\") + theme(plot.title = element_text(size = 26, face = \"bold\", color = \"black\"),axis.title.x = element_blank(),\naxis.title.y = element_blank(), legend.position = \"none\")\n\n\n\n\nThe results show almost all emissions are caused by Carbon Dioxide Gases or Carbon Dioxide Equivalent gases.\nWhile completing these visualizations, I realized some interesting results. In my first plot, it was slowly increasing until the peak in 2018 with a small decrease in 2019. The major information discovered is a sharp decline in emissions in 2020. Few reasons can be involved as COVID-19 began to affect the global economy by making people shelter in place with essential travel happening in the US such as getting groceries, going to work if you are an essential worker as an example for the decrease. Popularity in clean transportation and activism in climate change has caused people to change their lifestyle choices like not owning a car, switching to a EV, or public transportation reduces on-road emissions as well. As the United States emerged from the pandemic in 2021, a few factors caused an increase but not to the peak of 2018. People began to get vaccinated and feel more comfortable in public meaning people back into the office, participating in leisure activities, resulting in higher emission levels. However, due to the switch over to cleaner transportation and people continuing virtually slow down the increase.\nThe Sector of Transportation data set confirms that the majority of emissions caused by sector is road transportation. This is not a surprise to me as most people in the United States will own a car, drive to one destination from another, it is the most accessible mode of transportation in rural and urban areas. Jobs need cars to function as employees need vehicles to get to work, complete work, and leave in most cases. I expected domestic aviation to have more emissions as people begin to fly commercially for business, family or leisure as states open for travelers, but was surprised with a value comparable to international aviation which is regulated for COVID prone countries.\nMy final analysis states that the majority of gas emitted in the Transportation industry is Carbon Dioxide or Equivalent gases. I was surprised on how much little methane and nitrous oxide emissions were produced due to the aviation industry along with the production of the equipment. I did expect a substantial amount of Carbon Dioxide emissions as vehicles are the lifeline to our economy, and the country is going through electric vehicle transformation at a slow rate. In return, we need to utilize gas powered vehicles which cause higher carbon dioxide emissions. I expect this number to decrease as cleaner forms of transformation and power begin to become more prevalent."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#reflection",
    "href": "posts/Connor Skowyra's Final Project.html#reflection",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Reflection",
    "text": "Reflection\nThis was my first research project in Data Science that I have ever done as I decided on a career change when I enter the Data Analytics and Computational Social Science program. I began the project initially with little understanding in R, I prepared with a mini course in SQL but was unsure how that knowledge would transfer and be relevant to the project. Over time, I became more confident after doing the homework, trial and error, and asking for help from tutors. At first, I was planning to do a project based on Electric Vehicle Sales within Washington. However, I was unable to find a sufficient data set to join my data together regarding vehicle pricing, average lifespan of vehicle to determine if an EV was a good purchase for an average American so I decided to change my approach and look at overall emissions in the transportation industry. As someone who works in the utility industry and closely with our Fleet department. I wanted to see any correlation between greenhouse emissions and road transportation along with a comparison of other sectors. It would help me visualize, what I want to work to achieve in my personal work role. Our company goal is to become fully electrify our Fleet by 2050 and bring our carbon footprint to zero. I wanted to see as a country if our emissions were decreasing, which sector is most responsible and what type of gas is most responsible.\nMy most challenging problem was removing the data in scientific form into regular form. I was confused for a portion of time on why my numbers were so inaccurate, an example of this is the total methane emissions had a much higher number than carbon dioxide. Originally, the problem was related to how the data was initially downloaded and uploaded into R. Other data sets, I checked did not have this issue. I also knew that my data was false because vehicles run on gas which emits carbon dioxide while methane and nitrous oxide is more common in different industries like agriculture. I was unsure if the code was typed wrong, or if my charts were incomplete. I second guessed myself and made it seem a lot harder. Eventually, I needed help and I realized that it was the formatting in R that created the issue. My data set and code was correct with no errors. I typed in the option function, and correctly formatted my data for visualization.\nI did not experience much trouble with the visualizations which surprised me. I completed a visualization of the United States map for a different project and I could not find an answer to the function giving me errors for a few hours. Making the visualization in my different class, I learned how to modify the visualization to make it clean, understand what is important that individuals need to know, and code technical skills that definitely helped me complete this project.\nIn the project, I wished I had access to information regarding emissions by state. I would liked to compare how New England emissions compare to a similar region in population like Los Angeles or Maryland and Virginia to see if New England is leading emissions reduction in the United States. I could understand what are we doing right to reduce emissions or what should be do different if we are in the higher percentile.\nAnother idea, I wish that had a higher amount of information available is additional greenhouse gas emissions from other industries like agriculture and utilities to compare to the transportation industry. A wide business view on opinions and plans to reduce climate change would be interesting to learn about and analyze. Continuing the project to a new phase, I would add compare emissions in the transportation industry to the agriculture and the utility industries which have produced mass amounts of gases. The data would look to prove correlation if business in America is becoming more green or if an illusion is being shown and no meaningful change has happened."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#conclusion",
    "href": "posts/Connor Skowyra's Final Project.html#conclusion",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Conclusion",
    "text": "Conclusion\nOur research question states “How much emissions per ton are emitted annually and how this dilemma is changing since the push for clean energy beginning in 2015?”.\nThe analysis states that approximately 5,086,377,127 tons of emissions are emitted annually from 2015 to 2021. Over the last two years, this mean has been higher, meaning we have been decreasing our emissions average which shows positive change. The lower amount of emissions are the COVID-19 pandemic in 2020, the Electric implementation programs in Transportation, less commuting to the office are important factors in the reduction. People are seeing climate change as a significant threat and realize action must be taken to protect the future. This mindset encourages government to create rebates for clean transportation, investment for infrastructure and eco-friendly legislation.\nThe road transportation sector has been responsible for the majority of emissions from 2015 to 2021, with the majority of emissions being related to Carbon Dioxide. This information debunks our theory stating a significant amount of emissions are caused by daily tasks essential to life, getting groceries, commuting to work, seeing friends and family. Vehicles with engines are the vast majority of the United States car market due to lower initial price, quicker time to refuel, lack of infrastructure for vehicles powered with electric, hydrogen in most of the country. Work vehicles like trucks and construction equipment do not have the capability to use batteries as main sources of power as the technology is not available. The increase of non internal combustion vehicles becoming more accessible will reduce the amount of carbon dioxide being emitted in the road transportation industry and hopefully will extend to other transportation sectors like aviation.\nAs a result, we have determined that emissions have increased since 2020 due to the COVID-19 pandemic but less than pre-pandemic levels, projected to decrease in the coming years. The road transportation sector and carbon dioxide related gases are most responsible of emissions from our research. Using this information, we can begin to build a positive future to change the transportation industry to make a better tomorrow."
  },
  {
    "objectID": "posts/Connor Skowyra's Final Project.html#bibliography",
    "href": "posts/Connor Skowyra's Final Project.html#bibliography",
    "title": "Transportation Emissions from 2015-2022",
    "section": "Bibliography",
    "text": "Bibliography\nClimate TRACE, https://climatetrace.org/downloads. < Raw Data >\nSinger-Vine, Jeremy. “Big Emitters.” Data Is Plural, 16 Nov. 2022, https://www.data-is-plural.com/archive/2022-11-16-edition/. < Where Data was Obtained >\n“On Road.” Climate TRACE, 9 Nov. 2022, https://climatetrace.org/public/upload/files/62f50cfb415f6.pdf?v=1667641844. < How Transportation Data was Gathered >\nFederal funding programs (no date) U.S. Department of Transportation. Available at: https://www.transportation.gov/rural/ev/toolkit/ev-infrastructure-funding-and-financing/federal-funding-programs (Accessed: December 17, 2022). > Federal EV Programs >\n“Overview of Greenhouse Gases.” EPA, Environmental Protection Agency, 16 May 2022, https://www.epa.gov/ghgemissions/overview-greenhouse-gases. < Analysis of Gases >\nRabo, Olga. “What Is CO2E and How Is It Calculated?” Full Impact View of Your Investments - Cooler Future, 18 Nov. 2020, https://www.coolerfuture.com/blog/co2e. < CO2e Defined >\nRice, D., & Rolfe, M. (n.d.). Data Science Fundamentals: R Tutorials. Retrieved from https://classroom.google.com/u/1/w/MTIwNzU2NTkzNjI2/t/all < Class Tutorials >"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html",
    "href": "posts/danielseriy_finalproject_submission.html",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "",
    "text": "library(tidyverse) \n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(readxl) \n\nWarning: package 'readxl' was built under R version 4.2.2\n\nlibrary(ggplot2) \nlibrary(usmap)\n\nWarning: package 'usmap' was built under R version 4.2.2\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#introduction",
    "href": "posts/danielseriy_finalproject_submission.html#introduction",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Introduction",
    "text": "Introduction\nElections in recent history have been controversial and contentious. Are people in the United States really satisfied with our electoral system, not in the sense of the electoral college, but in the candidates that are available to them? Along with this, which electorate, in terms of states, are the most dissatisfied with elections, and do these dissatisfied electorates have the most impact on elections in general. The hope is that with the analysis of this data set, the answers to these questions can begin to be answered."
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#data",
    "href": "posts/danielseriy_finalproject_submission.html#data",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Data",
    "text": "Data\nThe data set being used in this study has been sourced from the MIT Election Data and Science Lab. This data set was created in 2017, and contains presidential election data for all 50 states for the 1976 - 2020 election years. The data set includes 15 distinct variables. Not all of these variable will be necessary in our analysis, and will be cleaned in order to make the analysis more streamlined. the summary of the original data set is included below.\n\n#Read in of election Dataset \nelection_orig <- read.csv(\"_data/1976-2020-president.csv\")\n\n#Summarize Dataset election_orig\nsummary(election_orig)\n\n      year         state             state_po           state_fips   \n Min.   :1976   Length:4287        Length:4287        Min.   : 1.00  \n 1st Qu.:1988   Class :character   Class :character   1st Qu.:16.00  \n Median :2000   Mode  :character   Mode  :character   Median :28.00  \n Mean   :1999                                         Mean   :28.62  \n 3rd Qu.:2012                                         3rd Qu.:41.00  \n Max.   :2020                                         Max.   :56.00  \n   state_cen        state_ic        office           candidate        \n Min.   :11.00   Min.   : 1.00   Length:4287        Length:4287       \n 1st Qu.:33.00   1st Qu.:22.00   Class :character   Class :character  \n Median :53.00   Median :42.00   Mode  :character   Mode  :character  \n Mean   :53.67   Mean   :39.75                                        \n 3rd Qu.:81.00   3rd Qu.:61.00                                        \n Max.   :95.00   Max.   :82.00                                        \n party_detailed      writein        candidatevotes       totalvotes      \n Length:4287        Mode :logical   Min.   :       0   Min.   :  123574  \n Class :character   FALSE:3807      1st Qu.:    1177   1st Qu.:  652274  \n Mode  :character   TRUE :477       Median :    7499   Median : 1569180  \n                    NA's :3         Mean   :  311908   Mean   : 2366924  \n                                    3rd Qu.:  199242   3rd Qu.: 3033118  \n                                    Max.   :11110250   Max.   :17500881  \n    version          notes         party_simplified  \n Min.   :20210113   Mode:logical   Length:4287       \n 1st Qu.:20210113   NA's:4287      Class :character  \n Median :20210113                  Mode  :character  \n Mean   :20210113                                    \n 3rd Qu.:20210113                                    \n Max.   :20210113"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#data-wrangling-mutation",
    "href": "posts/danielseriy_finalproject_submission.html#data-wrangling-mutation",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Data Wrangling & Mutation",
    "text": "Data Wrangling & Mutation\nIn order to begin the analysis, we must create a data frame that contains only crucial data from the original data set. In this case, we only needed to select the data that describes the outcome of each election cycle for each state. After this is completed, a new variable, percentvotes, was created to better represent the candidate with the highest percent of the votes for any election cycle. Finally, it was crucial to better format how write-in votes were represented. First for all rows that had writein == TRUE , “Write-In Votes” was replaced into the candidate column as well as the party_detailed column for the same row would be replaced with “Other”. finally, the writein column was removed since its data was now represented in the candidate and party_detailed columns.\nThis election1 data frame will be where all other data frames needed for the following visualizations will be derived from.\n\n#Select Pertinant Data from election_orig\nelection1 <- election_orig %>%\n  select(year, state, candidate, party_detailed, writein, candidatevotes, totalvotes) %>%\n  mutate(state = tolower(state)) %>%\n  mutate(percentvotes = (candidatevotes/totalvotes) * 100) %>%\n  mutate(candidate = case_when(\n    writein == TRUE ~ \"Write-In Votes\",\n    writein == FALSE ~ candidate)) %>%\n  mutate(party_detailed = case_when(\n    writein == TRUE ~ \"Other\",\n    writein == FALSE ~ party_detailed)) %>%\n  select(!writein)"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#visualization-1-heat-mapping-of-totalvotes",
    "href": "posts/danielseriy_finalproject_submission.html#visualization-1-heat-mapping-of-totalvotes",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Visualization #1: Heat Mapping of totalvotes",
    "text": "Visualization #1: Heat Mapping of totalvotes\nStep #1: Create data frame election_participation, that assigns the totalvotes for each state in each election year.\n\nelection_participation <- election1 %>%\n  select(year, state, totalvotes) %>%\n  distinct() %>%\n  pivot_wider(names_from = year,\n              values_from = totalvotes,\n              id_cols = state)\n\nStep #2: Create data frame mapdata, to represent the geographic data of the 50 states.\n\nThis is crucial in order to create the visualization, and will be used again for visualization #2.\n\n\nmapdata <- us_map() %>%\n  select(x, y, group, full) %>%\n  mutate(state = full) %>%\n  select(!full)\n\nmapdata$state <- tolower(mapdata$state)\n\nStep #3: Join mapdata and election_participation data frames by state & pivot_longer the newly joined data frame in order for data to be organized by year. This new data frame is called mapdata_totalvotes.\n\nThis new data frame, mapdata_totalvotes, will be what is used to create the visualization.\n\n\nmapdata_totalvotes <- left_join(mapdata, election_participation, by = \"state\") %>%\n  pivot_longer(cols = c(\"1976\" : \"2020\"),\n               names_to = \"year\",\n               values_to = \"total_votes\")\n\nStep #4: Create visualization of data, map_totalvotes, using ggplot2. The visualization will create 12 distinct heat maps for each election year using facet.\n\nmap_totalvotes <- ggplot(mapdata_totalvotes, aes(x = x,\n                                                 y = y,\n                                                 fill = total_votes,\n                                                 group = group)) +\n  facet_wrap(vars(year)) +\n  geom_polygon(color = \"black\") +\n  scale_fill_gradient(name = \"Total Votes\", low = \"white\", high = \"red\", na.value = \"grey50\") +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        rect = element_blank())\n\nmap_totalvotes"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#visualization-2-mapping-of-popular-vote-winner-by-state",
    "href": "posts/danielseriy_finalproject_submission.html#visualization-2-mapping-of-popular-vote-winner-by-state",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Visualization #2: Mapping of popular vote winner by state",
    "text": "Visualization #2: Mapping of popular vote winner by state\nStep #1: Create data frame election_winners, where each state for each election year is assigned the party_detailed with the highest percentvotes.\n\nelection_winners <- election1 %>%\n  select(!candidatevotes & !totalvotes & !candidate) %>%\n  group_by(year, state) %>%\n  slice_max(n = 1, percentvotes) %>%\n  pivot_wider(names_from = year,\n              values_from = party_detailed,\n              id_cols = state)\n\nStep #2: Join mapdata and election_winners data frames by state & pivto_longer in order for data to be organized by year and party in a new data frame, mapdata_winners.\n\nmapdata_winners <- left_join(mapdata, election_winners, by = \"state\") %>%\n  pivot_longer(cols = c(\"1976\" : \"2020\"),\n               names_to = \"year\",\n               values_to = \"Party\")\n\nStep #3: Create visualization, map_winners, where each election year is represented using facet. - In order for each state to be colored to match the party correctly, data frame color_list was created to assign each party with it’s respective color.\n\nmap_winners <- ggplot(mapdata_winners, aes(x = x,\n                                           y = y,\n                                           fill = Party,\n                                           group = group)) +\n  facet_wrap(vars(year)) +\n  geom_polygon(color = \"black\") +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        rect = element_blank())\n\ncolor_list <- c(\"DEMOCRAT\" = \"blue\", \"DEMOCRATIC-FARMER-LABOR\" = \"green\", \"other\" = \"yellow\", \"REPUBLICAN\" = \"red\")\n\nmap_winners + scale_colour_manual(values = color_list,aesthetics = c(\"colour\", \"fill\"))"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#visualization-3-4-growth-of-totalvotes",
    "href": "posts/danielseriy_finalproject_submission.html#visualization-3-4-growth-of-totalvotes",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Visualization #3 & #4: Growth of totalvotes",
    "text": "Visualization #3 & #4: Growth of totalvotes\nThe following 2 visualizations represent the same data. the first of two represent all the data on a singular graph to better show the outliers in growth, while the second of the two represents each state individually.\nStep #1: Create data frame total_votes_data to represent totalvotes for each state and year.\n\ntotal_votes_data <- election1 %>%\n  select(year, state, totalvotes) %>%\n  distinct()\n\nStep #2: Create visualization #3, total_votes_group.\n\ntotal_votes_group <- ggplot(total_votes_data, aes(x = year,\n                                                  y = totalvotes,\n                                                  group = state,\n                                                  color = state)) +\n  geom_line() +\n  ggtitle(\"Total Votes Growth\")\n\ntotal_votes_group\n\n\n\n\nStep #3: Create visualization #4, total_votes_indv.\n\ntotal_votes_indv <- ggplot(total_votes_data, aes(x = year,\n                                                 y = totalvotes,\n                                                 group = state,\n                                                 color = state)) +\n  facet_wrap(vars(state)) +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank()) +\n  geom_line() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Total Votes Growth by State\")\n\ntotal_votes_indv"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#visualization-5-6-growth-of-write-in-votes",
    "href": "posts/danielseriy_finalproject_submission.html#visualization-5-6-growth-of-write-in-votes",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Visualization #5 & #6: Growth of Write-In Votes",
    "text": "Visualization #5 & #6: Growth of Write-In Votes\nThe following to visualizations are completed in a similar manner to #3 & #4, but instead of using totalvotes, we are using the number of Write-In votes.\nStep #1: Create data frame writein_votes_data which represents how many write-in votes were recorded for each state in each election year.\n\nwritein_votes_data <- election1 %>%\n  filter(candidate == \"Write-In Votes\") %>%\n  select(year, state, candidatevotes)\n\nStep #2: Create visualization #5, writein_votes_group.\n\nwritein_votes_group <- ggplot(writein_votes_data, aes(x = year,\n                                                  y = candidatevotes,\n                                                  group = state,\n                                                  color = state)) +\n  geom_line() +\n  ggtitle(\"Write-In Votes Growth\")\n\nwritein_votes_group\n\n\n\n\nStep #3: Create visualization #6, writein_votes_indv.\n\nwritein_votes_indv <- ggplot(writein_votes_data, aes(x = year,\n                                                 y = candidatevotes,\n                                                 group = state,\n                                                 color = state)) +\n  facet_wrap(vars(state)) +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank()) +\n  geom_line() +\n  theme(legend.position = \"none\") + \n  ggtitle(\"Write-In Votes Growth by State\")\n\nwritein_votes_indv\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?"
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#analysis",
    "href": "posts/danielseriy_finalproject_submission.html#analysis",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Analysis",
    "text": "Analysis\n\nVisualization #1:\n\nFrom a purely visual interpretation, visualization #1 can give us insight into which states have larger electorates than others. It is clear to see that states on the coasts or states on the exterior parts of the US have more voters than states in the interior. This makes sense due to the fact that the majority of larger cities and population centers are in states closer to the ocean/boarders, while states in the interior tend to have smaller populations because of the nature of that land being the “breadbasket”, where most of the agricultural production takes place. There are 4 states in the visualization that stand out as outliers to having larger voting populations. These states are Florida, Texas, California & New York. This makes sense due to the fact that these states contain some of the largest cities in the US, with NYC containing the #1 largest city, NYC, with a population of ~8.8 Million people as of the 2020 census. California follows closely behind with the second largest city, Los Angeles, with a population of ~3.9 Million people as of the 2020 Census. Texas has the 4th largest city Houston, and Florida contains the 12th largest and 44th largest cities, Jacksonville and Miami, respectively. (infoplease)\n\nVisualization #2:\n\nWhile it would make sense to assume that states with larger populations have a disproportional impact in determining the outcome of elections, according to visualization #2 as well as the actual results of national elections, this assumption would be incorrect and not supported by the visualization. If we look at the four states mentioned above, while there are instances where the outcome of the states popular vote did match up with the outcome of the election, like California going Republican ’80 & ’84 for Reagan, this did not match up with California’s majority of Republican in ’76, and the winner of the election Jimmy Carter for the Democrats. This is only one example of the states, but this is true for all of the other big states. They do tend to match the results of the national election, but it is not consistent enough to say that there is a correlation between the states with total votes determining the outcome of the election.\n\nVisualization #3 & #4:\n\nThese visualizations help support my conclusion for visualization #1. This is because I concluded that the 4 states with the most total votes were California, Texas, New York, and Florida had the 4 highest total votes, and through visualizations #3 & #4, its is clearer to see that these states are the outliers compared to the rest of the states. Not only that, but these visualizations also show that compared to the other states in the US, the total amount of voters in these states is also growing more than the other 46 states, meaning that as time goes on, these 4 states will contribute more and more to the total popular vote.\nAnother Conclusion you can pull from these visualizations is that states with notably large cities like North Carolina with Charlotte and Illinois with Chicago, these cities are also growing with total votes, unlike states with notably small cities like Montana or Nebraska are staying rather constant.\n\nVisualization #5 & #6:\n\nIn order to answer the question of the satisfaction of the electorate with the current state of the electoral system, I have decided to use the amount & growth of write-in votes as the marker for said satisfaction/dissatisfaction.\nThe main conclusion that I made from these visualizations comes from visualization #5. this visualization clearly shows that going into the 2012 election, voters were beginning to write-in votes way more than they had in the past, and then would decrease down towards the 2020 election meaning that for the second election of Obama and the electoral race between Clinton and Trump, people were not satisfied with the candidates that were available to them.\nWhat makes this statistic more interesting is that the outliers for this data were California, Texas, and Washington DC. While I can understand that California and Texas had higher values due to the conclusions that we made above about their higher total voters, Washington DC makes an ironic point. The voters in the district have the least amount of representation in government and yet they are the most dissatisfied with the presidential candidates. I just find that to be interesting."
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#future-additionsimprovements",
    "href": "posts/danielseriy_finalproject_submission.html#future-additionsimprovements",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Future additions/improvements",
    "text": "Future additions/improvements\nWhile I think the work I have done here is a great start, I think there is a lot of additions I can make.\nFirst, I think that the addition of population data would give me more insight into the types of voters who are voting and the amount of eligible voters who are/aren’t voting. If I were to have included this data or found a data set with this data included, I could have created visualizations that were not just total votes, but total votes per capita (of eligible voters), I could have a better understanding of the trust in the voting system and understand how many people who can vote are voting.\nSecond, I think it would be helpful to have been able to add an overlay of the electoral college results on top of the popular vote election results. This would have allowed me to compare the differences between the two and possibly allow me to conclude which of the two systems is better representative of the electorate."
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#conclusion",
    "href": "posts/danielseriy_finalproject_submission.html#conclusion",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Conclusion",
    "text": "Conclusion\nIn Conclusion, This project allowed me to explore the world of R and data visualization, and see the possibilities that these tools have to offer. This project also allowed me to better understand presidential election data and what it can show.\nThere is a lot more work that can be done on this project to make more concrete conclusions, and I plan to continue this research into the future. There are millions of more data sets available on the internet to join into the data I have already compiled, and more complexity I can create. Not only that, but with the addition of statistical analysis, which I plan to include in the future, I will be able to make more concrete conclusions about the realities of presidential election data."
  },
  {
    "objectID": "posts/danielseriy_finalproject_submission.html#citations",
    "href": "posts/danielseriy_finalproject_submission.html#citations",
    "title": "DACSS 601 Final Project - Presidential Election Data from 1976 - 2020; an analysis of voter trends and satisfaction with the current electoral system",
    "section": "Citations",
    "text": "Citations\n\nResearch Citations:\n\nhttps://www.infoplease.com/us/cities/top-50-cities-us-population-and-rank\nhttp://www.iweblists.com/us/government/PresidentialElectionResults.html\n\nData Set Citation:\n\nMIT Election Data and Science Lab, 2017, “U.S. President 1976–2020”, https://doi.org/10.7910/DVN/42MVDX, Harvard Dataverse, V6, UNF:6:4KoNz9KgTkXy0ZBxJ9ZkOw== [fileUNF]"
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html",
    "href": "posts/Emma_Narkewicz_Final Project.html",
    "title": "Emma Narkewicz Final Project",
    "section": "",
    "text": "Code\n#| label: setup\n#| warning: false\n\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.2.2\n\n\nCode\nlibrary(tidycensus)\n\n\nWarning: package 'tidycensus' was built under R version 4.2.2\n\n\nCode\n  options(tigris_use_cache = TRUE)\nlibrary(tigris)\n\n\nWarning: package 'tigris' was built under R version 4.2.2\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nCode\nlibrary(RColorBrewer)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#introduction",
    "href": "posts/Emma_Narkewicz_Final Project.html#introduction",
    "title": "Emma Narkewicz Final Project",
    "section": "Introduction",
    "text": "Introduction\nFor my final project, I analyzed equity in access to Career & Technical Education (CTE) in 2021-2022 for 8,268,271 US high school students in each of of 3 underrepresented groups (Gender, Disability & Race), for each state, and in 3 distinct CTE career clusters:\n\nIT\nHealth Science\nSTEM\n\nI utilized the US Department of Education’s 2020-2021 Perkins V Data to analyze equity CTE participation. Equity in access was measured by the relative CTE participation of 3 student populations that are under represented in STEM careers:\n\nFemale Students\nStudents with Disabilities\nRacially & Ethnically Diverse Students\n\nThe portion of Perkins V Population data set read in for analysis spanned 52 geographic areas (50 states, Puerto Rico, & the District of Columbia), 3 CTE career clusters, 3 under represented student populations, & 7 racial & ethnic categories.\nIn the 2020-2021 school year 910,652 US high school students took Health Sciences CTE courses, 898,942 US high school students took IT CTE courses, and 666,222 US high school students took STEM CTE courses.\nTo accurately compare racial & ethnic diversity of students in CTE between states, the racial diversity of each state was calculated using US Census Bureau 2020 Decennial Census Data. The Perkins V data set was joined with 2020 Race & Ethnicity Census data for each state. Relative racial & ethnic diversity of state CTE programs was calculated by finding the difference between the state’s racial diversity in secondary CTE programs and the state’s racial diversity from census data.\nThe racial & ethnic categories used were:\n\nAmerican Indian or Alaskan Native\n\nAsian\n\nBlack or African American\n\nHispanic/Latino\n\nNative Hawaiian or Other Pacific Islander\n\nWhite\n\nTwo or More Races"
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#research-context",
    "href": "posts/Emma_Narkewicz_Final Project.html#research-context",
    "title": "Emma Narkewicz Final Project",
    "section": "Research Context",
    "text": "Research Context\nCareer & Technical Education (CTE) courses expose students to high demand careers through hands on training. Under the 2018 Strengthening Career & Technical Education (CTE) for the 21st Century Act (aka Perkins V), states receive federal funding to provide CTE. States must develop plans for CTE delivery, report on CTE participation & performance, and conduct Comprehensive Needs Assessments (CNAs) on CTE at the local level.\nStates have to allocate a portion of their Perkins V funds to recruiting Special Populations in CTE - an umbrella category which include students with disabilities (SWDs), and students in fields where less than 25% of individuals are of any gender. Barriers to CTE access for special populations include:\n\nLack of Training for CTE Instructors on Working With Students With Disabilities (i.e. Developing IEPs & Providing Accommodations)\nStigma & Misconceptions About the Capabilities of Students With Disabilities\nHistorical Inequities of Who is Permitted to Participate in STEM Fields\n\nI research equity in access and outcomes for secondary students with disabilities in CTE in my job at UMass Chan Medical School. To quote a policy brief I coauthored, “Research shows that CTE coursework in high school provides opportunities to improve employment and post-secondary outcomes for learners with disabilities, CTE has led to higher rates of on-time graduation, and more competitive paid jobs for secondary students with disabilities”  (McKay, Ellison, & Narkewicz, 2022)\nEven when female students, students with disabilities (SWDs), and racially diverse students are able to access CTE, they may be tracked into lower earning career clusters. The low participation of SWDs, female students, and Black Indigenous & People of Color (BIPOC) students in STEM fields inspired me to focus on CTE course in the IT, STEM, & Health Science career clusters."
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#research-questions",
    "href": "posts/Emma_Narkewicz_Final Project.html#research-questions",
    "title": "Emma Narkewicz Final Project",
    "section": "Research Questions",
    "text": "Research Questions\n\nRQ1. Did female secondary students equally participate in any CTE course & in STEM, Health Science, & IT career clusters in 2020-2021?\n\nWhat states had the highest & lowest female participation in CTE?\nWere female students under or over represented in any of the 3 career clusters?\n\nRQ2. What percentage of secondary students with disabilities participated in any CTE course & in STEM, Health Science, & IT career clusters in 2020-2021?\n\nWhat states had the highest & lowest participation of students with disabilities in CTE?\nWere students with disabilities under or over represented in any of the 3 CTE career clusters?\n\nRQ3. What is the racial & ethnic diversity of secondary CTE students in any CTE course & in STEM, Health Science, & IT career clusters in 2020-2021?\n\nWas participation in CTE comparable between students of different racial & ethnic groups?\nWas participation in STEM, Health, & IT CTE cluster comparable between students of different racial & ethnic groups?\nWere there any states where racial or ethnic groups are relatively under represented or over represented?\n\n\nNote: When interpreting answers to these research questions, it is important to note that all data is only for the 2020-2021 school year at the secondary CTE levels. Therefore these findings cannot be generalized to the participation of all CTE students in any school year."
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#reading-in-the-data",
    "href": "posts/Emma_Narkewicz_Final Project.html#reading-in-the-data",
    "title": "Emma Narkewicz Final Project",
    "section": "Reading In the Data",
    "text": "Reading In the Data\nFor easier visualization & analysis, rows & columns containing demographic categories and CTE career clusters outside the specified variables for analysis were deleted upon read in. Data on students with disabilities at the secondary (high school) level is based on the Individuals with Disabilities & Elementary and Secondary Education Acts (IDEA/ESEA). Rows containing CTE participation data the territory of Palau were not read in as there was not corresponding 2020 census data for this geography. The grand total column was read as neither the gender nor race demographic columns summed to the correct total. More information on the complete Perkins V participation data set can be found in Appendix A.\n\n\nCode\n# Reading in necessary variables\nCTE_DF <- read_excel(\"_Data/CTE_Enrollment_2020-2021.xlsx\",\n                       skip = 2,\n                     col_names = c(\"State\", \n                                   \"Year\", \n                                   \"Education_Level\",\n                                   \"Demographics\", \n                                   \"Total_CTE_Students\", \n                                   \"Delete_1\", \n                                   \"Delete_2\",\n                                   \"Delete_3\", \n                                   \"Delete_4\", \n                                   \"Delete_5\", \n                                   \"Delete_6\", \n                                   \"Delete_7\",\n                                   \"Health_Science\", \n                                   \"Delete_8\",\n                                   \"Delete_9\", \n                                   \"IT\", \n                                   \"Delete_10\",\n                                   \"Delete_11\", \n                                   \"Delete_12\", \n                                   \"STEM\", \n                                   \"Delete_13\",\n                                   \"Delete_14\")) %>%\n  select(!contains(\"Delete\")) %>%\n  filter(str_detect(`Education_Level`,\"Secondary\")) %>%\n   filter(!str_detect(`Demographics`,\"Male\")) %>%\n   filter(!str_detect(`Demographics`,\"Unknown\")) %>%\n   filter(!str_detect(`Demographics`,\"Individuals from Economically Disadvantaged Families\")) %>%\n   filter(!str_detect(`Demographics`,\"ADA\")) %>%\n   filter(!str_detect(`Demographics`,\"Individuals Preparing for Non-traditional Fields\")) %>%\n  filter(!str_detect(`Demographics`,\"English Learners\")) %>%\n   filter(!str_detect(`Demographics`,\"Single Parents\")) %>%\n   filter(!str_detect(`Demographics`,\"Out of Workforce Individuals\")) %>%\n   filter(!str_detect(`Demographics`,\"Homeless Individuals\")) %>%\n   filter(!str_detect(`Demographics`, \"Youth In Foster Care\")) %>%\n   filter(!str_detect(`Demographics`, \"Youth with Parent in Active Military\")) %>%\n  filter(!str_detect(`Demographics`, \"Migrant Students\")) %>%\n  filter(!str_detect(`State`, \"Palau\")) \n\n\nBelow is the Perkins V CTE data set after read in:\n\n\nCode\nCTE_DF\n\n\n# A tibble: 520 × 8\n   State  Year      Education_Level Demographics     Total…¹ Healt…² IT    STEM \n   <chr>  <chr>     <chr>           <chr>            <chr>   <chr>   <chr> <chr>\n 1 Alaska 2020-2021 Secondary       Grand Total      8249    1956    1528  1406 \n 2 Alaska 2020-2021 Secondary       Female           3400    1074    627   590  \n 3 Alaska 2020-2021 Secondary       American Indian… 1366    334     150   172  \n 4 Alaska 2020-2021 Secondary       Asian            495     73      105   54   \n 5 Alaska 2020-2021 Secondary       Black or Africa… 164     38      27    18   \n 6 Alaska 2020-2021 Secondary       Hispanic/Latino  590     118     116   89   \n 7 Alaska 2020-2021 Secondary       Native Hawaiian… 205     18      22    14   \n 8 Alaska 2020-2021 Secondary       White            4421    1153    931   880  \n 9 Alaska 2020-2021 Secondary       Two or More Rac… 1008    222     177   179  \n10 Alaska 2020-2021 Secondary       Individuals Wit… 989     233     201   175  \n# … with 510 more rows, and abbreviated variable names ¹​Total_CTE_Students,\n#   ²​Health_Science"
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#tidying-mutating-data",
    "href": "posts/Emma_Narkewicz_Final Project.html#tidying-mutating-data",
    "title": "Emma Narkewicz Final Project",
    "section": "Tidying & Mutating Data",
    "text": "Tidying & Mutating Data\nThe original CTE data set was neither tidy nor easy to read. To compare the diversity in CTE between states & career clusters, my unit of analysis will be the population of secondary students in CTE for the year 2020-2021, in a state, in a specific CTE cluster, with a specific demographic identity (race, gender, disability).\n\nObjectives of Tidying Data:\n\nRemoved redundant Year & Education Level columns as all data is for Secondary level students in the 2020-2021 year\nUsed pivot_longer() to combine the 3 CTE clusters & 1 state columns into 1 categorical Career Cluster column & 1 column of students counts\nConverted the number of students column from character type to numeric type using parse_number()\nUsed pivot_wider() to separate out gender, race, and disability variables from the Demographics column into distinct columns\n\n\n\nCode\n#Remove redundant columns\nTidy_CTE_DF <- CTE_DF %>%\n  select(!contains(\"Year\")) %>%\n  select(!contains(\"Level\")) %>%\n#Pivot Longer to Condense Career Clusters, also convert number of students to numeric type\n  pivot_longer(cols = \"Total_CTE_Students\": \"STEM\", \n               names_to = \"Career Cluster\",\n               values_to = \"Number of Students\") %>%\n  mutate(across(any_of(c(\"Number of Students\")), \n                parse_number)) %>%\n#Pivot wider demographic variables\npivot_wider(names_from = \"Demographics\", values_from = \"Number of Students\")\n\nTidy_CTE_DF\n\n\n# A tibble: 208 × 12\n   State    Career…¹ Grand…² Female Ameri…³ Asian Black…⁴ Hispa…⁵ Nativ…⁶  White\n   <chr>    <chr>      <dbl>  <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Alaska   Total_C…    8249   3400    1366   495     164     590     205   4421\n 2 Alaska   Health_…    1956   1074     334    73      38     118      18   1153\n 3 Alaska   IT          1528    627     150   105      27     116      22    931\n 4 Alaska   STEM        1406    590     172    54      18      89      14    880\n 5 Alabama  Total_C…  351521 191867    8864  4556  109087   31279     661 189561\n 6 Alabama  Health_…   26687  15305     802   511    7421    2150      63  15043\n 7 Alabama  IT          6635   2115      80   272    2000     552      19   3491\n 8 Alabama  STEM        4037    824     129   130     745     380      17   2507\n 9 Arkansas Total_C…  113868  55174     713  1789   20747   14869     906  71771\n10 Arkansas Health_…   23917  15685     146   494    4147    3161     161  15126\n# … with 198 more rows, 2 more variables: `Two or More Races` <dbl>,\n#   `Individuals With Disabilities (ESEA/IDEA)` <dbl>, and abbreviated variable\n#   names ¹​`Career Cluster`, ²​`Grand Total`,\n#   ³​`American Indian or Alaskan Native`, ⁴​`Black or African American`,\n#   ⁵​`Hispanic/Latino`, ⁶​`Native Hawaiian or Other Pacific Islander`\n\n\nThe tidy CTE data set hax dimensions of 208 x 12. There were 2 categorical columns (State, Career Cluster) & 10 numerical columns (The Grand Total of Students, Female Students, Students w/ Disabilities & the 7 columns for students counts of each of the 7 Race & Ethnicity Categories)\n\nThe 7 Race & Ethnicity Categories:\n\nAmerican Indian or Alaskan Native\nAsian\nBlack or African American\nHispanic/Latino\nNative Hawaiian or Other Pacific Islander\nWhite\nTwo or More Races\n\n\nLeaving racial data in un-pivoted format at this time allowed for the visualization of differences in CTE participation between states & CTE career clusters using US heat maps & facet_wrap.\n\n\nConverting Counts to Percentages\nTo appropriately compare diversity in CTE participation between states of different population sizes I:\n\nConverted counts of CTE students of each identity to percentages using mutate() and division by the grand total number of all secondary CTE students in the state\nRemoved counts from data frame for easier work with percentages\n\n\n\nCode\n#Converting participation from counts to percentages\nCTE_Data_Percentage <- Tidy_CTE_DF %>%\n  mutate(Percent_Female = round((`Female`/`Grand Total`)*100)) %>%\n  mutate(Percent_Disabilities= round((`Individuals With Disabilities (ESEA/IDEA)`/`Grand Total`)*100)) %>%\n  mutate (Percent_AmInd_AL_Ntv= round((`American Indian or Alaskan Native`/`Grand Total`)*100)) %>%\n  mutate (Percent_Asian= round((`Asian`/`Grand Total`)*100)) %>%\n  mutate (Percent_Black= round((`Black or African American`/`Grand Total`)*100)) %>%\n  mutate (Percent_Hispanic= round((`Hispanic/Latino`/`Grand Total`)*100)) %>%\n  mutate (Percent_NtvHI_PacIsld= round((`Native Hawaiian or Other Pacific Islander`/`Grand Total`)*100)) %>%\n  mutate(Percent_White= round((`White`/`Grand Total`)*100)) %>%\n  mutate(Percent_TwoMoreRaces= round((`Two or More Races`/`Grand Total`)*100)) %>%\n  rename(\"Delete1\" = \"Female\") %>%\n  rename(\"Delete2\" = \"American Indian or Alaskan Native\") %>%\n  rename(\"Delete3\" = \"Asian\") %>%\n  rename(\"Delete4\" = \"Black or African American\") %>%\n  rename(\"Delete5\" = \"Hispanic/Latino\") %>%\n  rename(\"Delete6\" = \"Native Hawaiian or Other Pacific Islander\") %>%\n  rename(\"Delete7\" = \"White\") %>%\n  rename(\"Delete8\" = \"Two or More Races\") %>%\n  rename(\"Delete9\" = \"Individuals With Disabilities (ESEA/IDEA)\") %>%\n  select(!contains(\"Delete\"))\n\n#Remove Nebraska N/A STEM row before combining with census data\nCTE_Data_Percentage <- CTE_Data_Percentage %>%\n  drop_na()\n\n\n\n\nCalculating Relative Racial Diversity Using tidycensus & Join\nTo calculate relative racial diversity of students participating in CTE I:\n\nObtained an API Key from the US Census Bureau to access census data\nIdentified appropriate race & data census variables from the 2020 decennial census using the tidy_census() package and load_variable() function\nCreated a data frame with the desired race & ethnicity variables & inherit geometry for future mapping using the get_decennial() function\nPivoted census data wider to create columns for each racial group\nConverted census race data from counts to percentages and removed count columns\nLeft joined CTE percentage data to the racial census percentage data by State\nCalculated relative CTE racial diversity in a state using the formula Delta RaceX CTE = (% of High Students in State CTE who are of X Race) - (% of People of X Race in State Census)\n\n\n\nCode\n#Loading tidycensus & 2020 decennial variables\n\nlibrary(tidycensus)\noptions(tigris_use_cache = TRUE)\n\ncensus_api_key(\"da16c3123512601e6f0d959c73fab921103c382d\")\n\n  v1 <- load_variables(year = 2020, dataset = \"pl\", cache = TRUE)\n\n\n\n\nCode\n# creating 2020 decennial census race data frame with desired variables and geometry\ncensus_2020_race_df <- get_decennial(\n  geography = \"state\",\n  variables = c ( \"Total_Census\" = \"P1_001N\", \n  \"White_Census\" = \"P1_003N\",\n  \"Black_Census\" = \"P1_004N\",\n  \"AmInd_ALNatve_Census\" = \"P1_005N\",\n  \"Asian_Census\"  = \"P1_006N\",\n  \"NativeHI_Census\" = \"P1_007N\",\n  \"TwoMore_Census\" = \"P1_009N\",\n  \"Hispanic_Census\" = \"P2_002N\"),\n  year = 2020,\n  sumfile = \"pl\",\n  geometry = TRUE,\n  resolution = \"20m\") %>%\n  shift_geometry() \n\n# pivot wider by race variables & remove \"Other\" variable to mirror the tidy CTE data format\n Census_Race_Date_Wider <- pivot_wider(\n census_2020_race_df, names_from = \"variable\", values_from = \"value\") %>%\n  select(!contains(\"Other\"))\n\n#Convert census race data to percentages & remove counts \nCensus_Race_Data_Percent  <- Census_Race_Date_Wider %>%\n  mutate (Percent_Census_AmInd_AL_Ntv= round((`AmInd_ALNatve_Census`/`Total_Census`)*100)) %>%\n  mutate (Percent_Census_Asian= round((`Asian_Census`/`Total_Census`)*100)) %>%\n  mutate (Percent_Census_Black= round((`Black_Census`/`Total_Census`)*100)) %>%\n  mutate (Percent_Census_NtvHI_PacIsld= round((`NativeHI_Census`/`Total_Census`)*100)) %>%\n  mutate (Percent_Census_Hispanic= round((`Hispanic_Census`/`Total_Census`)*100)) %>%\n  mutate(Percent_Census_White= round((`White_Census`/`Total_Census`)*100)) %>%\n  mutate(Percent_Census_TwoMoreRaces= round((`TwoMore_Census`/`Total_Census`)*100)) %>%\n  rename(\"State\" = \"NAME\") %>%\n  select(!contains(\"AmInd_ALNatve\")) %>%\n  select(!contains(\"Asian_Census\")) %>%\n  select(!contains(\"Black_Census\")) %>%\n  select(!contains(\"Hispanic_Census\")) %>%\n  select(!contains(\"NativeHI_Census\")) %>%\n  select(!contains(\"White_Census\")) %>%\n  select(!contains(\"TwoMore_Census\")) \n\n\nAfter some experimentation I discovered that joining the CTE data to the census data maintained the inherent tidycensus() geometry needed for mapping for the joined data set. I therefore left joined the CTE_Data_Percentage data set to the Census_Race_Data_Percent using the categorical State variable as the key.\n\n\nCode\n#Left join of CTE_Data in Percents to Census_Data in Percents\nJoined_CTE_Data <- Census_Race_Data_Percent %>%\n  left_join (CTE_Data_Percentage, by = \"State\")\n\n\nCalculating Relative Racial Diversity\nTo compare relative racial & ethnic CTE diversity between states & CTE concentrations, I calculated how many percentage points more or less the state’s CTE program or concentration racial & ethnic diversity was compared to the state’s overall racial & ethnic diversity.\nFor each of the 7 racial & ethnic categories, a Delta_Percentage was calculated using the formula (% of High Students in State CTE who are of X Race) - ( % of People of X Race in State Census)\n\n\nCode\n#Calculating Relative Racial Diversity, or Delta % Racial Group in CTE\nDelta_CTE_Data <- Joined_CTE_Data %>%\n  mutate (DeltaPercentAmIndNtv = round(`Percent_AmInd_AL_Ntv` - `Percent_Census_AmInd_AL_Ntv`)) %>%\n  mutate (DeltaPercentAsian =  round (`Percent_Asian` - `Percent_Census_Asian`)) %>%\n  mutate (DeltaPercentBlack = round (`Percent_Black` - `Percent_Census_Black`)) %>%\n  mutate (DeltaPercentHispanic = round (`Percent_Hispanic` - `Percent_Census_Hispanic`)) %>%\n  mutate (DeltaPercentNtvHIPacIsld = round (`Percent_NtvHI_PacIsld` - `Percent_Census_NtvHI_PacIsld`)) %>%\n  mutate (DeltaPercentWhite = round (`Percent_White` - `Percent_Census_White`)) %>%\n  mutate (DeltaPercentTwoMoreRaces = round (`Percent_TwoMoreRaces` - `Percent_Census_TwoMoreRaces`)) %>%\n  select(!contains(\"_AmInd\")) %>%\n  select(!contains(\"_Asian\")) %>%\n  select(!contains(\"_Black\")) %>%\n  select(!contains(\"_Hispanic\")) %>%\n  select(!contains(\"_NtvHI\")) %>%\n  select(!contains(\"_White\")) %>%\n  select(!contains(\"_TwoMoreRaces\"))"
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#results-visualizations",
    "href": "posts/Emma_Narkewicz_Final Project.html#results-visualizations",
    "title": "Emma Narkewicz Final Project",
    "section": "Results & Visualizations",
    "text": "Results & Visualizations\n\nFemale Participation in CTE - RQ 1\nOverall, nearly 4 million female high school students participated in US CTE courses in the 2020-2021 school year. In the same year, nearly 65,000 female students were in Health Science courses, just under 33,000 female students were in IT courses, and just over 18,000 female students were in STEM courses at the secondary level.\n47% of all secondary students participating in CTE in 2020-2021 were female.\n\n\nCode\n#Generating a tibble with the sum of all females in CTE, Sum of all students in CTE, & the % tot remale in CTE\nTidy_CTE_DF %>%\ngroup_by(`Career Cluster`) %>%\nselect(`State`, `Career Cluster`, `Female`, `Grand Total`) %>%\nsummarise(`TotalCountFemaleCTE` = sum(`Female`, na.rm=TRUE),  `SumGrandTotalCTE` = sum(`Grand Total`, na.rm=TRUE), `PercentTotFemale` =  round((`TotalCountFemaleCTE`/`SumGrandTotalCTE`)*100))\n\n\n# A tibble: 4 × 4\n  `Career Cluster`   TotalCountFemaleCTE SumGrandTotalCTE PercentTotFemale\n  <chr>                            <dbl>            <dbl>            <dbl>\n1 Health_Science                  649673           910652               71\n2 IT                              329541           898942               37\n3 STEM                            182701           666222               27\n4 Total_CTE_Students             3909965          8268271               47\n\n\nFig. 1 below is a heat map below of the percentage of female high school student in CTE statewide for the 2020-2011 year. The majority of states had CTE programs with 45%-50% female students. Across all career clusters, the majority of states came close to gender parity in total CTE participation. A handful of Southern states & Puerto Rico exceed total 55% female participation in CTE. In contrast, several Northeastern states and Alaska had total female participation under 40% or in the low 40%.\n\n\nCode\n#Select only CTE total rows for statewide CTE clusters\nTotCTEPercent <- Delta_CTE_Data %>%\n  filter(str_detect(`Career Cluster`, \"Total_CTE_Students\"))\n\n#Graph the Total Female Participation in any CTE course in %\nggplot(data = TotCTEPercent, aes(fill = `Percent_Female`)) +  geom_sf() + scale_fill_distiller(palette = \"RdPu\", direction = 1) + labs(title = \"Figure 1: % of Female High School Students in CTE Statewide (2020-2021)\",\n       caption = \"Source: US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"% Female Students\") + \n  theme_void() + facet_wrap(vars(`Career Cluster`)) + theme(legend.position = \"bottom\")\n\n\n\n\n\nFive states had more than 50% female participation in CTE at the secondary level in 2020-2021:\n\n55% of Alabama’s high school CTE students were female, the highest % of any state\n54% of Puerto Rico’s high school CTE students were female, the second highest %\n51% of the District of Columbia, Louisiana, & Mississippi’s high school CTE students were female\n\n\n\nCode\n#slicing top 10 states for nationwide female CTE participation\nTotCTEPercent %>%\n  as.data.frame %>%\n  filter(str_detect(`Career Cluster`, \"Total_CTE_Students\")) %>%\n  arrange(desc(`Percent_Female`)) %>%\n  select(`State`, `Percent_Female`) %>%\n  slice(1:10) %>%\n  rename(\"% Female Students in CTE\" = \"Percent_Female\")\n\n\n                  State % Female Students in CTE\n1               Alabama                       55\n2           Puerto Rico                       54\n3  District of Columbia                       51\n4             Louisiana                       51\n5           Mississippi                       51\n6             Minnesota                       50\n7              Missouri                       49\n8                Kansas                       49\n9          Rhode Island                       49\n10              Florida                       49\n\n\nTwo states had less than 40% female participation in CTE at the secondary level in 2020-2021. There were 7 states that had female participation in CTE between 40%-43%.\n\n37% of Maine’s highs school CTE students were female, the lowest of any state\n39% of Vermont’s high school CTE students were female, the second lowest\n41% of Alaska’s high school CTE students were female\n42% of New York & Pennsylvania’s high school CTE students were female\n43% of Michigan, Wyoming, New Hampshire, & Connecticut’s high school CTE students were female\n\n\n\nCode\n#slicing top 10 states for nationwide female CTE participation\nDelta_CTE_Data %>% \n  as.data.frame %>%\n  filter(str_detect(`Career Cluster`, \"Total_CTE_Students\")) %>%\n  arrange(`Percent_Female`) %>%\n  select(`State`, `Percent_Female`) %>%\n  slice(1:10) %>%\n  rename(\"% Female Students in CTE\" = \"Percent_Female\")\n\n\n           State % Female Students in CTE\n1          Maine                       37\n2        Vermont                       39\n3         Alaska                       41\n4       New York                       42\n5   Pennsylvania                       42\n6       Michigan                       43\n7        Wyoming                       43\n8  New Hampshire                       43\n9    Connecticut                       43\n10      Illinois                       44\n\n\nFemale students nationally are over represented in Health Science CTE courses and under represented in STEM & IT CTE courses \n\n71% of total high school students nationwide enrolled in a Health Science CTE course were female in the 2020-2021 year\n37% of total high school students nationwide enrolled in an IT CTE course were female in the 2020-2021 year\n27% of total high school students nationwide enrolled in a STEM CTE course were female in the 2020-2021 year\n\nPotential Health Science careers can have a lower earning potential than STEM & IT careers. For example home health aids, medical office assistance, & nursing assistants are Health Science careers, while software developer, statistician, engineer, and nuclear technicians fall under IT & STEM. It is therefor troubling that far fewer female students were in IT or STEM courses.\nFig. 2 below is a heat map of CTE participation of female students by CTE career cluster for the 2020-2021 high school year using facet_wrap(). The over-representation of female students in Health Science courses is shown by the deep purple-pink fill representing 80% female participation. The Total CTE heat map in Fig. 2 is paler than the heat map in Fig. 1, while had a much maximum female participation scale of only 55%. By visual contrast, there are far fewer than 50% female participating in IT & STEM. The IT & STEM heat maps are much lighter than the Total CTE Students one, showing the under-representation of female high school students in these 2 career clusters. Female participation in STEM courses was particularly low, resulting in a very pale pink heat map. Note that Nebraska is white in this heat map as there was participation data at the STEM level in the state\n\n\nCode\n#Graph comparing female CTE participation between IT, Health Science, STEM, Overall\n\nggplot(data = Delta_CTE_Data, aes(fill = `Percent_Female`)) +  geom_sf() + scale_fill_distiller(palette = \"RdPu\", direction = 1) + labs(title = \"Figure 2: % Female High School Students in CTE by Career Type (2020-2021)\",\n       caption = \"Source: US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"% Female Students\") + \n  theme_void() + facet_wrap(vars(`Career Cluster`)) + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nStudents with Disabilities Participation in CTE - RQ 2\nOverall, 935,962 total high school students with disabilities (SWDs) participated in US CTE courses in the 2020-2021 school year. Looking at US high school students in 2020-2021, there were 56,304 SWDs in Health Science courses, 96,0027 SWDs in IT courses, and 54,811 SWDs in STEM courses.\n11% of all secondary students participating in CTE in 2020-2021 had disabilities.\nAccording to the National Center for Education Statistics (2022), ~15% of US public school students ages 3-21 qualified for special education under IDEA in the 2020-2021 school year. This is not a high-school specific number, but suggests the participation of SWDs in high school CTE courses was comparable to the percentage of SWDs in US public schools for the 2020-2021 year.\n\n\nCode\n#Sum/Percentage SWDs in CTE nationwide & each career cluster\nTidy_CTE_DF %>%\ngroup_by(`Career Cluster`) %>%\nselect(`State`, `Career Cluster`, `Individuals With Disabilities (ESEA/IDEA)`, `Grand Total`) %>%\nsummarise(`TotalCountSWDCTE` = sum(`Individuals With Disabilities (ESEA/IDEA)`, na.rm=TRUE),  `SumGrandTotalCTE` = sum(`Grand Total`, na.rm=TRUE), `PercentSWDCTE` =  round((`TotalCountSWDCTE`/`SumGrandTotalCTE`)*100)) %>%\n select(!contains(\"Grand\")) %>%\n  rename(\"National Count SWDs\" =  \"TotalCountSWDCTE\") %>%\n  rename(\"National % SWDs\" =  \"PercentSWDCTE\") \n\n\n# A tibble: 4 × 3\n  `Career Cluster`   `National Count SWDs` `National % SWDs`\n  <chr>                              <dbl>             <dbl>\n1 Health_Science                     56304                 6\n2 IT                                 96027                11\n3 STEM                               54811                 8\n4 Total_CTE_Students                935962                11\n\n\nFig. 3 below is a heat map of percentage of high school students with a disability in CTE statewide for the 2020-2011 year. The majority of states have CTE programs made up of ~10% students with disabilities (SWDs). No states appear to have extremely low participation of students with disabilities. Several Northeastern states appear to have CTE programs made up of between 20-35% SWDs.\n\n\nCode\n#Graph  Total Student with Disabilities Participation in CTE \nggplot(data = TotCTEPercent, aes(fill = `Percent_Disabilities`)) +  geom_sf() + scale_fill_distiller(palette = \"Blues\", direction = 1) + labs(title = \"Figure 3: % High School Students w Disabilities in CTE Statewide (2020-2021)\",\n       caption = \"Source: US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"% Students w/ Disabilities\") + \n  theme_void() + facet_wrap(vars(`Career Cluster`)) + theme(legend.position = \"bottom\")\n\n\n\n\n\nSeven states had greater than 15% nationwide participation of students with disabilities in CTE at the secondary level in 2020-2021:\n\n36% of Vermont’s high school CTE students had disabilities, the highest % of any state\n31% of Pennsylvania’s high school CTE students had disabilities, the second highest % of any state\n24% of Maine’s high school CTE students had disabilities\n23% of New York’s high school CTE students had disabilities\n19% of Massachusetts’ high school CTE students had disabilities\n18% of Puerto Rico’s high school CTE students had disabilities\n16% of Oregon’s high school CTE students had disabilities\n\nIt is important to research further if the states where students with disabilities appear to be over-represented in CTE courses at the high school level if are being tracked into low-wage CTE career clusters.\n\n\nCode\n#slicing top 10 states for nationwide SWD in CTE participation\nTotCTEPercent %>%\n  as.data.frame %>%\n  filter(str_detect(`Career Cluster`, \"Total_CTE_Students\")) %>%\n  arrange(desc(`Percent_Disabilities`)) %>%\n  select(`State`, `Percent_Disabilities`) %>%\n  slice(1:10) %>%\n  rename(\"% SWD in CTE\" =  \"Percent_Disabilities\")\n\n\n           State % SWD in CTE\n1        Vermont           36\n2   Pennsylvania           31\n3          Maine           24\n4       New York           23\n5  Massachusetts           19\n6    Puerto Rico           18\n7         Oregon           16\n8       Delaware           15\n9     New Mexico           15\n10   Connecticut           15\n\n\nFour states had less that 9% national participation of students with disabilities in CTE at the secondary level in 2020-2021:\n\n5% of Montana’s high school CTE students had disabilities, the lowest % of any state\n7% of Alabama’s high school CTE students had disabilities\n8% of Nevada & Hawaii’s high school CTE students had disabilities\n\n\n\nCode\n#slicing top 10 states for nationwide SwD CTE participation\nDelta_CTE_Data %>% \n  as.data.frame %>%\n  filter(str_detect(`Career Cluster`, \"Total_CTE_Students\")) %>%\n  arrange(`Percent_Disabilities`) %>%\n  select(`State`, `Percent_Disabilities`) %>%\n  slice(1:10) %>%\n  rename(\"% SWD in CTE\" =  \"Percent_Disabilities\")\n\n\n            State % SWD in CTE\n1         Montana            5\n2         Alabama            7\n3          Nevada            8\n4          Hawaii            8\n5       Louisiana            9\n6        Michigan            9\n7         Arizona            9\n8     Mississippi            9\n9        Oklahoma            9\n10 South Carolina            9\n\n\nStudents with disabilities were under-represented in Health Sciences & STEM CTE courses nationally in 2020-2021\n\n6% of total high school students nationwide enrolled in a Health Science CTE course had a disability in the 2020-2021 year\n11% of total high school students nationwide enrolled in an IT CTE course had a disability in the 2020-2021 year\n8% of total high school students nationwide enrolled in a STEM CTE course were female in the 2020-2021 year\n\nThese results suggest SWDs may have been tracked out of Health Science & STEM CTE courses, but were not tracked out of IT CTE courses. This may be due to IT career pathways not requiring as much interpersonal skills as the Health Science career field.\nFig. 4 below is a heat map of CTE participation of SWDs by career cluster for the 2020-2021 high school year, using facet_wrap(). The national participation of high school SWDs in any CTE course closely resembles the participation of high school SWDs in IT courses for the 2020-2021 year. Both the Total CTE Students and IT facets of the heat map show a higher percentage of SWDs in Northeastern states.\nThe STEM heat map is slightly lighter than the Total CTE or IT heat maps in Fig. 4, suggesting a slight under-representation of SWDs. in STEM course, with the exception of in Puerto Rico. Note that Nebraska is white in this heat map as there was participation data at the STEM level in the state  The Health Science heat map is the lightest of all career clusters in Fig. 4, corresponding to the lowest percentage of SWDs in any career cluster. The difference in participation of students with disabilities between CTE clusters in Fig. 4 is much less dramatic than the difference in participation of female student in CTE clusters in Fig. 2.\n\n\nCode\n#Graph comparing SWD CTE participation between IT, Health Science, STEM, Overall\nggplot(data = Delta_CTE_Data, aes(fill = `Percent_Disabilities`)) +  geom_sf() + scale_fill_distiller(palette = \"Blues\", direction = 1) + labs(title = \"Figure 4: % High School Students w Disabilities in CTE by Career Type (2020-2021)\",\n       caption = \"Source: US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"% Students w/ Disability\") + \n  theme_void() + facet_wrap(vars(`Career Cluster`)) + theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nRacial & Ethnic Diversity in CTE Participation - RQ 3\nFrom highest to lowest, the following counts of students in racial & ethnic groups participated in secondary level CTE in the 2020-2021 year:\n\n3,857,162 White students\n2,325,769 Hispanic or Latino students\n1,304,866 Black students students\n377,667 Asian CTE students\n288,754 students who identify as being Two or More Races\n85,760 American Indian or Alaskan Native students\n28,274 Native Hawaiian or Other Pacific Islander students\n\nNote: CTE students can fall in more than 1 racial & ethnic category, meaning racial & ethnic percentages should not add up to 100. For example, a student could identify as both White and Hispanic \n\n\nCode\n#Sum of nationwide count CTE in each of 7 race & ethnicity categories\nSumCTE_Race <- Tidy_CTE_DF %>%\ngroup_by(`Career Cluster`) %>%\nselect(`State`, `Career Cluster`, `Grand Total`, `American Indian or Alaskan Native`, `Asian`, `Black or African American`, `Hispanic/Latino`, `Native Hawaiian or Other Pacific Islander`, `White`, `Two or More Races`) %>%\n  summarise(`SumGrndTotCTE` = sum(`Grand Total`, na.rm = TRUE),\n            `SumIndg_CTE` = sum(`American Indian or Alaskan Native`, na.rm=TRUE), \n            `SumAsn_CTE` = sum(`Asian`, na.rm=TRUE), \n            `SumBlk_CTE` = sum(`Black or African American`, na.rm=TRUE),\n            `SumHspnc_CTE` = sum(`Hispanic/Latino`, na.rm=TRUE), \n            `SumPacIsld_CTE` = sum(`Native Hawaiian or Other Pacific Islander`, na.rm=TRUE), \n            `SumWht_CTE` = sum(`White`, na.rm=TRUE),\n            `SumTwoMoreRaces_CTE` = sum(`Two or More Races`, na.rm=TRUE))\n\n\nCompared to 2020 Decennial Census, US high school CTE programs had a higher percentage of non-white students in 2020-2021 than the nation as a whole.\n\n61% of 2020 US Census participants were White, compared to only 46% of high school students in CTE\n20% of US Census participants were Hispanic, compared to 28% of high school students in CTE\n12% of US Census participants were Black, compared to 15% of high school students in CTE\n6% of US Census participants were Asian compared to 4.5% of high school students in CTE\n11% of US Census participants were Two or More Races, compared to 3.5% of high school students in CTE\n1% of US Census participants & high school students in CTE were American Indian or Alaskan Native\n<1% of US Census participants & high school students in CTE were Pacific Islander or Hawaiian Native\n\nBlack & Hispanic high students were relatively over-represented in CTE compared to census data, while White, Asian, & Two or More Races high school students were relatively under represented in CTE for the 2020-2021 year.\n\n\nCode\n# Percent Overall Race & Ethnicity in CTE\nPercent_TotRaceCTE <- SumCTE_Race %>%\n            mutate(`PercentIndgCTE` = round((`SumIndg_CTE`/`SumGrndTotCTE`)*100)) %>%\n            mutate(`PercentAsnCTE` = round((`SumAsn_CTE`/`SumGrndTotCTE`)*100)) %>%\n            mutate(`PercentBlkCTE` = round((`SumBlk_CTE`/`SumGrndTotCTE`)*100)) %>%\n            mutate(`PercentHspncCTE` = round((`SumHspnc_CTE`/`SumGrndTotCTE`)*100)) %>%\n            mutate(`PercentPacIsldCTE` = round((`SumPacIsld_CTE`/`SumGrndTotCTE`)*100)) %>%\n            mutate(`PercentWhtCTE` = round((`SumWht_CTE`/`SumGrndTotCTE`)*100)) %>%\n            mutate(`PercentTwoMoreRacesCTE` = round((`SumTwoMoreRaces_CTE`/`SumGrndTotCTE`)*100)) %>%\n            select(!contains(\"Sum\"))\n#Pivoted Percent Census Data\nCensus_Tot_Race_Percent <- Census_Race_Date_Wider %>%\n  as.data.frame %>%\nselect(`NAME`, `Total_Census`, `White_Census`, `Black_Census`, `AmInd_ALNatve_Census`, `Asian_Census`, `NativeHI_Census`, `TwoMore_Census`, `Hispanic_Census`) %>%\n#Sum Census Data by Race & Ethnicity\nsummarise(`Sum_Total_Census` = sum(`Total_Census`, na.rm=TRUE), \n            `Sum_Wht_Census` = sum(`White_Census`, na.rm=TRUE), \n            `Sum_Blk_Census` = sum(`Black_Census`, na.rm=TRUE),\n            `Sum_Indg_Census`= sum(`AmInd_ALNatve_Census`, na.rm=TRUE),\n            `Sum_Asn_Census` = sum(`Asian_Census`, na.rm=TRUE),\n            `Sum_PacIsld_Census` = sum(`NativeHI_Census`, na.rm=TRUE),\n            `Sum_TwoMoreRace_Census` = sum(`TwoMore_Census`, na.rm=TRUE),\n            `Sum_Hsp_Census` = sum(`Hispanic_Census`, na.rm=TRUE)) %>%\n  #Percent Census Data by Race & Ethnicity\n    mutate (Wht_ = round((`Sum_Wht_Census`/`Sum_Total_Census`)*100)) %>%\n    mutate (Blk_ = round((`Sum_Blk_Census`/`Sum_Total_Census`)*100)) %>%\n    mutate (Indg_ = round((`Sum_Indg_Census`/`Sum_Total_Census`)*100)) %>%\n    mutate (Asn_ = round((`Sum_Asn_Census`/`Sum_Total_Census`)*100)) %>%\n    mutate (PacIsld_= round(`Sum_PacIsld_Census`/`Sum_Total_Census`)*100) %>%\n    mutate (TwoMoreRaces_ = round((`Sum_TwoMoreRace_Census`/`Sum_Total_Census`)*100)) %>%\n    mutate(Hspnc_ = round((`Sum_Hsp_Census`/`Sum_Total_Census`)*100)) %>%\n    select(!contains(\"Sum\")) %>%\n  #pivot longer\n    pivot_longer(cols = contains(\"_\"), \n              names_to = \"Race or Ethnicity\",\n              values_to = \"Percent Census(%)\") \n\n\nI pivoted race & ethnicity data in percent for longer into a categorical Race or Ethnicity column & a numerical Percent Students in CTE column to examine national CTE participation by race & ethnicity prior to calculating relative participation based on the US census data. By using a stacked bar chart with percentages I could graph differences between CTE clusters and racial & ethnic groups at once, which I could not on the US heat map.\n\n\nCode\n#Pivot longer race & ethnicity categorical variables\nPivoted_Perc_CTE_Race  <- Percent_TotRaceCTE %>%\n  rename( `Indg_` = `PercentIndgCTE`) %>%\n  rename(`Asn_` = `PercentAsnCTE`) %>%\n  rename(`Blk_` = `PercentBlkCTE`) %>%\n  rename(`Hspnc_`= `PercentHspncCTE`) %>%\n  rename(`PacIsld_`= `PercentPacIsldCTE`) %>%\n  rename(`Wht_` = `PercentWhtCTE`) %>%\n  rename(`TwoMoreRaces_` = `PercentTwoMoreRacesCTE`) %>%\n  pivot_longer(cols = contains(\"_\"), \n              names_to = \"Race or Ethnicity\",\n              values_to = \"Percent Students in CTE (%)\")\n\n\nFig 5 is a stacked bar chart of the national percentage of high students by race & ethnicity across CTE clusters for the 2020-2021 school year. It enables a side by side comparison of the percentage of CTE students of each Race & Ethnicity between CTE career clusters.\n\nWhite studentswee the most common racial/ethnic group in CTE, making up just under 50% of all CTE participants nationwide.\nThere is a slightly larger percentage of White students in IT & STEM than in Health Science & Total CTE. There is a smaller percentage of students of Two or More Races in total CTE than in any of the STEM 3 concentrations.\nPacific Islander/Hawaiian Native & American Indian/Alaskan Native who each made up less that 1% of all CTE students & had negligible bar segments in Fig. 5\nHispanic students were the second most common racial/ethnic group in CTE, but there is a lower percentage of Hispanic students in IT & STEM than in Health Science & Total CTE courses.\nBlack students were the third most common racial/ethnic group in CTE at the the national level, but a lower percentage of Black students participated in STEM compared to the other 2 career clusters & Total high school CTE.\nLastly, there was a lower percentage of Asian students in Health Science & Total CTE than in STEM & IT, the opposite of what was observed for most racial/ethnic groups\n\n\n\nCode\n#Figure 5, CTE Race Bar Chart by %\nggplot(Pivoted_Perc_CTE_Race, aes(fill= `Race or Ethnicity`, y=`Percent Students in CTE (%)`, x = `Career Cluster`)) + geom_bar(position=\"fill\", stat=\"identity\") + ggtitle(\"Figure 5: % National CTE HS Participation by Race/Ethnicity & Cluster (2020-2021)\") + labs(x= \"Career Cluster\", y=\"Percentage Students in CTE (%)\") \n\n\n\n\n\nTo compare the relative CTE participation by race & ethnicity between states & career clusters, I created a heat map with facet_wrap() by CTE Cluster for each race & ethnicity category. As the calculated delta CTE participation by race displays if the state’s CTE cluster is more or less diverse than the state as whole, the divergent Spectral color palette was utilized for the heat map. Each racial & ethnic group was mapped, going from the group with the highest percentage of total CTE participation to the group with the lowest percentage of total CTE participation.\nNote Nebraska is white in all heat maps for the STEM CTE Career Cluster, as there was no data for this state & career cluster in the 2020-2021 year\nRelative White CTE Participation Heat Map\nThe relative percentage of White secondary students in CTE shown in Fig.6 ranged from -30% - 0%.This figure’s scale highlights how relatively under represented White students are in CTE compared to Census diversity. Across all CTE courses & clusters there appears to be more White students in Northern & Northeast states & Alaska, as indicated by orange-red fill. In contrast, White students are consistently under represented in Arizona across all career clusters, especially in Health Science.\nThere appears to be slightly lower percentage of White students in Health Science as opposed to IT & STEM, as indicated by deeper blue & green fill.\n\n\nCode\n ##Plot Relative White\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentWhite`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\", \n                       direction = -1) + \n  labs(title = \"Figure 6: Relative % of White High School Students in CTE (2020-2021)\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % White Students\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\")  \n\n\n\n\n\nRelative Hispanic CTE Participation Heat Map\nThe relative over-representation of Hispanic high school students in CTE in the 2020-2021 school year compared to Census diversity is apparent in Fig. 7. This is both because of the heat map scale ranging from -5% to +15%, & the orange-red South West portion of the Total CTE map, indicating 15% more Hispanic participation in CTE than % of Hispanic individuals in the state. Hispanic students are under-represented in IT courses in Utah & in STEM courses in New York, shown with the blue fill in those states. Hispanic high school students have higher relative participation in Health Science career courses than in IT or STEM courses.\n\n\nCode\n ##Plot Relative Hispanic\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentHispanic`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\", \n                       direction = -1) + \n  labs(title = \"Figure 7: Relative % of Hispanic High School Students in CTE (2020-2021)\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % Hispanic Students\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\")  \n\n\n\n\n\nRelative Black CTE Partipation Heat Map\nFig. 8 below shows that while Black students were relatively over represented in CTE when to census data, it was only slightly. This is indicated by Fig. 8’s fully positive scale of Delta % Black students in CTE in combination with all 4 facet_wrapped heat maps filled with green-blue indicating a Delta % Blacks students close to zero. Lastly, Black high school students participated less in STEM than in IT or Health Science CTE courses in 2020-2021.\n\n\nCode\n#Plot Relative Black\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentBlack`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\", \n                       direction = -1) + \n  labs(title = \"Figure 8: Relative % of Black High School Students in CTE (2020-2021)\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % Black Students\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\") \n\n\n\n\n\nRelative Two or More Races CTE Participation Heat Map\nFig. 9 is strikingly red & orange, but based on the legend scale of -50% to 0% this does not indicate high relative TE participation of students of Two or More Race. After closer examination, it appears that the heat map & scale is skewed by the dark blue outlier of Puerto Rico, which has a Delta % Students of Two or More Races as -50%. This suggests that there are 50% more students of Two or More Races in Puerto Rico Census data in high school CTE courses in 2020-2021.\n\n\nCode\n#Plot Delta Two More Races\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentTwoMoreRaces`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\",  direction = -1) + \n  labs(title = \"Figure 9: Relative % of High School Students Two More Races in CTE, 2020-2021\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % Students Two More Races\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\") \n\n\n\n\n\nI confirmed from the original Census & Perkins V the percentage of Puerto Rican’s of Two or More Races in the 2020-2021 was high, at 50%. In contrast, 0% of high school CTE students in Puerto Rico in 2020-2021 were of Two or More Races. This could be due to either disparities in participation or in how two or more races are measured in the Census vs. Perkins V.\n\n\nCode\n# % Puerto Rico Two More Races Census \nPR_Race_Percent <- Census_Race_Data_Percent %>%\n  as.data.frame %>%\n  filter(str_detect(`State`, \"Puerto\")) %>%\n  select(`State`, `Percent_Census_TwoMoreRaces`)\n\nPR_Race_Percent\n\n\n        State Percent_Census_TwoMoreRaces\n1 Puerto Rico                          50\n\n\n\n\nCode\n#% Puerto Rico Two More Races CTE Data\nPR_CTE_Percent <- CTE_Data_Percentage %>%\n  filter(str_detect(`State`, \"Puerto\")) %>%\n  select(`State`, `Percent_TwoMoreRaces`)\n\nPR_CTE_Percent\n\n\n# A tibble: 4 × 2\n  State       Percent_TwoMoreRaces\n  <chr>                      <dbl>\n1 Puerto Rico                    0\n2 Puerto Rico                    0\n3 Puerto Rico                    0\n4 Puerto Rico                    0\n\n\nRelative Asian CTE Participation Heat Map \nAs shown in Fig. 10 below, Asian Students were one of the racial & ethnic groups under represented in high school CTE courses in 2020-2021 when compared to Census diversity. Asian students were relatively under represented in Health Science in New York (Delta% = -5%), & over represented in Health Science in Hawaii (Delta% = +15%). Asian students were also relatively over represented in IT in the state of California (Delta% = +10%).\n\n\nCode\n#Plot Delta Asian\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentAsian`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\",  direction = -1) + \n  labs(title = \"Figure 10: Relative % of Asian High School Students in CTE, 2020-2021\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % Asian Students\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\") \n\n\n\n\n\nRelative Indigenous CTE Participication Heat Map \nFig. 11 shows the relative percentage of American Indians Native Alaskans in CTE courses on a heat map, facet_wrapped() by career clusters. I have relabeled this racial group to reflect the more commonly use term “Indigenous”.\n\nIndigenous students appeared relatively over represented in Oklahoma (Delta% = +3% to +5%) at the Total CTE Level & each of the 3 career clusters\nIndigenous students were relatively over represented in IT courses in Montana (Delta% = +5%) & relatively under-represented in Alaska (Delta% = -5%)\nIndigenous students were relatively under represented in Arizona & South Dakota STEM courses\n\n\n\nCode\n#Plot DeltaPercentAmIndNtv\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentAmIndNtv`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\",  direction = -1) + \n  labs(title = \"Figure 11: Relative % of Indigenous High School Students in CTE, 2020-2021\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % Indigenous Students\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\") \n\n\n\n\n\nRelative Pacific Islander CTE Participation Heat Map\nWhile nearly all states in Fig. 12 has a relative percentage of Pacific Islanders & Native Hawaiians of Delta% = 0% in CTE courses, in Hawaii, Pacific Islanders & Native Hawaiians were under represented in STEM courses, as indicated by the deep blue state in the STEM heat map facet (Delta% = -5%).\n\n\nCode\n#Plot DeltaPercentNtvHIPacIsld\n\nggplot(data = Delta_CTE_Data, aes(fill = `DeltaPercentNtvHIPacIsld`)) +  geom_sf() + scale_fill_distiller(palette = \"Spectral\", direction = -1) + \n  labs(title = \"Figure 12: Relative % of Pacific Islander High School Students in CTE, 2020-2021\",\n       caption = \"Sources:US Census Bureau 2020 Decennial Census; US Department of Education 2020-2021 Perkins V Enrollment Data\",\n       fill = \"Delta % Pacific Islanders Students\") + facet_wrap(vars(`Career Cluster`)) + theme_void()  + theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#conclusions-limitations-reflection",
    "href": "posts/Emma_Narkewicz_Final Project.html#conclusions-limitations-reflection",
    "title": "Emma Narkewicz Final Project",
    "section": "Conclusions, Limitations, & Reflection",
    "text": "Conclusions, Limitations, & Reflection\n\nConclusions\nWhile females made up 47% of all high school CTE students nationwide in 2020-2021, more needs to be done to increase gender parity in participation in the Health Science, IT, & STEM career clusters. Targeted recruitment efforts should be made to increase male participation in Health Science (71% female), & female participation in STEM (27% female) & IT courses (37% female)\nStudents with Disabilities represented 11% of all high school students in CTE nationwide in 2020-2021, roughly proportional to the estimated ~ 15% of students ages 3-21 with disabilities (NCES, 2020). Targeted recruitment & accommodations should be made to increase the participation of students with disabilities in Health Science (6% SwD) & STEM courses (8% SwD). Students with disabilities were over represented in some Northeast states, which bears further research on if SWDs are being tracked into certain CTE career clusters in those states.\nStudents participating in high school CTE courses in 2020-2021 were more racially & ethnically diverse (non-White) than Census data. Only 46% of high school students in CTE were White in the 2020-2021 year,compared to 61% of 2020 US Census participants. Program wide, Black & Hispanic high students were relatively over-represented in CTE courses compared to census data, while White, Asian & high students of Two or More Races were relatively under represented in CTE.\nHowever, by career cluster, a lower percentage of Hispanic students participated in IT & STEM courses than in the Health Sciences or any CTE course. In contrast, a higher percentage of White & Asian students participated in STEM & IT courses than any CTE course in the 2020-2021 year. A higher percentage of Black students participated in IT than in STEM career clusters. This highlights how high relative participation of racially & ethnically diverse students in CTE does not always equal consistent high participation of these racial & ethnic diverse student in high earning potential STEM career clusters.\n\n\nLimitation & Areas for Future Work\nA key limitation of this project was the limited scope of the Perkins V data read in & analyzed.CTE data only was for 2020-2021 program year & only participation data was read in. Future analysis should compare changes in CTE participation over time by reading in Perkins V data from year prior to 2020-2021 school years. Perkins V performance data can be analyzed in addition to Perkins V participation data to examine equity in both access and outcomes in CTE. Additionally, there are many other CTE special populations & career clusters that should also be analyzed for equitable access (see Appendix A).\nFor states where a student group was relatively “over” or “under” represented, further research should be conducted what barriers contribute to inequity in participation & how participation in CTE breaks down by career cluster. Having a third of students with disabilities participating in CTE course could mean an innovative program accommodating SWDs in in-demand career pathways OR that SWDs are being tracked into low-earning career pathways with little autonomy. More information is needed to assess what is actually occurring, & if it is good or bad for students.\nLastly, on a technical note, an area for future growth would be to write & replace areas of my code with more efficient and widely applicable functions. This would be particularly useful for future analysis reading in multiple Census & Perkins V data sets of the same structure but for different years.\n\n\nReflection\nThis DACSS 601 course was my first time even coding ever and learning R, & while my coding skills are still rudimentary, I am very proud of what I was able to accomplish in this course & final project. During Challenge 5, I struggled for days to create a single US Map visual. I became very comfortable working with map data on this final project & also I also joined two data sets & learned to use the tidycensus() package for this project, both new skills that would have terrified me a few weeks ago.\nI enjoyed seeing how I can use R to clean, analyze, and present data sets, including the Perkins V data I use in some work projects. Coding as a beginner has been an emotional roller coaster, but I am hopeful to apply this new found DACSS 601 learning to cleaning data creating research & policy briefs in R."
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#references",
    "href": "posts/Emma_Narkewicz_Final Project.html#references",
    "title": "Emma Narkewicz Final Project",
    "section": "References",
    "text": "References\nAdvance CTE. (n.d.) Career Clusters. Retrieved December 17, 2022, from https://careertech.org/career-clusters\nCALDER Center. (2017). Career and Technical Education, Inclusion, and Postsecondary Outcomes for Students with Disabilities. https://ccrscenter.org/sites/default/files/CTE_SWD_Infographic.pdf\nGrolemund, Garrett, & Wickham. (2017) R for Data Science. O’Reilly Media.\nHarvey, M.W. (2002). Comparison of Postsecondary Transitional Outcomes Between Students With and Without Disabilities by Secondary Vocational Education Participation: Findings from the National Education Longitudinal Study. Career Development for Exceptional Individuals, 25(2): p. 99-122.\nHehir, T., Dougherty, S.M. & Grindal, T. (2013). Students with Disabilities in Massachusetts Career and Technical Education Programs. Commonwealth of Massachusetts, Department of Elementary and Secondary Education. p. 16.\nLee, H., Rojewski, J.W. & Gregg, N.(2016) Causal Effects of Career-Technical Education on Postsecondary Work Outcomes of Individuals with High-Incidence Disabilities. Exceptionality, . 24(2): p. 79-92.\nMazzotti, V.L., et al., (2021). Secondary Transition Predictors of Postschool Success: An Update to the Research Base. Career Development and Transition. Career Development and Transition for Exceptional Individuals, 44(1): p. 47-64.\nMcKay, C.E,; Ellison, Langer, M.L., Narkewicz, E. L. (May 2022). Advancing Employment for Secondary Learners with Disabilities through CTE Policy and Practice. Transitions to Adulthood Center for Research, Department of Psychiatry, UMass Chan Medical School. p. 1-2. http://dx.doi.org/10.7191/pib.1182\nNational Alliance for Partnerships in Equity (NAPE). (2018). Special Populations in Perkins V. https://www.napequity.org/nape-content/uploads/NAPE-Perkins-V-Special-Populations-At-A-Glance_v3_10-15-18_ml.pdf\nNational Center for Education Statistics (NCES). (2022). Students with Disabilities Under IDEA Ages 3-21 (2020 -2021). https://nces.ed.gov/programs/coe/indicator/cgg/students-with-disabilities\nSmith, R. (2019, August 28). Advancing Racial Equity in Career and Technical Education Enrollment. Center for American Progress. Retrieved December 17, 2022, from https://www.americanprogress.org/article/advancing-racial-equity-career-technical-education-enrollment/\nTheobald, R. (2018) Career and Technical Education for Students with Disabilities. National Center for Analysis of Longitudinal Data in Education Research: Washington, DC. p. 7.\nTheobald, R. (2019), Career and Technical Education, Inclusion, and Postsecondary Outcomes for Students With Learning Disabilities. Journal of Learning Disabilities, 52(2): p. 109-119.\nU.S. Census Bureau. (2021). 2020 Census State Redistricting Data (Public Law 94-171). Summary File. https://www.census.gov/programs-surveys/decennial-census/about/rdo/summary-files.html#P1\nU.S. Department of Education, Office of Career, Technical, and Adult Education (OCTAE). (n.d.). Legislation and Regulations - Perkins V. PCRN: Perkins V. Retrieved December 17, 2022, from https://cte.ed.gov/legislation/perkins-v\nU.S. Department of Education, Office of Career, Technical, and Adult Education (OCTAE). (2021). Perkins V Enrollment Data. Perkins State Plans and Data Explorer. https://cte.ed.gov/dataexplorer/build_enrollment\nWagner, M. (1991) The Benefits of Secondary Vocational Education for Young People with Disabilities. Findings from the National Longitudinal Transition Study of Special Education Students. SRI International: Menlo Park, CA.\nWalker K, Herman M (2022). tidycensus: Load US Census Boundary and Attribute Data as ‘tidyverse’ and ‘sf’-Ready Data Frames. R package version 1.3, https://walker-data.com/tidycensus/."
  },
  {
    "objectID": "posts/Emma_Narkewicz_Final Project.html#appendix-a---full-perkin-v-participation-variables",
    "href": "posts/Emma_Narkewicz_Final Project.html#appendix-a---full-perkin-v-participation-variables",
    "title": "Emma Narkewicz Final Project",
    "section": "Appendix A - Full Perkin V Participation Variables",
    "text": "Appendix A - Full Perkin V Participation Variables\nThe original Perkins V Participation 2020-2021 data set contained data on 11,748,265 CTE students enrolled at the secondary and post-secondary level in the 50 states Puerto Rico, Washington DC, & Palau. The U.S. Department of Education requires states to annually report the gender, ethnicity, and disadvantaged identities of students participating in CTE classes. Enrollment data is available at the state-wide CTE level and broken down further into participation in 17 unique CTE career clusters.\n17 career clusters:\n\nAgri. Food & Nat. Res.\nArch. & Const.\nArts, AV, Tech & Comm.\nBusiness Mgmt. & Admin.\nEducation & Training\nFinance\nGovt. & Public Admin.\nHealth Science\nHospitality & Tourism\nHuman Services\nInformation Technology\nLaw,. Public Safety, Cor. & Sec.\nManufacturing\nMarketing\nSTEM\nTransp. Distr. & Logis.\nOther\n\n11 Disadvantaged Identities:\n\nIndividuals With Disabilities (ESEA/IDEA)\nIndividuals With Disabilities (ADA)\nIndividuals from Economically Disadvantaged Families\nIndividuals Preparing for Non-traditional Fields\nSingle Parents\nOut of Workforce Individuals\nEnglish Learners\nHomeless Individuals\nYouth In Foster Care\nYouth with Parent in Active Military\nMigrant Students\n\n8 Racial & Ethnic Indentities:\n\nAmerican Indian or Alaskan Native\n\nAsian\n\nBlack or African American\n\nHispanic/Latino\n\nNative Hawaiian or Other Pacific Islander\n\nWhite\n\nTwo or More Races\n\nUnknown"
  },
  {
    "objectID": "posts/Final Project - Kristin Abijaoude.html",
    "href": "posts/Final Project - Kristin Abijaoude.html",
    "title": "Citizenship Laws by Country",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   0.3.5\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ tidyr   1.2.1     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggplot2)\nlibrary(ggraph)\n\nError in library(ggraph): there is no package called 'ggraph'\n\nlibrary(igraph)\n\n\nAttaching package: 'igraph'\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\nThe following object is masked from 'package:base':\n\n    union\n\nlibrary(collapsibleTree)\n\nError in library(collapsibleTree): there is no package called 'collapsibleTree'\n\nlibrary(treemap)\n\nWarning: package 'treemap' was built under R version 4.2.2\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final Project - Kristin Abijaoude.html#asking-the-research-question",
    "href": "posts/Final Project - Kristin Abijaoude.html#asking-the-research-question",
    "title": "Citizenship Laws by Country",
    "section": "Asking the Research Question",
    "text": "Asking the Research Question\nEight billion humans. About 200 countries. Many laws with a million asterisks next to them.\nEach country handles citizenship and immigration differently. Some countries permit dual citizenship, while others restrict how many nationalities their citizens can have. Some countries give unconditional citizenship to any child born on their soil, while others do if their parents are citizens. Finally, there are countries that only restrict the path to citizenship to certain races, ethnic, or religious groups.\nA citizenship is a contract between the citizen and the country. The citizen pledges allegiance to the country (or countries); and in turn, the country (or countries) provide rights and protections for the citizen. For example, I am a dual American-Lebanese citizen; I was born in America, which provides unconditional birthright citizenship, and my parents passed the Lebanese nationality down to my sisters and me. The allegiance between the citizen and the country can break, just like a contract. In several cases, the citizenship can be revoked, from voluntary renunciation of a citizenship or involuntary renunciation due to disloyalty, criminal offenses, or even having dual citizenship. Note the word “several” since the situation like this is on a case-by-case analysis. Out of curiosity and interest, the purpose of this report is to analyze the bureaucratic nature of citizenship and immigration.\nQuestion: Which countries have more lax citizenship laws, and which countries have more restrictive citizenship laws?"
  },
  {
    "objectID": "posts/Final Project - Kristin Abijaoude.html#open-dataset",
    "href": "posts/Final Project - Kristin Abijaoude.html#open-dataset",
    "title": "Citizenship Laws by Country",
    "section": "Open Dataset",
    "text": "Open Dataset\nThe dataset, which is called GLOBALCIT Citizenship Law Dataset, originates from Italy-based European University Institute, specifically at the Cadmus EUI Research Repository. Maarten Peter Link et. al., from the Robert Schuman Centre for Advanced Studies collected data on different ways citizenship can be acquired or loss in each country as of 2020.\n\n# citizenship laws by country\ncitizenship <- read.csv(\"_data/GLOBALCIT Citizenship Law v1 EUI ResData/Data/data_v1.0_country-year.csv\")\n\nWarning in file(file, \"rt\"): cannot open file '_data/GLOBALCIT Citizenship Law\nv1 EUI ResData/Data/data_v1.0_country-year.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\ncitizenship\n\nError in eval(expr, envir, enclos): object 'citizenship' not found\n\n\n\ndim(citizenship)\n\nError in eval(expr, envir, enclos): object 'citizenship' not found\n\n\nThere are three datasets; one dataset about citizenship acquisition, another dataset about loss of citizenship; and a mega dataset containing both laws. Trying to merge the two former datasets would be like reinventing the wheel, so I opted to analyze the mega dataset, which consisted of 104 columns and 190 rows. Each row lists the country in the order of their country code in the Italian language.\n\ncolnames(citizenship)\n\nError in is.data.frame(x): object 'citizenship' not found\n\n\nTo make this report easier for the reader, as well as myself, I will be broadly focusing on whether the citizenship laws are enforced, regardless of the exceptions in place. For example, if a country allows parents to pass down citizenship to their children, I will classify that as “yes”, even if there’s an age limit to receiving citizenship, or if a country allows dual citizenship with certain countries, I will classify that as “Yes” as in they allow dual citizenship. However, I will keep the original dataset to manipulate more variables later."
  },
  {
    "objectID": "posts/Final Project - Kristin Abijaoude.html#tidy-and-manipulate-dataset",
    "href": "posts/Final Project - Kristin Abijaoude.html#tidy-and-manipulate-dataset",
    "title": "Citizenship Laws by Country",
    "section": "Tidy and Manipulate Dataset",
    "text": "Tidy and Manipulate Dataset\nThis is an extremely messy dataset, so with mutate() and case_when(), I reorganize and recode the variables according to the code guide provided in the ZIP file. The A stands for “acquisition”, as in “how citizenship is acquired”, and L stands for “loss” as in “how citizenship is loss”. The word bin, short for binary, means if the country has the citizenship law, and cat, short for category, means what the exceptions are in the citizenship law, but I will solely focus on the binary data for now.\nFirst, let’s recode the acquisition variables.\n\n# recode laws on acquisition of citizenship\ncitizen_tidy <- citizenship %>%\n  mutate(\"acq_descborn\" = case_when(A01a_bin == 1 ~ \"Yes\",   # Person born to a citizen of a country (birth in that country)\n                                     A01a_bin == 0 ~ \"No\"),\n         \"acq_descabroad\" = case_when(A01b_bin == 1 ~ \"Yes\", # Person born to a citizen of a country (birth abroad)\n                                       A01b_bin == 0 ~ \"No\"),\n         \"acq_birthright\" = case_when(A02a_bin == 1 ~ \"Yes\", # Person born in a country regardless of parent's nationality\n                                      A02a_bin == 0 ~ \"No\"),\n         \"acq_parents\"  = case_when (A02b_bin == 1 ~ \"Yes\", # Person born to a parent who was also born in same country\n                                     A02b_bin == 0 ~ \"No\"), \n         \"acq_found\" = case_when(A03a_bin == 1 ~ \"Yes\", # Child found in a country of unknown parentage\n                                 A03a_bin == 0 ~ \"No\"),\n         \"acq_parent_est\" = case_when(A04_bin == 1 ~ \"Yes\", # Establishment of parentage\n                                      A04_bin == 0 ~ \"No\"),\n         \"acq_residency\" = case_when(A06_bin == 1 ~ \"Yes\", # Residence-based acquisition\n                                     A06_bin == 0 ~ \"No\"),\n         \"acq_renounce\" = case_when(A06b_bin == 0 ~ \"No\", # Person must renounce old citizenship first\n                                    A06b_bin == 1 ~ \"Yes\",\n                                    TRUE ~ \"No\"),\n         \"acq_lang\" = case_when(A06c_bin == 0 ~ \"No\", # Person must know the language basics\n                                A06c_bin == 1 ~ \"Yes\",\n                                TRUE ~ \"No\"),\n         \"acq_good_chara\" = case_when(A06e_bin == 1 ~ \"Yes\", # Person must be of good character\n                                      A06e_bin == 0 ~ \"No\",\n                                      TRUE ~ \"No\"),\n         \"acq_econ\" = case_when(A06f_bin == 0 ~ \"No\", # Person must have sufficient income\n                                A06f_bin == 1 ~ \"Yes\",\n                                TRUE ~ \"No\"),\n         \"acq_childhood\" = case_when(A07_bin == 0 ~ \"No\", # Person with a certain period of residence or schooling as a minor\n                                     A07_bin == 1 ~ \"Yes\"),\n         \"acq_marriage\" = case_when(A08_bin == 0 ~ \"No\", # Person marries a citizen\n                                    A08_bin == 1 ~ \"Yes\"),\n         \"acq_transfer\" = case_when(A09_bin == 0 ~ \"No\", # Transfer to a child from a parent\n                                    A09_bin == 1 ~ \"Yes\"),\n         \"acq_adopt\" = case_when(A10_bin == 0 ~ \"No\", # Person who is adopted by a citizen\n                                 A10_bin == 1 ~ \"Yes\"),\n         \"acq_relative\" = case_when(A11_bin == 1 ~ \"Yes\", # Person who is another relative of a citizen\n                                    A11_bin == 0 ~ \"No\"),\n         \"acq_rel_former\" = case_when(A12a_bin == 1 ~ \"Yes\", # Person who is the relative of a former citizen\n                                      A12a_bin == 0 ~ \"No\"),\n         \"acq_rel_dead\" = case_when(A12b_bin == 1 ~ \"Yes\", # Person who is the relative of a deceased citizen\n                                    A12b_bin == 0 ~ \"No\"),\n         \"acq_spouse\" = case_when(A13_bin == 1 ~ \"Yes\", # Person who is the spouse or registered partner of citizen\n                                  A13_bin == 0 ~ \"No\"),\n         \"acq_dep_citizen\" = case_when(A14_bin == 0 ~ \"No\", # Person who is the dependent of the citizen\n                                       A14_bin == 1 ~ \"Yes\"),\n         \"acq_regain\" = case_when(A16_bin == 1 ~ \"Yes\", # Person who was once a former citizen and regains citizenship\n                                  A16_bin == 0 ~ \"No\"), \n         \"acq_specific\" = case_when(A18_bin == 0 ~ \"No\", # Person who possesses the citizenship of a specific country\n                                    A18_bin == 1 ~ \"Yes\"),\n         \"acq_cxn\" = case_when(A19_bin == 0 ~ \"No\", # Person who has a cultural affinity\n                               A19_bin == 1 ~ \"Yes\"),\n         \"acq_presume\" = case_when(A20_bin == 0 ~ \"No\", # Person who is a presumed citizen acted in good faith\n                                   A20_bin == 1 ~ \"Yes\"),\n         \"acq_longterm\" = case_when(A21_bin == 0 ~ \"No\", # Person who has resided in a country for a very long time\n                                    A21_bin == 1 ~ \"Yes\"),\n         \"acq_refugees\" = case_when(A22_bin == 0 ~ \"No\", # Person who is a recognised refugee\n                                    A22_bin == 1 ~ \"Yes\"),\n         \"acq_stateless\" = case_when(A23_bin == 0 ~ \"No\", # Person who is stateless or of undetermined citizenship\n                                     A23_bin == 1 ~ \"Yes\"),\n         \"acq_exceptional\" = case_when(A24_bin == 1 ~ \"Yes\", # Person who has special achievements\n                                       A24_bin == 0 ~ \"No\"),\n         \"acq_service\" = case_when(A25_bin == 1 ~ \"Yes\", # Person who is in the public service\n                                   A25_bin == 0 ~ \"No\"),\n         \"acq_invest\" = case_when(A26_bin == 0 ~ \"No\", # Person who invests in the country\n                                  A26_bin == 1 ~ \"Yes\"))\n\nError in mutate(., acq_descborn = case_when(A01a_bin == 1 ~ \"Yes\", A01a_bin == : object 'citizenship' not found\n\n# remove unnecessary columns \ncitizen_tidy <- citizen_tidy[,-5:-73]\n\nError in eval(expr, envir, enclos): object 'citizen_tidy' not found\n\n# sanity check point\ncitizen_tidy\n\nError in eval(expr, envir, enclos): object 'citizen_tidy' not found\n\n\nNext, time to recode laws dealing with loss of citizenship.\n\n# recode laws on loss of citizenship\ncitizen_tidy <- citizen_tidy %>%\n  mutate(\"loss_volunteer\" = case_when(L01_bin == 1 ~ \"Yes\", # Person who voluntarily renounces the citizenship of his/her country\n                                      L01_bin == 0 ~ \"No\"),\n         \"loss_abroad\" = case_when(L02_bin == 1 ~ \"Yes\", # Person who resides outside the country of which he/she is a citizen\n                                   L02_bin == 0 ~ \"No\"),\n         \"loss_foreignarmy\" = case_when(L03_bin == 1 ~ \"Yes\", # Person who renders military service to a foreign country\n                                        L03_bin == 0 ~ \"No\"),\n         \"loss_foreignserv\"  = case_when (L04_bin == 1 ~ \"Yes\", # Person who renders other services to a foreign country\n                                          L04_bin == 0 ~ \"No\"),\n         \"loss_newcitizen\" = case_when(L05_bin == 1 ~ \"Yes\", # Person who acquires a foreign citizenship\n                                       L05_bin == 0 ~ \"No\"),\n         \"loss_mustchoose\" = case_when(L06_bin == 1 ~ \"Yes\", # Non-renunciation of foreign citizenship (acquisition by birth)\n                                       L06_bin == 0 ~ \"No\"),\n         \"loss_disloyal\" = case_when(L07_bin == 1 ~ \"Yes\", # Loss of citizenship due to disloyalty or treason\n                                     L07_bin == 0 ~ \"No\"),\n         \"loss_crime\" = case_when(L08_bin == 1 ~ \"Yes\", # Loss of citizenship due to other criminal offenses\n                                  L08_bin == 0 ~ \"No\"),\n         \"loss_fraud\" = case_when(L09_bin == 1 ~ \"Yes\", # Person who has acquired citizenship by fraud\n                                  L09_bin == 0 ~ \"No\"),\n         \"loss_birth_acq\" = case_when(L10_bin == 1 ~ \"Yes\", # Person who retains a foreign citizenship other than birth\n                                      L10_bin == 0 ~ \"No\"),\n         \"loss_byparent\" = case_when(L11_bin == 1 ~ \"Yes\", # Person whose parent loses citizenship of a country\n                                     L11_bin == 0 ~ \"No\"),\n         \"loss_byspouse\" = case_when(L12_bin == 1 ~ \"Yes\", # Person whose partner loses citizenship of a country\n                                     L12_bin == 0 ~ \"No\"),\n         \"loss_parent_annul\" = case_when(L13a_bin == 1 ~ \"Yes\", # Person whose descent from a citizen is annulled\n                                         L13a_bin == 0 ~ \"No\"),\n         \"loss_adopt_abroad\" = case_when(L13b_bin == 1 ~ \"Yes\", # Loss through adoption or guardianship abroad\n                                         L13b_bin == 0 ~ \"No\"),\n         \"loss_former_stateless\" = case_when(L14_bin == 1 ~ \"Yes\", # Former stateless person who acquired foreigh citizenship\n                                             L14_bin == 0 ~ \"No\"))\n\nError in mutate(., loss_volunteer = case_when(L01_bin == 1 ~ \"Yes\", L01_bin == : object 'citizen_tidy' not found\n\n# remove unnecessary columns \ncitizen_tidy <- citizen_tidy[,-5:-34]\n\nError in eval(expr, envir, enclos): object 'citizen_tidy' not found\n\n# sanity check point\ncitizen_tidy\n\nError in eval(expr, envir, enclos): object 'citizen_tidy' not found\n\n\nI recoded countries that permit dual citizenship.\n\n# does the country allow dual citizenship?\ncitizen_tidy <- citizen_tidy %>%\n  mutate(\"dual_permit\" = case_when(dualcit_comb == 0 ~ \"No\",\n                                   dualcit_comb > 0 ~ \"Yes\")) %>%\n  select(-c(dualcit_comb)) %>%\n  relocate(\"dual_permit\", .after = \"year\") %>%\n  relocate(\"country\", .after = \"year\")\n\nError in mutate(., dual_permit = case_when(dualcit_comb == 0 ~ \"No\", dualcit_comb > : object 'citizen_tidy' not found\n\n# sanity check point\ncitizen_tidy\n\nError in eval(expr, envir, enclos): object 'citizen_tidy' not found"
  },
  {
    "objectID": "posts/Final Project - Kristin Abijaoude.html#reflection-and-conclusion",
    "href": "posts/Final Project - Kristin Abijaoude.html#reflection-and-conclusion",
    "title": "Citizenship Laws by Country",
    "section": "Reflection and Conclusion",
    "text": "Reflection and Conclusion\nI had no prior experience with R before starting my masters in data analytics, and after many classes, resources, and Google searches, I must say that I did a great job on this project. Let me explain:\nChoosing a topic that interested me made the project more manageable. I was always interested in learning about other countries, such as cultures, laws, and societies in each country. This dataset of citizenship laws by each country is no different. I have visited 10 countries, and continue to visit more in the foreseeable future.\nI felt like I hit the jackpot when I stumbled upon this dataset on the database archival website Data is Plural. It was messy, tedious, and confusing to read: it was the perfect dataset to clean, manipulate, and visualize. The handy guide attached in the ZIP file was the cherry on the top.\nThe most challenging part of data analytics as a whole is trying to figure out which codes to use for my desired results. A map? Try a bunch of these codes (we won’’t guarantee your map would look funky). Forgot to capitalize a letter? Your data doesn’t exist. I have the basic idea on what I want my end results to be, but I have to spend hours looking for the right code and find the tiny problem I have been overlooking for those past hours. I give up, until I don’t: the feeling of finding that one line of code is the feeling of success.\nHowever, I wish I learned to code earlier in my childhood. I would have caught up with the rest of the class if I had a basic idea of coding, but it’s better to learn now than never. I’m in the middle of my career change from marketing to data analytics, which permits me to work behind the scene.\nOverall, the world is not our oyster and we must follow the laws wherever we go. It is always ideal to do research beforehand or hire an immigration lawyer."
  },
  {
    "objectID": "posts/Final Project - Kristin Abijaoude.html#resources",
    "href": "posts/Final Project - Kristin Abijaoude.html#resources",
    "title": "Citizenship Laws by Country",
    "section": "Resources",
    "text": "Resources\nCookbook for R, http://www.cookbook-r.com/.\nGrolemund, Hadley Wickham and Garrett. R For Data Science. https://r4ds.had.co.nz/introduction.html.\nHoare, Jake. “How to Aggregate Data in R: R-Bloggers.” R, 12 July 2018, https://www.r-bloggers.com/2018/07/how-to-aggregate-data-in-r/.\nHoltz, Yan. “Help and Inspiration for R Charts.” The R Graph Gallery, https://r-graph-gallery.com/.\n“How to Count Number of Times a Character Appears in a Row.” Stack Overflow, 23 Nov. 2013, https://stackoverflow.com/questions/20100298/how-to-count-number-of-times-a-character-appears-in-a-row.\nVINK, Maarten Peter, et al. “Globalcit Citizenship Law Dataset.” Cadmus Home, European University Institute, 1 Jan. 2020, https://cadmus.eui.eu/handle/1814/73190."
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html",
    "href": "posts/Final Project _ JulianCastoro.html",
    "title": "Global Military Spending - An Analysis",
    "section": "",
    "text": "Code\n#\n#| label: setup\n#| warning: false\n#| message: false\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\n\nlibrary(leaflet)\n\n\noptions(digits = 3,decimals=2)\noptions(scipen = 999)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#data-choice-overview",
    "href": "posts/Final Project _ JulianCastoro.html#data-choice-overview",
    "title": "Global Military Spending - An Analysis",
    "section": "Data choice overview",
    "text": "Data choice overview\nI initially chose this data set because recent events abroad have certainly escalated a global fear of war and I wanted to see how the tides of military spending ebbed and flowed over the course of recent history. The more I cleaned the data and played around with visualizations, the more questions I had. I am excited to see what sort of trends come from analyzing this data set.\nI plan to show how different countries and regions value military spending and how that value changes over the course of time. Ideally I will be able to explain patterns in what I see with global or locally important historic events. How did the US spending change after 9/11 or more recently did we see anyone bolstering their defenses before news broke of the Russian invasion of Ukraine? How did spending change globally after the first and then second world war? While I am not a statistical expert and will not be able to refute a causation vs correlation argument, visualizations of these events and the corresponding spending patterns will still be interesting and hopefully provoking of conversation."
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#data",
    "href": "posts/Final Project _ JulianCastoro.html#data",
    "title": "Global Military Spending - An Analysis",
    "section": "Data",
    "text": "Data\n\nDescription\nBelow is a list of the available sheets in the SIPRI military spending data export. A few of the tabs are the same base information altered to reflect a specific currency or ratio. I am choosing to use the “Current USD” as my source of raw spending numbers as I believe it to be the easiest to understand and relate to. I will also be using “share of GDP” as well as “Share of Govt. spending” to provide context around a nations spending compared to their population they intend on defending as well as compared to overall spending. The Constant (2020) USD values are all in Millions.\nThere will be NA’s throughout the dataset which represent the fact that the data was unavailable at that time. This could mean that the country did not exist in that year or they were unwilling to share their military spending information with SIPRI and so the NAs have been left in to reflect that fact. Data exists for at least one country from 1949 to 2021. There are 173 total countries in the database with the full list as well as some additional information in the appendix.\n“Gross domestic product (GDP) is the total monetary or market value of all the finished goods and services produced within a country’s borders in a specific time period.” - Investopedia\n\n\nCode\nsheets <- excel_sheets(\"_data/SIPRI-Milex-data-1949-2021.xlsx\")\nsheets\n\n\n [1] \"Front page\"                     \"Regional totals\"               \n [3] \"Local currency financial years\" \"Local currency calendar years\" \n [5] \"Constant (2020) USD\"            \"Current USD\"                   \n [7] \"Share of GDP\"                   \"Per capita\"                    \n [9] \"Share of Govt. spending\"        \"Footnotes\"                     \n\n\n\n\nSource\nThe Military spending information I am analyzing comes from SIPRI, the Stockholm International Peace Research Institute. SIPRI was established in 1966 for the purpose of “research into conflict, armaments, arms control and disarmament”. The organization is funded by the Swedish government however it regularly works internationally with research centers around the globe. SIPRI collected this particular data set from the official reports of each of the included governments in the form of official publications such as public budgets.\nHere is a peek at what the raw information looks like.\n\n\nCode\nrawrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9])\n#knitr::kable(head(rawrawData_ShareOfGovSpend,10),\"simple\")\nhead(rawrawData_ShareOfGovSpend,10)"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#reading-in-raw-information",
    "href": "posts/Final Project _ JulianCastoro.html#reading-in-raw-information",
    "title": "Global Military Spending - An Analysis",
    "section": "Reading in raw information",
    "text": "Reading in raw information\nThe data provided by the Stockholm International Peace Research Institute(SIPRI) was separated into multiple tabs in one excel .xlsx file.\nIn order to begin working I imported the tabs I planned to utilize, skipping over some of the notes and title rows at the start of each tab.\n\n\nCode\nrawData_CurrentUSD <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[6],skip=5)\nrawData_ShareOfGDP <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[7],skip=5)\nrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9],skip=7)\n\n\n\n\n\nNext I delete the first row of NA as well as the Notes column for each tibble.\n\n\nCode\nrawData_CurrentUSD <- rawData_CurrentUSD[-1,]  %>%\n  select(-Notes)\nrawData_ShareOfGDP <- rawData_ShareOfGDP[-1,] %>%\n  select(-Notes)\nrawData_ShareOfGovSpend <- rawData_ShareOfGovSpend[-1,] %>%\n  select(-2:-3)"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#tidying-data",
    "href": "posts/Final Project _ JulianCastoro.html#tidying-data",
    "title": "Global Military Spending - An Analysis",
    "section": "Tidying Data",
    "text": "Tidying Data\nBefore pivoting my data I wanted to add a column for region. I chose to do this with an algorithm as an exercise in iteration however it would have been more practical to just hard code this column.\nThe list of countries where there are NA’s for every year:\n\n\nCode\nRegions <- rawData_CurrentUSD %>%\n  filter(is.na(`1949`))%>%\n  select(Country)\n\nRegions\n\n\n\n\n  \n\n\n\nYou will notice that some of these are continents and sub-continents, I wanted to break these into a Region field. The pattern I saw and chose to exploit was the fact that the overarching category would always be followed with a more specific category such as the Africa followed by North Africa values (see earlier print outs to see raw information).\nI then added an empty vector into the tibble and populated it according to the Country column using the pattern described above.\nOnce I had left flags in the region column dictating where each region was starting I could use fill(Region) in order to populate the rest of the column.\n\n\nCode\nemptyRegionCol <- as.numeric(vector(mode = \"character\",length = length(rawData_CurrentUSD$`1949`)))## doing as numeric here as a hacky way to fill this with NA's, any better way?\n\n#adding col to tibble to be populated later\nmutated_CurrentUSD <- rawData_CurrentUSD %>%\n  mutate(Region =emptyRegionCol,.before=2)\n\n# replacing the empty char in region with actual region if 2 NAs appear in a row\n  #iterates along the indices of the Country vector\nfor(i in seq_along(mutated_CurrentUSD$Country) ){\n   #if 2 nas in a row then do something\n  if(is.na(mutated_CurrentUSD$`1949`[i]) && is.na(mutated_CurrentUSD$`1949`[i+1]) ){\n    mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i+1] #this works because when referencing an index outside of the vector R will return NA - Great to know.\n  }\n  #if the row is a region\n  if(is.na(mutated_CurrentUSD$`1949`[i]) ){\n     mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i]\n  }\n}\n\nmutated_CurrentUSD <- mutated_CurrentUSD %>%\n  fill(Region)\n\nhead(mutated_CurrentUSD)\n\n\n\n\n  \n\n\n\nNow that I had my proof of concept for cleaning one of the sheets, in a format I wanted, it was time to clean the other sheets as well. I could have taken the above code and modified it to work for each of the other tabs individually however I took the opportunity to practice with functions and created the below function to clean any of the tabs once they were loaded in and lightly pre-processed."
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#clean-and-format-function",
    "href": "posts/Final Project _ JulianCastoro.html#clean-and-format-function",
    "title": "Global Military Spending - An Analysis",
    "section": "Clean and format function",
    "text": "Clean and format function\n\n\nCode\n## creating a function to clean the other sheets\n##Input: a partially cleaned tibble, must have been read in and had excess rows trimmed\n##output: tibble with regions correctly labeled and umbrella region categories i.e Africa above south Africa removed\n##Note: All sub categories contained the main category name in them, will use regex to create groupings in the future.\nCleanData <- function(inSheet){\n  emptyRegionCol <- as.numeric(vector(mode = \"character\",length = nrow(select(inSheet,2))))\n\n  #adding col to tibble to be populated later\n  output <- inSheet %>%\n    mutate(Region = emptyRegionCol,.before=2)\n\n\n  # replacing the empty char in region with actual region if 2 NAs appear in a row\n    #iterates along the indices of the Country vector\n  for(i in seq_along(output$Country)){\n  \n    if(is.na(output[[i,3]]) && is.na(output[[(i+1),3]]))\n      output$Region[i+1] <- output$Country[i+1]\n    \n    if(is.na(output[[i,3]]))\n       output$Region[i+1] <- output$Country[i]\n  }\n  \n  output <- output %>%\n    fill(Region)\n  \n  \n  # removing header rows\n  output <- output %>%\n    filter(!is.na(output[[3]]))\n}\n\n\nBelow I call the function on each of the tibbles loaded in earlier. For the sake of space I will only show one tab being cleaned.\nBefore:\n\n\n\n\n  \n\n\n\n\n\nCode\nCleaned_currentUSD <- CleanData(rawData_CurrentUSD)\nCleaned_ShareOfGovSpend <- CleanData(rawData_ShareOfGovSpend)\nCleaned_ShareOfGDP <- CleanData(rawData_ShareOfGDP)\n\n\nAfter:"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#pivoting-and-restructuring",
    "href": "posts/Final Project _ JulianCastoro.html#pivoting-and-restructuring",
    "title": "Global Military Spending - An Analysis",
    "section": "Pivoting and Restructuring",
    "text": "Pivoting and Restructuring\n\nPivot and Notation Changes\nNext I pivot the years, it should be noted that both the pivoting and Converting of notation could be handled by the clean function above, i kept the steps separate so that I could show the progression of the dataframe.\nI must also handle the xxx and … notations. From the information page of my data set I can see that the notation is described as follows:\n\n\n\n\n\n\n\nRaw Notation\nMeaning\n\n\n\n\n…\nData unavailable\n\n\nxxx\nCountry did not exist or was not independent during all or part of the year in question\n\n\n\nFor now I think I will just keep both as NA but will save this form of the information for future use.\nAfter pivot and notation changes:\n\n\nCode\nCleaned_currentUSD <- Cleaned_currentUSD %>%\n  pivot_longer(cols=3:ncol(Cleaned_currentUSD),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n  \nCleaned_ShareOfGovSpend <- Cleaned_ShareOfGovSpend %>%\n  pivot_longer(cols=3:ncol(Cleaned_ShareOfGovSpend),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n\nCleaned_ShareOfGDP <- Cleaned_ShareOfGDP %>%\n  pivot_longer(cols=3:ncol(Cleaned_ShareOfGDP),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n\n\ndf1<-head(Cleaned_currentUSD)\ndf2<-head(Cleaned_ShareOfGovSpend)\ndf3<-head(Cleaned_ShareOfGDP)\n\n\n#df1\n#df2\n# share of GDP\n#df3\n\n\n\n\nConverting Column Types\nFinally I will convert the column types to their correct representation.\n\n\nCode\n#Year to Number\nCleaned_currentUSD$Year<-as.integer(Cleaned_currentUSD$Year)\nCleaned_ShareOfGovSpend$Year<-as.integer(Cleaned_ShareOfGovSpend$Year)\nCleaned_ShareOfGDP$Year<-as.integer(Cleaned_ShareOfGDP$Year)\n\n# value to double\nCleaned_currentUSD$value <- as.numeric(Cleaned_currentUSD$value)\nCleaned_ShareOfGovSpend$value<- as.numeric(Cleaned_ShareOfGovSpend$value)\nCleaned_ShareOfGDP$value<- as.numeric(Cleaned_ShareOfGDP$value)\n\n\n\n\n\n\n\nJoining of the Different Tabs\nHere we do a full join to maintain data from all the different tabs. The years when share of Gov spend are unavailable will then show NA as intended.\n\n\nCode\nMilitarySpend<- full_join(Cleaned_currentUSD,Cleaned_ShareOfGovSpend,by=c(\"Country\",\"Year\",\"Region\"))\nMilitarySpend <- full_join(MilitarySpend,Cleaned_ShareOfGDP,by=c(\"Country\",\"Year\",\"Region\"))\n\nhead(MilitarySpend)"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#pivoting",
    "href": "posts/Final Project _ JulianCastoro.html#pivoting",
    "title": "Global Military Spending - An Analysis",
    "section": "Pivoting",
    "text": "Pivoting\nMean, Median and Standard Deviation for each of the tabs.\nFirst I wanted to get an idea of what my data looks like for each of the countries individually. In order to see this I grouped on country name and the types of data we have.\nIn order to get that done I need to pivot my data again.\n\n\nCode\ncombinedData <-MilitarySpend %>%\n  pivot_longer(c(MilitarySpend_currentUSD,MilitarySpend_ShareofGovSpend,MilitarySpend_ShareofGDP),names_to = \"viewOfSpend\",values_to = \"Spend\")\nhead(combinedData)\n\n\n\n\n  \n\n\n\nCode\ncombinedData<-na.omit(combinedData)\n\n\nTotal number of countries analyzed:\n\n\n\nList of Countries:\n\n\n\nNumber of countries by region:"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#statistics-by-region",
    "href": "posts/Final Project _ JulianCastoro.html#statistics-by-region",
    "title": "Global Military Spending - An Analysis",
    "section": "Statistics by region",
    "text": "Statistics by region\nNow that I have all of my data in a malleable state I will do some initial statistics on each tab of the initial dataset to give asome insight to what patterns or outliers may exist.\n\nMilitarySpend_currentUSD\nI will be using the current USD data for my analysis of major world events so I will not dive deeper at this point.\n\n\nCode\nbyRegionStats_USD<-combinedData%>%\n  group_by(Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE),\n            min = min(Spend, na.rm=TRUE),\n            max = max(Spend, na.rm=TRUE))%>%\n  arrange(desc(mean))%>%\n  filter(viewOfSpend==\"MilitarySpend_currentUSD\")\n\nbyRegionStats_USD\n\n\n\n\n  \n\n\n\n\n\nMilitarySpend_ShareOfGDP\n\n\nCode\nbyRegionStats_GDP<-combinedData%>%\n  group_by(Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE),\n            min = min(Spend, na.rm=TRUE),\n            max = max(Spend, na.rm=TRUE))%>%\n  arrange(desc(mean))%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGDP\")\n\nbyRegionStats_GDP\n\n\n\n\n  \n\n\n\nAfter sorting by mean share of GDP spend I see that the middle east has the highest by quite a large margin. This is odd because usally the US(North America), has the highest military spending. A max of 1.17 for GDP is also stageringly high when compared to the other regions. This prompts further investigation. A violin plot below gives a visualization of this distribution.\n\n\nCode\nbyRegionStats_GDP<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGDP\")\n\n\nbyRegionStats_GDP$Region<-as.factor(byRegionStats_GDP$Region)\n\nviolin_gdp<- na.omit(byRegionStats_GDP)%>%\n  ggplot(aes(x=Region,y=Spend))+\n  geom_violin()+\n  theme(axis.text.x=element_text(angle=60,hjust=1))\n\nviolin_gdp\n\n\n\n\n\nCode\n# removing NAs here because I cant plot them, I dont want to replace them with other values because it is still of interest when countries do not display their data.\n\n\nUpon investigation of the underlying data I see that Kuwait in 1991 is the culprit of the high military spending as percent of GDP number.\n\n\nCode\nbyRegionStats_GDP%>%\n  filter(Region==\"Middle East\")%>%\n  arrange(desc(Spend))%>%\n  head()\n\n\n\n\n  \n\n\n\nI initially thought this was a typo since there were two 1s in a row and it was such an insanely high spend. A share of GDP of 1.173 means Kuwait spent 117.3% of their GDP in that year. Upon further investigation, this is actually true and it was driven by the Persian Gulf War(aka Gulf War). The Gulf War was a conflict triggered by Iraqs invasion of Kuwait in 1990. This invasion was the first major international crisis since the Cold War and will certainly be a topic I add to the final analysis.\nhttps://www.britannica.com/event/Persian-Gulf-War\nLoosely I will plot Iraq, US and Kuwait spend from 1985-1995 to explore this time period.\n\n\nCode\ncombinedData%>%\n  filter(Year >= 1985 & Year<= 1995)%>%\n  filter(Country %in% c(\"United States of America\",\"Iraq\",\"Kuwait\"))%>%\n    ggplot(mapping=aes(y = Spend, x = Year))+  \n    geom_line(aes(color = Country),na.rm = FALSE)+\n    facet_wrap(vars(`viewOfSpend`),scales = \"free_y\")\n\n\n\n\n\nA new discovery! Iraq chose not to report their spending during the Gulf war. This is most likely due to the fact that SIPRI data collection is funded by the Swedish government who have pretty close ties to the United States. Sweden was one of the first countries to recognize US independence in 1783. Below I highlight reported Iraqi military spending overlaid with markings for the start and end of their reported data. We can see that in the time leading up to the gulf war military spending rose sharpley and even after the war they continued to spend above historical levels.\n\n\nCode\nstartOfNoData<- 1982\nendofNoData<- 2003\n\niraqMissingDataPLOT<-combinedData%>%\n  filter(Country %in% c(\"Iraq\"))%>%\n    ggplot(mapping=aes( x = Spend,y = Year))+\n    geom_point()+\n    facet_wrap(vars(`viewOfSpend`),scales = \"free_x\")+\n    scale_y_continuous(labels=seq(1949,2020,5),breaks =seq(1949,2020,5))\n\niraqMissingDataPLOT+\ngeom_hline(yintercept = 1982,color=\"red\")+\ngeom_hline(yintercept = 2003,color=\"red\")+\ngeom_hline(yintercept = 1985,color=\"blue\")+\ngeom_hline(yintercept = 1995,color=\"blue\")+\nannotate(\"text\",x=0,y=1982,label=\"1982\",hjust=-0.1,vjust=-0.1,color=\"red\")+\nannotate(\"text\",x=0,y=2003,label=\"2003\",hjust=-0.1,vjust=1.1,color=\"red\")+\nannotate(\"text\",x=0,y=1985,label=\"Start of Gulf war\",hjust=-0.1,vjust=-0.1,color=\"blue\")+\nannotate(\"text\",x=0,y=1995,label=\"End of Gulf war\",hjust=-0.1,vjust=-0.1,color=\"blue\")\n\n\n\n\n\n\n\nMilitarySpend_ShareofGovSpend\n\n\nCode\nbyRegionStats_GovSpend<-combinedData%>%\n  group_by(Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE),\n            min = min(Spend, na.rm=TRUE),\n            max = max(Spend, na.rm=TRUE))%>%\n  arrange(desc(mean))%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGovSpend\")\n\nbyRegionStats_GovSpend\n\n\n\n\n  \n\n\n\nThe share of Gov spend shows an insanely high max of sub-Saharan African spending as well as the Middle East.The Middle east spending has been explained above however below I will investigate the sub-sahara African spending.\n\n\nCode\nviolin_ShareOfSpend<- na.omit(byRegionStats_GovSpend)%>%\n  ggplot(aes(x=Region,y=Spend))+\n  geom_violin()+\n  theme(axis.text.x=element_text(angle=60,hjust=1))\n\nviolin_gdp\n\n\n\n\n\nZooming into Zimbabwe’s Violin plot we can see that they often have an exteremly high military spending when compared to overall spending. As suggested by [5], Zimbabwe’s military spending is highly influenced by internal political turbulence more so than economic factors. It should also be mentioned that their military spending is often determined by SIPRI from estimation and not necessarily accurate, for this reason I wont be looking further into the anomaly.\n\n\nCode\ncombinedData%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGovSpend\")%>%\n  filter(Region == \"sub-Saharan Africa\")%>%\n  filter(Country==\"Zimbabwe\")%>%\n  ggplot(aes(x=Country,y=Spend))+\n  geom_violin()+\n  theme(axis.text.x=element_text(angle=60,hjust=1))\n\n\n\n\n\nNow that some introductory statistics have been done to explain the data at first glance, we focus in on the heavy hitters in the military spending space and look into how recent global events have affected their spending habits.\nTop 6 mean spenders\n\n\nCode\ntop6_mean <- byRegionStats_USD %>%\n  head()%>%\n  pull(Region)\n\ntop6_mean\n\n\n[1] \"North America\"  \"East Asia\"      \"Eastern Europe\" \"Western Europe\"\n[5] \"Middle East\"    \"South Asia\"    \n\n\nThe top 6 spenders stand out not only in their mean spending habits but they continue to spend more and more.\n\n\nCode\nstatsViz2<- byRegionStats_USD%>%\n  mutate(test = fct_reorder(Region,desc(mean)))%>%\n  ggplot(mapping=aes(y = test, x = mean))+  \n  geom_point(aes(color = Region, alpha=0.9, size=max)) +\n  guides(alpha=\"none\",color=\"none\") +\n  labs(title = \"Certain Regions on average spend more than others\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\nstatsViz2\n\n\n\n\n\n\n\nTop 6 spenders over time\nThis data is scaled for each country on the Y axis so that you can see the shape of the spending compared to global averages. I found the below interesting in that it paints the story of the US setting the pace and overall shape of the graph with East Asia playing a bit of catchup, mostly driven by China.\n\n\nCode\nbyRegionStats_year_top6<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE))%>%\n  filter(viewOfSpend == 'MilitarySpend_currentUSD')%>%\n  filter(Region == top6_mean)%>%\n  arrange(desc(mean))\n\n\n# add global average overlay\nGlobalAverage<-combinedData%>%\n  group_by(viewOfSpend,Year)%>%\n  filter(viewOfSpend==\"MilitarySpend_currentUSD\")%>%\n  summarise(GlobalAverage = mean(Spend),GlobalStandardDev = sd(Spend))\n\nbyRegionplot_top6<- byRegionStats_year_top6%>%\n  ggplot(mapping=aes(x = Year, y = mean))+\n  geom_line(aes(color = Region)) +\n  geom_line(data= GlobalAverage,aes(x=Year,y=GlobalAverage))+\n  facet_wrap(vars(`Region`),scales = \"free_y\") +\n  labs(title = \"Raw Mean Spend in USD for top 6 Spenders with Global Average Overlay\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\n\nbyRegionplot_top6\n\n\n\n\n\n\n\nTop 6 spenders over time with GDP raw value overlay\nIn order to show the raw values of military spend and GDP for the top 6 spenders I back calculate the true GDP spend values from the percentage of military spend. This gives an idea of how the relationship of money spent on military causes and domestic production is shaped. We can see that for the most part raw GDP spend increases steadily, while we see more dramatic swings in military spending. An interesting point of note however is the increase in military spending in Eastern Europe and the Middle East associated with a dip in the regions GDP. If I were to continue to expand the report this would be an area of investigation.\n\n\nCode\nbyRegionStats_year_top6_GDP<-combinedData%>%\n  filter(Region %in% top6_mean)%>%\n  pivot_wider(names_from = viewOfSpend,values_from = Spend)%>%\n  mutate(rawGDP = `MilitarySpend_currentUSD`/`MilitarySpend_ShareofGDP`)%>%\n  group_by(Region,Year)%>%\n  summarize(meanUSD = mean(MilitarySpend_currentUSD, na.rm=TRUE), meanGDP =mean(rawGDP,na.rm=TRUE))\n  \n  \n\nbyRegionplot_top6_GDP<- byRegionStats_year_top6%>%\n  ggplot(mapping=aes(x = Year, y = mean))+\n  geom_line(aes(color = Region)) +\n  geom_line(data= byRegionStats_year_top6_GDP,aes(x=Year,y=meanGDP))+\n  facet_wrap(vars(`Region`),scales = \"free_y\") +\n  labs(title = \"Raw Mean Spend in USD for top 6 Spenders with raw GDP Overlay\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\n\nbyRegionplot_top6_GDP\n\n\n\n\n\n\n\nTop and bottom 5 spenders\n\n\nCode\ntopMean<-combinedData%>%\n  group_by(Country,Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE))%>%\n  filter(mean>=1880)%>%\n  arrange(desc(mean))\n\nbotMean<-combinedData%>%\n  group_by(Country,Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE))%>%\n  filter(mean>=1880)%>%\n  arrange(mean)\n\n\n\ncombinedData%>%\n  filter(Country == \"Afghanistan\")%>%\n  na.omit()%>%\n  mean(Spend)\n\n\n[1] NA\n\n\nTop 5 spenders on average:\n\n\n\n\n  \n\n\n\nBottom 5 spenders on average:"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#top-spenders-by-country",
    "href": "posts/Final Project _ JulianCastoro.html#top-spenders-by-country",
    "title": "Global Military Spending - An Analysis",
    "section": "Top spenders by Country",
    "text": "Top spenders by Country\nIn the following analysis I will be looking at significant events which impacted some if not all of the world. For the sake of clarity I will only be focusing on a select group of individual countries which are not directly involved. The top 10 countries which stand out as significant spenders in the last 10 years will be the ones focused on.\n\n\nCode\n# getting top 10 mean spenders in last 10 years\ntopMeanCountries<-combinedData%>%\n  group_by(Country,viewOfSpend)%>%\n  filter(Year>=2012)%>%\n  filter(viewOfSpend==\"MilitarySpend_currentUSD\")%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1))%>%\n  arrange(desc(mean))%>%\n  head(10)\ntopMeanCountriesVec<-topMeanCountries$Country\ntopMeanCountriesVec\n\n\n [1] \"United States of America\" \"China\"                   \n [3] \"Russia\"                   \"Saudi Arabia\"            \n [5] \"India\"                    \"United Kingdom\"          \n [7] \"France\"                   \"Japan\"                   \n [9] \"Germany\"                  \"Korea, South\"            \n\n\nCode\n# Raw data for these 10 countries\ntop10SpendCountriesbyYear<-combinedData%>%\n  group_by(Country,viewOfSpend,Year)%>%\n  filter(viewOfSpend==\"MilitarySpend_currentUSD\")%>%\n  filter(Country %in% topMeanCountriesVec)\n\n\n\n\nCode\ngrouped<-top10SpendCountriesbyYear%>%\n  ungroup()%>%\n  group_by(Country)\n\ntop10CountryTibbles<-grouped%>%\n  group_split(Country)\n\n# Using PURR here! Pretty powerful.\n  # result is list of tibbles which have been processed to contain YoY spend increases\ntop10CountryTibblesYoY<- map(top10CountryTibbles,~ .x %>% mutate(percentChangeYoY = ((Spend - lag(Spend))/Spend)*100))\n\n\n# I then stack these DF on top of eachother to create one single DF to interact with.\ntop10CountryYoY<-bind_rows(top10CountryTibblesYoY)\n\n\nPlots detailing YoY(Year over Year) spending for each of the top 10 spenders. Green and green line segments indicate that year having an increase or decrease in spending respectively. From this visualization we can see more often than not the spending of most nations is increasing. Germany, France, Japan, and the UK stand out as those who do the most jumping back and forth between spending more or less YoY.\n\n\nCode\nrect_data <- data.frame(  xmin=c(1990,1990),\n                          xmax=c(2020,2020),\n                          ymin=c(-100,0),\n                          ymax=c(0,20),\n                          col=c(\"green\",\"red\"))\n\nPercentChangeTop10Countries<-top10CountryYoY%>%\n  filter(Year>=1992)%>%\n  mutate(posOrNeg = case_when(\n    percentChangeYoY>0 ~ \"Positive\",\n    percentChangeYoY<0 ~ \"Negative\"\n  ))%>%\n  ggplot()+\n  geom_line(aes(x = Year, y = percentChangeYoY,color = as.factor(posOrNeg)))+\n  facet_wrap(vars(Country),scale=\"free_y\")+\n  theme(axis.text.x=element_text(angle=60,hjust=1))+\n  aes(group=NA)+guides(color = guide_legend(title = \"YoY Positive or negative spend\"))\n  \nPercentChangeTop10Countries\n\n\n\n\n\nTo give some perspective on just how much the US and now recently China spends on their military I have left the following vizualization of the top 10 spenders unscaled.\n\n\nCode\ntop10Countries<- top10SpendCountriesbyYear%>%\n  ggplot(mapping=aes(x = Year, y = Spend))+  \n  geom_point(aes(color = Country, alpha=0.9,)) +\n  theme_light() +\n  guides(alpha=\"none\") +\n  labs(title = \"East asia and NA dominate raw spend\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\ntop10Countries+\n  facet_wrap(vars(`Country`))+\n  theme(axis.text.x=element_text(angle=60,hjust=1))"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#conclusion",
    "href": "posts/Final Project _ JulianCastoro.html#conclusion",
    "title": "Global Military Spending - An Analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\n“Every gun that is made, every warship launched, every rocket fired signifies in the final sense, a theft from those who hunger and are not fed, those who are cold and are not clothed. > > This world in arms is not spending money alone. It is spending the sweat of its laborers, the genius of its scientists, the hopes of its children. This is not a way of life at all in any > true sense. Under the clouds of war, it is humanity hanging on a cross of iron.”\n\nDwight D. Eisenhower\nAfter reviewing the data at a high level and diving deeper into spending surrounding 9/11 and the Russo-Ukraine war I have found a few facts have held true through the years. Powerful country Military spending rises like ships in high tide, when one goes up, they all do. In the years leading up to an altercation the nations involved often increased spending both in raw USD as well as percentage of their government spend and GDP. In the future I would like to spend some more time analyzing the tabs other than USD spend and look a little bit more at the times of peace and how countries would prioritize spending that time period. I had a lot of fun with this project and can say now that I am competent with R even when starting with zero experience beforehand."
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#cleaning-with-intermitant-steps",
    "href": "posts/Final Project _ JulianCastoro.html#cleaning-with-intermitant-steps",
    "title": "Global Military Spending - An Analysis",
    "section": "Cleaning with intermitant steps",
    "text": "Cleaning with intermitant steps"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#reading-in-raw-information-1",
    "href": "posts/Final Project _ JulianCastoro.html#reading-in-raw-information-1",
    "title": "Global Military Spending - An Analysis",
    "section": "Reading in raw information",
    "text": "Reading in raw information\nThe data provided by the Stockholm International Peace Research Institute(SIPRI) was separated into multiple tabs in one excel .xlsx file.\nIn order to begin working I imported the tabs I planned to utilize, skipping over some of the notes and title rows at the start of each tab.\n\n\nCode\nrawData_CurrentUSD <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[6],skip=5)\nrawData_ShareOfGDP <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[7],skip=5)\nrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9],skip=7)\n\n\n\n\n\n\n  \n\n\n\nNext I delete the first row of NA as well as the Notes column for each tibble.\n\n\nCode\nrawData_CurrentUSD <- rawData_CurrentUSD[-1,]  %>%\n  select(-Notes)\nrawData_ShareOfGDP <- rawData_ShareOfGDP[-1,] %>%\n  select(-Notes)\nrawData_ShareOfGovSpend <- rawData_ShareOfGovSpend[-1,] %>%\n  select(-2:-3)"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#tidying-data-1",
    "href": "posts/Final Project _ JulianCastoro.html#tidying-data-1",
    "title": "Global Military Spending - An Analysis",
    "section": "Tidying Data",
    "text": "Tidying Data\nBefore pivoting my data I wanted to add a column for region. I chose to do this with an algorithm as an exercise in iteration however it would have been more practical to just hard code this column.\nThe list of countries where there are NA’s for every year:\n\n\nCode\nRegions <- rawData_CurrentUSD %>%\n  filter(is.na(`1949`))%>%\n  select(Country)\n\nRegions\n\n\n\n\n  \n\n\n\nYou will notice that some of these are continents and sub-continents, I wanted to break these into a Region field. The pattern I saw and chose to exploit was the fact that the overarching category would always be followed with a more specific category such as the Africa followed by North Africa values (see earlier print outs to see raw information).\nI then added an empty vector into the tibble and populated it according to the Country column using the pattern described above.\nOnce I had left flags in the region column dictating where each region was starting I could use fill(Region) in order to populate the rest of the column.\n\n\nCode\nemptyRegionCol <- as.numeric(vector(mode = \"character\",length = length(rawData_CurrentUSD$`1949`)))## doing as numeric here as a hacky way to fill this with NA's, any better way?\n\n#adding col to tibble to be populated later\nmutated_CurrentUSD <- rawData_CurrentUSD %>%\n  mutate(Region =emptyRegionCol,.before=2)\n\n# replacing the empty char in region with actual region if 2 NAs appear in a row\n  #iterates along the indices of the Country vector\nfor(i in seq_along(mutated_CurrentUSD$Country) ){\n   #if 2 nas in a row then do something\n  if(is.na(mutated_CurrentUSD$`1949`[i]) && is.na(mutated_CurrentUSD$`1949`[i+1]) ){\n    mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i+1] #this works because when referencing an index outside of the vector R will return NA - Great to know.\n  }\n  #if the row is a region\n  if(is.na(mutated_CurrentUSD$`1949`[i]) ){\n     mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i]\n  }\n}\n\nmutated_CurrentUSD <- mutated_CurrentUSD %>%\n  fill(Region)\n\nhead(mutated_CurrentUSD)\n\n\n\n\n  \n\n\n\nNow that I had my proof of concept for cleaning one of the sheets, in a format I wanted, it was time to clean the other sheets as well. I could have taken the above code and modified it to work for each of the other tabs individually however I took the opportunity to practice with functions and created the below function to clean any of the tabs once they were loaded in and lightly pre-processed."
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#clean-and-format-function-1",
    "href": "posts/Final Project _ JulianCastoro.html#clean-and-format-function-1",
    "title": "Global Military Spending - An Analysis",
    "section": "Clean and format function",
    "text": "Clean and format function\n\n\nCode\n## creating a function to clean the other sheets\n##Input: a partially cleaned tibble, must have been read in and had excess rows trimmed\n##output: tibble with regions correctly labeled and umbrella region categories i.e Africa above south Africa removed\n##Note: All sub categories contained the main category name in them, will use regex to create groupings in the future.\nCleanData <- function(inSheet){\n  emptyRegionCol <- as.numeric(vector(mode = \"character\",length = nrow(select(inSheet,2))))\n\n  #adding col to tibble to be populated later\n  output <- inSheet %>%\n    mutate(Region = emptyRegionCol,.before=2)\n\n\n  # replacing the empty char in region with actual region if 2 NAs appear in a row\n    #iterates along the indices of the Country vector\n  for(i in seq_along(output$Country)){\n  \n    if(is.na(output[[i,3]]) && is.na(output[[(i+1),3]]))\n      output$Region[i+1] <- output$Country[i+1]\n    \n    if(is.na(output[[i,3]]))\n       output$Region[i+1] <- output$Country[i]\n  }\n  \n  output <- output %>%\n    fill(Region)\n  \n  \n  # removing header rows\n  output <- output %>%\n    filter(!is.na(output[[3]]))\n}\n\n\nBelow I call the function on each of the tibbles loaded in earlier. For the sake of space I will only show one tab being cleaned.\nBefore:\n\n\n\n\n  \n\n\n\n\n\nCode\nCleaned_currentUSD <- CleanData(rawData_CurrentUSD)\nCleaned_ShareOfGovSpend <- CleanData(rawData_ShareOfGovSpend)\nCleaned_ShareOfGDP <- CleanData(rawData_ShareOfGDP)\n\n\nAfter:"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#pivoting-and-restructuring-1",
    "href": "posts/Final Project _ JulianCastoro.html#pivoting-and-restructuring-1",
    "title": "Global Military Spending - An Analysis",
    "section": "Pivoting and Restructuring",
    "text": "Pivoting and Restructuring\n\nPivot and Notation Changes\nNext I pivot the years, it should be noted that both the pivoting and Converting of notation could be handled by the clean function above, i kept the steps separate so that I could show the progression of the dataframe.\nI must also handle the xxx and … notations. From the information page of my data set I can see that the notation is described as follows:\n\n\n\n\n\n\n\nRaw Notation\nMeaning\n\n\n\n\n…\nData unavailable\n\n\nxxx\nCountry did not exist or was not independent during all or part of the year in question\n\n\n\nFor now I think I will just keep both as NA but will save this form of the information for future use.\nAfter pivot and notation changes:\n\n\nCode\nCleaned_currentUSD <- Cleaned_currentUSD %>%\n  pivot_longer(cols=3:ncol(Cleaned_currentUSD),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n  \nCleaned_ShareOfGovSpend <- Cleaned_ShareOfGovSpend %>%\n  pivot_longer(cols=3:ncol(Cleaned_ShareOfGovSpend),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n\nCleaned_ShareOfGDP <- Cleaned_ShareOfGDP %>%\n  pivot_longer(cols=3:ncol(Cleaned_ShareOfGDP),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n\n\ndf1<-head(Cleaned_currentUSD)\ndf2<-head(Cleaned_ShareOfGovSpend)\ndf3<-head(Cleaned_ShareOfGDP)\n\n\n#df1\n#df2\n# share of GDP\ndf3\n\n\n\n\n  \n\n\n\n\n\nConverting Column Types\nFinally I will convert the column types to their correct representation.\nBefore:\n\n\n\n\n  \n\n\n\n\n\nCode\n#Year to Number\nCleaned_currentUSD$Year<-as.integer(Cleaned_currentUSD$Year)\nCleaned_ShareOfGovSpend$Year<-as.integer(Cleaned_ShareOfGovSpend$Year)\nCleaned_ShareOfGDP$Year<-as.integer(Cleaned_ShareOfGDP$Year)\n\n# value to double\nCleaned_currentUSD$value <- as.numeric(Cleaned_currentUSD$value)\nCleaned_ShareOfGovSpend$value<- as.numeric(Cleaned_ShareOfGovSpend$value)\nCleaned_ShareOfGDP$value<- as.numeric(Cleaned_ShareOfGDP$value)\n\n\nAfter:\n\n\n\n\n  \n\n\n\n\n\n\n\n\nJoining of the Different Tabs\nHere we do a full join to maintain data from all the different tabs. The years when share of Gov spend are unavaiable will then show NA as intended.\n\n\nCode\n#nrow(Cleaned_currentUSD)\n#nrow(Cleaned_ShareOfGovSpend)\n#nrow(Cleaned_ShareOfGDP)\n\nMilitarySpend<- full_join(Cleaned_currentUSD,Cleaned_ShareOfGovSpend,by=c(\"Country\",\"Year\",\"Region\"))\n\nMilitarySpend <- full_join(MilitarySpend,Cleaned_ShareOfGDP,by=c(\"Country\",\"Year\",\"Region\"))\n\n\n\nhead(MilitarySpend)\n\n\n\n\n  \n\n\n\nCode\n#nrow(MilitarySpend)"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#extra-vizualizations",
    "href": "posts/Final Project _ JulianCastoro.html#extra-vizualizations",
    "title": "Global Military Spending - An Analysis",
    "section": "extra Vizualizations",
    "text": "extra Vizualizations\n\n\nCode\nbyRegionStats_year_top6<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE))%>%\n  filter(viewOfSpend == 'MilitarySpend_currentUSD')%>%\n  filter(Region == top6_mean)%>%\n  arrange(desc(mean))\n\n\n# add global average overlay\nGlobalAverage<-combinedData%>%\n  group_by(viewOfSpend,Year)%>%\n  filter(viewOfSpend==\"MilitarySpend_currentUSD\")%>%\n  summarise(GlobalAverage = mean(Spend),GlobalStandardDev = sd(Spend))\n\nbyRegionplot_top6<- byRegionStats_year_top6%>%\n  ggplot(mapping=aes(x = Year, y = mean))+\n  geom_point(aes(color = Region, alpha=0.9,)) +\n  geom_line(data= GlobalAverage,aes(x=Year,y=GlobalAverage))+\n  facet_wrap(vars(`Region`)) +\n  labs(title = \"Raw Mean Spend in USD for top 6 Spenders with Global Average Overlay\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\n\n\n\nbyRegionplot_top6"
  },
  {
    "objectID": "posts/Final Project _ JulianCastoro.html#animations",
    "href": "posts/Final Project _ JulianCastoro.html#animations",
    "title": "Global Military Spending - An Analysis",
    "section": "Animations",
    "text": "Animations\n\n\nCode\n# animationWIP<-combinedData%>%\n#   group_by(Country,viewOfSpend,Year)%>%\n#   pivot_wider(names_from = viewOfSpend,values_from = Spend)\n# \n# \n# ggplot(animationWIP, aes(x=`Year`,y=`MilitarySpend_currentUSD`, color = Country,size= MilitarySpend_ShareofGDP))+\n#   geom_point(alpha = 0.6,)+\n#   scale_radius(limits = c(0, NA), range = c(2, 12)) +\n#   facet_wrap(vars(Region),scales = \"free_y\")+\n#   labs(title = 'Year: {frame_time}', x = 'Share of Gov Spend', y= 'raw USD')+\n#   theme(legend.position = \"none\")\n# \n# myAnimation<- ggplot(animationWIP, aes(x=`Year`,y=`MilitarySpend_currentUSD`, color = Country,size= MilitarySpend_ShareofGDP))+\n#   geom_point(alpha = 0.6,)+\n#   scale_radius(limits = c(0, NA), range = c(2, 12)) +\n#   facet_wrap(vars(Region),scales = \"free_y\")+\n#   labs(title = 'Year: {frame_time}', x = 'Year', y= 'raw USD')+\n#   theme(legend.position = \"none\")+\n#   transition_time(`Year`)+\n#   ease_aes('linear')\n# \n# animate(myAnimation,duration = 15,fps = 20,width=600,height = 600, renderer = gifski_renderer())\n# anim_save(\"GovSpend_VS_RawUSD.gif\")\n\n\n\n\nCode\n# world_map<-map_data(\"world\")%>%\n#   filter(! long>180)\n#   \n# gsub(\"USA\",\"United states of America\",world_map)\n# \n# view(world_map)\n# \n# dataForMap<- combinedData %>%\n#   filter(viewOfSpend == \"MilitarySpend_currentUSD\")%>%\n#   rename(region=Country)\n# view(dataForMap)\n# \n# worldMapCountries<-world_map%>%\n#   distinct(region)\n# \n# mapAnimationData<-left_join(world_map,dataForMap)%>%\n#   filter(viewOfSpend==\"MilitarySpend_currentUSD\")\n# \n# \n# test<-mapAnimationData%>%\n#   group_by(long,lat,region)%>%\n#   summarize(mean = mean(Spend, na.rm=TRUE))\n# \n# \n# test%>%\n#   ggplot(aes(fill=mean, map_id=region))+\n#   geom_map(map=world_map)\n# \n# \n# test %>% \n#   ggplot(aes(map_id = region,fill=mean)) +\n#   geom_map(map = world_map) +\n#   expand_limits(x = world_map$long, y = world_map$lat)"
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html",
    "href": "posts/Final Project _HW3_ JulianCastoro.html",
    "title": "Global Military Spending - An Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\n\nlibrary(ggplot2)\nlibrary(gganimate)\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\n\nlibrary(leaflet)\n\noptions(digits = 3,decimals=2)\noptions(scipen = 999)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#introduction",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#introduction",
    "title": "Global Military Spending - An Analysis",
    "section": "Introduction",
    "text": "Introduction\nFor HW3 I am not outputting any of the cleaning however I am leaving it in for reference.\nThe intent of this homework for me was to show how playing around with the descriptive statistics is a great idea for exploring your data. The questions I originally had intended to answer with the graphs lead to new questions and at the end of this has added a new time period for my final project to investigate.\nHad I simply ommitted the outline I found in the data or changed my groupings to hide it, I would not have thought to highlight this time in history."
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#data",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#data",
    "title": "Global Military Spending - An Analysis",
    "section": "Data",
    "text": "Data\n\nBriefly describe the data\nBelow is a list of the available sheets in the SIPRI military spending data export. A few of the tabs are the same base information altered to reflect a specific currency or ratio. I am choosing to use the “Current USD” as my source of raw spending numbers as I believe it to be the easiest to understand and relate to. I will also be using “share of GDP” as well as “Share of Govt. spending” to provide context around a nations spending compared to their population they intend on defending as well as compared to overall spending.\n\n\nDescription of data I will use\n“Gross domestic product (GDP) is the total monetary or market value of all the finished goods and services produced within a country’s borders in a specific time period.” - investopedia\n\n\nCode\nsheets <- excel_sheets(\"_data/SIPRI-Milex-data-1949-2021.xlsx\")\n#sheets\n\n\n\n\nWhere did this information come from?\nThe Military spending information I am analyzing comes from SIPRI, the Stockholm International Peace Research Institute. SIPRI was established in 1966 for the purpose of “research into conflict, armaments, arms control and disarmament”. The organization is funded by the Swedish government however it regularly works internationally with research centers around the globe. SIPRI collected this particular data set from the official reports of each of the included governments in the form of official publications such as public budgets.\nHere is a peek at what the raw information looks like.\n\n\nCode\nrawrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9])\n#head(rawrawData_ShareOfGovSpend)\n\n\n\n\nReading in Raw information\nReading in raw information\nThe data provided by the Stockholm International Peace Research Institute(sipri) was separated into multiple tabs in one excel .xlsx file.\nIn order to begin working I imported the tabs I planned to utilize, skipping over some of the notes and title rows at the start of each tab.\n\n\nCode\nrawData_CurrentUSD <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[6],skip=5)\nrawData_ShareOfGDP <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[7],skip=5)\nrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9],skip=7)\n\n\n\n#head(rawData_CurrentUSD)\n#head(rawData_ShareOfGDP)\n#head(rawData_ShareOfGovSpend)\n\n\nNext I delete the first row of NA as well as the Notes column for each tibble.\n\n\nCode\n## trying Purr here to be cleaner, we have not covered this yet so please let me know if this could be better.\n\n#tibleList <- lst(rawData_CurrentUSD, rawData_ShareOfGDP, rawData_ShareOfGovSpend)\n\n# could pur this with my func i bet\n\n## not working ^^\n\n\n\n\nrawData_CurrentUSD <- rawData_CurrentUSD[-1,]  %>%\n  select(-Notes)\n\n\nrawData_ShareOfGDP <- rawData_ShareOfGDP[-1,] %>%\n  select(-Notes)\n\n\nrawData_ShareOfGovSpend <- rawData_ShareOfGovSpend[-1,] %>%\n  select(-2:-3)\n\n\n#head(rawData_CurrentUSD)\n#head(rawData_ShareOfGDP)\n#head(rawData_ShareOfGovSpend)  \n\n\n\n\nTidying Data\nBefore pivoting my data I wanted to add a column for region. I chose to do this with an algorithm as an exercise in iteration however it would have been more practical to just hard code this column.\n\nBefore Pivoting I am adding a column for region\nThe list of countries where there are NA’s for every year:\n\n\nCode\nRegions <- rawData_CurrentUSD %>%\n  filter(is.na(`1949`))%>%\n  select(Country)\n\n#Regions\n\n\nYou will notice that some of these are continents and sub-continents, I wanted to break these into a Region field. The pattern I saw and chose to exploit was the fact that the overarching category would always be followed with a more specific category such as the Africa followed by North Africa values (see earlier print outs to see raw information).\nI then added an empty vector into the tibble and populated it according to the Country column using the pattern described above.\nOnce I had left flags in the region column dictating where each region was starting I could use fill(Region) in order to populate the rest of the column.\n\n\nCode\nemptyRegionCol <- as.numeric(vector(mode = \"character\",length = length(rawData_CurrentUSD$`1949`)))## doing as numeric here as a hacky way to fill this with NA's, any better way?\n\n\n#adding col to tibble to be populated later\nmutated_CurrentUSD <- rawData_CurrentUSD %>%\n  mutate(Region =emptyRegionCol,.before=2)\n\n#mutated_CurrentUSD\n#head(mutated_CurrentUSD)\n\n\n\n# replacing the empty char in region with actual region if 2 NAs appear in a row\n  #iterates along the indices of the Country vector\nfor(i in seq_along(mutated_CurrentUSD$Country) ){\n  \n   #if 2 nas in a row then do something\n  if(is.na(mutated_CurrentUSD$`1949`[i]) && is.na(mutated_CurrentUSD$`1949`[i+1]) ){\n    mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i+1] #this works because when referencing an index outside of the vector R will return NA - Great to know.\n  }\n  \n  \n  #if the row is a region\n  if(is.na(mutated_CurrentUSD$`1949`[i]) ){\n     mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i]\n  }\n\n  \n}\n\nmutated_CurrentUSD <- mutated_CurrentUSD %>%\n  fill(Region)\n\n#mutated_CurrentUSD \n\n\nNow that I had my proof of concept for cleaning one of the sheets, in a format I wanted, it was time to clean the other sheets as well. I could have taken the above algorithm and modified it to work for each of the other tabs individually however I took the opportunity to practice with functions and created the below function to clean any of the tabs once they were loaded in and lightly cleaned."
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#pivoting-years-and",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#pivoting-years-and",
    "title": "Global Military Spending - An Analysis",
    "section": "Pivoting years and",
    "text": "Pivoting years and\nNext I pivot the years, it should be noted that both the pivoting and Converting of notation could be handled by the clean function above, i kept the steps separate so that I could show the progression of the dataframe.\nI must also handle the xxx and … notations. From the information page of my data set I can see that the notation is described as follows:\n\n\n\n\n\n\n\nRaw Notation\nMeaning\n\n\n\n\n…\nData unavailable\n\n\nxxx\nCountry did not exist or was not independent during all or part of the year in question\n\n\n\nFor now I think I will just keep both as NA but will save this form of the information for future use.\n\n\nCode\nCleaned_currentUSD <- Cleaned_currentUSD %>%\n  pivot_longer(cols=3:ncol(Cleaned_currentUSD),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n  \nCleaned_ShareOfGovSpend <- Cleaned_ShareOfGovSpend %>%\n  pivot_longer(cols=3:ncol(Cleaned_ShareOfGovSpend),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n\nCleaned_ShareOfGDP <- Cleaned_ShareOfGDP %>%\n  pivot_longer(cols=3:ncol(Cleaned_ShareOfGDP),names_to = \"Year\",values_drop_na = FALSE)%>%\n    na_if(\"...\") %>%\n      na_if(\"xxx\")\n\n\n\nConverting column types\nFinally I will convert the column types to their correct representation.\nBefore:\n\n\nCode\n#head(Cleaned_currentUSD)\n#head(Cleaned_ShareOfGovSpend)\n#head(Cleaned_ShareOfGDP)\n\n# Year to date\n#Cleaned_currentUSD$Year<-as_date(Cleaned_currentUSD$Year, format = \"%Y\",tz=NULL)\n#Cleaned_ShareOfGovSpend$Year<-as_date(Cleaned_ShareOfGovSpend$Year, format = \"%Y\",tz=NULL)\n#Cleaned_ShareOfGDP$Year<-as_date(Cleaned_ShareOfGDP$Year, format = \"%Y\",tz=NULL)\n\n#Year to Number\nCleaned_currentUSD$Year<-as.integer(Cleaned_currentUSD$Year)\nCleaned_ShareOfGovSpend$Year<-as.integer(Cleaned_ShareOfGovSpend$Year)\nCleaned_ShareOfGDP$Year<-as.integer(Cleaned_ShareOfGDP$Year)\n\n#Cleaned_currentUSD\n\n\n# value to double\nCleaned_currentUSD$value <- as.numeric(Cleaned_currentUSD$value)\nCleaned_ShareOfGovSpend$value<- as.numeric(Cleaned_ShareOfGovSpend$value)\nCleaned_ShareOfGDP$value<- as.numeric(Cleaned_ShareOfGDP$value)\n\n\n\n\nCode\n#view(Cleaned_ShareOfGovSpend)=\nCleaned_currentUSD <- Cleaned_currentUSD %>%\n  rename(MilitarySpend_currentUSD=value)\n\nCleaned_ShareOfGovSpend <- Cleaned_ShareOfGovSpend%>%\n  rename(MilitarySpend_ShareofGovSpend=value)\n\nCleaned_ShareOfGDP <- Cleaned_ShareOfGDP%>%\n  rename(MilitarySpend_ShareofGDP=value)\n\n#head(Cleaned_currentUSD)\n#head(Cleaned_ShareOfGovSpend)\n#head(Cleaned_ShareOfGDP)"
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#join-of-the-different-tabs",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#join-of-the-different-tabs",
    "title": "Global Military Spending - An Analysis",
    "section": "join of the different tabs",
    "text": "join of the different tabs\nI think we will want to do a full join as we dont want to lose any information.\n\n\nCode\n#nrow(Cleaned_currentUSD)\n#nrow(Cleaned_ShareOfGovSpend)\n#nrow(Cleaned_ShareOfGDP)\n\nMilitarySpend<- full_join(Cleaned_currentUSD,Cleaned_ShareOfGovSpend,by=c(\"Country\",\"Year\",\"Region\"))\n\nMilitarySpend <- full_join(MilitarySpend,Cleaned_ShareOfGDP,by=c(\"Country\",\"Year\",\"Region\"))\n\n\n\n#head(MilitarySpend)\n#nrow(MilitarySpend)"
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#hw3",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#hw3",
    "title": "Global Military Spending - An Analysis",
    "section": "HW3",
    "text": "HW3\n\ndescriptive stats\nmean median and std for each of the tabs Charts for the stats\nFirst I wanted to get an idea of what my data looks like for each of the countries individually. In order to see this I grouped on country name and the types of data we have.\nIn order to get that done I need to pivot my data again.\n\n\nCode\ncombinedData <-MilitarySpend %>%\n  pivot_longer(c(MilitarySpend_currentUSD,MilitarySpend_ShareofGovSpend,MilitarySpend_ShareofGDP),names_to = \"viewOfSpend\",values_to = \"Spend\")\nhead(combinedData)\n\n\n# A tibble: 6 × 5\n  Country Region        Year viewOfSpend                   Spend\n  <chr>   <chr>        <int> <chr>                         <dbl>\n1 Algeria North Africa  1949 MilitarySpend_currentUSD         NA\n2 Algeria North Africa  1949 MilitarySpend_ShareofGovSpend    NA\n3 Algeria North Africa  1949 MilitarySpend_ShareofGDP         NA\n4 Algeria North Africa  1950 MilitarySpend_currentUSD         NA\n5 Algeria North Africa  1950 MilitarySpend_ShareofGovSpend    NA\n6 Algeria North Africa  1950 MilitarySpend_ShareofGDP         NA\n\n\nCode\n#view(combinedData)\nna.omit(combinedData)\n\n\n# A tibble: 20,270 × 5\n   Country Region        Year viewOfSpend                 Spend\n   <chr>   <chr>        <int> <chr>                       <dbl>\n 1 Algeria North Africa  1963 MilitarySpend_currentUSD  66.4   \n 2 Algeria North Africa  1963 MilitarySpend_ShareofGDP   0.0251\n 3 Algeria North Africa  1964 MilitarySpend_currentUSD  99.9   \n 4 Algeria North Africa  1964 MilitarySpend_ShareofGDP   0.0350\n 5 Algeria North Africa  1965 MilitarySpend_currentUSD 106.    \n 6 Algeria North Africa  1965 MilitarySpend_ShareofGDP   0.0342\n 7 Algeria North Africa  1966 MilitarySpend_currentUSD 105.    \n 8 Algeria North Africa  1966 MilitarySpend_ShareofGDP   0.0351\n 9 Algeria North Africa  1967 MilitarySpend_currentUSD  99.2   \n10 Algeria North Africa  1967 MilitarySpend_ShareofGDP   0.0302\n# … with 20,260 more rows\n\n\nBelow is a graph depicting the completeness of the information available to us.\nTotal number of countries analyzed:\n\n\nCode\ncombinedData%>%\n  distinct(Country)%>%\n  count()\n\n\n# A tibble: 1 × 1\n      n\n  <int>\n1   173\n\n\nNumber of countries by region:\n\n\nCode\ncombinedData%>%\n  group_by(Region)%>%\n  summarise(numberCountries = n_distinct(Country))%>%\n  arrange(desc(numberCountries))\n\n\n# A tibble: 14 × 2\n   Region                            numberCountries\n   <chr>                                       <int>\n 1 sub-Saharan Africa                             47\n 2 Central Europe                                 20\n 3 Western Europe                                 20\n 4 Middle East                                    16\n 5 Central America and the Caribbean              13\n 6 South America                                  11\n 7 South East Asia                                11\n 8 Eastern Europe                                  8\n 9 East Asia                                       6\n10 South Asia                                      6\n11 Central Asia                                    5\n12 North Africa                                    4\n13 Oceania                                         4\n14 North America                                   2\n\n\nDue to the number of Regions I may need to focus on some in particular…\nby year, how many countries had data available\n\n\n\nStatistics by region\n#MilitarySpend_currentUSD\n\n\nCode\nbyRegionStats_USD<-combinedData%>%\n  group_by(Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE),\n            min = min(Spend, na.rm=TRUE),\n            max = max(Spend, na.rm=TRUE))%>%\n  arrange(desc(mean))%>%\n  filter(viewOfSpend==\"MilitarySpend_currentUSD\")\n\nbyRegionStats_USD\n\n\n# A tibble: 14 × 6\n# Groups:   Region [14]\n   Region                            viewOfSpend       mean    std    min    max\n   <chr>                             <chr>            <dbl>  <dbl>  <dbl>  <dbl>\n 1 North America                     MilitarySpend_… 1.49e5 2.23e5 426.   8.01e5\n 2 East Asia                         MilitarySpend_… 2.04e4 4.21e4   0    2.93e5\n 3 Eastern Europe                    MilitarySpend_… 1.09e4 3.52e4   0    2.46e5\n 4 Western Europe                    MilitarySpend_… 7.14e3 1.28e4   0    7.34e4\n 5 Middle East                       MilitarySpend_… 4.91e3 9.59e3   0    8.72e4\n 6 South Asia                        MilitarySpend_… 4.46e3 1.17e4   2.48 7.66e4\n 7 Oceania                           MilitarySpend_… 2.83e3 6.29e3   0    3.18e4\n 8 South America                     MilitarySpend_… 2.02e3 4.57e3   0    3.69e4\n 9 South East Asia                   MilitarySpend_… 1.94e3 3.16e3   0    3.30e4\n10 North Africa                      MilitarySpend_… 1.36e3 2.21e3   3.71 1.04e4\n11 Central Europe                    MilitarySpend_… 1.14e3 2.20e3   0    1.74e4\n12 Central Asia                      MilitarySpend_… 3.36e2 5.92e2   0    2.55e3\n13 Central America and the Caribbean MilitarySpend_… 2.86e2 9.54e2   0    8.68e3\n14 sub-Saharan Africa                MilitarySpend_… 2.58e2 6.35e2   0    6.85e3\n\n\n#MilitarySpend_ShareOfGDP\n\n\nCode\nbyRegionStats_GDP<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE),\n            min = min(Spend, na.rm=TRUE),\n            max = max(Spend, na.rm=TRUE))%>%\n  arrange(desc(mean))%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGDP\")\n\nbyRegionStats_GDP\n\n\n# A tibble: 1,022 × 7\n# Groups:   Region, viewOfSpend [14]\n   Region      viewOfSpend               Year  mean    std    min   max\n   <chr>       <chr>                    <int> <dbl>  <dbl>  <dbl> <dbl>\n 1 Middle East MilitarySpend_ShareofGDP  1991 0.167 0.305  0.0217 1.17 \n 2 Middle East MilitarySpend_ShareofGDP  1975 0.147 0.0893 0.0153 0.305\n 3 Middle East MilitarySpend_ShareofGDP  1959 0.146 0.112  0.0671 0.225\n 4 Middle East MilitarySpend_ShareofGDP  1976 0.139 0.0800 0.0175 0.292\n 5 Middle East MilitarySpend_ShareofGDP  1974 0.131 0.0765 0.0319 0.277\n 6 Middle East MilitarySpend_ShareofGDP  1982 0.117 0.0539 0.0430 0.193\n 7 Middle East MilitarySpend_ShareofGDP  1973 0.117 0.0828 0.0186 0.279\n 8 Middle East MilitarySpend_ShareofGDP  1978 0.115 0.0619 0.0419 0.229\n 9 Middle East MilitarySpend_ShareofGDP  1985 0.113 0.0661 0.0353 0.248\n10 Middle East MilitarySpend_ShareofGDP  1977 0.112 0.0632 0.0208 0.231\n# … with 1,012 more rows\n\n\n\n\nCode\nbyRegionStats_GDP\n\n\n# A tibble: 1,022 × 7\n# Groups:   Region, viewOfSpend [14]\n   Region      viewOfSpend               Year  mean    std    min   max\n   <chr>       <chr>                    <int> <dbl>  <dbl>  <dbl> <dbl>\n 1 Middle East MilitarySpend_ShareofGDP  1991 0.167 0.305  0.0217 1.17 \n 2 Middle East MilitarySpend_ShareofGDP  1975 0.147 0.0893 0.0153 0.305\n 3 Middle East MilitarySpend_ShareofGDP  1959 0.146 0.112  0.0671 0.225\n 4 Middle East MilitarySpend_ShareofGDP  1976 0.139 0.0800 0.0175 0.292\n 5 Middle East MilitarySpend_ShareofGDP  1974 0.131 0.0765 0.0319 0.277\n 6 Middle East MilitarySpend_ShareofGDP  1982 0.117 0.0539 0.0430 0.193\n 7 Middle East MilitarySpend_ShareofGDP  1973 0.117 0.0828 0.0186 0.279\n 8 Middle East MilitarySpend_ShareofGDP  1978 0.115 0.0619 0.0419 0.229\n 9 Middle East MilitarySpend_ShareofGDP  1985 0.113 0.0661 0.0353 0.248\n10 Middle East MilitarySpend_ShareofGDP  1977 0.112 0.0632 0.0208 0.231\n# … with 1,012 more rows\n\n\nCode\nbyRegionStats_GDP<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGDP\")\n\n\nbyRegionStats_GDP$Region<-as.factor(byRegionStats_GDP$Region)\n\nviolin_gdp<- na.omit(byRegionStats_GDP)%>%\n  ggplot(aes(x=Region,y=Spend))+\n  geom_violin()\n\nviolin_gdp\n\n\n\n\n\nCode\n# removing NAs here because I cant plot them, I dont want to replace them with other values because it is still of interest when countries do not display thier data.\n\n\nSomething crazy going on with the middle east spend…\nInvestigation:\n\n\nCode\nbyRegionStats_GDP%>%\n  filter(Region==\"Middle East\")%>%\n  arrange(desc(Spend))%>%\n  head()\n\n\n# A tibble: 6 × 5\n# Groups:   Region, viewOfSpend, Year [6]\n  Country Region       Year viewOfSpend              Spend\n  <chr>   <fct>       <int> <chr>                    <dbl>\n1 Kuwait  Middle East  1991 MilitarySpend_ShareofGDP 1.17 \n2 Kuwait  Middle East  1990 MilitarySpend_ShareofGDP 0.485\n3 Kuwait  Middle East  1992 MilitarySpend_ShareofGDP 0.318\n4 Israel  Middle East  1975 MilitarySpend_ShareofGDP 0.305\n5 Israel  Middle East  1976 MilitarySpend_ShareofGDP 0.292\n6 Israel  Middle East  1973 MilitarySpend_ShareofGDP 0.279\n\n\nI initially thought this was a typo since there were two 1s in a row and it was such an insanely high spend. A share of GDP of 1.173 means Kuwait spent 117.3% of their GDP in that year. Upon further investigation, this is actually true and it was driven by the Persian Gulf War(aka Gulf War). The Gulf War was a conflict triggered by Iraqs invasion of Kuwait in 1990. This invasion was the first major international crisis since the Cold War and will certainly be a topic I add to the final analysis.\nhttps://www.britannica.com/event/Persian-Gulf-War\nLoosely I will plot Iraq, US and Kuwait spend from 1985-1995 to explore this time period.\n\n\nCode\ncombinedData%>%\n  filter(Year >= 1985 & Year<= 1995)%>%\n  filter(Country %in% c(\"United States of America\",\"Iraq\",\"Kuwait\"))%>%\n    ggplot(mapping=aes(y = Spend, x = Year))+  \n    geom_point(aes(color = Country, alpha=0.9),na.rm = FALSE)+\n    facet_wrap(vars(`viewOfSpend`),scales = \"free_y\")\n\n\n\n\n\nA new discovery! Iraq chose not to report their spending during the Gulf war.\n\n\nCode\ncombinedData%>%\n  filter(Country %in% c(\"Iraq\"))%>%\n  filter(Year>1960&is.na(Spend))\n\n\n# A tibble: 87 × 5\n   Country Region       Year viewOfSpend                   Spend\n   <chr>   <chr>       <int> <chr>                         <dbl>\n 1 Iraq    Middle East  1961 MilitarySpend_ShareofGovSpend    NA\n 2 Iraq    Middle East  1962 MilitarySpend_ShareofGovSpend    NA\n 3 Iraq    Middle East  1963 MilitarySpend_ShareofGovSpend    NA\n 4 Iraq    Middle East  1964 MilitarySpend_ShareofGovSpend    NA\n 5 Iraq    Middle East  1965 MilitarySpend_ShareofGovSpend    NA\n 6 Iraq    Middle East  1966 MilitarySpend_ShareofGovSpend    NA\n 7 Iraq    Middle East  1967 MilitarySpend_ShareofGovSpend    NA\n 8 Iraq    Middle East  1968 MilitarySpend_ShareofGovSpend    NA\n 9 Iraq    Middle East  1969 MilitarySpend_ShareofGovSpend    NA\n10 Iraq    Middle East  1970 MilitarySpend_ShareofGovSpend    NA\n# … with 77 more rows\n\n\nCode\nstartOfNoData<- 1982\nendofNoData<- 2003\n\niraqMissingDataPLOT<-combinedData%>%\n  filter(Country %in% c(\"Iraq\"))%>%\n    ggplot(mapping=aes( x = Spend,y = Year))+\n    geom_point()+\n    facet_wrap(vars(`viewOfSpend`),scales = \"free_x\")+\n    scale_y_continuous(labels=seq(1949,2020,5),breaks =seq(1949,2020,5))\n\niraqMissingDataPLOT+\ngeom_hline(yintercept = 1982,color=\"red\")+\ngeom_hline(yintercept = 2003,color=\"red\")+\nannotate(\"text\",x=0,y=1982,label=\"1982\",hjust=-0.1,vjust=-0.1,color=\"red\")+\nannotate(\"text\",x=0,y=2003,label=\"2003\",hjust=-0.1,vjust=1.1,color=\"red\")\n\n\n\n\n\nBelow I continue to explore the dataset. This might be too much for HW3 however I was doing it anyway for the final so I chose to include it.\n#MilitarySpend_ShareofGovSpend\nNow to explore the statistics within the Share of Gov Spend tab.\n\n\nCode\nbyRegionStats_GovSpend<-combinedData%>%\n  group_by(Region,viewOfSpend)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE,sigfig=1),\n            std = sd(Spend, na.rm=TRUE),\n            min = min(Spend, na.rm=TRUE),\n            max = max(Spend, na.rm=TRUE))%>%\n  arrange(desc(mean))%>%\n  filter(viewOfSpend==\"MilitarySpend_ShareofGovSpend\")\n\nbyRegionStats_GovSpend\n\n\n# A tibble: 14 × 6\n# Groups:   Region [14]\n   Region                            viewOfSpend      mean    std     min    max\n   <chr>                             <chr>           <dbl>  <dbl>   <dbl>  <dbl>\n 1 Middle East                       MilitarySpend… 0.161  0.0757 1.83e-2 0.575 \n 2 South Asia                        MilitarySpend… 0.122  0.0511 3.65e-2 0.274 \n 3 Eastern Europe                    MilitarySpend… 0.115  0.0887 7.71e-3 0.350 \n 4 sub-Saharan Africa                MilitarySpend… 0.103  0.260  3.90e-3 5.82  \n 5 South East Asia                   MilitarySpend… 0.0984 0.0749 7.83e-3 0.392 \n 6 North Africa                      MilitarySpend… 0.0973 0.0369 2.81e-2 0.178 \n 7 East Asia                         MilitarySpend… 0.0837 0.0503 1.73e-2 0.225 \n 8 South America                     MilitarySpend… 0.0728 0.0392 1.73e-2 0.348 \n 9 North America                     MilitarySpend… 0.0600 0.0386 2.51e-2 0.124 \n10 Central Asia                      MilitarySpend… 0.0518 0.0269 1.36e-2 0.158 \n11 Oceania                           MilitarySpend… 0.0441 0.0155 1.54e-2 0.0879\n12 Central Europe                    MilitarySpend… 0.0439 0.0283 6.72e-4 0.290 \n13 Central America and the Caribbean MilitarySpend… 0.0354 0.0218 2.20e-3 0.214 \n14 Western Europe                    MilitarySpend… 0.0348 0.0189 8.58e-3 0.164 \n\n\n\n\nCode\nlibrary(forcats)\n\nstatsViz2<- byRegionStats%>%\n  mutate(test = fct_reorder(Region,desc(mean)))%>%\n  ggplot(mapping=aes(y = test, x = mean))+  \n  geom_point(aes(color = Region, alpha=0.9, size=max)) +\n  guides(alpha=\"none\",color=\"none\") +\n  labs(title = \"Certain Regions on average spend more than others\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\n\nError in mutate(., test = fct_reorder(Region, desc(mean))): object 'byRegionStats' not found\n\n\nCode\nstatsViz2\n\n\nError in eval(expr, envir, enclos): object 'statsViz2' not found\n\n\nFrom this we can see the top 6 spenders reside in a cluster of there own when you look at their mean and max spend.\nTop 6\n\n\nCode\ntop6_mean <- byRegionStats %>%\n  head()%>%\n  pull(Region)\n\n\nError in head(.): object 'byRegionStats' not found\n\n\nCode\ntop6_mean\n\n\nError in eval(expr, envir, enclos): object 'top6_mean' not found"
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#faceted-by-region-show-mean-spend-of-each-country",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#faceted-by-region-show-mean-spend-of-each-country",
    "title": "Global Military Spending - An Analysis",
    "section": "Faceted by Region, show mean spend of each country",
    "text": "Faceted by Region, show mean spend of each country\n\n\nCode\nbyRegionStats_year<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE))%>%\n  filter(viewOfSpend == 'MilitarySpend_currentUSD')%>%\n  arrange(desc(mean))\n\nbyRegionStats_year\n\n\n# A tibble: 1,022 × 4\n# Groups:   Region, viewOfSpend [14]\n   Region        viewOfSpend               Year    mean\n   <chr>         <chr>                    <int>   <dbl>\n 1 North America MilitarySpend_currentUSD  2021 413561.\n 2 North America MilitarySpend_currentUSD  2020 400839.\n 3 North America MilitarySpend_currentUSD  2011 386841.\n 4 North America MilitarySpend_currentUSD  2010 378660.\n 5 North America MilitarySpend_currentUSD  2019 378369.\n 6 North America MilitarySpend_currentUSD  2012 372829.\n 7 North America MilitarySpend_currentUSD  2009 362427.\n 8 North America MilitarySpend_currentUSD  2018 352610.\n 9 North America MilitarySpend_currentUSD  2013 348872.\n10 North America MilitarySpend_currentUSD  2008 338049.\n# … with 1,012 more rows\n\n\nCode\nbyRegionplot<- byRegionStats_year%>%\n  ggplot(mapping=aes(x = Year, y = mean))+  \n  geom_point(aes(color = Region, alpha=0.9,)) +\n  theme_light() +\n  guides(alpha=\"none\") +\n  labs(title = \"East asia and NA dominate raw spend\" ,caption = \"Data from SPIRI Military Expediture Database\")\n  \n\nbyRegionplot"
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#to-be-a-little-less-noisy-showing-the-top-6-spenders-over-time",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#to-be-a-little-less-noisy-showing-the-top-6-spenders-over-time",
    "title": "Global Military Spending - An Analysis",
    "section": "To be a little less noisy, showing the top 6 spenders over time",
    "text": "To be a little less noisy, showing the top 6 spenders over time\n\n\nCode\nbyRegionStats_year_top6<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  summarize(mean = mean(Spend, na.rm=TRUE))%>%\n  filter(viewOfSpend == 'MilitarySpend_currentUSD')%>%\n  filter(Region == top6_mean)%>%\n  arrange(desc(mean))\n\n\nError in `filter()`:\n! Problem while computing `..1 = Region == top6_mean`.\nℹ The error occurred in group 1: Region = \"Central America and the Caribbean\",\n  viewOfSpend = \"MilitarySpend_currentUSD\".\nCaused by error in `mask$eval_all_filter()`:\n! object 'top6_mean' not found\n\n\nCode\nbyRegionplot_top6<- byRegionStats_year_top6%>%\n  ggplot(mapping=aes(x = Year, y = mean))+  \n  geom_point(aes(color = Region, alpha=0.9,)) +\n  facet_wrap(vars(`Region`),scales = \"free_y\") +\n  labs(title = \"Raw Mean Spend in USD for top 6 Spenders\" ,caption = \"Data from SPIRI Military Expediture Database\")\n\n\nError in ggplot(., mapping = aes(x = Year, y = mean)): object 'byRegionStats_year_top6' not found\n\n\nCode\nbyRegionplot_top6\n\n\nError in eval(expr, envir, enclos): object 'byRegionplot_top6' not found\n\n\nFrom here I choose to investigate Eastern Europe.\n\n\nCode\nbyRegionStats_year_EasternEurope<-combinedData%>%\n  group_by(Region,viewOfSpend,Year)%>%\n  filter(viewOfSpend == 'MilitarySpend_currentUSD')%>%\n  filter(Region == 'Eastern Europe')\n\nEasternEuropePlot<- byRegionStats_year_EasternEurope%>%\n  ggplot(mapping=aes(x = Year, y = Spend))+  \n  geom_point(aes(color = Country, alpha=0.9)) +\n  facet_wrap(vars(`Country`)) +\n  labs(title = \"Raw Mean Spend in Eastern Europe\" ,caption = \"Data from SPIRI Military Expediture Database\")\n  \nEasternEuropePlot\n\n\n\n\n\nI left the scaling fixed so that you could see how high USSR spending was and then they become Russia and spend slowley increases but not to the levels of the USSR.\nWe see here that USSR spending looks to be about late 1990s, following some research I found…\n\n\nCode\nstartOfNoData<- 1985\nendofNoData<- 1995\n\nUSSRSpendPlot<-combinedData%>%\n  filter(Country %in% c(\"USSR\",\"Russia\"))%>%\n  filter(viewOfSpend == 'MilitarySpend_currentUSD')%>%\n    ggplot(mapping=aes( x = Year,y = Spend,color = `Country`))+\n    geom_point()\n\n\nUSSRSpendPlot+\ngeom_vline(xintercept = 1987,color=\"red\")+\nannotate(\"text\",x=1987,y=0,label=\"1987\",hjust=1,vjust=-0,color=\"red\")+\n  \ngeom_vline(xintercept = 1990,color=\"red\")+\nannotate(\"text\",x=1990,y=0,label=\"1990\",hjust=-0,vjust=-0,color=\"red\")+\n\ngeom_vline(xintercept = 1945,color=\"black\")+\nannotate(\"text\",x=1945,y=0,label=\"Start of Cold War, Defeat of Germany and Japan\",hjust=-0,vjust=-0,color=\"black\",angle='90')+\n  \ngeom_vline(xintercept = 1985,color=\"black\")+\nannotate(\"text\",x=1985,y=0,label=\"Mikhail Gorbachev becomes leader of the USSR\",hjust=-0.1,vjust=-0,color=\"black\",angle='90')+\n  \ngeom_vline(xintercept = 1991,color=\"black\")+\nannotate(\"text\",x=1991,y=0,label=\"1991 End of Cold War, USSR disbanded\",hjust=-0.1,vjust=1,color=\"black\",angle='90')\n\n\n\n\n\nFrom this view of the information it leads one to believe the USSR spending was very high during the cold war, I am unsure why we were given access to their spending towards the end of the cold war, it could have been a show of good faith, more research to come. Once the war was declared over, Russian spending fell to 0 in the following year and slowly started creeping back up.\nAnother area of research i get from this would be what happened in 2003- 2004 to spike the trend in Russian spending."
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#hw-questions",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#hw-questions",
    "title": "Global Military Spending - An Analysis",
    "section": "HW Questions",
    "text": "HW Questions\n\nwhat questions are left unanswered?\nRussian spending during 2003-2004\nThe rest of my outlined possible questions from HW2 will still be answered, their answer was just not obvious from the summary statistics.\n\n\nwhat may be unclear?\nIn order to improve some of these charts I could include some information about global spending at the time or perhaps augment the information to display it as a portion of global military spend? - If i showed percent of global(or engaged parties) military spend it would be interesting to show if just spending more would win the wars.\n\n\nHow could I improve these vizualizations\nI could change the view of some of these visualizations so that they are more appealing. I liked the use of the violin plot for the Gulf War analysis and will use more variety in graphs for the final."
  },
  {
    "objectID": "posts/Final Project _HW3_ JulianCastoro.html#conclusion",
    "href": "posts/Final Project _HW3_ JulianCastoro.html#conclusion",
    "title": "Global Military Spending - An Analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\n“Every gun that is made, every warship launched, every rocket fired signifies in the final sense, a theft from those who hunger and are not fed, those who are cold and are not clothed. > > This world in arms is not spending money alone. It is spending the sweat of its laborers, the genius of its scientists, the hopes of its children. This is not a way of life at all in any > true sense. Under the clouds of war, it is humanity hanging on a cross of iron.”\n\nDwight D. Eisenhower\nCitations: https://www.goodreads.com/quotes/tag/military-budget#:~:text=%E2%80%9CEvery%20gun%20that%20is%20made,is%20not%20spending%20money%20alone.\nhttps://www.sipri.org/about\nR Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media.\nhttps://www.investopedia.com/terms/g/gdp.asp\nAppendix: Full descriptions as provided by SIPRI:\n\nIntroduction\nEstimates of world, regional and sub-regional totals in constant (2019) US$ (billions).\nData for military expenditure by country in current price local currency, presented according to each country’s financial year.\nData for military expenditure by country in current price local currency, presented according to calendar year.\nData for military expenditure by country in constant price (2019) US$ (millions), presented according to calendar year, and in current US$m. for 2020.\nData for military expenditure by country in current US$ (millions), presented according to calendar year.\nData for military expenditure by country as a share of GDP, presented according to calendar year.\nData for military expenditure per capita, in current US$, presented according to calender year. (1988-2020 only)\nData for military expenditure as a percentage of general government expenditure. (1988-2020 only)"
  },
  {
    "objectID": "posts/Final Project __ HW2__ JulianCastoro.html",
    "href": "posts/Final Project __ HW2__ JulianCastoro.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Final Project __ HW2__ JulianCastoro.html#introduction",
    "href": "posts/Final Project __ HW2__ JulianCastoro.html#introduction",
    "title": "Homework 2",
    "section": "Introduction",
    "text": "Introduction\n\nData choice overview\nI initially chose this data set because recent events abroad have certainly escalated a global fear of war and I wanted to see how the tides of military spending ebbed and flowed over the course of what we have been tracking. As I cleaned the data more and more questions bled from the raw information infront of me. I am excited to see what sort of trends come from analyzing this data set.\nI plan to show how different countries value military spending and how that value changes over the course of time. Ideally I will be able to explain patterns in what I see with global or locally important historic events. How did the US spending change after 9/11 or more recently did we see anyone bolstering their defenses before news broke of the Russian invasion of Ukraine? How did spending change globally after the first and then second world war? While I am not a statistical expert and will not be able to refute a causation vs correlation argument, visualizations of these events and the corresponding spending patterns will still be interesting and hopefully provoking of conversation."
  },
  {
    "objectID": "posts/Final Project __ HW2__ JulianCastoro.html#data",
    "href": "posts/Final Project __ HW2__ JulianCastoro.html#data",
    "title": "Homework 2",
    "section": "Data",
    "text": "Data\n\nBriefly describe the data\nBelow is a list of the available sheets in the SIPRI military spending data export. A few of the tabs are the same base information altered to reflect a specific currency or ratio. I am choosing to use the “Current USD” as my source of raw spending numbers as I believe it to be the easiest to understand and relate to. I will also be using “share of GDP” as well as “Share of Govt. spending” to provide context around a nations spending compared to their population they intend on defending as well as compared to overall spending.\n\n\nCode\nsheets <- excel_sheets(\"_data/SIPRI-Milex-data-1949-2021.xlsx\")\nsheets\n\n\n [1] \"Front page\"                     \"Regional totals\"               \n [3] \"Local currency financial years\" \"Local currency calendar years\" \n [5] \"Constant (2020) USD\"            \"Current USD\"                   \n [7] \"Share of GDP\"                   \"Per capita\"                    \n [9] \"Share of Govt. spending\"        \"Footnotes\"                     \n\n\n\n\nNarative and variables in the dataset\nThe data provides a narrative around the military spending for all countries where the information was accessible at the time. The variables in each sheet are different however they are all reflective of spending on military budget for those countries in different forms i.e. USD, SHare of GDP, per capita, etc.\nHere is a peek at what the raw information looks like.\n\n\nCode\nrawrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9])\nhead(rawrawData_ShareOfGovSpend)\n\n\n# A tibble: 6 × 37\n  Military e…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10 ...11 ...12\n  <chr>        <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n1 \"Countries … <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n2 \"Figures ar… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n3 \"Data for g… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n4 \"Figures in… <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n5 \"\\\". .\\\" = … <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n6  <NA>        <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA> \n# … with 25 more variables: ...13 <chr>, ...14 <chr>, ...15 <chr>, ...16 <chr>,\n#   ...17 <chr>, ...18 <chr>, ...19 <chr>, ...20 <chr>, ...21 <chr>,\n#   ...22 <chr>, ...23 <chr>, ...24 <chr>, ...25 <chr>, ...26 <chr>,\n#   ...27 <chr>, ...28 <chr>, ...29 <chr>, ...30 <chr>, ...31 <chr>,\n#   ...32 <chr>, ...33 <chr>, ...34 <chr>, ...35 <chr>, ...36 <chr>,\n#   ...37 <chr>, and abbreviated variable name\n#   ¹​`Military expenditure by country as percentage of government spending, 1949-2021         © SIPRI 2021`\n\n\n\n\nReading in Raw information\nReading in raw information\nThe data provided by the Stockholm International Peace Research Institute(sipri) was separated into multiple tabs in one excel .xlsx file.\nIn order to begin working I imported the tabs I planned to utilize, skipping over some of the notes and title rows at the start of each tab.\n\n\nCode\nrawData_CurrentUSD <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[6],skip=5)\nrawData_ShareOfGDP <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[7],skip=5)\nrawData_ShareOfGovSpend <- read_excel(\"_data/SIPRI-Milex-data-1949-2021.xlsx\",sheet=sheets[9],skip=7)\n\n\n\nhead(rawData_CurrentUSD)\n\n\n# A tibble: 6 × 75\n  Country   Notes `1949` `1950` `1951` `1952` `1953` `1954` `1955` `1956` `1957`\n  <chr>     <chr> <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n1 <NA>      <NA>  <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n2 Africa    <NA>  <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n3 North Af… <NA>  <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n4 Algeria   §4    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n5 Libya     ‡§¶16 xxx    xxx    ...    ...    ...    ...    ...    ...    ...   \n6 Morocco   §17   xxx    xxx    xxx    xxx    xxx    xxx    xxx    23.71… 35.40…\n# … with 64 more variables: `1958` <chr>, `1959` <chr>, `1960` <chr>,\n#   `1961` <chr>, `1962` <chr>, `1963` <chr>, `1964` <chr>, `1965` <chr>,\n#   `1966` <chr>, `1967` <chr>, `1968` <chr>, `1969` <chr>, `1970` <chr>,\n#   `1971` <chr>, `1972` <chr>, `1973` <chr>, `1974` <chr>, `1975` <chr>,\n#   `1976` <chr>, `1977` <chr>, `1978` <chr>, `1979` <chr>, `1980` <chr>,\n#   `1981` <chr>, `1982` <chr>, `1983` <chr>, `1984` <chr>, `1985` <chr>,\n#   `1986` <chr>, `1987` <chr>, `1988` <chr>, `1989` <chr>, `1990` <chr>, …\n\n\nCode\n#head(rawData_ShareOfGDP)\n#head(rawData_ShareOfGovSpend)\n\n\nNext I delete the first row of NA as well as the Notes column for each tibble.\n\n\nCode\n## trying Purr here to be cleaner, we have not covered this yet so please let me know if this could be better.\n\n#tibleList <- lst(rawData_CurrentUSD, rawData_ShareOfGDP, rawData_ShareOfGovSpend)\n\n#modify(tibleList,select(-1))\n#map(tibleList,slice(-1))\n\n## not working ^^\n\n\n\n\nrawData_CurrentUSD <- rawData_CurrentUSD[-1,]  %>%\n  select(-Notes)\n\n\nrawData_ShareOfGDP <- rawData_ShareOfGDP[-1,] %>%\n  select(-Notes)\n\n\nrawData_ShareOfGovSpend <- rawData_ShareOfGovSpend[-1,] %>%\n  select(-2:-3)\n\n\nhead(rawData_CurrentUSD)\n\n\n# A tibble: 6 × 74\n  Country  `1949` `1950` `1951` `1952` `1953` `1954` `1955` `1956` `1957` `1958`\n  <chr>    <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n1 Africa   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n2 North A… <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n3 Algeria  xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n4 Libya    xxx    xxx    ...    ...    ...    ...    ...    ...    ...    ...   \n5 Morocco  xxx    xxx    xxx    xxx    xxx    xxx    xxx    23.71… 35.40… 41.69…\n6 Tunisia  xxx    xxx    xxx    xxx    xxx    xxx    xxx    3.714… 6.411… 9.523…\n# … with 63 more variables: `1959` <chr>, `1960` <chr>, `1961` <chr>,\n#   `1962` <chr>, `1963` <chr>, `1964` <chr>, `1965` <chr>, `1966` <chr>,\n#   `1967` <chr>, `1968` <chr>, `1969` <chr>, `1970` <chr>, `1971` <chr>,\n#   `1972` <chr>, `1973` <chr>, `1974` <chr>, `1975` <chr>, `1976` <chr>,\n#   `1977` <chr>, `1978` <chr>, `1979` <chr>, `1980` <chr>, `1981` <chr>,\n#   `1982` <chr>, `1983` <chr>, `1984` <chr>, `1985` <chr>, `1986` <chr>,\n#   `1987` <chr>, `1988` <chr>, `1989` <chr>, `1990` <chr>, `1991` <chr>, …\n\n\nCode\n#head(rawData_ShareOfGDP)\n#head(rawData_ShareOfGovSpend)\n\n\n\n\nTidying Data\nBefore pivoting my data I wanted to add a column for region. I chose to do this with an algorithm as an exercise in iteration however it would have been more practical to just hard code this column.\n\nBefore Pivoting I am adding a column for region\nThe list of countries where there are NA’s for every year:\n\n\nCode\nRegions <- rawData_CurrentUSD %>%\n  filter(is.na(`1949`))%>%\n  select(Country)\n\nRegions\n\n\n# A tibble: 18 × 1\n   Country                          \n   <chr>                            \n 1 Africa                           \n 2 North Africa                     \n 3 sub-Saharan Africa               \n 4 Americas                         \n 5 Central America and the Caribbean\n 6 North America                    \n 7 South America                    \n 8 Asia & Oceania                   \n 9 Oceania                          \n10 South Asia                       \n11 East Asia                        \n12 South East Asia                  \n13 Central Asia                     \n14 Europe                           \n15 Central Europe                   \n16 Eastern Europe                   \n17 Western Europe                   \n18 Middle East                      \n\n\nYou will notice that some of these are continents and sub-continents, I wanted to break these into a Region field. The pattern I saw and chose to exploit was the fact that the overarching category would always be followed with a more specific category such as the Africa followed by North Africa values (see earlier print outs to see raw information).\nI then added an empty vector into the tibble and populated it according to the Country column using the pattern described above.\nOnce I had left flags in the region column dictating where each region was starting I could use fill(Region) in order to populate the rest of the column.\n\n\nCode\nemptyRegionCol <- as.numeric(vector(mode = \"character\",length = length(rawData_CurrentUSD$`1949`)))## doing as numeric here as a hacky way to fill this with NA's, any better way?\n\n\n#adding col to tibble to be populated later\nmutated_CurrentUSD <- rawData_CurrentUSD %>%\n  mutate(Region =emptyRegionCol,.before=2)\n\n#mutated_CurrentUSD\n#head(mutated_CurrentUSD)\n\n\n\n# replacing the empty char in region with actual region if 2 NAs appear in a row\n  #iterates along the indices of the Country vector\nfor(i in seq_along(mutated_CurrentUSD$Country) ){\n  \n   #if 2 nas in a row then do something\n  if(is.na(mutated_CurrentUSD$`1949`[i]) && is.na(mutated_CurrentUSD$`1949`[i+1]) ){\n    mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i+1] #this works because when referencing an index outside of the vector R will return NA - Great to know.\n  }\n  \n  \n  #if the row is a region\n  if(is.na(mutated_CurrentUSD$`1949`[i]) ){\n     mutated_CurrentUSD$Region[i+1] <- mutated_CurrentUSD$Country[i]\n  }\n\n  \n}\n\nmutated_CurrentUSD <- mutated_CurrentUSD %>%\n  fill(Region)\n\nmutated_CurrentUSD \n\n\n# A tibble: 191 × 75\n   Country Region `1949` `1950` `1951` `1952` `1953` `1954` `1955` `1956` `1957`\n   <chr>   <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n 1 Africa  <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 2 North … Africa <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 3 Algeria North… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 4 Libya   North… xxx    xxx    ...    ...    ...    ...    ...    ...    ...   \n 5 Morocco North… xxx    xxx    xxx    xxx    xxx    xxx    xxx    23.71… 35.40…\n 6 Tunisia North… xxx    xxx    xxx    xxx    xxx    xxx    xxx    3.714… 6.411…\n 7 sub-Sa… North… <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 8 Angola  sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 9 Benin   sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n10 Botswa… sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n# … with 181 more rows, and 64 more variables: `1958` <chr>, `1959` <chr>,\n#   `1960` <chr>, `1961` <chr>, `1962` <chr>, `1963` <chr>, `1964` <chr>,\n#   `1965` <chr>, `1966` <chr>, `1967` <chr>, `1968` <chr>, `1969` <chr>,\n#   `1970` <chr>, `1971` <chr>, `1972` <chr>, `1973` <chr>, `1974` <chr>,\n#   `1975` <chr>, `1976` <chr>, `1977` <chr>, `1978` <chr>, `1979` <chr>,\n#   `1980` <chr>, `1981` <chr>, `1982` <chr>, `1983` <chr>, `1984` <chr>,\n#   `1985` <chr>, `1986` <chr>, `1987` <chr>, `1988` <chr>, `1989` <chr>, …\n\n\nWith my tibble in this state I can now remove the header rows.\n\n\nCode\nmutated_CurrentUSD <- mutated_CurrentUSD %>%\n  filter(!is.na(`1949`))\nmutated_CurrentUSD\n\n\n# A tibble: 173 × 75\n   Country Region `1949` `1950` `1951` `1952` `1953` `1954` `1955` `1956` `1957`\n   <chr>   <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n 1 Algeria North… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 2 Libya   North… xxx    xxx    ...    ...    ...    ...    ...    ...    ...   \n 3 Morocco North… xxx    xxx    xxx    xxx    xxx    xxx    xxx    23.71… 35.40…\n 4 Tunisia North… xxx    xxx    xxx    xxx    xxx    xxx    xxx    3.714… 6.411…\n 5 Angola  sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 6 Benin   sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 7 Botswa… sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 8 Burkin… sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n 9 Burundi sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n10 Camero… sub-S… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n# … with 163 more rows, and 64 more variables: `1958` <chr>, `1959` <chr>,\n#   `1960` <chr>, `1961` <chr>, `1962` <chr>, `1963` <chr>, `1964` <chr>,\n#   `1965` <chr>, `1966` <chr>, `1967` <chr>, `1968` <chr>, `1969` <chr>,\n#   `1970` <chr>, `1971` <chr>, `1972` <chr>, `1973` <chr>, `1974` <chr>,\n#   `1975` <chr>, `1976` <chr>, `1977` <chr>, `1978` <chr>, `1979` <chr>,\n#   `1980` <chr>, `1981` <chr>, `1982` <chr>, `1983` <chr>, `1984` <chr>,\n#   `1985` <chr>, `1986` <chr>, `1987` <chr>, `1988` <chr>, `1989` <chr>, …"
  },
  {
    "objectID": "posts/Final Project __ HW2__ JulianCastoro.html#pivoting-years",
    "href": "posts/Final Project __ HW2__ JulianCastoro.html#pivoting-years",
    "title": "Homework 2",
    "section": "Pivoting years",
    "text": "Pivoting years\nNext I pivot the years\nBefore:\n\n\nCode\n#cols to pivot\nhead(mutated_CurrentUSD)\n\n\n# A tibble: 6 × 75\n  Country Region  `1949` `1950` `1951` `1952` `1953` `1954` `1955` `1956` `1957`\n  <chr>   <chr>   <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n1 Algeria North … xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n2 Libya   North … xxx    xxx    ...    ...    ...    ...    ...    ...    ...   \n3 Morocco North … xxx    xxx    xxx    xxx    xxx    xxx    xxx    23.71… 35.40…\n4 Tunisia North … xxx    xxx    xxx    xxx    xxx    xxx    xxx    3.714… 6.411…\n5 Angola  sub-Sa… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n6 Benin   sub-Sa… xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx    xxx   \n# … with 64 more variables: `1958` <chr>, `1959` <chr>, `1960` <chr>,\n#   `1961` <chr>, `1962` <chr>, `1963` <chr>, `1964` <chr>, `1965` <chr>,\n#   `1966` <chr>, `1967` <chr>, `1968` <chr>, `1969` <chr>, `1970` <chr>,\n#   `1971` <chr>, `1972` <chr>, `1973` <chr>, `1974` <chr>, `1975` <chr>,\n#   `1976` <chr>, `1977` <chr>, `1978` <chr>, `1979` <chr>, `1980` <chr>,\n#   `1981` <chr>, `1982` <chr>, `1983` <chr>, `1984` <chr>, `1985` <chr>,\n#   `1986` <chr>, `1987` <chr>, `1988` <chr>, `1989` <chr>, `1990` <chr>, …\n\n\nAfter:\n\n\nCode\nclean_currentUSD <- mutated_CurrentUSD %>%\n  pivot_longer(cols=3:ncol(mutated_CurrentUSD),names_to = \"Year\",values_drop_na = FALSE)\n  \nhead(clean_currentUSD)\n\n\n# A tibble: 6 × 4\n  Country Region       Year  value\n  <chr>   <chr>        <chr> <chr>\n1 Algeria North Africa 1949  xxx  \n2 Algeria North Africa 1950  xxx  \n3 Algeria North Africa 1951  xxx  \n4 Algeria North Africa 1952  xxx  \n5 Algeria North Africa 1953  xxx  \n6 Algeria North Africa 1954  xxx  \n\n\n\nConverting notations\nNow I must handle the xxx and … notations. From the information page of my data set I can see that the notation is described as follows:\n\n\n\n\n\n\n\nRaw Notation\nMeaning\n\n\n\n\n…\nData unavailable\n\n\nxxx\nCountry did not exist or was not independent during all or part of the year in question\n\n\n\nFor now I think I will just keep both as NA but will save this form of the information for future use.\n\n\nCode\ncleanNA_currentUSD <-clean_currentUSD%>%\n  na_if(\"...\") %>%\n  na_if(\"xxx\")\n\n\nhead(cleanNA_currentUSD)\n\n\n# A tibble: 6 × 4\n  Country Region       Year  value\n  <chr>   <chr>        <chr> <chr>\n1 Algeria North Africa 1949  <NA> \n2 Algeria North Africa 1950  <NA> \n3 Algeria North Africa 1951  <NA> \n4 Algeria North Africa 1952  <NA> \n5 Algeria North Africa 1953  <NA> \n6 Algeria North Africa 1954  <NA> \n\n\n\n\nConverting column types\nFinally I will convert the column types to their correct representation.\nBefore:\n\n\nCode\nglimpse(cleanNA_currentUSD)\n\n\nRows: 12,629\nColumns: 4\n$ Country <chr> \"Algeria\", \"Algeria\", \"Algeria\", \"Algeria\", \"Algeria\", \"Algeri…\n$ Region  <chr> \"North Africa\", \"North Africa\", \"North Africa\", \"North Africa\"…\n$ Year    <chr> \"1949\", \"1950\", \"1951\", \"1952\", \"1953\", \"1954\", \"1955\", \"1956\"…\n$ value   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"66.43…\n\n\nCode\n# Year to date\ncleanNA_currentUSD$Year<-as_date(cleanNA_currentUSD$Year, format = \"%Y\",tz=NULL)\n\n\n#i could get just year by adding this but then it is a double?\n#%>%\n # year()\n\n\n\n# value to double\ncleanNA_currentUSD$value <- as.numeric(cleanNA_currentUSD$value)\n\n\nAfter:\n\n\nCode\nhead(cleanNA_currentUSD)\n\n\n# A tibble: 6 × 4\n  Country Region       Year       value\n  <chr>   <chr>        <date>     <dbl>\n1 Algeria North Africa 1949-01-01    NA\n2 Algeria North Africa 1950-01-01    NA\n3 Algeria North Africa 1951-01-01    NA\n4 Algeria North Africa 1952-01-01    NA\n5 Algeria North Africa 1953-01-01    NA\n6 Algeria North Africa 1954-01-01    NA"
  },
  {
    "objectID": "posts/Final Project __ HW2__ JulianCastoro.html#potential-research-questions",
    "href": "posts/Final Project __ HW2__ JulianCastoro.html#potential-research-questions",
    "title": "Homework 2",
    "section": "Potential research questions",
    "text": "Potential research questions\n\nGlobal events which may have led to increases of spending.\nGrouped view of allies\nGrouped view of regions\nWhich countries are steady vs decrease/increase\nCould overlay US per capita spending with public opinion of military\nShare of govt spending"
  },
  {
    "objectID": "posts/Final Project __ HW2__ JulianCastoro.html#conclusion",
    "href": "posts/Final Project __ HW2__ JulianCastoro.html#conclusion",
    "title": "Homework 2",
    "section": "Conclusion",
    "text": "Conclusion\n\n“Every gun that is made, every warship launched, every rocket fired signifies in the final sense, a theft from those who hunger and are not fed, those who are cold and are not clothed. > > This world in arms is not spending money alone. It is spending the sweat of its laborers, the genius of its scientists, the hopes of its children. This is not a way of life at all in any > true sense. Under the clouds of war, it is humanity hanging on a cross of iron.”\n\nDwight D. Eisenhower\nCitations: https://www.goodreads.com/quotes/tag/military-budget#:~:text=%E2%80%9CEvery%20gun%20that%20is%20made,is%20not%20spending%20money%20alone.\nAppendix: Full descriptions as provided by SIPRI:\n\nIntroduction\nEstimates of world, regional and sub-regional totals in constant (2019) US$ (billions).\nData for military expenditure by country in current price local currency, presented according to each country’s financial year.\nData for military expenditure by country in current price local currency, presented according to calendar year.\nData for military expenditure by country in constant price (2019) US$ (millions), presented according to calendar year, and in current US$m. for 2020.\nData for military expenditure by country in current US$ (millions), presented according to calendar year.\nData for military expenditure by country as a share of GDP, presented according to calendar year.\nData for military expenditure per capita, in current US$, presented according to calender year. (1988-2020 only)\nData for military expenditure as a percentage of general government expenditure. (1988-2020 only)"
  },
  {
    "objectID": "posts/Final Project_DarronBunt.html#introduction",
    "href": "posts/Final Project_DarronBunt.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nThe pervasiveness of social media within contemporary culture only continues to increase. A Pew Research Center survey conducted in 2021 indicated that roughly 70% of American adults use at least one social media site – a proportion that increases to over 80% when considering younger generations. When it comes to American teens, 97% report using the Internet daily to connect with others, engage with content, and seek out information of interest.\nThe prominence of social media necessitates that businesses and service-oriented organizations maintain a presence on the platforms their target audiences frequent. Institutions of higher education are not exempt from this expectation; rather, the number of accounts associated with any particular college or university continues to increase as various audiences, including prospective and current students, alumni, and community members, seek out information and content they are interested in. Given this, institutions now employ entire teams of communications specialists to manage their online presence. Social media has become an increasingly important component of their overall communication and promotional strategies.\nThis research project seeks to explore how the 50 US flagship state public colleges are using Twitter. The included institutions were defined as the “flagship” based on the following characteristics: the institution is the most prominent public university within its state; it was usually the first public university to be established, and may include additional distinguishing characteristics such as boasting the largest research profile; and it has a NCAA Division I athletics program. In addition, the listed university may host the most doctoral programs and advanced degrees (including law and medicine) and may also receive the largest proportion of financial support from its parent state see this list as an example. The main Twitter profile of each of the most commonly listed 50 US flagship state schools were compiled for evaluation. More specifically, I intend to perform analysis on two facets of their social media presence: what is being produced by each college’s account, and what the response to those posts is. Overall, this research seeks to explore three primary questions:\n\nAre there consistencies in how colleges are using Twitter?\nWhat makes some posts more successful than others?\nAre there takeaways on how colleges can most effectively use Twitter?"
  },
  {
    "objectID": "posts/Final Project_DarronBunt.html#data",
    "href": "posts/Final Project_DarronBunt.html#data",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nFor this project, I have used the social media analytics tool Brandwatch to collect every Twitter post authored by one of the 50 flagship US colleges during the month of November 2022. I then exported this data to a .csv file that I subsequently loaded into R.\n\nLoad the dataset\n\n#import data\nFlagshipTwitter <- read_csv(\"_data/FlagshipTwitterUpdated.csv\")\n#import Author > School Name data\nTWAuthor2School <- read_csv(\"_data/Author_SchoolName.csv\")\n#import enrollment data from CCIHE \nEnrollmentData <- read_csv(\"_data/CCIHE2021PublicData.csv\")\n#remove rows where the Page Type isn't twitter\nFlagshipTwitter2 <- subset(FlagshipTwitter, PageType =='twitter')\n#rename the columns I intend to use for analysis\nFlagshipTwitter3 <- rename(FlagshipTwitter2, c(Tweet = 'Full Text', MentionedAuthors = 'Mentioned Authors', TWFollowers = 'Twitter Followers', TWReply = 'Twitter Reply Count', TWRetweets = 'Twitter Retweets', TWLikes = 'Twitter Likes', Reach = 'Reach (new)', EngType = 'Engagement Type', URL = Url)) %>%\nreplace_na(list(EngType = \"OG\"))\n#put columns I plan to use first \nFlagshipTwitterUse <- select(FlagshipTwitter3, Author, Tweet, EngType, Date, TWFollowers, Impressions, Reach, TWLikes, TWRetweets, TWReply,  Sentiment, everything())\n#separate dates into respective date and time column\nTwitterUse2 <- separate(FlagshipTwitterUse, Date, into = c(\"Date\", \"Time\"), sep = \" \")\nTwitterUse2$Date <- parse_date(TwitterUse2$Date, format = \"%m/%d/%Y\")\nTwitterUse2$Time <- parse_time(TwitterUse2$Time, format = \"%H:%M\")\n#add weekdays column\nTwitterUse2$Weekday <- weekdays(TwitterUse2$Date)\n#create the version you're going to build from for this project\nTW_Enr_Full <- TwitterUse2 %>%\n  left_join(TWAuthor2School, by = \"Author\") %>%\n left_join(EnrollmentData, by = \"SchoolName\")\nTW_Enr_Full$SizeSetting <- str_replace_all(TW_Enr_Full$SizeSetting, c(\"Four-year, large, primarily residential\" =\"LargePriRez\", \"Four-year, large, highly residential\" = \"LargeHighRez\", \"Four-year, large, primarily nonresidential\" = \"LargeNonRez\", \"Four-year, medium, primarily nonresidential\" = \"MedNonRez\", \"Four-year, medium, primarily nonresidential\" = \"MedPriRez\", \"Four-year, small, highly residential\" = \"SmallHighRez\", \"Four-year, medium, primarily residential\" = \"MedPriRez\"))\nTW_Enr_Full %>%\n  select(SchoolName, TWFollowers, F20Enrollment, SizeSetting, Date, Time, Weekday, EngType, Impressions, Reach, TWLikes, TWRetweets, TWReply, Sentiment, Tweet) \n\n# A tibble: 5,658 × 15\n   SchoolName   TWFol…¹ F20En…² SizeS…³ Date       Time  Weekday EngType Impre…⁴\n   <chr>          <dbl>   <dbl> <chr>   <date>     <tim> <chr>   <chr>     <dbl>\n 1 The Univers…  189187   37840 LargeP… 2022-11-30 23:55 Wednes… OG       189187\n 2 University …   33885   18025 LargeP… 2022-11-30 23:50 Wednes… RETWEET   33885\n 3 University …   90003   37437 LargeP… 2022-11-30 23:47 Wednes… RETWEET   90003\n 4 University …  143148   30092 LargeH… 2022-11-30 23:30 Wednes… RETWEET  143148\n 5 University …   31394   20722 LargeN… 2022-11-30 23:30 Wednes… OG        33378\n 6 University …   33887   18025 LargeP… 2022-11-30 23:24 Wednes… OG        33887\n 7 University …  168145   31115 LargeN… 2022-11-30 23:10 Wednes… OG       233177\n 8 University …   72683    8899 MedNon… 2022-11-30 23:03 Wednes… RETWEET   72683\n 9 University …   72683    8899 MedNon… 2022-11-30 23:02 Wednes… REPLY     72683\n10 University …  126437   33081 LargeP… 2022-11-30 23:00 Wednes… RETWEET  126437\n# … with 5,648 more rows, 6 more variables: Reach <dbl>, TWLikes <dbl>,\n#   TWRetweets <dbl>, TWReply <dbl>, Sentiment <chr>, Tweet <chr>, and\n#   abbreviated variable names ¹​TWFollowers, ²​F20Enrollment, ³​SizeSetting,\n#   ⁴​Impressions\n\n\n\n\nDescribe the dataset\nThe dataset is comprised of the 5,658 posts that were made by the 50 US flagship colleges in November 2022. For each post, there are several associated variables that will be used for analysis. The 14 variables that are of particular interest for this project are:\n\nSchool Name: Which school authored each post.\nTwitter Followers: The number of Twitter followers the account had at the time of posting.\nF20 Enrollment: The enrollment at each school in the Fall of 2020.\nSize Setting: The size and setting designation for each school.\nDate: The date each post was authored.\nTime: The time each post was posted.\nWeekday: The day of the week each post was made.\nEngagement Type: A designation of whether the post was an original post (OG), a retweet of someone else’s post (RETWEET), a reply to another account’s post (REPLY), or quote tweet, a retweet of another account’s post with added commentary (QUOTE).\nImpressions: The sum of the followers of a tweet’s author and the followers of any retweeting authors.\nReach: An estimate of how many people have actually seen/read a given post.\nTwitter Likes: The number of times Twitter users “liked” a given post.\nTwitter Retweets: The number of times Twitter users retweeted a given post on their own Twitter.\nTwitter Replies: The number of times Twitter users left a comment on a given post.\nSentiment: An AI-driven interpretation of the content of each tweet that subsequently labels the post as either Positive, Negative, or Neutral.\nTweet: The content of the tweet authored."
  },
  {
    "objectID": "posts/Final Project_DarronBunt.html#visualization",
    "href": "posts/Final Project_DarronBunt.html#visualization",
    "title": "Final Project",
    "section": "Visualization",
    "text": "Visualization\n\nHow are the flagship colleges using Twitter?\nIf you consult with ten different social media managers you will very likely receive ten different answers about how they use Twitter to achieve their goals. Accordingly, I first want to explore how the different flagship college accounts are being used, namely: how often they are posting, when those posts are being published, and what types of posts they are.\nTotal number of posts made by each college\n\n#number of posts made by each college\nby_college <- TwitterUse2 %>%\ncount(Author) %>%\n  rename(\"Total_Posts\" = \"n\") %>%\n arrange(desc(Total_Posts))\n\n#add column breaking down total posts by range\nby_college_range2 <- by_college %>%\n mutate(\n   post_range = case_when(\n      Total_Posts < 50 ~ \"Under 50 posts\", \n      Total_Posts < 100 ~ \"50-100 posts\",\n       Total_Posts < 150 ~ \"100-149 posts\",\n       Total_Posts < 200 ~ \"150-200 posts\",\n       Total_Posts > 200 ~ \"200+ posts\"))\n\n#make a graph of number of posts made by each college\nby_college_range2 %>%\n  arrange(Total_Posts) %>%\nmutate(Author=factor(Author, levels=Author)) %>%\n  ggplot(aes(x=Author, y=Total_Posts)) +\n  geom_bar(stat='identity', fill=\"forest green\", width = 0.4) +\n   labs(title = \"Number of Posts Made by Each School\", x = \"School\",y  = \"Total Number of Posts\") +\n  theme(axis.text = element_text(size = 5, angle=90)) +\n  facet_wrap(~post_range, scales=\"free_x\", ncol = 3)  \n\n\n\n\n\n#summary statistics for Posts by College\nsummary(by_college$Total_Posts)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    2.0    50.5    93.5   113.2   155.2   337.0 \n\n\nIn an ideal world, this visualization would be sorted with under 50 posts first and the remaining facets following sequentially. Nonetheless, this visualization indicates that post volume varies significantly depending on the flagship college in question - from 337 (Rutgers) to only two (University of South Dakota).\nIn addition to post volume having a wide range overall (335), the IQR is also quite high (105). Further, the difference in posts (20) between the median (93) and the mean (113) also points to a high degree of variance and a lack of consistency in terms of how frequently each flagship college is posting on Twitter.\nNumber of original posts, retweets, quote tweet, and comments by each college\nThere are four different types of posts that can be made on Twitter: original posts, retweets, quote tweets, and comments. We can add further context to how frequently each account is posting by also considering what kind of posts they are making. This will also allow us to derive insight into whether different schools are employing strategies that lean more heavily into particular types of posts (and later, whether engagement varies depending on the type of post being made).\nBecause each college’s post volume varies widely, it is more prudent to examine engagement types by college by their proportion and not total volume.\n\n#number of OG posts, retweets, comments made by each college\npost_type_by_college <- TwitterUse2 %>%\n  replace_na(list(EngType = \"OG\")) %>%\ngroup_by(Author, EngType) %>%\n  summarize(Count=n()) %>%\n  pivot_wider(names_from = EngType, values_from = Count)\npost_types_by_college <-  merge(by_college, post_type_by_college, by=\"Author\")\n\n#proportional breakdown of posts by each college\npost_prop_college <- post_types_by_college %>%\n  mutate(OGProp = OG/Total_Posts*100) %>%\nmutate(QuoteProp = QUOTE/Total_Posts*100) %>%\nmutate(ReplyProp = REPLY/Total_Posts*100) %>%\nmutate(RetweetProp = RETWEET/Total_Posts*100) %>%\n  select(Author, OGProp, QuoteProp, RetweetProp, ReplyProp)\nsummary(post_prop_college[c(\"OGProp\", \"QuoteProp\", \"ReplyProp\", \"RetweetProp\")])\n\n     OGProp         QuoteProp         ReplyProp        RetweetProp    \n Min.   : 11.57   Min.   : 0.4132   Min.   : 0.2967   Min.   : 3.636  \n 1st Qu.: 31.44   1st Qu.: 2.1682   1st Qu.: 2.1971   1st Qu.:28.212  \n Median : 44.64   Median : 4.0270   Median : 5.8258   Median :44.835  \n Mean   : 49.21   Mean   : 5.6112   Mean   : 8.0246   Mean   :43.945  \n 3rd Qu.: 63.44   3rd Qu.: 5.9041   3rd Qu.:10.8108   3rd Qu.:58.251  \n Max.   :100.00   Max.   :30.7692   Max.   :37.2881   Max.   :86.647  \n                  NA's   :12        NA's   :12        NA's   :4       \n\n\nOriginal posts are the most common type of post, comprising on average almost half (49%) of posts made by the colleges. Retweets are the next most common type of engagement (40%), while quote tweets and replies are typically least utilized (4% and 6% respectively).\n\n#visualizing post type frequency by school\nggplot(data=TW_Enr_Full) +\ngeom_bar(\n  mapping = aes(x = SchoolName, fill=EngType),\n  position=\"fill\") +\n labs(title = \"Post Type Frequency by School\", x = \"School\",y  = \"Proportion of Posts\", fill = \"Engagement Type\") +\ntheme(axis.text = element_text(size = 3, angle=90)) \n\n\n\n\nThis visualization helps to show that the majority of schools are employing two posts types with the greatest frequency – original posts, and retweets. There are indeed outliers for both; the University of South Dakota and University of Wyoming made only original posts, while the University of Iowa and University of New Hampshire’s posts were over 90% original (99% and 93% respectively). Conversely, the University at Buffalo and Rutgers University produce fewer original posts (13% and 12% respectively) and instead focus their posting strategy heavily on retweets (79% and 87% respectively).\nWhile replies are not a typical engagement strategy for most of the colleges, two schools (the University of Idaho and the University of Kentucky) have a proportion of replies comprising over a third of their total volume (37% and 32% respectively). Similarly, while the majority of schools are not producing a high volume of quote tweets, the University of Vermont (35%), the University of Tennessee (23%), and UMass Amherst (18%) are utilizing this post type more frequently as part of their posting strategy.\nNumber of posts made on each day of the week\n\n#number of posts made on each day of the week\nTwitterUse2$Weekday <- weekdays(TwitterUse2$Date)\nweekday_by_college <-TwitterUse2 %>%\n  group_by(Author, Weekday) %>%\n  summarize(Count=n()) %>%\npivot_wider(names_from = Weekday, values_from = Count) %>%\n  select(Author, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday)\nweekday_by_college[is.na(weekday_by_college)] <- 0 \n weekday_by_college2 <- weekday_by_college %>%\n   mutate(total = rowSums(across(c(Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday))))\n\n#proportion of posts made on each day of the week\n weekday_prop_college <- weekday_by_college2 %>%\n  mutate(MondayProp = Monday/total*100) %>% \nmutate(TuesdayProp = Tuesday/total*100) %>%\n    mutate(WednesdayProp = Wednesday/total*100) %>%\n    mutate(ThursdayProp = Thursday/total*100) %>%\n    mutate(FridayProp = Friday/total*100) %>%\n    mutate(SaturdayProp = Saturday/total*100) %>%\n    mutate(SundayProp = Sunday/total*100) %>%\nselect(Author, MondayProp, TuesdayProp, WednesdayProp, ThursdayProp, FridayProp, SaturdayProp, SundayProp)\n\n#visualizing posts by day by school\nggplot(data=TW_Enr_Full) +\ngeom_bar(\n  mapping = aes(x = SchoolName, fill=Weekday),\n  position=\"fill\") +\n labs(title = \"Posts by Day by School\", x = \"School\",y  = \"Proportion of Posts by Day\") +\ntheme(axis.text = element_text(size = 3, angle=90))\n\n\n\n\n\nsummary(weekday_prop_college[c(\"MondayProp\", \"TuesdayProp\", \"WednesdayProp\", \"ThursdayProp\", \"FridayProp\", \"SaturdayProp\", \"SundayProp\")])\n\n   MondayProp     TuesdayProp    WednesdayProp     ThursdayProp  \n Min.   : 0.00   Min.   : 0.00   Min.   : 7.692   Min.   : 0.00  \n 1st Qu.:11.49   1st Qu.:17.49   1st Qu.:14.704   1st Qu.:10.23  \n Median :14.21   Median :20.60   Median :18.524   Median :13.45  \n Mean   :15.63   Mean   :20.81   Mean   :19.484   Mean   :13.06  \n 3rd Qu.:18.05   3rd Qu.:24.94   3rd Qu.:22.980   3rd Qu.:16.27  \n Max.   :37.88   Max.   :30.77   Max.   :50.000   Max.   :23.08  \n   FridayProp     SaturdayProp      SundayProp    \n Min.   : 3.75   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:12.17   1st Qu.: 2.776   1st Qu.: 2.505  \n Median :15.26   Median : 7.020   Median : 5.897  \n Mean   :16.06   Mean   : 8.209   Mean   : 6.739  \n 3rd Qu.:18.30   3rd Qu.:12.180   3rd Qu.: 9.617  \n Max.   :50.00   Max.   :34.783   Max.   :22.034  \n\n\nAcross the dataset, the most common day of the week to post is Tuesday (21%) with Wednesday close behind (19%). The proportions of posts made on Fridays, Mondays (16% each), and Thursdays (13%) are quite similar. Weekends had the fewest posts (8% on Saturdays; 7% on Sundays).\nNumber of posts made during each time period of the day\nFor this analysis I have taken the timestamp from each post in the dataset and assigned it to one of four time periods: afternoon (noon to 6pm), evening (6pm to midnight), overnight (midnight to 6am), and morning (6am to noon) in order to determine the proportion of posts made by each school during each time block.\n\n#add a time period label to each post based on when it was made\ntime_of_day <- TW_Enr_Full %>%  \n   mutate(TimePeriod = format(Time, format=\"%H:%M:%S\")) %>%\n  mutate(TimePeriod = replace(TimePeriod, TimePeriod >= \"00:00:00\" & TimePeriod < \"05:59:59\", \"overnight\")) %>%\n  mutate(TimePeriod = replace(TimePeriod, TimePeriod >= \"06:00:00\" & TimePeriod < \"11:59:59\", \"morning\")) %>%\n  mutate(TimePeriod = replace(TimePeriod, TimePeriod >= \"12:00:00\" & TimePeriod < \"17:59:59\", \"afternoon\")) %>%\n mutate(TimePeriod = replace(TimePeriod, TimePeriod >= \"18:00:00\" & TimePeriod < \"23:59:59\", \"evening\")) %>%\n  select(Author, Date, Time, TimePeriod, Tweet)\n\n#pivot data to provide a summary of posts by time of day by college\ntime_of_day_by_college <- time_of_day %>%\ngroup_by(Author, TimePeriod) %>%\n  summarize(Count=n()) %>%\n  pivot_wider(names_from = TimePeriod, values_from = Count)\ntime_of_day_by_college[is.na(time_of_day_by_college)] <- 0 \n\n#adding a total row to the time of day breakdown\ntime_of_day_by_college2 <- time_of_day_by_college %>%\n  mutate(total = rowSums(across(c(afternoon, evening, morning, overnight)))) \n\n#proportion of posts made during each time period\n time_prop_college <- time_of_day_by_college2 %>%\n  mutate(AftProp = afternoon/total*100) %>% \nmutate(EveProp = evening/total*100) %>%\n    mutate(MornProp = morning/total*100) %>%\n    mutate(OvernightProp = overnight/total*100) %>%\nselect(Author, AftProp, EveProp, MornProp, OvernightProp)\n\n#visualizing posts by time period by school\nggplot(data=time_of_day) +\ngeom_bar(\n  mapping = aes(x = Author, fill=TimePeriod),\n  position=\"fill\") +\n labs(title = \"Posts by Time Period by School\", x = \"School\",y  = \"Proportion of Posts by Time Period\", fill = \"Time Period\") +\ntheme(axis.text = element_text(size = 3, angle=90))\n\n\n\n\n\nsummary(time_prop_college[c(\"AftProp\", \"EveProp\", \"MornProp\", \"OvernightProp\")])\n\n    AftProp          EveProp         MornProp       OvernightProp   \n Min.   :  3.39   Min.   : 0.00   Min.   : 0.0000   Min.   : 0.000  \n 1st Qu.: 35.45   1st Qu.:39.55   1st Qu.: 0.0000   1st Qu.: 3.673  \n Median : 41.73   Median :44.20   Median : 0.0000   Median : 7.380  \n Mean   : 42.07   Mean   :46.54   Mean   : 0.9940   Mean   :10.390  \n 3rd Qu.: 53.84   3rd Qu.:54.04   3rd Qu.: 0.8656   3rd Qu.:12.973  \n Max.   :100.00   Max.   :95.45   Max.   :15.1260   Max.   :60.000  \n\n\nAcross the dataset, evening and afternoon posts were most common (47% and 42% respectively). Only 1% of posts were made during the morning time period.\nI do believe that this time data may not be completely accurate; given the small proportion of morning posts it seems likely that the timestamps provided in the dataset (i) do not reflect local time when the post was made; and (ii) were also likely given in UTC and not in one of the more common time zones in the US. Given this, I am wary of deriving any insights from this data and would want to make adjustments to each time by adjusting it to the proper time zone for each respective college before continuing.\n\n\nWhat is the response to the posts made by each college?\nMaking posts is certainly an important part of having a social media presence, but how your audience responds to those posts is crucial data that helps to guide social media strategy. Shouting into a vacuum is both inefficient and poor strategy; if no one is consuming and engaging with your content, why invest in creating it to begin with?\nWhile there are several metrics that relate to post engagement – including likes, retweets, and replies by one’s Twitter audience – the metric I want to focus on for this analysis is reach. Reach is an estimate of the number of people that have actually seen/read a given post, and the reach listed in this dataset has been calculated using Brandwatch’s proprietary algorithm.\nAverage Reach by School\nThe first, and most basic, level of analysis we have performed is to look at the average reach of the posts made by each school during the month of November.\n\n#Average reach for posts made by each college\neng_met_by_college <- TW_Enr_Full %>%\ngroup_by(SchoolName) %>%\n  summarize(\nReach_Mean = mean(Reach,na.rm = TRUE),\nReach_Median = median(Reach, na.rm = TRUE),\nReach_Max = max(Reach, na.rm = TRUE), \nReach_Min = min(Reach, na.rm = TRUE)) %>%\narrange(desc(Reach_Mean))\n\n#add column breaking down reach by range\neng_met_by_college_range <- eng_met_by_college %>%\n mutate(\n   post_range = case_when(\n      Reach_Mean < 10000 ~ \"Under 10k\", \n      Reach_Mean < 15000 ~ \"10k-15k\",\n       Reach_Mean < 20000 ~ \"15k-20k\",\n       Reach_Mean < 25000 ~ \"20-25k\",\n      Reach_Mean < 30000 ~ \"25k-30k\",\n       Reach_Mean > 30000 ~ \"30k+\"))\n\n#make a graph of number of average reach by each college\neng_met_by_college_range %>%\n  arrange(Reach_Mean) %>%\nmutate(SchoolName=factor(SchoolName, levels=SchoolName)) %>%\n  ggplot(aes(x=SchoolName, y=Reach_Mean)) +\n  geom_bar(stat='identity', fill=\"forest green\", width = 0.5) +\n   labs(title = \"Average Reach by School\", x = \"School\",y  = \"Average Reach\") +\n  theme(axis.text = element_text(size = 5, angle = 90)) \n\n\n\n\nThere was very clearly one outlier during the analysis period – the University of Kentucky – whose average reach was more than double (86,807) the next closest school (the University of Florida, at 40,746). Kentucky had several viral tweets during the month of November related to an incident in their residences where a drunk white female student assaulted a Black woman who was on duty as a desk clerk in a residence hall while also calling her racial epithets. This highlights the impact that viral incidents – in this case, a disturbing and disappointing one, although incidents with more positive sentiment can and indeed also do sometimes go viral – can have on an account’s metrics.\n\n#make a graph of number of average reach by each college without Kentucky\neng_met_by_college_range %>%\n  arrange(Reach_Mean) %>%\n  filter(SchoolName != \"University of Kentucky\") %>%\nmutate(SchoolName=factor(SchoolName, levels=SchoolName)) %>%\n  ggplot(aes(x=SchoolName, y=Reach_Mean)) +\n  geom_bar(stat='identity', fill=\"forest green\", width = 0.5) +\n   labs(title = \"Average Reach by School (Without Kentucky)\", x = \"School\",y  = \"Average Reach\") +\n  theme(axis.text = element_text(size = 5, angle = 90)) \n\n\n\n\nWhen we remove the University of Kentucky from the dataset, a range of average reach by school still remains, but there is greater consistency and a more steady increase from the school with the lowest average reach (the University of Wyoming at 3,073) to the school with the next-highest reach (the University of Florida at 40,746).\nHow does the size of each school impact its number of followers and reach?\nUnlike businesses, colleges arguably do not have an endless pool of followers to draw from. Rather, it is likely that the majority of the people who follow a particular college’s account have an association with that school – most prominently, audiences like prospective and current students, alumni – and given this, schools with a larger enrollment base should be expected to have a larger following than smaller schools.\n\n#calculating followers as a proportion of enrollment\nfoll_prop_enroll <- TW_Enr_Full %>%\n  mutate(FollowersByEnrollment = TWFollowers/F20Enrollment) %>%\n  mutate(ReachByFollowers = Reach/TWFollowers) %>%\n  mutate(ReachByEnrollment = Reach/F20Enrollment) %>%\nselect(SchoolName, Author, FollowersByEnrollment, ReachByFollowers, ReachByEnrollment, TWFollowers, F20Enrollment, Reach, SizeSetting) \n\n#summarizing metrics by school\nfoll_prop_enroll_summary <- foll_prop_enroll %>%\ngroup_by(SchoolName, Author) %>%\n  summarize(avgFollByEnroll = mean(FollowersByEnrollment),\n            avgReachbyFoll = mean(ReachByFollowers),\n            avgReachbyEnroll = mean(ReachByEnrollment),\n            avgReach = mean(Reach),\n            TWFollowers = max(TWFollowers)) %>%\n            left_join(EnrollmentData, by = \"SchoolName\") %>%\nleft_join(by_college, by = \"Author\") %>%\n  select(SchoolName, Author, Total_Posts, avgReach, avgFollByEnroll, avgReachbyFoll, avgReachbyEnroll, TWFollowers, F20Enrollment, SizeSetting)\nfoll_prop_enroll_summary$SizeSetting <- str_replace_all(foll_prop_enroll_summary$SizeSetting, c(\"Four-year, large, primarily residential\" =\"Large Mostly Rez\", \"Four-year, large, highly residential\" = \"Large High Rez\", \"Four-year, large, primarily nonresidential\" = \"Large Non Rez\", \"Four-year, medium, primarily nonresidential\" = \"Med Non Rez\", \"Four-year, medium, primarily nonresidential\" = \"Med Mostly Rez\", \"Four-year, small, highly residential\" = \"Small High Rez\", \"Four-year, medium, primarily residential\" = \"Med Mostly Rez\"))\n\n# Create a scatterplot mapping enrollment and followers\nggplot(foll_prop_enroll_summary, \n       aes(x=F20Enrollment, y=TWFollowers)) +\n  geom_point(color=\"red\") + \n    geom_text_repel(aes(label = SchoolName), size = 1.5, max.overlaps = Inf) + \n  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +\n   labs(title = \"Relationship Between Enrollment and Followers\", x = \"Enrollment\",y  = \"Followers\") +\n  geom_smooth(formula = y ~ x, method=lm, se=FALSE)\n\n\n\n\nThere is indeed a correlation between the enrollment at a college and its Twitter followers; larger institutions should have more followers than smaller ones. On average, the flagship colleges have an average of 3.6 followers for every enrolled student. There is, however, a fairly large IQR (3). It would be of value to perform a deeper analysis of the difference between schools like Louisiana State, the University of Arkansas at Little Rock, and West Virginia University, who all have more than 7 followers for each enrolled student and schools like the University of Wyoming and the University at Buffalo, who have less than one follower for each of their enrolled students.\n\n#summary statistics for average followers by enrollment\nsummary(foll_prop_enroll_summary[c(\"avgFollByEnroll\")])\n\n avgFollByEnroll  \n Min.   : 0.3954  \n 1st Qu.: 1.9101  \n Median : 3.5899  \n Mean   : 3.6477  \n 3rd Qu.: 4.9360  \n Max.   :10.5433  \n\n\nWhat is the relationship between followers and reach for the flagship college Twitter accounts?\nFollowing a Twitter account means that that account’s posts are likely to appear on that user’s Twitter feed. Accordingly, accounts with a higher number of Twitter followers have a greater likelihood that their posts will have a larger reach. Increased likelihood, however, is not a guarantee; posts from smaller accounts can go viral, while posts from larger accounts can fail to resonate with their intended audience. Plotting the relationship between Twitter followers and average post reach should show a correlation between the two variables; it should also provide us with some insight into which schools are more/less successful at generating the expected level of reach, and those schools could then be examined in greater detail.\n\nggplot(foll_prop_enroll_summary, \n       aes(x=TWFollowers, y=avgReach)) +\n  geom_point(color=\"red\") + \n    geom_text_repel(aes(label = SchoolName), size = 1.5, max.overlaps = Inf) + \n  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +\n  scale_x_continuous(labels = function(y) format(y, scientific = FALSE)) +\n   labs(title = \"Relationship Between Followers and Reach\", x = \"Followers\", y  = \"Average Post Reach\") +\n  geom_smooth(formula = y ~ x, method=lm, se=FALSE)\n\n\n\n\nThere is indeed a strong correlation between number of followers and the average reach of each school’s posts. What’s immediately evident is that there is also one very significant outlier – the University of Kentucky, whose average reach this month was skewed significantly after a significant negative occurrence on campus and the response to it went viral. There are five other universities whose average post reach outpaced what would be expected given their following: the University of Idaho; the University of Virginia; the University of Tennessee, the University of South Carolina, and the University of Florida. There were tragic violent events involving students at both Idaho and Virginia, while Florida’s Board of Trustees selected a contentious candidate as their new President-elect. Meanwhile, Tennessee and South Carolina’s average reach was largely bolstered by athletics content, and in particular, posts about football.\nCan we visualize a relationship between enrollment, followers, reach, and institution size?\nI was really curious what would happen if I tried to plot college enrollment, Twitter followers, institution size and setting (as per Carnegie Classifications), and post reach.\n\n#summarizing average followers by enrollment by reach for each school\nggplot(foll_prop_enroll_summary, \n       aes(x=F20Enrollment, y=TWFollowers, size=avgReach, color=SizeSetting)) +\n    geom_point(alpha=0.7) +\n  scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +\n      labs(title = \"Relationship Between Enrollment, Followers, Reach, and Institution Size\", x = \"Enrollment\",y  = \" Twitter Followers\", color = \"School Size & Setting\", size=\"Post Reach\") \n\n\n\n\nThere are not many medium-sized flagship colleges, but it’s interesting to see all but one of them clumped together in the bottom left corner. The one outlying medium school is the University of Arkansas and I would love to further explore the why behind that.\n\n\nFuture work - more textual analysis\nAs a primarily qualitative researcher who performs a lot of content analysis, I’m particularly interested in diving deeper into what is being shared in each of the college’s tweets; I’m very interested in exploring how variables like topics, themes, and word choices impact engagement, and what kinds of similarities and differences exist across institutions of different sizes and regions.\nTo gain some experience and explore what I might be able to do with textual analysis in R, I started experimenting with some ways in which I might be able to examine tweet text.\nCounting the number of characters in each Twitter post\nWhile tweets are (currently) limited to only 280 characters, This could potentially be useful in determining whether post length contributes to or detracts from engagement.\n\n#count the number of characters in each Twitter post\nTwitterUse2$TweetCharCount = str_length(TwitterUse2$Tweet)\nTwitterUse2 %>%\n  select(Author, TweetCharCount, Tweet) \n\n# A tibble: 5,658 × 3\n   Author      TweetCharCount Tweet                                             \n   <chr>                <int> <chr>                                             \n 1 UofAlabama             302 \"Juggling, snakes and cooking - oh my! ICYMI: the…\n 2 uhmanoa                218 \"RT @ManoaGrad Summer and Fall 2022 candidates - …\n 3 CUBoulder              255 \"RT @CU_PTS We are thrilled to welcome two new el…\n 4 UNC                    221 \"RT @ChapelHillFD #CHtraffic Alert: Country Club …\n 5 unevadareno            301 \"Save the date! @unevadareno will hold the 2022 W…\n 6 uhmanoa                289 \"$3.4 million to grow Native Hawaiian physicians …\n 7 uarizona               176 \"University of Arizona researchers are teaming up…\n 8 UArkansas              295 \"RT @ArkansasPBS LIVESTREAMING: Join @HillaryClin…\n 9 UArkansas               51 \"@prehistormic Your No. 1 song of the year, we ho…\n10 UUtah                  262 \"RT @UofUIT If you haven’t already, check out @Ma…\n# … with 5,648 more rows\n\n#find average number of characters in each college's posts\nTweetAvgCharacters <- TwitterUse2 %>%\n  group_by(Author) %>%\n  summarize(\n    AvgTweetCharCount = mean(TweetCharCount, na.rm = TRUE))\nTweetAvgCharacters\n\n# A tibble: 50 × 2\n   Author      AvgTweetCharCount\n   <chr>                   <dbl>\n 1 CUBoulder                230.\n 2 IndianaUniv              234.\n 3 LSU                      214.\n 4 Mizzou                   189.\n 5 OhioState                140.\n 6 OleMiss                  168.\n 7 penn_state               221.\n 8 RutgersU                 213.\n 9 uafairbanks              219.\n10 uarizona                 240.\n# … with 40 more rows\n\n\nAthletics content on the flagship Twitter accounts\nThe use of athletics content to drive engagement is another topic that I have a particular interest in, and I was curious just how many of November’s 5,638 used seven keywords that would indicate with (near) certainty that the posts were about sports and sporting events.\n\n#number of posts from November that were about athletics\nDissectPosts <- TwitterUse2 %>%\n  select(Author, TWFollowers, Tweet, EngType, TweetCharCount, Reach, TWLikes, TWRetweets, TWReply)\nDissectPostsAthletics <- DissectPosts %>%\n  filter(str_detect(Tweet, \"FB|(?i)football|basketball|MBB|WBB|athletics|NCAA\"))\n  DissectPostsAthletics\n\n# A tibble: 764 × 9\n   Author        TWFollowers Tweet EngType Tweet…¹ Reach TWLikes TWRet…² TWReply\n   <chr>               <dbl> <chr> <chr>     <int> <dbl>   <dbl>   <dbl>   <dbl>\n 1 WestVirginiaU      197557 \"RT … RETWEET     102 21844       0       0       0\n 2 UUtah              126435 \"RT … RETWEET     246 17988       0       0       0\n 3 UMassAmherst        57996 \"Wha… OG          255 13263       5       1       0\n 4 Mizzou             137999 \"RT … RETWEET     302 18695       0       0       0\n 5 UNLincoln           50787 \"RT … RETWEET     140 11842       0       0       0\n 6 UUtah              126436 \"RT … RETWEET     304 17988       0       0       0\n 7 RutgersU           141602 \"RT … RETWEET     140 18907       0       0       0\n 8 uarizona           168142 \"RT … RETWEET     275 20377       0       0       0\n 9 WestVirginiaU      197557 \"We’… OG          106 48002     239      41       9\n10 UTAustin           239275 \"RT … RETWEET     315 23706       0       0       0\n# … with 754 more rows, and abbreviated variable names ¹​TweetCharCount,\n#   ²​TWRetweets\n\n#determine how many athletics posts were made by each college\nAthleticsPostsByCollege <- DissectPostsAthletics %>%\n  group_by(Author) %>%\n  summarize(Count=n()) %>%\nleft_join(post_types_by_college, by = \"Author\") %>%\nselect(Author, Count, Total_Posts) %>%\n mutate(AthAvg = Count/Total_Posts*100) %>%\n  arrange(desc(AthAvg))\nAthleticsPostsByCollege\n\n# A tibble: 42 × 4\n   Author         Count Total_Posts AthAvg\n   <chr>          <int>       <int>  <dbl>\n 1 OhioState         29          69   42.0\n 2 UofSC             24          74   32.4\n 3 RutgersU          97         337   28.8\n 4 UTAustin          41         158   25.9\n 5 UUtah             78         311   25.1\n 6 UofIllinois       26         108   24.1\n 7 UofOklahoma       48         203   23.6\n 8 umontana           8          34   23.5\n 9 UofAlabama        54         242   22.3\n10 universityofga    39         189   20.6\n# … with 32 more rows\n\n\n13% of all posts made by the college flagship Twitter accounts were about athletics. Ohio State had the greatest proportion of posts relating to athletics (42%), while the University of Iowa had the smallest proportion (one post, or 1.3% of their posts for the month). Rutgers had the largest volume of athletics posts (97), comprising 29% of their total posting volume.\nMost common words used in Tweets\nI also endeavored to make a word cloud to highlight the words that occurred most frequently in the college’s posts.\n\n#Create a vector containing only the text\nTweetText <- DissectPosts$Tweet\n# Create a corpus  \nTweetCorpus <- Corpus(VectorSource(TweetText))\n\nTweetCorpus <- TweetCorpus %>%\n  tm_map(removeNumbers) %>%\n  tm_map(removePunctuation) %>%\n  tm_map(stripWhitespace)\nTweetCorpus <- tm_map(TweetCorpus, content_transformer(tolower))\nTweetCorpus <- tm_map(TweetCorpus, removeWords, stopwords(\"english\"))\n\ndtm <- TermDocumentMatrix(TweetCorpus) \nmatrix <- as.matrix(dtm) \nwords <- sort(rowSums(matrix),decreasing=TRUE) \ndf <- data.frame(word = names(words),freq=words)\n\nwordcloud(words = df$word, freq = df$freq, min.freq = 100,           max.words=50, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(4, \"Dark2\"))\n\n\n\n\nHaving created this, I see a lot of opportunity to improve, and ideally, derive greater insight from this type of work with text. I’d be interested in exploring ways to look for phrases in addition to single keywords, as this may provide more topical and thematic insight. I would also want to build out more custom stopwords; I can see words like “make”, “now”, “take”, and “see” in the cloud, and it would be very difficult to derive any sort of meaningful insight from their use. Similarly, “’s” appears in the word cloud, and I would certainly want to exclude this and any other contractions."
  },
  {
    "objectID": "posts/Final Project_DarronBunt.html#conclusion",
    "href": "posts/Final Project_DarronBunt.html#conclusion",
    "title": "Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nWhile all 50 flagship colleges are using Twitter, the ways in which they elect to use it vary greatly; the volume of posts produced by each account and the type of posts that they choose to employ are the two primary variables that support this assertion.\nWhen it comes to who follows the Twitter accounts of the flagship colleges, there is a relationship between each institution’s enrollment and its number of followers. Smaller institutions typically have fewer followers, while larger institutions have more. Understanding this relationship provides an opportunity to examine what the factors are that contribute to deviation from the expected trendline.\nSimilarly, there is also a correlative relationship between the number of followers that each account has and the average reach of their posts. I am particularly curious about what this relationship looks like with a larger dataset, as I suspect that examining a year’s worth of data as opposed to a month’s work would lessen the impact of viral posts related to significant events.\nThere are a lot of questions I still want to answer related to the factors that contribute to greater engagement on Twitter. I would like to continue layering different variables (the data related to posts by day of the week, or time posted, for example) and how this does/does not impact engagement."
  },
  {
    "objectID": "posts/Final Project_DarronBunt.html#reflection",
    "href": "posts/Final Project_DarronBunt.html#reflection",
    "title": "Final Project",
    "section": "Reflection",
    "text": "Reflection\nI knew that I wanted to explore something related to higher education social media use as this aligns with what I do for work and I’m very interested in adding to my “toolbox” as it relates to new ways to compile, break down and analyze social media data and provide insights related to what I’ve derived. Working through this project has also given me a lot of ideas about how incorporating work with R into our analysis process affords not just new opportunities for analysis, but process efficiency as well.\nI didn’t come into the project with a fully developed research question; rather, I knew that I wanted to explore some aspect of higher ed social media data in a way that I have not done previously. I gravitated towards tweets that were authored by the colleges as I knew I could pull a dataset that would require less validation; unlike creating a dataset online conversation about the different institutions, which will inevitably bring in some degree of irrelevant data (ie. mentions that are not actually relevant to analysis), focusing on content produced by the colleges facilitated developing a robust dataset with fewer variables that could affect relevancy.\nThere are a lot of institutions of higher education in the US; in 2021, Carnegie provided classification data for 3,939 institutions. I knew it wasn’t going to be realistic or feasible to look at data for all US colleges, so I began to think about what might be a logical way to create a smaller dataset of colleges with some type of unifying characteristic. I decided on the flagship colleges as that characteristic; this served to decrease the number of colleges to a far more manageable 50. I knew I was going to build a query in Brandwatch, a social media analytics tool, and also knew that they receive more data from Twitter than any other social media platform, so I elected to focus exclusively on posts authored for Twitter.\nI found a list identifying the 50 flagship US colleges and then compiled a list of the primary Twitter accounts associated with each of them. I used that information to write a Brandwatch query and pulled a dataset of all the posts made by the colleges between December 1 2021 and November 30 2022. This returned 78,700 mentions. I was originally wanted to use this larger dataset, but I ran into trouble exporting to R; while there is apparently an R Package to connect the Brandwatch API to R, I was unsuccessful in figuring out how to use it. My next option was to export from Brandwatch to .csv; as I am only able to download in batches of 10,000 mentions, I elected to narrow my dataset to fit within those parameters and accordingly reduced the time frame to November 1-30 2022.\nThe most challenging part of the project was narrowing my focus from exploratory analysis to answering specific questions about the data. There are so many directions and possibilities for this data and I found myself exploring multiple pathways but struggling to settle on a foundational question to answer. I found that this resulted in me overcommitting myself in terms of what, specifically, I wanted to analyze, and quickly realized that I needed to narrow my focus for my analysis. Accordingly, there are many different aspects of the dataset that I want to explore in greater detail.\nOne of the next steps I would very much be interested in making were I to continue with this project would be to explore using data related to the relationships between enrollment and Twitter followers, and between Twitter followers and average post reach to do some predictive modeling. Colleges are frequently interested in how their performance compares to that of their competitors, and this type of analysis could help to answer those questions and help to guide insights relating to why a particular institution’s data does/does not align with predictions.\nWhile I learned a lot about R and its capabilities throughout this course and this project, I am very aware of how greater experience and familiarity with both R as a tool and programming languages in general would make my research process both easier and faster. Many of my code blocks are functional but clunky, and I look forward to increasing my knowledge related to what R can do and how I can execute different analysis tactics.\nWhile I’m happy with the visualizations that I created for this project, I can similarly see how more time experimenting with ggplot and its capabilities would be beneficial so that I can produce visuals that more clearly express the assertions that I am trying to make."
  },
  {
    "objectID": "posts/Final Project_DarronBunt.html#bibliography",
    "href": "posts/Final Project_DarronBunt.html#bibliography",
    "title": "Final Project",
    "section": "Bibliography",
    "text": "Bibliography\n\nSOURCE: Data from twitter.com, compiled using brandwatch.com social analytics software.\nR Core Team (2022). R: A free software environment for statistical computing and graphics. The R Foundation for Statistical Computing, Vienna, Austria. R-project.org\nWickham, Hadley & Grolemund, Garrett (2017). R For Data Science.\nThe R Graph Gallery r-graph-gallery.com\nPew Research Center (2021). Social Media Fact Sheet. pewresearch.org\nPew Research Center (2022). Teens’ Views About Social Media. pewresearch.org\nPew Research Center (2022). Teens, Social Media and Technology 2022. pewresearch.org\nThe Carnegie Classification of Institutions of Higher Education (2021). 2021 Update Public File carnegieclassifications.ascent.edu\nOglethorpe list of 50 flagship institutions and tuition by state (2022). olethorpe.edu\nDIY College Rankings: What are flagship universities? (2022). diycollegerankings.com"
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html",
    "href": "posts/finalfinalproject_abbybalint.html",
    "title": "Final Project - Abby Balint",
    "section": "",
    "text": "The dataset I have chosen for my final project is part of a study titled “Facing the Music: The Current State of Streaming Services in the Music Industry” done by Dr. Silviana Falcon and Jessica Korver out of Florida Southern College, published in 2020. The part of the research I will be focusing on is a survey conducted around music streaming platform usage and consumer sentiment around the fairness of artist compensation from these platforms. This survey was conducted among 163 respondents who use various music streaming platforms in varying frequency, and are likely peers of the researchers. This means a sample that likely will skew younger in age but otherwise varying in demographic background. The survey itself contains 15 questions, 8 of which center around music streaming usage, 3 around artist compensation, 3 demographic questions, and one open-ended commentary.\nThe data file for the survey results contains one row (case) for each of the 161 respondent’s survey responses. There are 45 columns of data in the original dataset due to some of the survey questions being multi-select or including an Other text write-in option. Once I recode some variables for analysis, the amount of columns in the dataset will increase. I should not need to pivot longer, so my amount of rows (1 per respondent) will stay the same throughout.\nBelow I am reading in my dataset and opening the packages I’ll be using.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(crosstable)\nlibrary(gmodels)\nlibrary(readxl)\nlibrary(forcats)\n\n\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\n\nstreaming <- read_excel(\"_data/musicstreamingsurveyfinal2.xlsx\", skip=1)\n\n\ndim(streaming)\n\n[1] 161  45"
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#the-goal",
    "href": "posts/finalfinalproject_abbybalint.html#the-goal",
    "title": "Final Project - Abby Balint",
    "section": "The Goal",
    "text": "The Goal\nBecause this is survey data that reflects response options in numerical codes as opposed to the actual text of the survey, my first task will be to recode some of the survey variables so that I can see the actual text associated with the codes within my dataset. Luckily, the survey itself was provided as part of this research project in addition to the Excel data, so that is what I will be using to recode.\nMy goal is to transform the dataset in a way that will allow me to make crosstabs and charts to compare key survey questions against each other to see if there is a relationship between demographics, music streaming usage, and sentiment towards artist compensation. Below, I am focusing on recoding the variables that will be key in my analysis so that I can generate charts and visualizations that will reflect the text of the response options rather than the numeric codes, as charts and tables with numerical variable values would be meaningless to a reader. I decided to recode the variables into new variables as opposed to changing the existing column values because there may be times when I want to reference or use the original codes in my visualization or analysis, for example adding up values across columns.\nTo do this, I used the available survey that was run in Qualtrics, a platform I am familiar with from my work. My prior knowledge of Qualtrics allows me to know how the programming interface works, so by reading the survey, looking at the logical patterns, and using my knowledge of Qualtrics, I was able to determine which numeric codes belonged with each response option for almost every question in the survey. The only one I was not able to do this for was the first question regarding genre as the number of codes did not align with the number of response options in the written survey, however this question was not part of my planned analysis."
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#cleaning-tidying",
    "href": "posts/finalfinalproject_abbybalint.html#cleaning-tidying",
    "title": "Final Project - Abby Balint",
    "section": "Cleaning & tidying",
    "text": "Cleaning & tidying\n\nstreaming2 <- streaming %>%\n  mutate(`Total Music Time` = dplyr::case_when(\n    `How often do you listen to music per week?` == 1 ~ \"0-10 hours\",\n    `How often do you listen to music per week?` == 2 ~ \"11-20 hours\",\n    `How often do you listen to music per week?`== 4 ~ \"21+ hours\")) %>%\n  mutate(`Total Streaming Time` = dplyr::case_when(\n    `How often do you listen to streaming services per week?` == 1 ~ \"0-10 hours\",\n    `How often do you listen to streaming services per week?` == 2 ~ \"11-20 hours\",\n    `How often do you listen to streaming services per week?`== 3 ~ \"21+ hours\")) %>%\n   mutate(`Streaming Used Most` = dplyr::case_when(\n    `Which music streaming service would you say you use the MOST? - Selected Choice` == 1 ~ \"Spotify\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice` == 2 ~ \"Apple Music\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice`== 3 ~ \"Pandora\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice`== 4 ~ \"Amazon Music\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice`== 5 ~ \"Google Play\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice`== 6 ~ \"Other\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice`== 7 ~ \"YouTube\",\n    `Which music streaming service would you say you use the MOST? - Selected Choice`== 9 ~ \"Tidal\")) %>%\n   mutate(`Premium User` = dplyr::case_when(\n    `Are you on a Premium plan for any music streaming services (where you pay a monthly fee to stream music)?` == 1 ~ \"Yes\",\n    `Are you on a Premium plan for any music streaming services (where you pay a monthly fee to stream music)?` == 2 ~ \"No\")) %>%\n  mutate(`Streaming Earnings Guess` = dplyr::case_when(\n    `What is your best guess for how much an artist (on average) earned for 100,000 streams in 2018 (from Spotify/Apple Music)?` == 1 ~ \"Lower than $500\",\n    `What is your best guess for how much an artist (on average) earned for 100,000 streams in 2018 (from Spotify/Apple Music)?` == 2 ~ \"$501-$1,000\",\n    `What is your best guess for how much an artist (on average) earned for 100,000 streams in 2018 (from Spotify/Apple Music)?` == 3 ~ \"$1,001-$10,000\",\n    `What is your best guess for how much an artist (on average) earned for 100,000 streams in 2018 (from Spotify/Apple Music)?` == 4 ~ \"$10,001-$50,000\",\n    `What is your best guess for how much an artist (on average) earned for 100,000 streams in 2018 (from Spotify/Apple Music)?` == 5 ~ \"$10,001-$50,000\",\n    `What is your best guess for how much an artist (on average) earned for 100,000 streams in 2018 (from Spotify/Apple Music)?` == 6 ~ \"More than $100,000\")) %>%\n    mutate(`Suffient Compensation Agreement` = dplyr::case_when(\n    `According to the Trichordist Streaming Price Bible of 2018, it was found that artists earn an average of $413 for 100,000 streams. In your opinion, does that amount seem sufficient for compensation?` == 1 ~ \"Yes\",\n    `According to the Trichordist Streaming Price Bible of 2018, it was found that artists earn an average of $413 for 100,000 streams. In your opinion, does that amount seem sufficient for compensation?` == 2 ~ \"No\",\n    `According to the Trichordist Streaming Price Bible of 2018, it was found that artists earn an average of $413 for 100,000 streams. In your opinion, does that amount seem sufficient for compensation?` == 3 ~ \"I'm not sure\")) %>%\n  mutate(`Ideal Compensation` = dplyr::case_when(\n    `What do you believe would be an ideal rate of compensation for 100,000 streams?` == 1 ~ \"Lower than $500\",\n    `What do you believe would be an ideal rate of compensation for 100,000 streams?` == 2 ~ \"$501-$1,000\",\n    `What do you believe would be an ideal rate of compensation for 100,000 streams?` == 3 ~ \"$1,001-$10,000\",\n    `What do you believe would be an ideal rate of compensation for 100,000 streams?` == 4 ~ \"$10,001-$50,000\",\n    `What do you believe would be an ideal rate of compensation for 100,000 streams?` == 5 ~ \"$50,001-$100,000\",\n    `What do you believe would be an ideal rate of compensation for 100,000 streams?` == 6 ~ \"More than $100,000\")) %>%\n    mutate(`Age Group 1` = dplyr::case_when(\n    `Age` == 1 ~ \"18-21\",\n    `Age` == 2 ~ \"22-35\",\n    `Age` == 3 ~ \"36-50\",\n    `Age` == 4 ~ \"51+\")) %>%\n     mutate(`Age Group` = dplyr::case_when(\n    `Age` == 1 ~ \"18-21\",\n    `Age` == 2 ~ \"22-35\",\n    `Age` > 2 ~ \"36+\")) %>%\n    mutate(`Education` = dplyr::case_when(\n    `Current Education Level - Selected Choice` == 1 ~ \"High school\",\n    `Current Education Level - Selected Choice` == 2 ~ \"Associates\",\n    `Current Education Level - Selected Choice` == 3 ~ \"Bachelors\",\n    `Current Education Level - Selected Choice` == 4 ~ \"Post Graduate\",\n    `Current Education Level - Selected Choice` == 5 ~ \"Prefer not to say\",\n    `Current Education Level - Selected Choice` == 6 ~ \"Other\")) %>%\n    mutate(`Gender` = dplyr::case_when(\n    `Gender - Selected Choice` == 1 ~ \"Female\",\n    `Gender - Selected Choice` == 2 ~ \"Male\",\n    `Gender - Selected Choice` >2 ~ \"Non-Binary/Gender neutral/other\"))\n\n\nA little more tidying\nBelow is an example of where I used the original survey codes to add up values across rows. I generated a new variable to be able to see how many streaming platforms a respondent has ever used based on a multi-select survey question. I thought this would be a little more interesting than just generating what percentage of respondents said they had ever used any music streaming platforms, as it is common knowledge that Apple Music and Spotify are the top two most widely used. With the below variable transformation, I can see how many platforms the average consumer is using even if they have one as their “primary”.\n\nstreaming2 <- streaming2 %>%\n  rowwise() %>%\n  mutate(`Number of Platforms Ever Used` = sum(across(starts_with(\"Which music streaming services have you ever used? Select all that apply. - Selected Choice\")), na.rm = T))\n\nI think this survey data is useful in two separate ways, with some overlap. First, I want to look at general usage of streaming services as reported by respondents as I think it can be a good basic level representation of the streaming market. Next, I want to look at sentiment around artist compensation based on some demographics or platforms used.\nFirst I want to generate a few visualizations about what the data tells us at the highest level. From the below four charts, I can gather the following. Most of the respondent base is Age 18-21. Most are premium streaming users, and the amount of time spent streaming is varied, but most commonly in the range of 10-20 hours per week. Finally, most report having ever used somewhere between 2-4 streaming platforms. This doesn’t necessarily speak to how many a consumer uses often, as the question text only specifies “ever”, not a certain timeframe.\n\nstreaming2 %>%\n  filter(!is.na(`Age Group`))%>%\n  ggplot(aes(x=`Age Group`, fill=`Age Group`,  y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  scale_y_continuous(labels = scales::percent_format()) +\n  ggtitle(\"Age of Respondents\")\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Premium User`))%>%\n  ggplot(aes(x=`Premium User`, fill=`Premium User`,  y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  scale_y_continuous(labels = scales::percent_format()) +\n  ggtitle(\"Premium Users\")\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Total Streaming Time`))%>%\n  ggplot(aes(x=`Total Streaming Time`, fill=`Total Streaming Time`,  y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  scale_y_continuous(labels = scales::percent_format()) +\n  ggtitle(\"Total Time Spent Streaming Music\")\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Number of Platforms Ever Used`))%>%\n  ggplot(aes(x=`Number of Platforms Ever Used`, fill=`Number of Platforms Ever Used`,  y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  ggtitle(\"Total Number of Music Streaming Platforms Ever Used\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_x_continuous(breaks = seq(0, 6, by = 1))"
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#music-streaming-platform-usage",
    "href": "posts/finalfinalproject_abbybalint.html#music-streaming-platform-usage",
    "title": "Final Project - Abby Balint",
    "section": "Music Streaming Platform Usage",
    "text": "Music Streaming Platform Usage\nDiving into my analysis, I am starting off by generating a few crosstabs to get a high level view of the data. I decided to use the crosstable package becaus the style of charts it generates allows me to see both percentages and sample sizes with clear headers that display the n values.\nBelow, I decided to look at Age by streaming service used the most. Looking at this crosstab, I noticed that the sample size for Age 36+ is relatively small, so I decided to go back and group up the last two age groups in my tidying code chunk to create this crosstab.\n\nstreaming2%>%\n  filter(!is.na(`Age Group 1`))%>%\n  crosstable(c(`Streaming Used Most`), by=`Age Group 1`, total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n    as_flextable(compact=TRUE, header_show_n=1:6)\n\n\nAge Group 1Total18-21 (N=99)22-35 (N=34)36-50 (N=13)51+ (N=11)Streaming Used MostAmazon Music8 (8.1%)3 (9.1%)2 (18.2%)2 (25.0%)15 (9.9%)Apple Music27 (27.3%)7 (21.2%)1 (9.1%)2 (25.0%)37 (24.5%)Google Play0 (0%)1 (3.0%)0 (0%)0 (0%)1 (0.7%)Other1 (1.0%)0 (0%)1 (9.1%)0 (0%)2 (1.3%)Pandora2 (2.0%)4 (12.1%)1 (9.1%)3 (37.5%)10 (6.6%)Spotify60 (60.6%)17 (51.5%)4 (36.4%)1 (12.5%)82 (54.3%)Tidal1 (1.0%)0 (0%)2 (18.2%)0 (0%)3 (2.0%)YouTube0 (0%)1 (3.0%)0 (0%)0 (0%)1 (0.7%)NA01236Total99 (63.1%)34 (21.7%)13 (8.3%)11 (7.0%)157 (100.0%)\n\n\nLooking at the below crosstab, I can see that Spotify is most popular within every age range, but in particular 18-21. Among the 36+ group the sample is distributed more evenly between the platforms. As many of these other platforms are free to use, I can hypothesize that Ages 36+ are less likely to pay for a premium music subscription.\n\nstreaming2%>%\n  filter(!is.na(`Age Group`))%>%\n  crosstable(c(`Streaming Used Most`), by=`Age Group`, total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n    as_flextable(compact=TRUE, header_show_n=1:6)\n\n\nAge GroupTotal18-21 (N=99)22-35 (N=34)36+ (N=24)Streaming Used MostAmazon Music8 (8.1%)3 (9.1%)4 (21.1%)15 (9.9%)Apple Music27 (27.3%)7 (21.2%)3 (15.8%)37 (24.5%)Google Play0 (0%)1 (3.0%)0 (0%)1 (0.7%)Other1 (1.0%)0 (0%)1 (5.3%)2 (1.3%)Pandora2 (2.0%)4 (12.1%)4 (21.1%)10 (6.6%)Spotify60 (60.6%)17 (51.5%)5 (26.3%)82 (54.3%)Tidal1 (1.0%)0 (0%)2 (10.5%)3 (2.0%)YouTube0 (0%)1 (3.0%)0 (0%)1 (0.7%)NA0156Total99 (63.1%)34 (21.7%)24 (15.3%)157 (100.0%)\n\n\nHere is the same data represented visually two ways. The first chart shows the differing breakdown of age between each platform, while the second chart visually represents both the differences in age group, as well as the popularity of each streaming platform.\n\nstreaming2 %>%\n  filter(!is.na(`Streaming Used Most`))%>%\n  ggplot(aes(`Streaming Used Most`, fill = `Age Group`)) + \n  geom_bar(position=\"fill\"\n           ) + \n  labs(title = \"Music Streaming Platform Most Used by Age\", x=\"Streaming Platform\", y=\"Age\") + \n  theme_bw() +\n  scale_fill_discrete(name=\"Age\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Streaming Used Most`))%>%\n  ggplot(aes(`Streaming Used Most`, fill = `Age Group`)) + \n  geom_bar(position=\"dodge\"\n           ) + \n  labs(title = \"Music Streaming Platform Most Used by Age\", x=\"Streaming Platform\", y=\"Age\") + \n  theme_bw() +\n  scale_fill_discrete(name=\"Age\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nBelow I can see that 81% of the youngest age group subscribes to the premium version of a platform, whereas only 58% of the oldest age group does, speaking to my above prediction. It will be interesting to see how this plays into sentiment around streaming compensation, as we can hypothesize that those not willing to pay for a premium version of a streaming platform may not be inclined to support increased artist compensation.\n\nstreaming2%>%\n  filter(!is.na(`Age Group`))%>%\n  crosstable(c(`Premium User`), by=`Age Group`, showNA=\"no\", total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n    as_flextable(compact=TRUE, header_show_n=1:6)\n\n\nAge GroupTotal18-21 (N=99)22-35 (N=34)36+ (N=24)Premium UserNo18 (18.2%)9 (27.3%)8 (42.1%)35 (23.2%)Yes81 (81.8%)24 (72.7%)11 (57.9%)116 (76.8%)Total99 (63.1%)34 (21.7%)24 (15.3%)157 (100.0%)\n\n\nI generated the below crosstab to investigate which platforms had the highest percentage of premium users. Only Apple Music and Spotify had enough sample to really look at this distribution, but out of the two Apple Music does lead over Spotify.\n\nstreaming2%>%\n  filter(!is.na(`Streaming Used Most`))%>%\n  crosstable(c(`Premium User`), by=`Streaming Used Most`, total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n  as_flextable(compact=TRUE, header_show_n=1:10)\n\n\nStreaming Used MostTotalAmazon Music (N=15)Apple Music (N=37)Google Play (N=1)Other (N=2)Pandora (N=10)Spotify (N=85)Tidal (N=3)YouTube (N=1)Premium UserNo11 (73.3%)2 (5.4%)0 (0%)0 (0%)9 (90.0%)12 (14.1%)1 (33.3%)0 (0%)35 (22.7%)Yes4 (26.7%)35 (94.6%)1 (100.0%)2 (100.0%)1 (10.0%)73 (85.9%)2 (66.7%)1 (100.0%)119 (77.3%)Total15 (9.7%)37 (24.0%)1 (0.6%)2 (1.3%)10 (6.5%)85 (55.2%)3 (1.9%)1 (0.6%)154 (100.0%)\n\n\nAnd represented more visually:\n\nstreaming2 %>%\n  filter(!is.na(`Streaming Used Most`))%>%\n  ggplot(aes(`Streaming Used Most`, fill = `Premium User`)) + \n  geom_bar(position=\"dodge\"\n           ) + \n  labs(title = \"Music Streaming Platform Most Used by Age\", x=\"Streaming Platform\", y=\"Age\") + \n  theme_bw() +\n  scale_fill_discrete(name=\"Premium User\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nIn the below crosstabs, I can see that the number of platforms used remains relatively consistent across demos. However, I believe it would be likely that those who do not have premium subscriptions would be more likely to bounce between platforms, so lets investigate that next.\n\nstreaming2 %>%\n  filter(!is.na(`Age Group`))%>%\n  crosstable(c(`Number of Platforms Ever Used`), by=`Age Group`,showNA=\"no\", total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n    as_flextable(compact=TRUE, header_show_n=1:6)\n\n\nAge GroupTotal18-21 (N=99)22-35 (N=34)36+ (N=24)Number of Platforms Ever UsedMin / Max1.0 / 6.00 / 6.00 / 6.00 / 6.0Med [IQR]3.0 [2.0;4.0]3.0 [2.0;4.0]3.0 [1.8;3.2]3.0 [2.0;4.0]Mean (std)3.4 (1.3)2.9 (1.4)2.6 (1.8)3.2 (1.5)N (NA)99 (0)34 (0)24 (0)157 (0)\n\n\n\ncrosstable(streaming2, c(`Number of Platforms Ever Used`), by=`Gender`,showNA=\"no\", total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n    as_flextable(compact=TRUE, header_show_n=1:6)\n\n\nGenderTotalFemale (N=111)Male (N=44)Non-Binary/Gender neutral/other (N=3)Number of Platforms Ever UsedMin / Max0 / 6.00 / 6.03.0 / 5.00 / 6.0Med [IQR]3.0 [2.0;4.0]3.0 [2.0;4.2]4.0 [3.5;4.5]3.0 [2.0;4.0]Mean (std)3.0 (1.4)3.4 (1.5)4.0 (1.0)3.1 (1.5)N (NA)111 (0)44 (0)3 (0)161 (0)\n\n\nThe data shows this is actually not true, with the median of 3 platforms being the same and the mean being within .2.\n\ncrosstable(streaming2, c(`Number of Platforms Ever Used`), by=`Premium User`,showNA=\"no\", total=\"both\",\n           percent_pattern=\"{n} ({p_col})\", percent_digits=1) %>%\n    as_flextable(compact=TRUE, header_show_n=1:6)\n\n\nPremium UserTotalNo (N=35)Yes (N=119)Number of Platforms Ever UsedMin / Max1.0 / 6.01.0 / 6.00 / 6.0Med [IQR]3.0 [3.0;4.0]3.0 [2.0;4.0]3.0 [2.0;4.0]Mean (std)3.4 (1.1)3.2 (1.4)3.1 (1.5)N (NA)35 (0)119 (0)161 (0)"
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#streaming-usage-conclusion",
    "href": "posts/finalfinalproject_abbybalint.html#streaming-usage-conclusion",
    "title": "Final Project - Abby Balint",
    "section": "Streaming Usage Conclusion",
    "text": "Streaming Usage Conclusion\nTo conclude this first part of my analysis, I think this survey data paints a good base-level depiction of streaming usage. As previously noted, the demographic makeup of the sample is a bit limited, but I think the results do paint a relatively accurate picture of which streaming services are most popular and the prominence of premium streaming subscriptions. I think it also reflects an industry-wide sentiment that varying generations feel differently towards streaming, as those in the Gen Z age range have been exposed to streaming most of their lives and therefore are more likely to see it as the implied way to consume music, particularly in current American society."
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#sentiment-around-artist-streaming-compensation",
    "href": "posts/finalfinalproject_abbybalint.html#sentiment-around-artist-streaming-compensation",
    "title": "Final Project - Abby Balint",
    "section": "Sentiment Around Artist Streaming Compensation",
    "text": "Sentiment Around Artist Streaming Compensation\nThe second half of the survey focuses on asking respondents what they currently know about how much artists are compensated on streaming platforms for 100,000 streams, and then what they believe would be an ideal rate of compensation for 100,000 streams. Although simplistic, I think this is an effective way to gauge what consumers know about how the streaming industry has changed the way artists make money, and to show if consumers feel for artists and wish that they would get paid more.\nBelow we can see that most respondents expect artists to earn low wages per 100,000 streams, with over 20% saying they would guess “under $500”.\n\nstreaming2 %>%\n  filter(!is.na(`Streaming Earnings Guess`))%>%\n  ggplot(aes(`Streaming Earnings Guess`, y= ..prop.., group=1), stat=count) + \n  geom_bar() + \n  labs(title = \"Respondents Predictions for Streaming Earnings\", x=\"Streaming Earnings Prediction\") + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_discrete(limits=c(\"Lower than $500\", \"$501-$1,000\", \"$1,001-$10,000\", \n            \"$10,001-$50,000\", \"$50,001-$100,000\", \"More than $100,000\"))\n\n\n\n\nBelow I generated a series of four charts, the first breaking down by Age and Gender whether respondents believe artists are fairly compensated. Males are slightly more likely to think “no”, while age 36+ is slightly more likely to think “yes”, however across the board the majority response was that no one thinks artists are fairly compensated.\n\nstreaming2 %>%\n  filter(!is.na(`Age Group`))%>%\n  ggplot(aes(x=`Suffient Compensation Agreement`, fill=`Suffient Compensation Agreement`, y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  ggtitle(\"Do respondents believe artists are compensated fairly? By Age\") +\n  facet_wrap (~`Age Group`) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  scale_x_discrete(limits=c(\"Yes\",\"No\", \"I'm not sure\"))\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Gender`))%>%\n  ggplot(aes(x=`Suffient Compensation Agreement`, fill=`Suffient Compensation Agreement`, y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  ggtitle(\"Do respondents believe artists are compensated fairly? By Gender\") +\n  facet_wrap (~`Gender`) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  scale_x_discrete(limits=c(\"Yes\",\"No\", \"I'm not sure\"))\n\n\n\n\nThe following charts demonstrate what respondents think fair pay would be, with a varied response. Most reported somewhere in the middle range of the response options. Males had a more varied response than females. Looking at this distribution, I find it frustrating that the response options contained such wide ranges such as $1,000-$10,000, or $10,001-$50,000, as it makes it difficult to see how respondents really feel because there is a huge difference between the high and low end of these ranges.\nWhat we can see is that users support artists being paid more than what they predicted they are currently paid, which shows consumer support for increased artist wages.\n\nstreaming2 %>%\n  filter(!is.na(`Age Group`)) %>%\n  ggplot(aes(x=`Ideal Compensation`, fill=`Ideal Compensation`, y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  ggtitle(\"Ideal Compensation Rate for Artist by Respondent Age\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  scale_x_discrete(limits=c(\"Lower than $500\", \"$501-$1,000\", \"$1,001-$10,000\", \n            \"$10,001-$50,000\", \"$50,001-$100,000\", \"More than $100,000\"))\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Age Group`)) %>%\n  ggplot(aes(x=`Ideal Compensation`, fill=`Ideal Compensation`, y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  ggtitle(\"Ideal Compensation Rate for Artist by Respondent Age\") +\n  facet_wrap (~`Age Group`) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  scale_x_discrete(limits=c(\"Lower than $500\", \"$501-$1,000\", \"$1,001-$10,000\", \n            \"$10,001-$50,000\", \"$50,001-$100,000\", \"More than $100,000\"))\n\n\n\n\n\nstreaming2 %>%\n  filter(!is.na(`Gender`))%>%\n  ggplot(aes(x=`Ideal Compensation`, fill=`Ideal Compensation`, y=..prop.., group=1), stat=count) + \n  geom_bar()+\n  ggtitle(\"Ideal Compensation Rate for Artist by Respondent Gender\") +\n  facet_wrap (~`Gender`) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  scale_x_discrete(limits=c(\"Lower than $500\", \"$501-$1,000\", \"$1,001-$10,000\", \n            \"$10,001-$50,000\", \"$50,001-$100,000\", \"More than $100,000\"))"
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#artist-compensation-conclusion",
    "href": "posts/finalfinalproject_abbybalint.html#artist-compensation-conclusion",
    "title": "Final Project - Abby Balint",
    "section": "Artist Compensation Conclusion",
    "text": "Artist Compensation Conclusion\nWrapping up this section, I think it’s interesting that across age and gender, the sentiment that artists are unfairly compensated remains true. I think an interesting expansion of this would be to poll consumers on how much they would be wiling to pay for their preferred premium streaming service monthly, as an increase in compensation for artists would likely be followed by a rise in the price of premium services."
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#overall-conclusion",
    "href": "posts/finalfinalproject_abbybalint.html#overall-conclusion",
    "title": "Final Project - Abby Balint",
    "section": "Overall Conclusion",
    "text": "Overall Conclusion\nOverall, I found this dataset to be interesting but limiting in terms of sample size and range of questions. I think this survey serves as a great jumping off point that could be expanded upon if ran again in the future. For example, how much is the public willing to pay for ad-free streaming? Would they be interested in different purchasing options, like tiered price plans based on the amount of ads/features included? I think another piece that can’t be ignored is the sample size limitations of this survey. With a much larger sample, or even more importantly, a more varied respondent base, I think a survey similar to this could give real insight into how streaming users are feeling about this premium service in the market.\nAnother question type that I feel this survey would benefit from is 0-10 or likert scale questions. For example, “How willing would you be to pay more for a streaming service if you knew it paid more to its artists than it’s competitors?”. Or, “On a scale of 1-10, how would you rate your user experience on the following platforms?” for the platforms that each respondent has reported using in the past. These sorts of attitudinal questions, or questions that speak to specific attributes about each platform, could be interesting to combine with data about artist compensation. I would hypothesize that many who report supporting higher compensation for artists support it more in theory than in practice, as users likely look to the most competitive pricing already when choosing a streaming service to use."
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#reflection",
    "href": "posts/finalfinalproject_abbybalint.html#reflection",
    "title": "Final Project - Abby Balint",
    "section": "Reflection",
    "text": "Reflection\nGoing into this class and project as someone who had never used R with no technical background at all, this was challenging overall for me. I decided to use a survey data set because it is what I am most familiar with for my career. What was most challenging for me was picking out focus in on in the dataset in terms of recoding and transforming the data, as well as where to start with analyzing. Although I work with survey data for my job, I do not analyze data but rather work with it from an operational and project management perspective. I tried to focus on the beginnings of analysis here to push myself, and if I was to continue with this project I would look for deeper ways to cross tab the data, like maybe combining questions/demos to create persona-like segments in the data for analysis. As mentioned above, if I was running this survey there are a lot of ideas I had of how it could be expanded upon.\nI realized while working on this project that relatively simple survey data like this that isn’t numerical in nature doesn’t lend itself to the most interesting charts. Rather, the most to discover lays in crosstabs and simple charts and figures that act as more of an accompaniment to a verbal explanation. I tried to use a mix of both charts and crosstabs and break charts out by segment when useful to expand on the limited data as much as possible. To do similar work in the future I would look for a more robust data source, such as a survey with more sample or more questions, or both.\nOverall, I think this project was useful in helping me to utilize what I learned in this class, and wrapping up I feel that I have a good basic understanding of the functionality of R as well as how to structure basic codes. I found myself having an easier and easier time attempting each challenge and leading into the final project, although I still fill as though I only touched the very very tip of the iceberg with an infinite amount still to learn. My goal in learning R is to contribute at my job by having another tool that I am familiar with do things like filter data, provide specific data cuts, and clean or transform where necessary. I think this project helped me in honing those skills."
  },
  {
    "objectID": "posts/finalfinalproject_abbybalint.html#bibliography",
    "href": "posts/finalfinalproject_abbybalint.html#bibliography",
    "title": "Final Project - Abby Balint",
    "section": "Bibliography",
    "text": "Bibliography\nSurvey dataset:\nKorver, Jessica; Falcon, Dr. Silviana (2020), “Facing the Music: The Current State of Streaming Services in the Music Industry”, Mendeley Data, V1, doi: 10.17632/kjv64jz4kz.1\nLink: https://data.mendeley.com/datasets/kjv64jz4kz/1\nCC BY 4.0 : https://creativecommons.org/licenses/by/4.0/\nAccompanying questionnaire:\nKorver, Jessica (2020), “Facing the Music: The Current State of Streaming Services in the Music Industry”, Florida Southern College, pages 50-52.\nLink: https://repository.flsouthern.edu/server/api/core/bitstreams/fc765e08-cd79-440d-a364-7141bb427be1/content\nCourse textbook:\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media.\nLink: https://r4ds.had.co.nz/explore-intro.html\nR programming language:\nR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nR Packages:\nTidyverse: Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686.\nggplot2: H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\ndpylr: Wickham H, François R, Henry L, Müller K (2022). dplyr: A Grammar of Data Manipulation. R package version 1.0.10, https://CRAN.R-project.org/package=dplyr.\njanitor: Firke S (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 2.1.0, https://CRAN.R-project.org/package=janitor.\ncrosstable: Chaltiel D (2022). crosstable: Crosstables for Descriptive Analyses. R package version 0.5.0, https://CRAN.R-project.org/package=crosstable.\ngmodels: Warnes GR, Bolker B, Lumley T, SAIC-Frederick RCJCfRCJaC, Program IFbtIR, NIH ot, Institute NC, NO1-CO-12400. CfCRuNC (2022). gmodels: Various R Programming Tools for Model Fitting_. R package version 2.18.1.1, https://CRAN.R-project.org/package=gmodels.\nreadxl: Wickham H, Bryan J (2022). readxl: Read Excel Files. R package version 1.4.1, https://CRAN.R-project.org/package=readxl.\nforcats: Wickham H (2022). forcats: Tools for Working with Categorical Variables (Factors). R package version 0.5.2, https://CRAN.R-project.org/package=forcats."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html",
    "href": "posts/FinalProject_KavyaHarlalka.html",
    "title": "Final Project - Layoffs since pandemic",
    "section": "",
    "text": "Code\n# Import important libraries needed for functions\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(summarytools)\nlibrary(dplyr)\nlibrary(formattable)\nlibrary(lubridate)\nlibrary(gridExtra)\nlibrary(treemapify)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#introduction",
    "href": "posts/FinalProject_KavyaHarlalka.html#introduction",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Introduction",
    "text": "Introduction\nOne of the biggest challenges faced by people today are the layoffs going on across the globe. The layoffs are affecting not just a single country or industry but almost all of them. Companies like Amazon, Meta, Twitter, Netflix, Coinbase have all had mass layoffs this year. And it seems to be continuing further with more and more companies joining the list. Mass layoffs are devastating not just for the individuals being laid off but also for the economony as a whole.\nThe layoffs are a result of the overhiring that happened when the economy recovered after the pandemic. Tech companies were especially prone to it - Microsoft, Meta and Alphabet increased their company size by 20% after the pandemic. Now, all these companies have to deal with the costs of overhiring and the result is cutting jobs. More than 105,000 people have lost their jobs and as per Bloomberg, the layoff count has hit near pandemic level. Some companies have imposed a hiring freeze instead of laying employees off.\nThe tech sector is not the only one affected, housing sales have also slowed with high home prices and rising interest rates on loans and this has caused large layoffs in the real estate market. Investment banks and financial institutions have been affected as well with mass layoffs being announced in companies like Goldman Sachs, Wells Fargo and Credit Suisse.\nThe aim of this research project is to delve into these layoffs and analyze which countries and industries have been affected the most by it. We will also look into finding some useful insights and patterns to these layoffs."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#dataset",
    "href": "posts/FinalProject_KavyaHarlalka.html#dataset",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Dataset",
    "text": "Dataset\nThe dataset being used is a pre-existing one originally picked from layoffs.fyi, a crowdsourced database of tech layoffs, taken till 12/29/22. It gives details of all the layoffs that have happened since March 2020 when the pandemic hit. It contains 1696 mass layoff details which have taken place in 161 cities of 55 countries and affecting 28 industries since March 2020. The data shows that 1,495 tech companies have sacked 246,267 employees since the onset of Covid-19, but 2022 has been the worst year for the tech sector and early 2023 can be even grimmer.\nThe following columns in the dataset are of note :-\n\nCompany - The organization from which the layoffs took place. This piece of information would give us an idea which companies are having the largest layoffs all over the world.\nLocation - This gives us the City in which the layoff has happened. This can give us an idea about which areas are affected the most.\nLaid_Off_Count - This gives us the number of employees laid off. It will give the total counts for any kind of grouping we would want to do to further analyze the data.\nPercentage - It gives the value of percentage that the laid off employees form giving an insight on how much percent of employees were let go and which companies are going through the highest percentage lay offs.\nDate - Gives the date on which the layoff took place, will help give a distribution of the layoffs taking place providing a good idea about the peaks and the lowest points of the layoff periods.\nStage - The stage of funding for the company, giving an insight into which categories are facing the biggest problems in this period.\nCountry - The country in which the layoff took place. The highest layoffs are currently going on in the United States and that is clearly reflected in the data analysis done below of this dataset."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#possible-research-queries",
    "href": "posts/FinalProject_KavyaHarlalka.html#possible-research-queries",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Possible research queries",
    "text": "Possible research queries\nSome of the possible research questions that come up when we look at this dataset are :-\n\nWhich sector/industry has been impacted the most with this layoffs trend?\nWhich countries are suffering from the highest number of layoffs?\nWhat kind of funding stage are most companies at when laying off people?\nWhen was the peak observed in the layoffs trend? What could be the possible reason for it?"
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#the-data",
    "href": "posts/FinalProject_KavyaHarlalka.html#the-data",
    "title": "Final Project - Layoffs since pandemic",
    "section": "The Data",
    "text": "The Data\nWe are now going to fetch the data and view it. There are two columns that are there in the dataset we do not need for our analysis. These are Source (the source of the lay off information) and List_of_Employees_Laid_Off. Hence, after reading in the data, we ignore these two columns before continuing analysis on it.\n\n\nCode\n# Read data from the csv file\ndata <- read.csv('_data/Dataset_detailing_layoffs.csv')\n\n# Remove unnecessary columns\ndata <- subset(data, select = -c(Source, List_of_Employees_Laid_Off))\n\n# Printing the read data as a paged table\npaged_table(data)\n\n\n\n\n  \n\n\n\nWe can see all of the data in the paged table above. As it can be seen, the dataset is huge and hence cannot be analyzed as is. We will have to order and sort the dataset to get a better idea of what it can convey."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#summary-of-data",
    "href": "posts/FinalProject_KavyaHarlalka.html#summary-of-data",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Summary of data",
    "text": "Summary of data\nFirst step is going to be printing and analyzing the summary of the complete dataset to give us an idea how to approach it.\n\n\nCode\n# Get and print summary of the dataset to help us analyze it.\nprint(dfSummary(data))\n\n\nData Frame Summary  \ndata  \nDimensions: 1696 x 10  \nDuplicates: 0  \n\n-------------------------------------------------------------------------------------------------------------------\nNo   Variable         Stats / Values               Freqs (% of Valid)    Graph                 Valid      Missing  \n---- ---------------- ---------------------------- --------------------- --------------------- ---------- ---------\n1    Company          1. Uber                         5 ( 0.3%)                                1696       0        \n     [character]      2. Better.com                   4 ( 0.2%)                                (100.0%)   (0.0%)   \n                      3. DataRobot                    4 ( 0.2%)                                                    \n                      4. Gopuff                       4 ( 0.2%)                                                    \n                      5. Latch                        4 ( 0.2%)                                                    \n                      6. Loft                         4 ( 0.2%)                                                    \n                      7. Netflix                      4 ( 0.2%)                                                    \n                      8. OYO                          4 ( 0.2%)                                                    \n                      9. Patreon                      4 ( 0.2%)                                                    \n                      10. Shopify                     4 ( 0.2%)                                                    \n                      [ 1411 others ]              1655 (97.6%)          IIIIIIIIIIIIIIIIIII                       \n\n2    Location         1. SF Bay Area               452 (26.7%)           IIIII                 1696       0        \n     [character]      2. New York City             193 (11.4%)           II                    (100.0%)   (0.0%)   \n                      3. Boston                     74 ( 4.4%)                                                     \n                      4. Los Angeles                72 ( 4.2%)                                                     \n                      5. Seattle                    60 ( 3.5%)                                                     \n                      6. Bengaluru                  54 ( 3.2%)                                                     \n                      7. London                     51 ( 3.0%)                                                     \n                      8. Toronto                    46 ( 2.7%)                                                     \n                      9. Sao Paulo                  42 ( 2.5%)                                                     \n                      10. Berlin                    41 ( 2.4%)                                                     \n                      [ 151 others ]               611 (36.0%)           IIIIIII                                   \n\n3    Industry         1. Finance                   208 (12.3%)           II                    1696       0        \n     [character]      2. Retail                    141 ( 8.3%)           I                     (100.0%)   (0.0%)   \n                      3. Healthcare                122 ( 7.2%)           I                                         \n                      4. Transportation            108 ( 6.4%)           I                                         \n                      5. Food                      107 ( 6.3%)           I                                         \n                      6. Marketing                 106 ( 6.2%)           I                                         \n                      7. Real Estate                99 ( 5.8%)           I                                         \n                      8. Consumer                   81 ( 4.8%)                                                     \n                      9. Media                      75 ( 4.4%)                                                     \n                      10. Other                     74 ( 4.4%)                                                     \n                      [ 18 others ]                575 (33.9%)           IIIIII                                    \n\n4    Laid_Off_Count   Mean (sd) : 197.2 (572.9)    229 distinct values   :                     1197       499      \n     [integer]        min < med < max:                                   :                     (70.6%)    (29.4%)  \n                      3 < 70 < 11000                                     :                                         \n                      IQR (CV) : 119 (2.9)                               :                                         \n                                                                         :                                         \n\n5    Percentage       Mean (sd) : 0.3 (0.3)        69 distinct values    : :                   1137       559      \n     [numeric]        min < med < max:                                   : :                   (67.0%)    (33.0%)  \n                      0 < 0.2 < 1                                        : : .                                     \n                      IQR (CV) : 0.2 (1)                                 : : : .           .                       \n                                                                         : : : : :         :                       \n\n6    Date             1. 2020-04-02                  27 ( 1.6%)                                1696       0        \n     [character]      2. 2020-04-03                  25 ( 1.5%)                                (100.0%)   (0.0%)   \n                      3. 2020-03-27                  23 ( 1.4%)                                                    \n                      4. 2020-04-01                  21 ( 1.2%)                                                    \n                      5. 2020-04-08                  21 ( 1.2%)                                                    \n                      6. 2022-11-15                  20 ( 1.2%)                                                    \n                      7. 2022-08-09                  17 ( 1.0%)                                                    \n                      8. 2020-03-31                  16 ( 0.9%)                                                    \n                      9. 2022-06-29                  16 ( 0.9%)                                                    \n                      10. 2020-04-07                 15 ( 0.9%)                                                    \n                      [ 390 others ]               1495 (88.1%)          IIIIIIIIIIIIIIIII                         \n\n7    Funds_Raised     Mean (sd) : 877.5 (6450.5)   549 distinct values   :                     1575       121      \n     [numeric]        min < med < max:                                   :                     (92.9%)    (7.1%)   \n                      0 < 132 < 121900                                   :                                         \n                      IQR (CV) : 336.5 (7.4)                             :                                         \n                                                                         :                                         \n\n8    Stage            1. Unknown                   292 (17.2%)           III                   1696       0        \n     [character]      2. Series B                  239 (14.1%)           II                    (100.0%)   (0.0%)   \n                      3. Series C                  235 (13.9%)           II                                        \n                      4. IPO                       232 (13.7%)           II                                        \n                      5. Series D                  180 (10.6%)           II                                        \n                      6. Series A                  141 ( 8.3%)           I                                         \n                      7. Acquired                  117 ( 6.9%)           I                                         \n                      8. Series E                   90 ( 5.3%)           I                                         \n                      9. Seed                       58 ( 3.4%)                                                     \n                      10. Series F                  41 ( 2.4%)                                                     \n                      [ 5 others ]                  71 ( 4.2%)                                                     \n\n9    Date_Added       1. 2020-03-28 20:52:49         40 ( 2.4%)                                1696       0        \n     [character]      2. 2020-03-29 22:30:11          1 ( 0.1%)                                (100.0%)   (0.0%)   \n                      3. 2020-03-29 22:30:34          1 ( 0.1%)                                                    \n                      4. 2020-03-29 22:31:11          1 ( 0.1%)                                                    \n                      5. 2020-03-29 22:31:33          1 ( 0.1%)                                                    \n                      6. 2020-03-29 22:31:57          1 ( 0.1%)                                                    \n                      7. 2020-03-29 22:32:22          1 ( 0.1%)                                                    \n                      8. 2020-03-29 23:31:27          1 ( 0.1%)                                                    \n                      9. 2020-03-30 13:58:13          1 ( 0.1%)                                                    \n                      10. 2020-03-30 13:59:33         1 ( 0.1%)                                                    \n                      [ 1647 others ]              1647 (97.1%)          IIIIIIIIIIIIIIIIIII                       \n\n10   Country          1. United States             1119 (66.0%)          IIIIIIIIIIIII         1696       0        \n     [character]      2. India                      104 ( 6.1%)          I                     (100.0%)   (0.0%)   \n                      3. Canada                      77 ( 4.5%)                                                    \n                      4. Brazil                      54 ( 3.2%)                                                    \n                      5. United Kingdom              52 ( 3.1%)                                                    \n                      6. Germany                     48 ( 2.8%)                                                    \n                      7. Israel                      37 ( 2.2%)                                                    \n                      8. Australia                   32 ( 1.9%)                                                    \n                      9. Singapore                   24 ( 1.4%)                                                    \n                      10. Indonesia                  20 ( 1.2%)                                                    \n                      [ 45 others ]                 129 ( 7.6%)          I                                         \n-------------------------------------------------------------------------------------------------------------------\n\n\nWe can see that there are more than 1400 companies data, 161 cities in 55 countries and affecting 28 industries. Thus we cannot analyze each and every value of each and every column since. We will only be analyzing the ones that have been impacted the most since it would not give us in-depth insight we need if we consider all the values of such a large datasets. Reducing the number of values also produce better and more insightful visualizations."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#analysis-of-country-wise-data",
    "href": "posts/FinalProject_KavyaHarlalka.html#analysis-of-country-wise-data",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Analysis of Country wise data",
    "text": "Analysis of Country wise data\nLet us start with the most critical analysis, which countries have been impacted the most by this layoff trend. We have 28 countries in the dataset and the data needs to be grouped for each unique country for us to be able to get this information.\n\n\nCode\n# Group dataset by Country, ignoring values that have country as NA\ndataGroupedByCountry <- data %>%\n  group_by(Country) %>% \n  filter(!is.na(Country))\n\n# Analyze the grouped data by finding the total laid off count per country\ndataGroupedByCountry <- dataGroupedByCountry %>%\n  summarise(\n    Total_Laid_Off = sum(Laid_Off_Count, na.rm = TRUE)\n  ) %>%\n  arrange(Country)\npaged_table(dataGroupedByCountry)\n\n\n\n\n  \n\n\n\nThough we get an idea on the number of layoffs in each country, we do not get any real comparison just from the table. We need visualizations to be able to better compare and analyze the data we see in this table.\n\n\nCode\ndataGroupedByCountry %>%\n  top_n(20) %>%\n  ggplot(aes(y=Country, x=Total_Laid_Off, fill = Country)) +\n      geom_col() +\n      theme(axis.text.x=element_text(size=20, angle=30, hjust=1),\n            axis.title.x = element_text(size=20),\n            axis.text.y=element_text(size=20),\n            axis.title.y = element_text(size=20)) +\n       labs(title = \"Layoffs by Country 2022\",\n            x = \"Country\", \n            y = \"Laid off count\")\n\n\n\n\n\nThis graph helps us gain perspective of layoffs in the highest 20 countries. As can be seen from the bar chart, the United States has the highest layoff count with more than 150000 layoffs. None of the other countries has such a significant number of layoffs. The second highest is India with around 30000 layoffs.\nThe above visualization gives us an idea in count, now we should consider percentages and composition.\n\n\nCode\ndataGroupedByCountryPercentage <- dataGroupedByCountry %>%\n  mutate(across(where(is.numeric), ~ formattable::percent(./sum(.)))) %>%\n  arrange(desc(Total_Laid_Off)) %>%\n  top_n(20)\n\nggplot(dataGroupedByCountryPercentage, aes(x=\"\", y=Total_Laid_Off, fill=Country)) +\n  geom_bar(stat=\"identity\", width=1, color=\"white\") +\n  coord_polar(\"y\", start=0)\n\n\n\n\n\nThe above pie chart gives us the distribution of layoffs taking place across the top 20 countries. United States constitutes almost 65% of the total layoffs all across the world.\nThere are a few main reasons that one could speculate for the United States having the highest number of layoffs.\n\nOne of the biggest reasons is the inflation that is currently affecting the country. A lot of United States Dollars was printed during the pandemic which resulted in the inflation. In response to higher inflation, the Federal Reserve has raised the effective Federal Funds interest rate from 0.08% in January 2022 to 3.08% at the end of September 2022. Hence this brought about recession, taking debit has become costlier for the startups and investors are also not readily investing. This has directly affected the economy and caused the startups and organizations to compensate with laying off employees.\nAnother reason is the overhiring that occured especially in the FAANG companies (Facebook, Apple, Amazon, Netflix and Google) mainly in the United States during the post pandemic period. They increased their headcount by over 80% between 2019 and 2021 and are now correcting for their optimism."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#citywise-layoffs-in-the-united-states",
    "href": "posts/FinalProject_KavyaHarlalka.html#citywise-layoffs-in-the-united-states",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Citywise Layoffs in the United States",
    "text": "Citywise Layoffs in the United States\nNow we check citywise layoffs in the country with the highest layofffs - United States.\n\n\nCode\ndataUnitedStates <- data %>%\n  subset(Country=='United States') %>%\n  group_by(Location) %>%\n  summarise(\n              Total_Laid_Off = sum(Laid_Off_Count, na.rm = TRUE)\n            ) %>%\n  mutate(across(where(is.numeric), ~ formattable::percent(./sum(.))))\n\ndataUnitedStates %>%\n  ggplot(aes(area = Total_Laid_Off, fill = Location, label = Location)) +\n  geom_treemap() +\n  geom_treemap_text() +\n  theme(legend.position=\"none\")\n\n\n\n\n\nSF Bay Area has been the center of Mass layoffs. From Salesforce to Twitter to Meta, thousands of U.S. workers have lost their jobs in brutal mass layoffs in 2022.\nFor a growing number of companies, there have been second and third rounds of cuts. These include Stripe, which cut around 1,000 in November after laying off around 50 people (from TaxJar, a Stripe acquisition) earlier this year, and Lyft, which slashed 683 from its team after laying off 60 people in July. In May, Netflix cut 150 staff members from its workforce and laid off 450 more in June.\nMeta announced it is laying off 13% of its staff, or more than 11,000 employees, through a staff letter CEO Mark Zuckerberg wrote on Nov. 3. The company saw overall sales fall 4% to $27.71 billion in the latest quarter. Operating income dropped 46% from the previous year, while costs and expenses rose 19% to $22.1 billion.\nAfter the deal to take over Twitter for $44 billion closed, the social media company’s new owner, Elon Musk, fired Twitter’s CEO along with several top executives. It was previously suggested he would cut 75% of its pre-takeover workforce. He has since walked that notion back but the company did announce layoffs to half its workforce, with smaller cuts for the team responsible for preventing the spread of misinformation.\nMost of the tech companies are based off SF Bay Area and hence the highest number of layoffs can be observed to be there itself."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#companies-with-highest-layoffs",
    "href": "posts/FinalProject_KavyaHarlalka.html#companies-with-highest-layoffs",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Companies with highest layoffs",
    "text": "Companies with highest layoffs\nOut of the complete dataset, we check which companies have had the highest number of layoffs.\n\n\nCode\ndataTop20Companies <- data %>%\n  top_n(10, Laid_Off_Count)\n\ndataTop20Companies %>%\n  mutate(Company = fct_reorder(Company, Laid_Off_Count)) %>%\n  ggplot(aes(x=Company, y=Laid_Off_Count)) + \n  geom_col()\n\n\n\n\n\nThe companies with the highest layoffs are clearly Meta and Amazon\nFacebook parent company Meta announced last week it would cut 13% of its staff or 11,000 workers, with Meta CEO Mark Zuckerberg saying he overestimated how long the pandemic’s e-commerce boom would last.\nEarly last week, reports indicated Amazon expected to cut 10,000 employees – a small fraction of its 1.5 million workers – would include tech as well as corporate staffers, according the The New York Times. The layoffs, which would be the largest in the company’s history, are expected to continue into next year.\nOne common thread among the companies laying off large numbers of workers in a booming economy is a mismatch between the number of managers and the number of lower-end workers,"
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#funding-stages",
    "href": "posts/FinalProject_KavyaHarlalka.html#funding-stages",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Funding Stages",
    "text": "Funding Stages\nNow we look at the distribution of layoffs among the funding stage in each of the top 20 countries.\n\n\nCode\ndataTop20Countries <- data[data$Country %in% dataGroupedByCountryPercentage$Country,]\n\ndataTop20Countries %>%\n  ggplot(aes(fill=Stage, y=Country, x=Laid_Off_Count)) + \n      geom_bar(position=\"stack\", stat=\"identity\")\n\n\n\n\n\nAs can be seen from the graph, companies at the funding stage of IPO have the highest layoffs count. This is because the largest mass layoffs were all in the big tech firms (FAANG). So even though startups did have a lot of layoffs due to the debt crisis and recession, the number of people laid off is much higher for the large firms. Also, the publicly traded firms (like Meta) saw huge stock price tanks and had to reduce workforce to compensate for the huge losses."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#layoffs-from-industry-perspective",
    "href": "posts/FinalProject_KavyaHarlalka.html#layoffs-from-industry-perspective",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Layoffs from industry perspective",
    "text": "Layoffs from industry perspective\nLet us now compare layoffs with the industries for each of the top 20 countries.\n\n\nCode\ntest <- dataTop20Countries %>% \n  group_by(Country) %>%\n  nest() %>% \n  # Add a new column with the ggplot objects\n  mutate(plots = pmap(.l = list(data, as.character(Country)), \n                      ~ ggplot(data = (..1 %>%      # first element of .l\n                                         group_by(Industry) %>%\n                                         summarise(\n                                            Total_Laid_Off = sum(Laid_Off_Count, na.rm = TRUE)\n                                          )  %>%\n                                          arrange(desc(Total_Laid_Off))) %>%\n                                          mutate(across(where(is.numeric), ~ formattable::percent(./sum(.))))) + \n                          aes(x = Industry, # expose the x and y variables\n                              y = Total_Laid_Off) +\n                          geom_point(size=3) +\n                          geom_segment(aes(x=Industry, \n                             xend=Industry, \n                             y=0, \n                             yend=Total_Laid_Off)) +\n                          coord_flip() +\n                          labs(title = str_to_title(str_glue('{..2}')))))\n\ngrid.arrange( grobs = test$plots, nrow = 5 )\n\n\n\n\n\nCOVID 19 affected specific sectors badly like transportation, consumer and real estate due to social distancing to control outbreak. As vaccination rates inched up, these sectors did start performing better, but 2022 recession fears have led to renewed layoffs in almost all industries. Retail and consumer are again hit the hardest because of inflation in 2022.\nFood sector was also hit in some countries first because of supply-demand crunch during COVID 19 and then due to inflation in 2022. Finance was also affected because of rapid economic downturns since 2020.\nHealthcare, media, marketing and logistics remained relatively prone to layoffs and were doing fine during COVID 19 outbreak. Vaccinations for COVID 19 is still crucial so healthcare is doing fine in 2022 as well. Crypto noticed a rapid rise with bitcoin halving in 2020. Several crypto startups were found and the field thrived in 2020 and 2021. In 2022 there were layoffs in crypto sector as well. Fitness industry took a hit because of social distancing as well."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#layoffs-over-time",
    "href": "posts/FinalProject_KavyaHarlalka.html#layoffs-over-time",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Layoffs over time",
    "text": "Layoffs over time\nLet us now analyze layoffs since 2020.\n\n\nCode\ndata %>%\n  mutate(\n    Date2 = ymd(Date) # convert date field to date type (currently string)\n  ) %>%\n  ggplot(aes(y=Laid_Off_Count, x=Date2)) + \n      geom_point() +\n       labs(title = \"Layoffs over time\",\n            x = \"Date\", \n            y = \"Laid off count\")\n\n\n\n\n\nIn 2020 there was a surge in layoffs, especially for startups and specific sectors, due to the efforts to contain COVID 19 and it led businesses to suspend operations or close, resulting in a record number of temporary layoffs.\nBig tech was generally doing fine. After COVID 19, there was an immediate recovery in the economy, which was not stable at all. Startups that were benefitting from pandemic boom are feeling the pressure in 2022 because debt has become costlier in FED’s efforts to control inflation. Massive overhiring in tech sector during periods of rapid growth is leading to mass layoffs in 2022. The public markets have been hit hard in 2022, and that’s trickled down to the private markets. Recession concerns, rising interest rates and geopolitical issues have all contributed to a roller-coaster stock market.\nOne more reason for layoffs could be that the publicly traded companies working on a fiscal or calendar year are about to hit a very important stockholder time frame. For example, it’s Q4 earnings report and business review for calendar year operating corporations, which usually means hiring freezes and quarterly reviews. Hence, combined with recession fears, this adds to the huge surge in layoffs during Q4 of 2022."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#reflection",
    "href": "posts/FinalProject_KavyaHarlalka.html#reflection",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Reflection",
    "text": "Reflection\nI learnt a lot throughout the process of analyzing and visualizing from the Layoffs dataset. The reason I chose this dataset is its relevance today and how it is affecting everyone around me. The dataset in itself is interesting and taken from a crowdsourced site. I initially started off with getting a summary of the dataset. This helps me understand the kind of values that are there in each of the columns and how I could possibly use the columns to answer important research queries. I began the project with the most basic research query that came to my mind when I looked at the dataset - which country is most affected by this layoffs season. To gain an insight into this, I had to group the dataset by country and then get the total number of layoffs in each of the countries. Then selecting the top 20 countries with the highest total laid off count gave me an idea on which countries were impacted the most. In this case, it was the United States.\nUpon further analysis, I found that the layoffs were occuring in higher amounts not just in specific countries but also in specific locations. So I decided to check the country with the highest layoffs (United States) and group them using location. This helped my gain insight into the fact that SF Bay Area was affected the most by the layoffs and most of these layoffs were from big tech firms. Then I looked at the companies with highest layoffs by simply sorting the dataset by number of layoffs and getting the top ones. Indeed Meta, Amazon and Uber were the among top ones.\nI also looked at the funding stages that the layoffs were occuring at. This helped me understand whether the startups or big firms were the ones causing higher lay offs. To get these, I took the countries with the highes layoffs and got their distribution with respect to the Funding Stage. It was clear that layoffs were highest for organizations at the IPO stage.\nNext, I faced my biggest challenge in the project. It was time to delve into the industries being affected. I did not want to visualize it for just one country but for all the ones with high layoff count. To do this, I had to learn to print graphs with clarity on a small scale and print multiple graphs in the same row. It took research but I was able to figure out a relatively clean implementation that clearly conveyed what I wanted to show. The sectors affected in each of the countries were clearly visible and we are able to draw inferences from those visualizations.\nFinally, I decided to look at layoffs over the period of time starting with the pandemic till the last date in the dataset. I used scatterplot to plot the layoffs taking place on each date. I had to convert the date column from a char column to a proper date column to be able to print a visually clear and correct scatterplot. It helped me analyze further into the topic and draw conclusions on the layoffs trend that has been going on since the pandemic period.\nTHough I am happy with the results, I believe I could provide better visualizations. There are certain features I could not get as properly working like facet wrap and could be made better use of. Also, there could be more versatile kinds of graphs that could be used throughout for the visualizations instead of mainly the bar graph."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#conclusion",
    "href": "posts/FinalProject_KavyaHarlalka.html#conclusion",
    "title": "Final Project - Layoffs since pandemic",
    "section": "Conclusion",
    "text": "Conclusion\nIn this research, we analyzed the layoffs that have been taking place since the start of the pandemic and upto the recent recession. We have delved into the various aspect of the layoffs and compared it with respect to the location, industry, company, funding stage and date. We have seen that the highest layoffs have taken place in the United States (especially in the SF Bay Area) and analyzed what factors could possibly be the reason contributing to such mass layoffs. We have also looked at the companies like Meta and Amazon having the highest layoffs and the possible reasons behind them. We checked the funding stages at which the companies are laying off to see how much startups are contributing to the lay offs as compared to big firms. Finally, we looked at the trend of layoffs that have been continuing since the pandemic period.\nThe winter of layoffs is here. Large-scale layoffs have taken place across the world, as the US is staring at an impending recession.In 2008, tech companies laid off about 65,000 employees, and a similar number of workers lost their livelihoods in 2009, according to data by global outplacement & career transitioning firm Challenger, Gray & Christmas. In comparison, 965 tech companies have laid off more than 150,000 employees this year globally, surpassing the Great Recession levels of 2008-2009, According to a MarketWatch report, layoffs are part of a strategy by tech firms to maintain viability through 2023 and beyond."
  },
  {
    "objectID": "posts/FinalProject_KavyaHarlalka.html#references",
    "href": "posts/FinalProject_KavyaHarlalka.html#references",
    "title": "Final Project - Layoffs since pandemic",
    "section": "References",
    "text": "References\n\nlayoffs.fyi\nR programming language\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media.\nhttps://www.investopedia.com/biggest-layoffs-2022-6826521\nhttps://mondo.com/insights/mass-layoffs-in-2022-whats-next-for-employees/\nhttps://www.bloomberg.com/news/articles/2022-11-08/the-pace-of-tech-job-cuts-is-reaching-early-pandemic-levels?leadSource=uverify%20wall\nhttps://abc7news.com/tech-layoff-tracker-bay-area-layoffs-doordash-meta/12434385/\nhttps://www.computerworld.com/article/3680448/what-amazon-twitter-meta-and-others-got-wrong-with-layoffs.html"
  },
  {
    "objectID": "posts/finalproject_RyanODonnell.html",
    "href": "posts/finalproject_RyanODonnell.html",
    "title": "Final Project - Samplepalooza",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(dataRetrieval)\nlibrary(rvest)\nlibrary(sf)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/finalproject_RyanODonnell.html#final-project-goals",
    "href": "posts/finalproject_RyanODonnell.html#final-project-goals",
    "title": "Final Project - Samplepalooza",
    "section": "Final Project Goals",
    "text": "Final Project Goals\nIn addition to meeting the requirements for DACSS 601, my goal with this final project is to catch up with the data analysis on all six years of Samplepalooza data. I last did a comprehensive report on the data through 2019. We have since collected two more years’ of data and I owe the participants and other stakeholders a thorough look at the data collected so we can decide on the future of Samplepalooza.\nThe main question that needs to be answered by this data is what is the impact of each tributary on the watershed as a whole. When water samples are collected, the results are measured in concentrations: how much nitrogen/phosphorus/chloride is present in a liter of water. To measure the impact, we also need to know how much water there is at this concentration; this is called loading. To calculate loading, we multiply the concentration by the flow. We can also use the loading to calculate the yield, which is loading per area of land. This way we can see which tributaries are contributing the most of the measured parameter to the system by comparing the loads and which are maybe contributing more of their fair share by comparing the yields. Once the loading and yields are calculated, this can be used to direct nutrient reduction efforts within the watershed to have the most impact.\nThis project will start the process of developing tools that I can use moving forward to sort, clean, analyze, and store all of the data I collect at work in a more efficient way. I have spent the past few years making the same charts over and over for each river, each year in Excel and I’m ready to move beyond that!"
  },
  {
    "objectID": "posts/finalproject_RyanODonnell.html#sample-results",
    "href": "posts/finalproject_RyanODonnell.html#sample-results",
    "title": "Final Project - Samplepalooza",
    "section": "Sample Results",
    "text": "Sample Results\nSamplepalooza samples were collected annually(ish) at locations throughout the Connecticut River watershed, which encompasses parts of Vermont, New Hampshire, Massachusetts, and Connecticut, from the Canadian border down to Long Island Sound. It occurred in 2014-2015 and 2018-2021. Not all sites were sampled for all years. Sites were always sampled for two parameters: total nitrogen (TN) and total phosphorus (TP); some years they were also sampled for chloride (Cl). It is typically done on a single day, but in 2021, severe weather caused some sites to be sampled on one day and the rest on another day. Because of this 2021 anomaly, the only date stored in this data is for 2021 and the rest just have an associated year.\nThis data was originally stored in a wide format with each sampling site as its own row and the columns were the site metadata and then each year-parameter combo. This was how the data came to me after 2015 and I continued using this format, but it was quickly becoming unwieldy! I have since learned that the best practice for storing and analyzing this data (and is the way required by all online databases I need to upload to) is that each individual sample should have its own row; therefore, a case is site/site info/date/parameter.\n\n\nCode\n# Created from homeworks 2-3\n\nsplza_orig <- read_xlsx(\"_data/14_21SamplepaloozaResults.xlsx\",\n                        skip = 2,\n                        col_names = c(\"SiteName\", \"SiteID\", \"SiteType\", \"State\", \"EcoRegion\", \"2014-TN\", \"2015-TN\", \"2018-TN\", \"2019-TN\", \"2020-TN\", \"2021-TN\", \"2014-TP\", \"2015-TP\", \"2018-TP\", \"2019-TP\", \"2020-TP\", \"2021-TP\", \"2014-Cl\", \"2015-Cl\", \"2020-Cl\", \"2021-Cl\", \"Date2021\",\"Lat\", \"Lon\"))\n\n# step 1 - recode chloride data \n# step 2 - convert column types\n# step 3 - pivot data and relocate new columns\n# step 4 - add sample date and result unit columns\n# step 5 - remove 2021Date column\n\nsplza_tidy <- splza_orig %>%\n  mutate(`2014-Cl` = recode(`2014-Cl`, \"<3\" = \"1.5\", \"< 3\" = \"1.5\"),\n         `2015-Cl` = recode(`2015-Cl`, \"<3\" = \"1.5\", \"< 3\" = \"1.5\"),\n         `2021-Cl` = recode(`2021-Cl`, \"<3\" = \"1.5\", \"< 3\" = \"1.5\"),\n         .keep = \"all\") %>%\n  type_convert() %>%\n  pivot_longer(col = starts_with(\"20\"),\n               names_to = c(\"Year\", \"Parameter\"),\n               names_sep = \"-\",\n               values_to = \"ResultValue\",\n               values_drop_na = TRUE,\n               ) %>%\n  relocate(Year:ResultValue, .before = SiteType) %>%\n  mutate(\"SampleDate\" = case_when(\n    Year == 2014 ~ ymd(\"2014-08-06\"),\n    Year == 2015 ~ ymd(\"2015-09-10\"),\n    Year == 2018 ~ ymd(\"2018-09-20\"),\n    Year == 2019 ~ ymd(\"2019-09-12\"),\n    Year == 2020 ~ ymd(\"2020-09-17\"),\n    Year == 2021 ~ ymd(paste(`Date2021`))), \n    .after = `Year`) %>%\n  mutate(\"ResultUnit\" = case_when(\n    Parameter == \"TN\" ~ \"mg-N/L\",\n    Parameter == \"TP\" ~ \"\\U03BCg-P/L\" ,\n    Parameter == \"Cl\" ~ \"mg-Cl/L\"),\n    .after = `ResultValue`) %>%\n  select(-`Date2021`)\n  \n  splza_tidy\n\n\n\n\n  \n\n\n\nWhile reading in the original data, I named the columns to make pivoting them later on more easy by using a “Year-Parameter” pattern for each. I recoded the results that were below the detection limit of the testing method. The standard practice is to use half of the detection limit (or twice, if it was above the limit) in calculations. With the non-numeric data now replace, I converted the columns so that all the results were stored as numeric values. I used pivot_longer() to establish each individual case of a single sample result. I created a “Date” column and either pulled the 2021 date value from the original data or manually added the rest of the dates. Finally, I added Now my data was much tidier! I used this version of the data for some practice visualizations in my Homework 3."
  },
  {
    "objectID": "posts/finalproject_RyanODonnell.html#streamflow-and-loading",
    "href": "posts/finalproject_RyanODonnell.html#streamflow-and-loading",
    "title": "Final Project - Samplepalooza",
    "section": "Streamflow and Loading",
    "text": "Streamflow and Loading\nThe second data set needed is the estimated daily streamflow, measured in cubic feet per second (cfs). The United States Geological Survey (USGS) has gages that collect continuous streamflow information at many locations throughout the watershed. A few of the Samplepalooza sites are co-located with gaging stations, some are on the same river as a gaging station but not at the same location, and many are on ungaged streams. I can estimate the streamflow at most locations by taking the streamflow (cfs) from 1 or 2 gaging stations either on the same river or nearby, dividing by the drainage area (sq mi) to the gage (cfs /sq mi), and then multiplying by the drainage area for the sampling location (back to cfs). While there are certainly more complicated methods of modeling streamflow that would take into account precipitation, land use, elevation, etc. into account, this is sufficient for what is already a fairly low resolution sampling project.\nTo make things more complicated, there are a couple sites that also have flow information from US Army Corps of Engineers (USACOE) flood control dams. New England’s USACOE flood control dams usually only hold back water during significant rainfall or snowmelt to prevent flooding downstream, then slowly release it in the days after. This means that the flows may be significantly different from nearby streams without these dams. While one such site also has a USGS gage downstream of the dam, one only has the USACOE data, so I also needed to be able to incorporate this type of data into the calculations as well.\nI started with the code written by my volunteer, Travis (see Appendix for complete code). It consisted almost entirely of “For” loops and I didn’t understand it at all, so Dr. Rolfe graciously re-interpreted it into the tidyr format. I then went back over the code, from beginning to end, and tweaked it to get everything working properly. I’ll go through each step to show the process for gathering the streamflow data and calculating the loading and yield.\n\nStep 1 - USGS Gage and USACOE IDs\n\n\nCode\n# Comments starting with ### are from volunteer, ## are from MR helping convert the original code to tidyr, # are from Ryan\n\n### Disables scientific notation\noptions(scipen=999)\n\n### Bring in data on sampling locations (drainage area and USGS gages to use for flow estimation)\n# Add leading zero for USGS gages\n\nsplza_gages_orig <- read_xlsx(\"_data/Samplepalooza_gages.xlsx\") %>%\n  mutate(Gage1 = ifelse(is.na(Gage1), NA_character_, \n                        str_c(\"0\", Gage1, sep=\"\")),\n         Gage2 = ifelse(is.na(Gage2), NA_character_, \n                        str_c(\"0\", Gage2, sep=\"\"))) %>%\n  select(-c(\"Name\",\"Notes\",\"Lat\",\"Lon\"))\n\n# Attach Gage and Dam IDs to Results\nsplza_results_gages <- left_join(\n  splza_tidy,\n  splza_gages_orig,\n  by = \"SiteID\")\n\nresults_gages_display <- splza_results_gages %>%\n  select(-c(\"Year\", \"SiteType\", \"State\", \"EcoRegion\", \"Lat\", \"Lon\", \"OtherFlowSource\"))\n\nresults_gages_display\n\n\n\n\n  \n\n\n\nFirst, I needed to read in a second Excel sheet that contained the sample locations, drainage areas, and 1-2 USGS gage IDs and/or a USACOE dam ID. Then I joined it to the results by the site ID.\n\n\nStep 2 - Pull USGS Gage Flows and Drainage Areas\n\n\nCode\n# Create list of gages and sample dates to pull from USGS database\n# readNWISdv is slow so reducing the number of queries as much as possible speeds up the following steps\n\nsplza_gages_dates <- splza_results_gages %>%\n  select(c(\"SampleDate\",\"Gage1\",\"Gage2\")) %>%\n  pivot_longer(cols = c(\"Gage1\",\"Gage2\"),\n    names_to = NULL,\n    values_to = \"GageID\") %>%\n  drop_na(\"GageID\") %>%\n  distinct()\n\n# map streamflows from USGS for each date/gage combo\n# thanks to MR for simplifying this for me!\n\ngage_flows_by_date <-  purrr::map2_dfr(splza_gages_dates$GageID, splza_gages_dates$SampleDate,\n                     ~readNWISdv(siteNumbers=.x, \n                                 parameterCd = \"00060\",\n                                 startDate = .y,\n                                 endDate = .y)) %>%\n  rename(\"GageID\" = \"site_no\",\n         \"daily_cfs\" = \"X_00060_00003\") %>%\n  select(c(\"GageID\", \"Date\", \"daily_cfs\")) %>%\n  tibble()\n\n# USGS gage drainage areas\n\ngages_only <- splza_gages_dates %>%\n  select(\"GageID\") %>%\n  unique()\n\ngage_drainage <- purrr::map_dfr(gages_only$GageID,\n             ~readNWISsite(siteNumbers = .x)) %>%\n  rename(\"GageID\" = \"site_no\",\n         \"gage_drainage\" = \"drain_area_va\") %>%\n  select(c(\"GageID\", \"gage_drainage\")) %>%\n  tibble()\n\n# calculate cfsm (cubic feet per second per square mile)\n\ngages_cfsm <- left_join(\n  gage_flows_by_date,\n  gage_drainage,\n  by = \"GageID\") %>%\n  mutate(\"gage_daily_cfsm\" = daily_cfs/gage_drainage) %>%\n  select(-c(\"daily_cfs\", \"gage_drainage\"))\n\ngages_cfsm\n\n\n\n\n  \n\n\n\nUSGS offers an R package called dataRetrieval, which allows you to access their hydrologic and water quality data directly through R. To reduce the numbers of queries to their database, I created a list of unique gage and date combinations from the Samplepalooza data. I used Purr to map the readNWISdv() function which retrieves the daily streamflow data from the USGS National Water Information System (NWIS) from a specific data and gage. I further refined the list to unique gage numbers and used readNWISsite(), which pulls all the available information on a gage and pulled out the drainage area for each gage. Finally, using this combination of data, I calculated the daily streamflow per square mile (cfsm) for each gage and date combination.\n\n\nStep 3 - Pull USACOE Dam Flows\n\n\nCode\n# USACoE Dam Flow Data\n# Thanks to MR for initially taming this beast\n\n# Dates and Dam codes for inquiry\n\nsplza_dams_dates <- splza_results_gages %>%\nselect(c(\"SampleDate\",\"USACOE_dam_id\", \"USACOE_dam_drainage\")) %>%\n  drop_na(\"USACOE_dam_id\") %>%\n  distinct()  \n\n### In some instances, two USGS gages were used and the values for each gage were averaged. When USACOE dams were used, the R script downloads the hourly inflow and outflow values, averages them to make a daily mean inflow and daily mean outflow, and then proceeds as if they were stream gage discharges.\n\n# function to read hourly dam outflow information and calculate average daily cfs\n\ndam_daily_cfs <-function(date, dam_id){  \n  url <- paste0(\"https://reservoircontrol.usace.army.mil/nae_ords/cwmsweb/cwms_web.apitable.table_display?gagecode=\",dam_id,\"&days=1&enddate=\",format.Date(date,\"%Y-%m-%d\"),\"&interval_hrs=1\")\n  dam_html <- read_html(url)\n  dam_tables <- html_nodes(dam_html,\"table\")\n  dam_info_list <- html_table(dam_tables[2], fill = TRUE, header = TRUE, trim = TRUE)\n  dam_flow_tibble <- dam_info_list[[1]]\n  dam_daily_flow <- summarize(dam_flow_tibble, \n                              \"dam_daily_cfs\" = mean(parse_number(dam_flow_tibble$`Flow(cfs)`))) %>%\n    mutate(\"SampleDate\" = rep(date),\n           \"USACOE_dam_id\" = rep(dam_id))\n  \n  return(dam_daily_flow)\n}\n\n# calculate daily cfs\n\ndam_cfs <- purrr::map2_dfr(splza_dams_dates$SampleDate, splza_dams_dates$USACOE_dam_id,\n                          ~dam_daily_cfs(date = .x,\n                                        dam_id = .y))\n\n# calculate  dams daily cfsm\n\ndams_cfsm <- left_join(\n  splza_dams_dates,\n  dam_cfs,\n  by= c(\"USACOE_dam_id\", \"SampleDate\")) %>%\n  mutate(\"dam_cfsm\" = dam_daily_cfs/USACOE_dam_drainage) %>%\n  select(-c(\"USACOE_dam_drainage\", \"dam_daily_cfs\"))\n\ndams_cfsm\n\n\n\n\n  \n\n\n\nNext, I needed to do the same thing for the two sites with USACOE dam information. Unfortunately, there wasn’t an R package that interfaces easily with their data. They do post their flows online and there is a tidyverse package called rvest that can be used to “harvest” data from a website. Dr. Rolfe suggested writing a function to pull the web info and then using Purrr to map the dam IDs and dates to the function. The original code pulled the hourly data and then created a daily average from those numbers, so I did that as well. The drainage to the dam could also be pulled from the web using the same procedure, but in this case, the dam drainage areas were already in the spreadsheet with gage information. I was then able to follow the same calculation as above to calculate the daily cfsm for these dam ID and date combinations.\n\n\nStep 4 - Estimate Flows, Loading and Yield\n\n\nCode\n# bind flows to results and estimate flows at site\n\nsplza_est_flows <- left_join(\n  splza_results_gages,\n  gages_cfsm,\n  by = c(\"SampleDate\" = \"Date\", \"Gage1\" = \"GageID\")) %>%\n  \n  rename(\"Gage1_cfsm\" = gage_daily_cfsm) %>%\n  \n  left_join(\n    .,\n    gages_cfsm,\n    by = c(\"SampleDate\" = \"Date\", \"Gage2\" = \"GageID\")) %>%\n  \n  rename(\"Gage2_cfsm\" = gage_daily_cfsm) %>%\n  \n  left_join(\n    .,\n    dams_cfsm,\n    by = c(\"SampleDate\", \"USACOE_dam_id\")) %>%\n  # Multiply cfsm by drainage\n  # 25-CNT has a special  calculation due to its location\n  # See notes on gages for reasoning\n  group_by(SiteID, SampleDate) %>%\n    mutate(\"Est_cfs\" = case_when(\n       SiteID == \"25-CNT\" ~ (Gage1_cfsm * (Drainage_sq_mi - 221) + (Gage2_cfsm * 221)),\n       SiteID != \"25-CNT\" ~ (mean(c(Gage1_cfsm, Gage2_cfsm, dam_cfsm), na.rm = TRUE)) * Drainage_sq_mi)) %>%\n  ungroup() %>%\n  select(-c(Gage1:dam_cfsm))\n  \n### Calculate loading\n### Note: 5.393771 and 0.005394 are calculated conversion factors to result in pounds/day\n\n### mg_L_conversion_factor = mg/L * ft3/second * 28.3168 L/1 ft3 * 1 lb/453592 mg * 86400 second/1 day\n### ug_L_conversion_factor = mg/L * ft3/second * 28.3168 L/1 ft3 * 1 lb/453592370 ug * 86400 second/1 day\n\nmg_L_conversion_factor <- 5.393771\nug_L_conversion_factor <- 0.005394\n\nsplza_results_loading <- splza_est_flows %>%\n  mutate(\n    \"Est_loading\" = case_when(\n    str_detect(ResultUnit, \"mg\") ~ (Est_cfs * ResultValue * mg_L_conversion_factor),\n    str_detect(ResultUnit, \"\\u03bcg\") ~ (Est_cfs * ResultValue * ug_L_conversion_factor)),\n    \"Est_yield\" = (Est_loading / Drainage_sq_mi)\n  )\n\nsplza_results_loading\n\n\n\n\n  \n\n\n\nFinally, I joined the cfsm values to the results and estimated the flows at each site. For all except one site, this is done by multiplying the average of the cfsm values available by the drainage area to the sites. There is one site with a slightly different method due to its proximity to various gages and dams. I then multiplied the estimated flows by the concentrations from the sample results and a conversion factor that puts the loading values into pounds per day. Then I did one last calculation by dividing the loading by the site drainage area to get the yield in pounds per day per square mile."
  },
  {
    "objectID": "posts/finalproject_RyanODonnell.html#graphs",
    "href": "posts/finalproject_RyanODonnell.html#graphs",
    "title": "Final Project - Samplepalooza",
    "section": "Graphs",
    "text": "Graphs\nRaw water quality data is difficult for people to understand without visualizations that guide them towards understanding whether the result is good or bad.\n\nRecreating Old Graphs\nFor all of these graphs, I made sure that the sites were listed from North to South. This is useful because as the Connecticut River flows from north to south, the river acquires more water (and pollutants) and starts to flow from mountains which have naturally low nutrients to valley floors which naturally have higher nutrients.\n\n\nCode\nggplot(mainstem, aes(y = fct_reorder(SiteName, Lat), x = ResultValue, fill = Year)) +\n  geom_col(color = \"black\", position = \"dodge\") +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(x = \"Results\", y = \"Site Location\") +\n  theme_light() +\n  facet_wrap(vars(Parameter), scales = \"free_x\", labeller = labeller(Parameter = parameter_labs))\n\n\n\n\n\nSamplepalooza Mainstem Results\n\n\n\n\nThis is a look at all of the results from the mainstem sites over the years. As you can see, Connecticut was added later in the project life. In general, the concentrations increase from north to south, though they seem to peak at Middletown. Essex is brackish, so the chloride is omitted for that location since the water is naturally salty.\n\n\nCode\nggplot(tributary, aes(y = fct_reorder(SiteName, Lat), x = ResultValue, fill = Year)) +\n  geom_col(color = \"black\", position = \"dodge\") +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(x = \"Results\", y = \"Site Location\") +\n  theme_light() +\n  facet_grid(cols = vars(Parameter), \n             rows = vars(EcoRegion), \n             scales = \"free\", \n             labeller = labeller(Parameter = parameter_labs, EcoRegion = label_both))\n\n\n\n\n\nSamplepalooza Tributary Results\n\n\n\n\nThese are the results from all the tributaries, split up by EcoRegion and parameter. We expect to see higher nutrients in EcoRegion XIV than in VIII, which is demonstrated here. The tributaries are arranged from north to south again, but each tributary is affected also by how much urbanization and agriculture is within it, rather than its north-south orientation.\n\n\nCode\nggplot(tributary, aes(x = Est_loading, y = fct_reorder(SiteName, Drainage_sq_mi), fill = State)) +\n  geom_bar(color = \"black\", stat = \"summary\", fun = \"median\") +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(x = \"Median Loading (lbs/day)\", y = \"Site Locations\") +\n  theme_light() +\n  facet_grid(\n    cols = vars(Parameter), \n    rows = vars(EcoRegion), \n    scales = \"free\", \n    labeller = labeller(Parameter = loading_labs, EcoRegion = label_both))\n\n\n\n\n\nLoading by Watershed Size\n\n\n\n\nThis graph has been very popular when I presented the results in 2019. I was easily able to also split it by EcoRegion using facet_grid(). If each tributary was contributing its “fair share” of chloride, nitrogen, and phosphorus, they would also (more or less) go from largest loading to smallest in each EcoRegion; we would also expect EcoRegion XIV. Some noteable over contributers are the Farmington, Millers, Scantic, and Fort Rivers.\n\n\nNew Data Exploration\n\n\nCode\nggplot(tributary, aes(y = Est_loading, x = ResultValue, color = State)) +\n  geom_point() +\n  scale_fill_brewer(palette = \"Spectral\") +\n  labs(x = \"Concentration\", y = \"Loading (lbs/day\") +\n  theme_light() +\n  facet_wrap(vars(Parameter), \n             scales = \"free\",\n             labeller = labeller(Parameter = loading_labs))\n\n\n\n\n\nConcentration versus Loading in Samplepalooza Tributaries\n\n\n\n\nThis graph shows the concentration versus the loading of each tributary sample. It is interesting that most of the sites contributing the highest loading of each parameter do not have the highest concentration, especially with chloride. If we wanted to direct restoration efforts on particular tributaries, those efforts should be targeted to watersheds that fall in the upper right quandrant of these graphs to have the biggest impact.\n\n\nCode\nggplot(tributary, aes(x = State, y = ResultValue, fill = Parameter)) +\n  geom_violin() +\n  scale_fill_brewer(palette = \"Spectral\") +\n  theme_light() +\n  guides(x = guide_axis(n.dodge = 2), fill = \"none\") +\n  facet_grid(rows = vars(Parameter), \n             cols = vars(EcoRegion), \n             scales = \"free\",\n             labeller = labeller(EcoRegion = label_both, Parameter = parameter_labs))\n\n\n\n\n\nDistribution of Tributary Concentrations\n\n\n\n\nI was particularly interested in the utility of the violin plots. I have been criticized in the past by other professionals in my field for my reluctance to use box and whisker plots. Aside from the fact that they are nearly impossible to make in Excel, I have conducted some informal polls and found that non-scientists do not understand the information a box and whisker plot is trying to convey. These violin plots seem to present the distribution a little more intuitively. In this graph, we can see that Vermont and New Hampshire have bottom heavy plots (which is what we would like to see) and Connecticut and Massachusetts have more even distribution across the ranges of concentrations.\n\n\nCode\nggplot(tributary, aes(x = State, y = Est_loading, fill = Parameter)) +\n  geom_violin() +\n   scale_fill_brewer(palette = \"Spectral\") +\n  labs(y = \"Estimated Loading (lbs/day)\") +\n  theme_light() +\n  guides(fill = \"none\") +\n   facet_grid(rows = vars(Parameter), \n              scales = \"free\",\n              labeller = labeller(Parameter = loading_labs))\n\n\n\n\n\nDistribution of Loading\n\n\n\n\nThis is the same graph but looking at loading. Here, we can see that all the plots are bottom heavy with Connecticut and Massachusetts having the heavier hitting tributaries compared to New Hampshire and Vermont."
  },
  {
    "objectID": "posts/finalproject_RyanODonnell.html#maps",
    "href": "posts/finalproject_RyanODonnell.html#maps",
    "title": "Final Project - Samplepalooza",
    "section": "Maps",
    "text": "Maps\n\n\nCode\n# Watershed States\nwatershed_states <- map_data(\"state\") %>% \n  filter(region %in% c(\"connecticut\", \"massachusetts\", \"vermont\", \"new hampshire\")) %>% \n           filter(!subregion %in% c(\"martha's vineyard\", \"nantucket\"))\n# map files\ntrib_map <- st_read(\"_data/maps/Splza_tribs.shp\") %>%\n  fortify() %>%\n  mutate(SiteID = str_trim(SiteID))\n\nmainstem_map <- st_read(\"_data/maps/Splza_main.shp\") %>%\n  fortify() %>%\n  mutate(SiteID = str_trim(SiteID))\n\n# join with data \ntrib_map_data <- splza_results_loading_no_outliers %>%\n   filter(SiteType == \"Tributary\") %>%\n  full_join(.,\n  trib_map,\n  by = c(\"SiteID\" = \"SiteID\")) %>% #not sure why it made me do this, but just putting SiteID didn't work\n    st_as_sf()\n\nmain_map_data <- splza_results_loading_no_outliers %>%\n   filter(SiteType == \"Mainstem\") %>%\n  full_join(.,\n  mainstem_map,\n  by = c(\"SiteID\" = \"SiteID\")) %>%\n    st_as_sf()\n\n\nMaking maps is also very important for presenting water quality data in a way that makes sense to the average viewer. I primarily present my work to the volunteers who collect the data, at public meetings, or to school children. The easier to understand, the better!\nTo set up these maps, I created a base of the four watershed states using the maps package, which I first did in Homework 3. I used the USGS StreamStats program and GIS to make a shapefile of all the tributaries and the mainstem sites that would be easy to join with the data in R. This map layer already existed prior to this project. I read this file in using the sf package and joined it with the data. Then the shapes were ready to be plotted in ggplot2.\n\n\nCode\nfilter(trib_map_data, Parameter == \"TN\") %>%\nggplot() +\n  geom_polygon(data = watershed_states, aes(x = long, y = lat, group = region), color = \"dark green\", linetype = \"dotted\", fill=\"green\", alpha=0.3) +\n  expand_limits(x = watershed_states$long, y = watershed_states$lat) +\n  scale_shape_identity() +\n  coord_map() +\n  geom_sf(aes(fill = ResultValue, color = Exceeds_criteria)) +\nscale_fill_distiller(palette = \"Blues\", direction = 1, breaks = c(0, 0.4, 0.7, 1.5, 3), name = \"Total Nitrogen\\n(mg-N/L)\") +\n  scale_color_discrete(name = \"EcoRegion Criteria\") +\n  theme_void() +\n  facet_wrap(vars(Year))\n\n\n\n\n\nTributary Nitrogen Concentrations by Year\n\n\n\n\nUsing ggplot2 and facet_wrap() to create these maps is very useful. I used to have to make a different layer for each year in GIS and export it separately to put in my reports.\nIn this figure, you can see the relative concentrations and the outline indicates whether or not the watershed meets the criteria for its EcoRegion. It is particularly stark that only the Scantic River watershed is the darkest blue.\n\n\nCode\nfilter(main_map_data, Parameter == \"TP\") %>%\nggplot() +\n  geom_polygon(data = watershed_states, aes(x = long, y = lat, group = region), color = \"dark green\", linetype = \"dotted\", fill=\"green\", alpha=0.3) +\n  expand_limits(x = watershed_states$long, y = watershed_states$lat) +\n  scale_shape_identity() +\n  coord_map() +\n  geom_sf(aes(fill = ResultValue, color = Exceeds_criteria)) +\nscale_fill_distiller(palette = \"Reds\", direction = 1, breaks = c(0, 10, 31, 60, 125), name = \"Total Phosphorus\\n(\\u03bc/L)\") +\n  scale_color_discrete(name = \"EcoRegion Criteria\") +\n  theme_void() +\n  facet_wrap(vars(Year))\n\n\n\n\n\nMainstem Phosphorus Concentrations by Year\n\n\n\n\nThis is an example of a similar map with the mainstem sites instead of the tributaries.\n\n\nCode\nfilter(trib_map_data, Parameter == \"Cl\") %>%\n  group_by(SiteName) %>%\n  mutate(\"avg_yield\" = mean(Est_yield)) %>%\nggplot() +\n  geom_polygon(data = watershed_states, aes(x = long, y = lat, group = region), color = \"dark green\", linetype = \"dotted\", fill=\"green\", alpha=0.3) +\n  expand_limits(x = watershed_states$long, y = watershed_states$lat) +\n  scale_shape_identity() +\n  geom_sf(aes(fill = avg_yield)) +\n  scale_fill_distiller(palette = \"Purples\", direction = 1, name = \"Chloride\\nAverage Yield\\n(lbs/day/sq mi)\") +\n  theme_void()\n\n\n\n\n\nTributary Chloride Yields\n\n\n\n\nFinally, I made an example of the yields. The yields are the loading (lbs/day) adjusted for the area of the watershed. This helps adjust the concentrations to the impact of the watershed. Since the sizes of the watersheds are represented in the image, the yield is useful for mapping loading."
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html",
    "href": "posts/Final_MatthewNorberg.html",
    "title": "Olympic Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(ggpubr)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#olympic-success",
    "href": "posts/Final_MatthewNorberg.html#olympic-success",
    "title": "Olympic Analysis",
    "section": "Olympic Success",
    "text": "Olympic Success\nFor most of us, competing in the Olympic Games is nothing more than a fantasy. It would be quite an accomplishment to represent your country in the Games, let alone earn a medal. However, for many of the athletes competing in the Games, the goal is to earn a gold medal, rather than just competing. So what does it mean to be a “successful” athlete in the Olympic Games? Are you successful if you make it to the Games or are you only successful if you earn a medal? For the purposes of this paper, I am going to define Olympic success as having earned a medal in the Olympic Games.\nWhat do the most successful athletes in the Olympic Games have in common? Do athletes who win a lot of medals have similar physical characteristics in common that could explain why they have been so successful? In this paper, I analyzed results the results from each Olympic Games dating back to the first modern games in 1896 up to the Olympic Games in held in 2016 to answer these questions."
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#data-description",
    "href": "posts/Final_MatthewNorberg.html#data-description",
    "title": "Olympic Analysis",
    "section": "Data Description",
    "text": "Data Description\nThe dataset comes as a set of two csv files: ‘athlete_events’ and ‘noc_regions’ where each file represents a table in a relational database. Each row in this table contains information about an athlete who competed in an Olympic event. Note that athletes can occur in the table more than once if they competed in multiple events or games. A good example of an athlete who appears many times in the dataset is Michael Phelps. Consequently, a case should be treated as the combination of the athlete name, year, and event.\n\n# Find all the rows containing information about Michael Phelps\nfilter(athletes, str_detect(Name, \"Michael\") & str_detect(Name, \"Phelps\")) %>%\n  select(-c(ID, NOC, Sex, Height, Weight, Season))\n\n\n\n  \n\n\n\n\nBasic Information About The Data\nEach athlete in the dataset is given a unique id to identify them. This is useful in cases where two athletes have the same name, but are different people. In total, there are 135,404 distinct id’s in the dataset indicating that the total number of athletes in the dataset is 135,404. The following tibble lists the number of athletes who compete for each region in the dataset. Additionally, the graph proceeding the tibble illustrates how the number of athletes competing in the games has grown over time.\n\nathletes %>%\n  group_by(region) %>%\n  summarise(num_athletes = n_distinct(ID)) %>%\n  arrange(region)\n\nathletes %>%\n  group_by(Year, Season) %>%\n  summarise(n = n_distinct(ID), .groups = \"keep\") %>%\n  ggplot(mapping = aes(x = Year, y = n, color = Season)) +\n  geom_point() + \n  geom_line() +\n  labs(y = \"Numbr of Athletes\", title = \"Number of Athletes Competing In Games Over Time\")\n\n\n\n  \n\n\n\n\n\n\nThe dataset covers Olympic data from 1896 to 2016. Throughout this time span, there was 29 occurrences of the Summer Games and 22 occurrences of the Winter Games which is verified with the query below. I found it surprising that there were 29 occurrences of the Summer Games instead of 31. If we use the common knowledge that the games are held every four years and we assume that none of the years are missing, then 29 occurrences of the Summer Games would indicate that the last year in the dataset is 2008 (4 * (29 - 1) + 1896 = 2008), not 2016. There are two reasons which explain why there are only 29 occurrences instead of 31:\n\nThe Games were cancelled4 in 1916, 1940, and 1944.\nThe Games were held in 1906 even though they were held in 1904 and 1908 which means there was period in time when the Games were not held every four years.\n\n\nathletes %>%\n  group_by(Season) %>%\n  summarise(Num_Games = n_distinct(Year))\n\n\n\n  \n\n\n\nIn total there are 66 sports included in the dataset. Each sport includes some number of events for athletes to compete in. There are 765 events in the dataset of which 554 are male events and the remaining 269 are female. The following graph illustrates how the number of events in each game has grown over time.\n\nathletes %>%\n  group_by(Year, Season) %>%\n  summarise(n = n_distinct(Event), .groups = \"keep\") %>%\n  ggplot(mapping = aes(x = Year, y = n, color = Season)) +\n  geom_point() + \n  geom_line() +\n  labs(y = \"Numbr of Events\", title = \"Number of Events Included In The Games Over Time\")"
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#cleaning-the-data",
    "href": "posts/Final_MatthewNorberg.html#cleaning-the-data",
    "title": "Olympic Analysis",
    "section": "Cleaning The Data",
    "text": "Cleaning The Data\n\nInitial Transformations\nWe can start by dropping the ‘Games’ column because each entry in the column is a combination of the ‘Year’ and ‘Season’ columns. Additionally, we can drop the ‘notes’ column which comes from the ‘noc’ file because most of the values are not available and the values which are available are not relevant to this analysis.\nWe can clean up the ‘Medal’ column by replacing the values that are not available with the string “None”. Based on the context of the data set, we can assume that a NA in this column means that the athlete did not win a Medal in that event. Therefore, replacing these values with “None” seems logical.\n\n# Get rid of games and notes columns\nathletes <- select(athletes, -c(Games, notes))\n\n# Switch NA in Medal to None\nathletes <- athletes %>% replace_na(list(Medal = \"None\"))\n\n\n\nHandling Missing Values\nThere are four columns in the data set which contain missing values: ‘Region’, ‘Age’, ‘Height’, and ‘Weight’. I started by handling the missing values in the ‘region’ column first. We can find the ‘NOC’ associated with unknown regions by querying the ‘noc_regions’ data set.\n\n# Display NA in region\nfilter(noc, is.na(region))\n\n\n\n  \n\n\n\nWe know that any row in the ‘athletes’ tibble that has an ‘NOC’ value of ‘ROT’, ‘TUV’, or ‘UNK’ will have a value in the ‘Region’ column that is NA. I think it would be sensible to replace the NA values according to the table below. The chunk of code below was used to replace the missing values according to the mapping in the table.\n\n\n\nNOC\nRegion\nNotes\n\n\n\n\nROT\nNone\nRefugee Olympic Team\n\n\nTUV\nTuvala\nTuvala\n\n\nUNK\nUnknown\nUnknown\n\n\n\n\n# Fill in missing values in Region column\nathletes <- athletes %>%\n            mutate(Temp = case_when(NOC == \"ROT\" ~ \"None\",\n                                      NOC == \"TUV\" ~ \"Tuvala\",\n                                      NOC == \"UNK\" ~ \"Unknown\")) %>%\n            mutate(Region = case_when(is.na(region) ~ Temp,\n                                      !is.na(region) ~ region)) %>%\n            select(-c(region, Temp))\n\nLet’s handle the missing values in the ‘Age’, ‘Height’, and ‘Weight’ columns next. Here’s a simple algorithm that we can use to replace all the values in those columns. I will use the ‘Age’ column to describe the algorithm but the steps are the same for the other columns.\nSuppose we find a row in the data set which has a missing value in the ‘Age’ column. We know that the value in the ‘Year’ column of that row is not missing because the ‘Year’ column does not contain any missing values. The same is true for the ‘Sex’ and ‘Sport’ columns. Therefore, we can compute the average athlete age during the given year, sex, and event (ignoring other missing values) and replace the missing value with the average value.\nFirst, let’s calculate the average age, height, and weight for each year, sex, and event grouping in the data set. This is calculated and stored in the tibble using the code chunk below.\n\n# Compute average age, height, and weight by years\nAverageValues <- athletes %>%\n                 group_by(Year, Sex, Event) %>%\n                 summarise(AverageAge = round(mean(Age, na.rm = TRUE), digits = 0),\n                           AverageHeight = round(mean(Height, na.rm = TRUE), digits = 0),\n                           AverageWeight = round(mean(Weight, na.rm = TRUE), digits = 1),\n                           .groups = \"keep\")\n# Display the results\nAverageValues\n\n\n\n  \n\n\n\nWe can see that many of the values in the tibble displayed above are not a number (‘NaN’). The ‘Nan’ values in the tibble occur when every athlete that is included in the average calculation for a group has a missing value. For example, the first ‘Nan’ value in the tibble occurs for the (1896, ‘M’, ‘Athletics Men’s 800 metres’) grouping in the ‘AverageHeight’ and ‘AverageWeight’ column. This is due to the fact that every male athlete in the dataset who competed in the ‘Athletics Men’s 800 metres’ event has a missing age and weight as confirmed by the query below.\n\nathletes %>%\n  filter(Year == 1896 & Sex == 'M' & Event == 'Athletics Men\\'s 800 metres') %>%\n  select(Name, Height, Weight)\n\n\n\n  \n\n\n\nDue to the presence of the ‘NaN’ values in the tibble, we will not be able to replace all of the NA values in the tibble. However, using this approach we will be able to replace many of them. In the case that we can not resolve a missing value, we will simply leave it is NaN and exclude it from further calculations.\n\nathletes <- inner_join(athletes, AverageValues, by = c('Year', 'Sex', 'Event')) %>%\n            mutate(Age = case_when(!is.na(Age) ~ Age,\n                                    is.na(Age) & !is.nan(AverageAge) ~ AverageAge)) %>%\n            mutate(Height = case_when(!is.na(Height) ~ Height,\n                                       is.na(Height) & !is.nan(AverageHeight) ~ AverageHeight)) %>%\n            mutate(Weight = case_when(!is.na(Weight) ~ Weight,\n                                       is.na(Weight) & !is.nan(AverageWeight) ~ AverageWeight)) %>%\n            select(-c(AverageAge, AverageHeight, AverageWeight))\n\n\n\nCleaning Sport & Event\nThere is another transformation I would like to perform to make the data set a bit cleaner. Take a look at the first few values in the ‘Sport’ and ‘Event’ columns presented below.\n\n# Display first few rows of Sport and Event columns\nathletes %>%\n  select(Sport, Event, Sex) %>%\n  head()\n\n\n\n  \n\n\n\nNotice how the first word or set of words in the ‘Event’ column is the same exact string in the ‘Sport’ column. If this pattern is true for the entire dataset, then we can remove the ‘Sport’ portion of the event column to make it look nicer. The code chunk below was used to test whether or not this pattern is true throughout the entire dataset. I used the testCols function to confirm the pattern in the ‘Sport’ and ‘Event’ column was true for the entire dataset. After I confirmed that the property was true, I ran the cleanEvent function to clean the columns.\n\ntestCols <- function() {\n  # Iterate along sport column\n  for (i in seq_along(athletes$Sport)) {\n    # Get the sport and event string at row i\n    # Split each string on space character\n    sport <- str_split(athletes$Sport[[i]], \" \")[[1]]\n    event <- str_split(athletes$Event[[i]], \" \")[[1]]\n    # Iterate over items in sport list\n    for (j in seq_along(sport)) {\n      # Check to see if each item in sport is equivalent to corresponding item in event\n      if (sport[[j]] != event[[j]]) {\n        # If it is not equivalent -> stop and display sport and event which breaks pattern\n        cat(\"BREAK\\n\", sport, \" \", event, \"\\nPattern does not hold\")\n        return(FALSE)\n      }\n    }\n  }\n  # If we get here, then the property holds for the entire data set\n  cat(\"Pattern holds for entire data set\\n\")\n  return(TRUE)\n}\n\n# Test Columns\ntestCols()\n\n# Function to clean event column\ncleanEvent <- function() {\n  # Iterate along and event columns\n  for (i in seq_along(athletes$Sport)) {\n    # Split sport and event string at row i on space character\n    sport <- str_split(athletes$Sport[[i]], \" \")[[1]]\n    event <- str_split(athletes$Event[[i]], \" \")[[1]]\n    # Empty string to store new event\n    newEvent <- \"\"\n\n    # Iterate along event list\n    for (j in seq_along(event)) {\n      # Once we get past the length of sport, start piecing together newEvent\n      if (j > length(sport)) {\n        newEvent = str_c(newEvent, event[[j]], \" \")\n      }\n    }\n\n    # Reset newEvent string at position i\n    athletes$Event[[i]] <- newEvent\n  }\n\n  # Return cleaned column to save changes\n  return(athletes$Event)\n}\n\n# Perform event column cleaning\nathletes$Event <- cleanEvent()\n\nIt may be tempting to remove the ‘Sex’ portion of the ‘Event’ column as well. For instance, in the tibble shown above, the first row is (‘Basketball’, ‘Basketball Men’s Basketball’, ‘M’). We could technically change the value in the ‘Event’ column to just ‘Basketball’. In doing so, we would not lose any information as we know the athlete is a male by looking at the sex column. However, the pattern that is present in the output above where the sex of the athlete follows the value in the ‘Sport’ column is not true for every row in the dataset. Therefore, we would be losing information by removing the term following the value in the ‘Sport’ column.\n\n\nReordering, Sorting, & Saving\nLastly, let’s finish the cleaning process by reordering the columns and sorting them to make the data set easier to look at. Afterwards, I saved the clean dataset so I would not need to repeat the cleaning operations perfomed above.\n\n# Change column order and sort by year\nathletes <- athletes %>%\n  relocate(Team, .after = Name) %>%\n  relocate(NOC, .after = Team) %>%\n  relocate(Region, .after = NOC) %>%\n  arrange(Year)\n\n# Save cleaned data set\nwrite_csv(athletes, './Data/athletes_clean.csv')"
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#comparing-medalists-vs-non-medalists",
    "href": "posts/Final_MatthewNorberg.html#comparing-medalists-vs-non-medalists",
    "title": "Olympic Analysis",
    "section": "Comparing Medalists Vs Non-Medalists",
    "text": "Comparing Medalists Vs Non-Medalists\nLet’s start by investigating the age, height, and weight columns of the data set for the set of medalists and non-medalists. This was a tricky calculation to perform because athletes can occur in the dataset more than once and we need to be careful not to double count anyone.\nI started by computing a tibble which contains the unique ID’s of all the athletes who have won an at least one event. Then I created another tibble which contains all the athletes who never won an event. Note that these tibbles are disjoint and the union of the two tibbles gives us the set of all athletes in the dataset. After I had the computed both sets of ids, I was able to make use of the inner_join operation to create two new tibbles. The first tibble contains the information about athletes who have won at least one medal over the span of their career. The second tibble contains information about athletes who never won an event in their career. Note that these tibbles can not be constructed using the dplyr filter function alone. In doing so, we would end up with athletes who appear in both the medalist tibble and the non medalist tibble. Using the approach I described, it is impossible for an athlete to be placed in both tibbles.\nAt this point, we have separated the data into two groups, but an athlete can still appear in each group more than once. To solve this, I computed the average age, height, and weight for each athlete in both groups. This operation resulted in two tibbles where each athlete is only recorded once in each tibble. Finally, I could use this information to compute the average age, height, and weight for each group.\n\n# Read in the clean dataset\ndf <- read_csv('./Data/athletes_clean.csv')\n\n# Helper function to compute average age, height, and weight for each athlete in a tibble\ngetBasicStats <- function(dfView) {\n  toReturn <- dfView %>%\n              group_by(ID, Sex) %>%\n              summarise(Count = n(),\n                        AverageAge = mean(Age, na.rm = TRUE),\n                        AverageHeight = mean(Height, na.rm = TRUE),\n                        AverageWeight = mean(Weight, na.rm = TRUE),\n                        .groups = \"keep\")\n  return(toReturn)\n}\n\n# Helper function to compute average age, height, and weight for a set of athletes in a tibble\nanalyseBasicStats <- function(dfView) {\n  toReturn <- dfView %>%\n              group_by(Sex) %>%\n              summarise(Count = n(),\n                        AvAge = mean(AverageAge, na.rm = TRUE),\n                        SDAge = sd(AverageAge, na.rm = TRUE),\n                        AvHeight = mean(AverageHeight, na.rm = TRUE),\n                        SDHeight = sd(AverageHeight, na.rm = TRUE),\n                        AvWeight = mean(AverageWeight, na.rm = TRUE),\n                        SDWeight = sd(AverageWeight, na.rm = TRUE))\n  return(toReturn)\n}\n\n# Find the ids of all the medalists\nmedalistIDs <- df %>%\n               filter(Medal != \"None\") %>%\n               distinct(ID)\n\n# Find the ids of people who have not won a medal\nnonMedalistIDs <- setdiff(df %>% distinct(ID), medalistIDs)\n\n# Get the statistics for the medalist group\nmedals <- getBasicStats(inner_join(df, medalistIDs, by = \"ID\")) %>%\n          analyseBasicStats(.) %>%\n          mutate(Sex = case_when(Sex == 'F' ~ 'Female Medalist',\n                 Sex == 'M' ~ 'Male Medalist')) %>%\n          rename('Gender/Medal Status' = Sex)\n\n# Get the statistics for the non medalist group\nnonMedals <- getBasicStats(inner_join(df, nonMedalistIDs, by = \"ID\")) %>%\n             analyseBasicStats(.) %>%\n             mutate(Sex = case_when(Sex == 'F' ~ 'Female Non-Medalist',\n                    Sex == 'M' ~ 'Male Non-Medalist')) %>%\n             rename('Gender/Medal Status' = Sex)\n\n# Combine results\nbind_rows(medals, nonMedals) %>% arrange('Gender/Medal Status')\n\n\n\n  \n\n\n\nNotice that sum of the values in the ‘Count’ column above is 135,404 which is the same as the number of athletes in the dataset. This indicates that we have avoided double counting athletes.\nThe tibble displayed above is interesting, but it’s not easy to tell if there are any differences between normal athletes and medalists based solely on the output. Let’s create density plots of the age, height, and weight variables for the athletes and the medalists to get a better view of the data.\n\nmedalsData <- df %>% inner_join(medalistIDs, by = \"ID\") %>%\n              getBasicStats(.) %>%\n              rename(Age = AverageAge) %>%\n              rename(Height = AverageHeight) %>%\n              rename(Weight = AverageWeight) %>%\n              filter(!is.na(Age)) %>%\n              filter(!is.na(Height)) %>%\n              filter(!is.na(Weight)) %>%\n              mutate(Sex = case_when(Sex == 'F' ~ 'Female Medalist',\n                                     Sex == 'M' ~ 'Male Medalist'))\n\nnonMedalsData <- df %>% inner_join(nonMedalistIDs, by = \"ID\") %>%\n                 getBasicStats(.) %>%\n                 rename(Age = AverageAge) %>%\n                 rename(Height = AverageHeight) %>%\n                 rename(Weight = AverageWeight) %>%\n                 filter(!is.na(Age)) %>%\n                 filter(!is.na(Height)) %>%\n                 filter(!is.na(Weight)) %>%\n                 mutate(Sex = case_when(Sex == 'F' ~ 'Female Non-Medalist',\n                                        Sex == 'M' ~ 'Male Non-Medalist'))\n\nggplot() +\ngeom_density(mapping = aes(x = Age, colour = Sex),\n             adjust = 2,\n             alpha = 0.1,\n             data = medalsData) +\ngeom_density(mapping = aes(x = Age, colour = Sex),\n             adjust = 2,\n             alpha = 0.1,\n             data = nonMedalsData) +\nlabs(title = \"Age & Sex vs Olympic Outcomes\")\n\nggplot() +\ngeom_density(mapping = aes(x = Height, colour = Sex),\n             adjust = 2,\n             alpha = 0.1,\n             data = medalsData) +\ngeom_density(mapping = aes(x = Height, colour = Sex),\n             adjust = 2,\n             alpha = 0.1,\n             data = nonMedalsData) +\nlabs(title = \"Height & Sex vs Olympic Outcomes\")\n\nggplot() +\ngeom_density(mapping = aes(x = Weight, colour = Sex),\n             adjust = 2,\n             alpha = 0.1,\n             data = medalsData) +\ngeom_density(mapping = aes(x = Weight, colour = Sex),\n             adjust = 2,\n             alpha = 0.1,\n             data = nonMedalsData) +\nlabs(title = \"Weight & Sex vs Olympic Outcomes\")\n\n\n\n\n\n\n\n\n\n\nThe density plots generated above for the medalists and non-medalists groups of athletes are very similar to each other. However, it is interesting that the peaks for the non-medalist group are taller than the medalist group for each gender in both the ‘Height’ and ‘Weight’ diagrams. This indicates that a larger proportion of non-medalists are centered around the average value than medalists in both the height and weight category.\nBased on the plots generated in the previous code block, it appears that it would be difficult to predict whether or not an athlete would win a medal based solely on their physical characteristics such as age, height, and weight due to the fact that the density plots are very similar. This means that we need to look at the data in some other ways to gain insight on Olympic status."
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#comparing-top-athletes-against-thier-competitors",
    "href": "posts/Final_MatthewNorberg.html#comparing-top-athletes-against-thier-competitors",
    "title": "Olympic Analysis",
    "section": "Comparing Top Athletes Against Thier Competitors",
    "text": "Comparing Top Athletes Against Thier Competitors\nThe previous section showed that it is hard to find differences between medalists and non-medalists based off their physical characteristics. Instead, let’s compare the most successful Olympians who have won the most medals to everyone else. Hopefully, we will see a difference between the physical characteristics of the most successful athletes and their competitors. The following chunk of R code find the athletes that have won the most medals and shows the number of each type of medal earned by the athlete.\n\n# Medal count by athlete\nbestAthletes <- df %>%\n                filter(Medal != \"None\") %>%\n                group_by(ID, Name, Sport, Sex) %>%\n                count(Medal) %>%\n                pivot_wider(names_from = Medal, values_from = n) %>%\n                mutate(TotalMedals = Bronze + Silver + Gold) %>%\n                relocate(Bronze, .after = Sport) %>%\n                relocate(Silver, .after = Bronze) %>%\n                relocate(Sex, .after = Name) %>%\n                ungroup() %>%\n                arrange(desc(TotalMedals))\n\n# Display the top athletes\nbestAthletes\n\n\n\n  \n\n\n\nLet’s see how some of the top athletes who earned the most medals compare to the rest of the athletes in their sport. The following chunk of code generates height and weight histograms for the top three athletes which show where they stand in relation to their competitors.\n\ntop3 <- bestAthletes %>%\n        head(n = 3)\n\ncompAverages <- function() {\n  averages <- tibble(Name = character(), AverageHeight = numeric(), AverageWeight = numeric(), AverageAge = numeric())\n  for (i in seq_along(top3$Name)) {\n    id <- top3$ID[[i]]\n    name <- top3$Name[[i]]\n    averages <- bind_rows(averages, df %>% \n                          filter(ID == id & Name == name) %>%\n                          group_by(ID, Name) %>%\n                          summarise(AverageHeight = mean(Height, na.rm = TRUE), \n                                    AverageWeight = mean(Weight, na.rm = TRUE), \n                                    AverageAge = mean(Age, na.rm = TRUE), \n                                    .groups = \"keep\"))\n  }\n  return (averages)\n}\n\ncompAthletes <- function() {\n  \n  # Compute average height and weight for each athlete\n  averages <- compAverages()\n\n  for (i in seq_along(top3$Name)) {\n    # Collect name, sport, event, and gender\n    name <- top3$Name[[i]]\n    sport <- top3$Sport[[i]]\n    gender <- top3$Sex[[i]]\n    \n    # Get height and weight for current athlete from averages\n    cHeight <- (averages %>% filter(Name == name))$AverageHeight[[1]]\n    cWeight <- (averages %>% filter(Name == name))$AverageWeight[[1]]\n    cAge <- (averages %>% filter(Name == name))$AverageAge[[1]]\n    \n    # Get height and weight for all athletes \n    others <- df %>% \n              filter(Sport == sport & Sex == gender) %>%\n              group_by(ID) %>%\n              summarise(Height = mean(Height, na.rm = TRUE),\n                        Weight = mean(Weight, na.rm = TRUE),\n                        Age = mean(Age, na.rm = TRUE),\n                        .groups = \"keep\")\n    mHeight <- (others %>% filter(!is.nan(Height)) %>% summarise(mean = mean(Height), na.rm = TRUE, .groups = \"keep\"))$mean[[1]]\n    mWeight <- (others %>% filter(!is.nan(Weight)) %>% summarise(mean = mean(Weight), na.rm = TRUE, .groups = \"keep\"))$mean[[1]]\n    mAge <- (others %>% filter(!is.nan(Age)) %>% summarise(mean = mean(Age), na.rm = TRUE, .groups = \"keep\"))$mean[[1]]\n    \n    # Reset gender variable for graph aesthetics\n    gender <- if (gender == \"M\") \"Male\" else \"Female\"\n    \n    # Graph For Height Information\n    l <- others %>%\n         filter(!is.nan(Height)) %>%\n         ggplot() +\n         geom_density(mapping = aes(x = Height),\n                      adjust = 2,\n                      alpha = 0.1) +\n         geom_vline(xintercept = cHeight, color=\"red\") +\n         geom_vline(xintercept = mHeight, color=\"blue\") +\n         labs(y = \"\")\n    \n    # Graph For Weight Information\n    m <- others %>%\n         filter(!is.nan(Weight)) %>%\n         ggplot() +\n         geom_density(mapping = aes(x = Weight),\n                      adjust = 2,\n                      alpha = 0.1) +\n         geom_vline(xintercept = cWeight, color=\"red\") +\n         geom_vline(xintercept = mWeight, color=\"blue\") + \n         labs(y = \"\")\n    \n    # Graph For Weight Information\n    r <- others %>%\n         filter(!is.nan(Age)) %>%\n         ggplot() +\n         geom_density(mapping = aes(x = Age),\n                      adjust = 2,\n                      alpha = 0.1) +\n         geom_vline(xintercept = cAge, color=\"red\") +\n         geom_vline(xintercept = mAge, color=\"blue\") + \n         labs(y = \"\")\n    \n    # Display Graphs\n    grid.arrange(top = str_c(name, \" vs All Athletes In \", gender, \" \", sport),\n                 bottom = str_c(\"Red -> \", name, \"; Blue -> mean\"),\n                 left = \"density\",\n                 l, m, r, ncol = 3)\n  }\n  \n}\n\ncompAthletes()\n\n\n\n\n\n\n\n\n\n\nIt is hard to see a consistent pattern in the charts above in the statistics displayed above. For example, Michael Phelps was taller than the average competitors, but Nikolay Yefimovich as quite a bit smaller the average competitor. Based on what we have seen in the previous two subsections, it appears that way need to restrict our analysis to a singular sport and possibly to even a single event within a sport in order to uncover characteristics which separate medalists from non-medalists. For instance, when we compared medalists to non-medalists, we were comparing the two groups across every sport in the dataset which explains why there was no major difference in the density plots. In the previous section, we compared the most successful athletes to their competitors. There were categories where these athletes were indeed outliers, but there is no pattern linking all these athletes together which is likely caused by the fact that they compete in different sports."
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#comparing-countries-by-number-of-medals",
    "href": "posts/Final_MatthewNorberg.html#comparing-countries-by-number-of-medals",
    "title": "Olympic Analysis",
    "section": "Comparing Countries By Number of Medals",
    "text": "Comparing Countries By Number of Medals\nIn the previous section, we compared the most successful athletes to their competitors. In this section, we will switch focus and investigate whether an athletes country has an impact on their success. To start investigating how medals relates to country, let’s find out how many medals of each type each country has and how many total medals each country has. This has been done with the following chunk of R code.\n\ncountryMedal <- df %>%\n                group_by(Region) %>%\n                count(Medal) %>%\n                pivot_wider(names_from = Medal, values_from = n) %>%\n                relocate(Bronze, .after = None) %>%\n                relocate(Silver, .after = Bronze) %>%\n                arrange(desc(Gold)) %>%\n                mutate(TotalMedals = sum(Bronze,Silver,Gold, na.rm = TRUE))\ncountryMedal\n\n# Display Information\ncountryMedal %>%\n  head(n = 25) %>%\n  ggplot(aes(x = fct_reorder(Region, TotalMedals), y = TotalMedals)) +\n    geom_bar(stat = \"identity\", fill=\"#f68060\", alpha=.6, width=.4) +\n    coord_flip() +\n    theme_bw() +\n    labs(x = \"Country\", title = \"Number of Medals Per Country\")\n\n\n\n  \n\n\n\n\n\n\nThe skewness seen in the medal count could be explained by some countries having more athletes competing in the games than others. Let’s map each country to the number of athletes that have competed for them. Note that in the calculation below, each athlete is counted only once, even if they competed in multiple events or Olympic games.\n\nathleteCount <- df %>%\n                group_by(Region, Name) %>%\n                group_by(Region) %>%\n                count() %>%\n                rename(Athletes = n) %>%\n                arrange(desc(Athletes))\nathleteCount\n\n\n\n  \n\n\n\nNot surprisingly, many of the counties that have won a lot of medals also have the most athletes. This could explain the skewness that is seen Number of Countries vs Number of Medals plot above. The chart below show the statistics for the number of medals earned by each country normalized by the total number of athletes that competed for the country.\n\nnormed <- inner_join(countryMedal, athleteCount, by = \"Region\") %>%\n          mutate(MedalsPerAthlete = TotalMedals / Athletes) %>%\n          arrange(desc(MedalsPerAthlete))\n\nnormed\n\nnormed %>%\n  head(n = 125) %>%\n  ggplot(aes(x = fct_reorder(Region, MedalsPerAthlete), y = MedalsPerAthlete)) +\n    geom_bar(stat = \"identity\", fill=\"#f68060\", alpha=.6, width=.4) +\n    coord_flip() +\n    theme_bw() +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank()) +\n    labs(x = \"Country\", title = \"Number of Medals Per Athlete Per Country\")\n\n\n\n  \n\n\n\n\n\n\nIn the chart above, I removed the country labels so we could display the number of medals per athlete for many countries in the chart. You can figure out which countries correspond to the top bars by looking at the tibble above it. The chart reveals that the distribution is still very skewed even if we normalize the number of medals per country by the number of athletes who have competed.\nOne last way that we can visualize the number of medals earned by with each country is with an evolution graph. The visual below shows the medal progression for the top 15 medal earning countries in the data set.\n\ndf %>%\n  filter(Region %in% (countryMedal %>% head(15))$Region) %>%\n  group_by(Region, Year) %>%\n  count(Medal) %>%\n  pivot_wider(names_from = Medal, values_from = n) %>%\n  mutate(TotalMedals = sum(Bronze,Silver,Gold, na.rm = TRUE)) %>%\n  group_by(Region) %>%\n  mutate(CumulativeMedals = cumsum(TotalMedals)) %>%\n  ggplot(mapping = aes(x = Year, y = CumulativeMedals, color = Region)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Total Number of Medals Earned At Time Period X\",\n       title = \"Region Medal Accumulation Over Time\")\n\n\n\n\nThe results shown above seem to indicate that athletes from some countries may be more likely to win Olympic medals than others. Additionally, the likelihood that an athlete would win a medal based on their country changes over time as indicated by the chart above. For example the likelihood that an athlete from the United States wins a medal in 2010 appears to be more likely than in 1896 due to the fact that the United States has accumulated many more medals in 2010."
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#process",
    "href": "posts/Final_MatthewNorberg.html#process",
    "title": "Olympic Analysis",
    "section": "Process",
    "text": "Process\nI started the analysis by cleaning the data set and handling the missing values. Doing so made it much easier to generate the figures and perform calculations. After cleaning the data, I analyzed the athletes by generating histograms for the medalist athletes and the non-medalist athletes to compare them. I was hoping there would be noticeable differences between the groups which could explain why one group won Olympic medals and the other group did not. Unfortunately, there was not any substantial noticeable differences that could be observed in the graphs. My next idea was to compare the most successful athletes to the rest of the athletes in their respective sports. This approach provided some insight as to why these athletes were successful, but there was no pattern which could be observed across the ones which were analyzed. At this point, it became clear that in order to uncover the characteristics of successful athletes, the scope of the analysis needs to be narrowed down to a particular sport of interest. My last idea was to analyze the amount of medals earned by each country. This analysis showed that some countries have earned many more medals than others and that an athlete’s country may be an indicator of Olympic success. That is, athletes from some countries are more likely to win medals than athletes than from others."
  },
  {
    "objectID": "posts/Final_MatthewNorberg.html#next-steps",
    "href": "posts/Final_MatthewNorberg.html#next-steps",
    "title": "Olympic Analysis",
    "section": "Next Steps",
    "text": "Next Steps\nIf I were to continue investigating the dataset, I would narrow down my analysis to a single sport. The analysis above did not reveal many observable differences between medalists and non-medalists. As I alluded to previously, I think this is due to the fact that I was searching for patterns that differentiated the two groups across each sport in the dataset. However, I now believe that this approach is too broad which made it difficult to find substantial results. In addition to narrowing down analysis to one sport, I would also look to see if combinations of variables can be used to predict Olympic success. Previously, I was looking at singular variables and comparing them to Olympic success. However, we may see more interesting results if we compare pairs of variables to Olympic success. For example, the graph below shows the outcomes of male Olympic swimmers based on their height and weight. In the graph, it appears as though most of the gold medalists appear in the upper right hand corner indicating that the combination of an athletes height and weight are important in male swimming.\n\ndf %>%\n  filter(Sex == \"M\" & \n         Sport == \"Swimming\" & \n         Medal != \"None\" & \n         !is.na(Height) & \n         !is.na(Weight)) %>%\n  ggplot() +\n  geom_point(aes(x = Height, y = Weight, color = Medal)) +\n  labs(title = \"Height Weight & Outcome For Male Olympic Swimmers\",\n       caption = \"*Non-medalists ommitted\")\n\n\n\n\nIn addition to investigating how multiple variables could impact medal status, I would also continue to look into the effect that an athlete’s country has on performance. However, in the future, I would compare countries against each other for one specific sport rather than comparing them across every sport. I doing so, I would hope to get a better idea of whether an athlete would win a medal or not by looking at their country and their sport. For instance, we can look at how well each country has done in the male swimming event. Looking at the chart below, I would predict that athletes from the United States are much more likely to earn a medal than athletes from Cuba.\n\ndf %>%\n  filter(Sex == \"M\" & Sport == \"Swimming\") %>%\n  group_by(Region) %>%\n  count(Medal) %>%\n  pivot_wider(names_from = Medal, values_from = n) %>%\n  mutate(TotalMedals = sum(Bronze,Silver,Gold, na.rm = TRUE)) %>%\n  arrange(desc(TotalMedals)) %>%\n  head(n = 25) %>%\n  ggplot(aes(x = fct_reorder(Region, TotalMedals), y = TotalMedals)) +\n    geom_bar(stat = \"identity\", fill=\"#f68060\", alpha=.6, width=.4) +\n    coord_flip() +\n    theme_bw() +\n    labs(x = \"Country\", y = \"Total Medals\", title = \"Number of Medals Per Country In Men's Swimming\")"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html",
    "href": "posts/Final_Project- Said Arslan.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(countrycode)\n\n\nWarning: package 'countrycode' was built under R version 4.2.2\n\n\nCode\nlibrary(summarytools)\n\n\nWarning: package 'summarytools' was built under R version 4.2.2\n\n\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\nlibrary(htmlTable)\nlibrary(lessR)\n\n\nWarning: package 'lessR' was built under R version 4.2.2\n\n\n\nlessR 4.2.4                         feedback: gerbing@pdx.edu \n--------------------------------------------------------------\n> d <- Read(\"\")   Read text, Excel, SPSS, SAS, or R data file\n  d is default data frame, data= in analysis routines optional\n\nLearn about reading, writing, and manipulating data, graphics,\ntesting means and proportions, regression, factor analysis,\ncustomization, and descriptive statistics from pivot tables.\n  Enter:  browseVignettes(\"lessR\")\n\nView changes in this and recent versions of lessR.\n  Enter: news(package=\"lessR\")\n\n**New Feature**: Interactive analysis of your data\n  Enter: interact()\n\n\nAttaching package: 'lessR'\n\nThe following object is masked from 'package:summarytools':\n\n    label\n\nThe following objects are masked from 'package:dplyr':\n\n    recode, rename\n\n\nCode\nlibrary(RColorBrewer)\nlibrary(corrplot)\n\n\ncorrplot 0.92 loaded\n\n\nCode\nload(\"Final_project- Said Arslan.RData\")\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#read-the-primary-dataset",
    "href": "posts/Final_Project- Said Arslan.html#read-the-primary-dataset",
    "title": "Final Project",
    "section": "Read the primary dataset",
    "text": "Read the primary dataset\n\n\nCode\n# ac <-\n#    read_csv(\"C:/Users/saida/Documents/african_crises.csv\",\n#     col_types = cols(\n#         case = col_integer(),\n#         cc3 = col_factor(ordered = TRUE),\n#         country = col_character(),\n#         year = col_date(format = \"%Y\"),\n#         exch_usd = col_double(),\n#         gdp_weighted_default = col_double(),\n#         inflation_annual_cpi = col_double(),\n#         systemic_crisis = col_integer(),\n#         domestic_debt_in_default = col_integer(),\n#         sovereign_external_debt_default = col_integer(),\n#         independence = col_integer(),\n#         currency_crises = col_integer(),\n#         inflation_crises = col_integer(),\n#         banking_crisis = col_character()\n#     )\n#   )\n# \n# sample_n(ac, 10) %>% htmlTable()\n\n\nIn the main dataset, each variable has its own column, each observation has its own row, and each value has its own cell.\n\nThe variables in the primary dataset\n\ncase is a factor variable that identifies the index for each country from the original, global dataset.\ncc3 is a factor variable that identifies each country with a three letter code.\ncountry is a character variable that names each country.\nyear is a date variable that identifies the year of the observation.\nsystemic_crisis is a binary variable that identifies whether a systemic crisis is observed in that year or not. Systemic crisis means the nationwide banking crisis which its impact spreads to the other sectors.\nexch_usd is a numeric variable that gives exchange rate of USD to the country’s currency. For a country, the lower the exchange rate the more valuable its currency.\ndomestic_debt_in_default is a binary variable that shows if the government failed in repaying its domestic debts or not.\nsovereign_external_debt_default is a binary variable that identifies if sovereign external debt default occurred or not. It is a failure of a government to honor some or all of its debt obligations to the external lenders.\ngdp_weighted_default is a numeric variable which gives the ratio of debt to gdp when debt default is observed.\ninflation_annual_cpi is numeric variable that shows inflation rate based on consumer price index.\nindependence is a binary variable that indicates if the country is independent or not as of observation year.\ncurrency_crises is a binary variable that identifies currency crisis occured or not in the observation year.\ninflation_crises is a binary variable that identifies inflation crisis occured or not in the observation year.\nbanking_crisis is a binary variable that identifies banking crisis occured or not in the observation year.\n\nFor consistency I will first change column names with “crises” to “crisis” as well as “inflation_annual_cpi” to “inflation_rate. Second, I will remove case number that is not needed in my analysis. Then, I will recode”banking_crisis” variable which is differently coded in the dataset than the other crisis indicator variables.\n\n\nCode\ncat(unique(ac$banking_crisis), sep = \"\\n\")\n\n\n1\n0\n\n\n\n\nCode\n# ac <- ac %>% dplyr::rename(\"inflation_crisis\" = \"inflation_crises\", \n#                            \"currency_crisis\" = \"currency_crises\",\n#                            \"inflation_rate\" = \"inflation_annual_cpi\") %>% \n#   select(-case) %>% \n#   mutate(banking_crisis = case_when(\n#          banking_crisis == \"crisis\" ~ 1L,\n#          banking_crisis == \"no_crisis\" ~ 0L)\n#   )"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#initial-analysis",
    "href": "posts/Final_Project- Said Arslan.html#initial-analysis",
    "title": "Final Project",
    "section": "Initial analysis",
    "text": "Initial analysis\nI would like to supplement different economic indicators to the original dataset to better analyze the economic crises, however, there is no financial and economic data available for African countries that goes back to the 1800s or early 1900s.\nThat’s why, I wanted to first examine timeline of crisis in these African countries along with dates of their gaining independency.\n\n\nCode\n#I'll create a timeline that shows independence of each country and include point for years of crises in the plot\n\nfilt_crs <- ac %>% \n  filter(if_any(ends_with(\"crisis\"), ~ .x == 1)) %>%\n  pivot_longer(cols = ends_with(\"crisis\"), \n               names_to = \"type_of_crisis\",\n               values_to = \"crisis_or_not\") %>% \n  filter(crisis_or_not == 1)\n\n\nac %>% ggplot(aes(year, independence)) + \n  geom_line() +\n  geom_area(fill = \"#FFDB6D\", alpha = 0.3) +\n  geom_point(data = filt_crs, \n             mapping = aes(year, crisis_or_not, color=type_of_crisis)) +\n  guides(color=guide_legend(\"Types of Crisis\", override.aes = list(size = 3))) +\n  scale_y_continuous(name = \"Independence\", breaks = c(0,1)) +\n  scale_x_date(name = \"Year\", \n               date_breaks = \"20 years\", \n               date_labels = \"%Y\", \n               limits = c(as.Date(\"1860-01-01\"), NA), \n               expand=c(0,0)) +\n  facet_wrap(vars(country), ncol = 2) +\n  theme_light()\n\n\n\n\n\nThe plots above reveal crucial situation for my further analysis. As we can see, most of the economic crisis in African countries actually took place after they gained independence and most of the countries got their independency around 1960s. Especially, all the systemic crises are after1960s. Therefore, I decided to focus on the data after 60s in analysis. Also, it appears that including additional economic indicators, which are usually start from 60s, to the main dataset would comply with this perspective"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#merging-commodity-import-export-price-index-obtained-by-web-scraping-with-the-main-dataset",
    "href": "posts/Final_Project- Said Arslan.html#merging-commodity-import-export-price-index-obtained-by-web-scraping-with-the-main-dataset",
    "title": "Final Project",
    "section": "Merging Commodity Import & Export Price Index obtained by web scraping with the main dataset",
    "text": "Merging Commodity Import & Export Price Index obtained by web scraping with the main dataset\nIn order to expand the features of the main dataset, I scrape additional data provided by the Commodity of Terms of Trade Database of International Monetary Fund (IMF). While the IMF provides many formats including excel and csv file formats but I instead wanted to invest time on scraping the rendered html using their online table viewer: https://data.imf.org/?sk=2CDDCCB8-0B59-43E9-B6A0-59210D5605D2&sId=1434492942851.\nUsing the viewer I selected the desired data and used inspect in Chrome to view the rendered html for the current page. Then copied the div representing the entire table in html and pasted the text into local files.\nFirst I wrote a function to convert the html, into a data frame matching the online table viewer.\n\n\nCode\nimf_html_to_df <-  function (html_filename){\n\n  # getting column labels by looking at the custom filename\n  filename_pieces = str_split(html_filename, \"_\")[[1]]\n\n  # read all the html in the file\n  html_str = readLines(html_filename)\n\n  # function returning the elements matched by the regex\n  html_to_byRowList = function(full_file_str){\n    regex_exp = \"<div class=\\\"PPTSCellConText.*?>(.*?)<\"\n    result = stringr::str_match_all(full_file_str, regex_exp)\n    return(result[[1]][, 2])\n  }\n\n  # all elements in the df by row\n  elements = html_to_byRowList(html_str)\n\n  columns = c(\"country\", seq(as.integer(filename_pieces[3]), as.integer(filename_pieces[4]), by=1))\n  num_columns = length(columns)\n  schema = matrix(ncol = num_columns, nrow = 0)\n  df = data.frame(schema)\n  colnames(df) = columns\n\n  # grab elements, ignoring column headers\n  elements = elements[num_columns:length(elements)]\n\n  # populate df iterating through element by df row\n  start = 1\n  while(start + num_columns - 1 <= length(elements)){\n    curr = elements[start:(start+num_columns-1)]\n    curr = do.call(c, list(list(curr[1]), as.double(curr[2:(length(curr))])))\n    df[nrow(df) + 1,] = curr\n    start = start + num_columns\n  }\n\n  # add cc3 labels used in joining later\n  df$cc3 = as_factor(countrycode(df$country, origin = 'country.name', destination = 'iso3c'))\n  return(df)\n}\n\n\nApplying the transformer function on html export price index files. They are in 20 year grouping since at most 20 columns render.\nJoining them together.\n\n\nCode\nexport_df <-  merge(x=export_df_1, y=export_df_2, by=c(\"cc3\", \"country\")) %>% \n  as_tibble()\nhtmlTable(export_df)\n\n\n\n\n\n\ncc3\ncountry\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n\n\n\n\n1\nAGO\nAngola\n40.83\n40.36\n40.41\n40.57\n40.36\n39.65\n40.86\n40.79\n40.92\n61.84\n58.33\n60.86\n62.37\n59.86\n78.01\n81.05\n77.91\n76.25\n73.29\n71.88\n71.08\n52.01\n55.29\n48.22\n51.89\n57.39\n52.33\n52.04\n49.36\n48.07\n48.8\n52.2\n51.85\n44.63\n51.52\n62.81\n59.69\n60.81\n64.12\n70.06\n80.31\n86.37\n89.05\n99.55\n82.15\n90.83\n100.7\n100\n100.87\n97.7\n\n\n2\nCAF\nCentral African Republic\n99.9\n99.88\n99.96\n100.01\n99.71\n100.03\n99.6\n99.4\n100.37\n100.08\n99.24\n100.93\n101.43\n100.54\n101.13\n100.69\n99.51\n99.64\n99.68\n100.08\n99.3\n99.45\n99.65\n99.15\n99.01\n98.67\n98.51\n98.22\n99.45\n100.46\n100.2\n99.5\n99.78\n98.94\n98.79\n98.48\n97.65\n97.79\n98.33\n98.29\n98.61\n99.03\n99.36\n99.6\n99.26\n99.88\n100.99\n100\n99.65\n99.79\n\n\n3\nCIV\nCôte d'Ivoire\n92.84\n94.73\n95.44\n96.71\n98.11\n95.79\n92.49\n93.04\n98.01\n102.28\n97.72\n104.17\n110.29\n106.54\n108.21\n104.6\n100.47\n99.06\n100.58\n101.51\n100.17\n96.25\n95.4\n91.42\n89.6\n89.87\n88.03\n87.2\n87.5\n90.65\n90.77\n90.29\n91.5\n89.98\n87.91\n87.44\n87.49\n91.67\n92.44\n92.11\n93.66\n95.28\n97.34\n100.54\n99.01\n101.95\n103.6\n100\n100.09\n101.64\n\n\n4\nDZA\nAlgeria\n59.16\n58.8\n58.99\n59.06\n58.87\n58.13\n58.65\n58.55\n58.94\n73.33\n71.72\n73.73\n76.05\n75.29\n85.38\n87.85\n87.27\n87.6\n86.12\n85.24\n84.64\n72.86\n72.43\n67.65\n69.39\n72.97\n69.81\n69.07\n68.21\n66.69\n66.99\n70.31\n69.96\n64.81\n69.02\n79.31\n77.66\n76.94\n81.18\n84.43\n92.25\n94.6\n95.7\n102.89\n90.26\n94.47\n100.26\n100\n100.79\n99.54\n\n\n5\nEGY\nEgypt\n94\n93.9\n94.01\n94.04\n93.97\n93.8\n93.9\n93.87\n94.12\n96.92\n96.34\n96.73\n97.02\n96.77\n98.5\n98.79\n98.43\n98.18\n98.05\n97.87\n97.61\n95.37\n95.83\n94.96\n95.4\n96.05\n95.29\n95.1\n94.74\n94.72\n94.93\n95.31\n95.24\n94.19\n94.97\n96.5\n96.11\n96.17\n96.73\n97.34\n98.36\n98.98\n99.21\n100.11\n98.43\n99.43\n100.37\n100\n100.06\n99.81\n\n\n6\nKEN\nKenya\n100.22\n100.12\n100.01\n99.71\n99.57\n99.79\n99.41\n99.39\n99.36\n100.01\n99.56\n100.36\n101.6\n100.63\n100.85\n100.43\n99.81\n99.82\n100.11\n100.75\n99.84\n99.34\n98.7\n98.38\n98.39\n98.31\n97.82\n97.8\n97.72\n98.28\n98.09\n97.98\n98.65\n98.39\n98.37\n98.53\n97.9\n97.87\n98.13\n98.24\n98.78\n99.18\n99.11\n99.63\n99.39\n99.8\n100.28\n100\n99.56\n99.51\n\n\n7\nMAR\nMorocco\n100.54\n100.44\n100.51\n100.57\n100.86\n100.78\n100.35\n100.67\n101.5\n101.16\n100.19\n100.81\n101.3\n101\n101.24\n100.72\n100.51\n100.65\n100.48\n100.45\n100.35\n99.87\n100.1\n99.92\n99.6\n99.66\n99.15\n99.22\n98.96\n99.06\n99.11\n98.96\n98.85\n98.88\n98.81\n99\n98.82\n98.87\n98.9\n98.96\n99.46\n100.23\n100.17\n99.94\n99.51\n100.27\n100.52\n100\n100.46\n100.61\n\n\n8\nMUS\nMauritius\n97.15\n95.56\n95.72\n95.51\n100.29\n100.92\n101\n105.49\n107.96\n116.83\n110.86\n105.49\n101.99\n100.17\n100.79\n109.86\n103.87\n97.64\n97.6\n93.12\n91.15\n94.14\n94.32\n96.74\n98.14\n97.87\n94.32\n94.63\n95.26\n96.82\n97.15\n95.86\n95.74\n94.01\n91.03\n93.27\n93.42\n91.26\n91.92\n92.19\n94.63\n98.24\n94.24\n95.79\n99.35\n100.86\n102.57\n100\n99.07\n98.73\n\n\n9\nNGA\nNigeria\n66.78\n66.47\n66.48\n66.61\n66.52\n65.89\n66.69\n66.67\n66.91\n80.46\n78.3\n79.9\n80.88\n79.48\n89.46\n90.94\n89.31\n88.41\n86.96\n86.22\n85.73\n74.55\n76.57\n72.01\n74.34\n77.75\n74.58\n74.38\n72.67\n71.82\n72.35\n74.6\n74.33\n69.47\n73.99\n80.97\n79.2\n79.92\n81.96\n85.25\n90.65\n93.67\n94.98\n99.91\n91.51\n95.86\n100.42\n100\n100.4\n98.94\n\n\n10\nTUN\nTunisia\n90.58\n90.5\n90.48\n90.55\n90.56\n90.34\n90.58\n90.64\n90.88\n95.07\n94.2\n94.78\n95.07\n94.58\n97.53\n97.78\n97.24\n97.06\n96.65\n96.38\n96.23\n92.85\n93.51\n92.11\n92.82\n93.83\n92.77\n92.74\n92.18\n91.94\n92.13\n92.77\n92.73\n91.14\n92.6\n94.74\n94.18\n94.36\n94.95\n95.92\n97.45\n98.43\n98.75\n99.96\n97.69\n98.96\n100.19\n100\n100.21\n99.89\n\n\n11\nZAF\nSouth Africa\n96.79\n96.63\n96.54\n96.57\n96.78\n96.89\n96.58\n96.6\n96.93\n97.68\n97.73\n98.01\n98.18\n97.8\n98.14\n98.06\n97.76\n97.81\n97.45\n97\n96.96\n96.25\n96.14\n96.43\n96.54\n96.66\n96.16\n96.11\n95.64\n95.77\n96.13\n96.03\n95.95\n95.48\n95.4\n95.87\n96.05\n95.92\n96.11\n97.18\n97.73\n98.46\n98.86\n99.93\n98.82\n100.22\n100.85\n100\n99.97\n99.46\n\n\n12\nZMB\nZambia\n98.46\n101.68\n94.67\n96.79\n99.69\n97.81\n89.51\n87.97\n96.39\n95.77\n83.13\n84.89\n82.63\n81.15\n85.89\n84.96\n79.64\n77.2\n78.65\n75.88\n76.44\n74.76\n77.5\n82.54\n84\n82.38\n79.05\n78.69\n75.66\n78.83\n82.53\n77.82\n78.37\n73.48\n72.71\n75.25\n73.43\n73.47\n75.33\n82.71\n86.84\n99.46\n100.03\n98.29\n92.1\n100.23\n103.24\n100\n98.75\n97.31\n\n\n13\nZWE\nZimbabwe\n98.88\n98.72\n98.98\n99.17\n99.6\n100.04\n99.61\n99.89\n100.64\n100.71\n99.94\n100.32\n100.15\n99.23\n99.68\n99.76\n98.68\n97.72\n97.86\n97.59\n97.12\n96.15\n96.75\n99.2\n99.35\n98.24\n97.37\n96.69\n96.01\n96.97\n97.76\n97.12\n97.02\n95.71\n95.91\n97.15\n96.15\n96.37\n97.56\n98.48\n98.69\n100.53\n101.47\n100.06\n98.92\n100.96\n101.71\n100\n99.59\n99.8\n\n\n\n\n\nLastly I need to join IMF dataframes to the main dataset, ac. There are two complexities:\n\nreformatting the year columns as a single new ‘year’ column\njoin on year & cc3 code, since Ivory Coast has different names in ac and export_df.\n\nWrote a function to transform the IMF data frame into a joinable format:\n\n\nCode\nimf_df_transform <- function(df, col_name){\n  \n  df = df %>% arrange(cc3)\n\n  # select each grouping of data I need\n  just_cpi_df = subset(df, select = -c(cc3, country))\n  cpi = unlist(just_cpi_df, use.names=FALSE)\n  cc3 = unlist(subset(df, select = c(cc3)), use.names=FALSE)\n  country = unlist(subset(df, select = c(country)), use.names=FALSE)\n  year = colnames(just_cpi_df)\n\n  row_repeats = length(cpi) / length(cc3)\n  col_repeats = length(cpi) / length(year)\n  as.date=function(x){\n    year_starts = paste(as.character(x), rep(\"01\", length(x)), rep(\"01\", length(x)), sep=\"-\" )\n    return(as.Date(year_starts, format=\"%Y-%m-%d\"))\n  }\n\n  # reconstruct the joinable df\n  df_joinable = data.frame(\n    country=rep(country, times=row_repeats),\n    cc3=rep(cc3, times=row_repeats),\n    year=rep(as.date(year),each=col_repeats)\n  )\n  df_joinable[col_name] = cpi\n\n  # remove the country column as not needed for join\n  df_joinable = subset (df_joinable, select = -country)\n  return(df_joinable)\n}\n\n\nDefining new object to distinguish original dataset from the merged and truncated one.\nLastly, I use this function and inner join the export price data to main dataset.\n\n\nCode\nexport_df_joinable <- imf_df_transform(export_df, \"exp_pi\")\nhtmlTable(head(export_df_joinable, n=10))\n\n\n\n\n\n\ncc3\nyear\nexp_pi\n\n\n\n\n1\nDZA\n1965-01-01\n59.16\n\n\n2\nAGO\n1965-01-01\n40.83\n\n\n3\nCAF\n1965-01-01\n99.9\n\n\n4\nCIV\n1965-01-01\n92.84\n\n\n5\nEGY\n1965-01-01\n94\n\n\n6\nKEN\n1965-01-01\n100.22\n\n\n7\nMUS\n1965-01-01\n97.15\n\n\n8\nMAR\n1965-01-01\n100.54\n\n\n9\nNGA\n1965-01-01\n66.78\n\n\n10\nZAF\n1965-01-01\n96.79\n\n\n\n\n\n\n\nCode\nac_imf <- inner_join(ac_imf, export_df_joinable, by = c(\"cc3\", \"year\"))\n\n\nNOw repeating the same scraping process for the import price index data:\nMerging two import data and then converting into a joinable format.\n\n\nCode\nimport_df <- merge(x=import_df_1, y=import_df_2, by=c(\"cc3\", \"country\"))\nhtmlTable(import_df)\n\n\n\n\n\n\ncc3\ncountry\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n\n\n\n\n1\nAGO\nAngola\n99.53\n99.5\n99.47\n99.35\n99.47\n99.49\n99.34\n99.39\n100.11\n100.8\n100.07\n99.89\n99.83\n99.78\n100.04\n100.11\n99.83\n99.47\n99.52\n99.43\n99.22\n98.67\n98.63\n98.73\n98.78\n98.76\n98.48\n98.51\n98.46\n98.59\n98.63\n98.67\n98.56\n98.34\n98.19\n98.36\n98.31\n98.48\n98.59\n98.78\n98.93\n99.26\n99.44\n99.93\n99.48\n99.74\n100.15\n100\n99.98\n99.81\n99.28\n99.35\n\n\n2\nCAF\nCentral African Republic\n98.57\n98.52\n98.6\n98.55\n98.58\n98.5\n98.43\n98.53\n99.19\n100.24\n99.55\n99.48\n99.32\n99.22\n99.81\n100.06\n99.64\n99.27\n99.23\n99.06\n98.76\n98.1\n98.31\n98.22\n98.44\n98.45\n98.1\n98.17\n98.19\n98.32\n98.42\n98.47\n98.33\n97.8\n97.85\n98.24\n98.08\n98.22\n98.4\n98.58\n98.88\n99.34\n99.48\n99.97\n99.4\n99.73\n100.27\n100\n99.94\n99.78\n99.02\n99.02\n\n\n3\nCIV\nCôte d'Ivoire\n92.85\n92.79\n92.88\n92.75\n92.75\n92.35\n92.18\n92.43\n93.76\n97.7\n96.06\n96.35\n96.63\n96.29\n98.61\n99.09\n98.54\n97.77\n97.28\n96.88\n96.61\n93.45\n94.08\n92.9\n93.42\n94.2\n93.19\n93.19\n92.45\n92.35\n92.59\n93.14\n92.9\n91.49\n92.53\n94.27\n93.56\n93.96\n94.47\n95.5\n97.04\n98.09\n98.45\n100.29\n98.06\n99.15\n100.31\n100\n100.28\n99.74\n96.74\n96.52\n\n\n4\nDZA\nAlgeria\n99.56\n99.53\n99.49\n99.37\n99.47\n99.5\n99.25\n99.38\n100.81\n101.62\n100.5\n100.31\n99.92\n99.7\n100.05\n100.08\n99.45\n98.99\n99.12\n98.84\n98.36\n97.75\n97.45\n97.87\n98.12\n97.78\n97.27\n97.43\n97.45\n97.86\n98.09\n98.18\n97.86\n97.16\n96.75\n97\n97.06\n97.4\n97.54\n97.78\n97.93\n98.67\n99.11\n99.87\n98.92\n99.36\n100.33\n100\n99.82\n99.53\n98.65\n98.59\n\n\n5\nEGY\nEgypt\n97.98\n97.94\n97.81\n97.64\n97.7\n97.69\n97.33\n97.31\n98.94\n100.08\n98.78\n98.84\n98.65\n98.46\n99.09\n98.92\n98.32\n98.07\n98.16\n97.91\n97.38\n96.08\n95.82\n95.95\n96.26\n96.17\n95.59\n95.79\n95.7\n95.84\n96.01\n96.35\n96.01\n95.17\n95.02\n95.71\n95.75\n96.13\n96.45\n96.93\n97.32\n98.14\n98.72\n99.77\n98.41\n99.19\n100.35\n100\n99.94\n99.52\n98.09\n97.86\n\n\n6\nKEN\nKenya\n93.97\n93.81\n93.8\n93.66\n93.7\n93.67\n93.73\n93.66\n94.24\n97.49\n96.51\n96.6\n96.75\n96.46\n98.32\n98.56\n98.09\n97.62\n97.48\n97.41\n97.01\n94.46\n94.9\n94.23\n94.67\n95.15\n94.44\n94.51\n94.09\n94.15\n94.42\n94.82\n94.73\n93.68\n94.31\n95.54\n95.17\n95.54\n95.99\n96.66\n97.55\n98.37\n98.89\n99.93\n98.25\n99.2\n100.23\n100\n99.91\n99.55\n97.39\n97.14\n\n\n7\nMAR\nMorocco\n90.21\n90.09\n89.98\n89.88\n89.9\n89.76\n89.72\n89.76\n91.14\n95.89\n94.32\n94.62\n94.76\n94.21\n97.1\n97.45\n96.52\n96.03\n95.79\n95.42\n94.77\n91.16\n91.32\n90.47\n91.29\n92\n90.62\n90.68\n90.27\n90.3\n90.66\n91.42\n91.1\n89.06\n89.95\n92.37\n91.97\n92.27\n93.18\n94.38\n95.87\n97.32\n98.11\n100.12\n96.86\n98.44\n100.5\n100\n99.91\n99.2\n95.59\n94.92\n\n\n8\nMUS\nMauritius\n91.84\n91.73\n91.58\n91.34\n91.48\n91.19\n90.77\n91.31\n93.8\n97.96\n95.44\n96.18\n96.53\n95.96\n98.87\n99.19\n98.23\n97.35\n96.81\n96.27\n95.74\n91.79\n92.5\n91.25\n91.69\n92.56\n90.99\n90.97\n90.19\n90.28\n90.5\n91.04\n90.75\n88.78\n89.78\n92.05\n91.32\n91.89\n92.74\n94.05\n95.72\n97.16\n97.91\n100.27\n96.99\n98.91\n100.9\n100\n100.43\n99.76\n95.76\n95.63\n\n\n9\nNGA\nNigeria\n99.52\n99.47\n99.44\n99.39\n99.49\n99.41\n99.29\n99.45\n100.01\n100.52\n100.01\n100.02\n99.98\n99.89\n100.16\n100.31\n100.06\n99.81\n99.77\n99.61\n99.49\n99.12\n99.18\n99.13\n99.16\n99.15\n98.88\n98.94\n98.8\n98.9\n98.99\n99\n98.9\n98.62\n98.61\n98.86\n98.75\n98.85\n98.94\n99.08\n99.34\n99.69\n99.73\n100.05\n99.68\n99.92\n100.19\n100\n100.07\n99.96\n99.45\n99.5\n\n\n10\nTUN\nTunisia\n91.69\n91.69\n91.43\n91.33\n91.41\n91.2\n90.98\n91.13\n92.82\n96.68\n94.86\n95.32\n95.48\n95.05\n97.59\n97.6\n96.65\n96.17\n96.19\n95.89\n95.19\n91.99\n92.1\n91.47\n92.13\n92.69\n91.38\n91.41\n91.14\n91.24\n91.6\n92.31\n91.98\n90.05\n90.7\n92.99\n92.66\n92.96\n93.9\n94.92\n96.22\n97.64\n98.44\n100.14\n96.95\n98.53\n100.43\n100\n100.02\n99.41\n96.02\n95.32\n\n\n11\nZAF\nSouth Africa\n94.08\n94.02\n93.98\n93.98\n93.99\n93.88\n93.94\n93.94\n94.31\n97.02\n96.36\n96.67\n96.87\n96.55\n98.33\n98.49\n98.1\n97.82\n97.6\n97.45\n97.25\n95.06\n95.47\n94.75\n95.2\n95.75\n95.05\n95.02\n94.64\n94.58\n94.76\n95.16\n95.09\n94.01\n94.82\n96.14\n95.78\n95.99\n96.42\n97.16\n98.08\n98.74\n99.1\n100.03\n98.47\n99.36\n100.2\n100\n100.03\n99.74\n97.76\n97.45\n\n\n12\nZMB\nZambia\n94.39\n94.44\n94.09\n94.12\n94.24\n94.08\n93.75\n93.6\n94.47\n97.18\n95.84\n96.11\n96.12\n95.77\n97.71\n97.82\n97.19\n96.75\n96.68\n96.4\n96.21\n93.88\n94.37\n93.97\n94.47\n94.92\n94.09\n94.07\n93.52\n93.64\n94.03\n94.23\n94.09\n92.83\n93.48\n94.81\n94.4\n94.68\n95.17\n96.24\n97.26\n98.52\n98.98\n99.9\n98.04\n99.23\n100.3\n100\n99.93\n99.49\n97.38\n97.02\n\n\n13\nZWE\nZimbabwe\n91.59\n91.51\n91.33\n91.25\n91.39\n91.35\n91.24\n91.09\n92.06\n96.07\n94.79\n95.04\n95.02\n94.44\n97.06\n97.28\n96.58\n95.95\n95.87\n95.58\n95.21\n91.83\n92.23\n91.85\n92.52\n93.12\n92.01\n91.92\n91.29\n91.3\n91.72\n92.4\n92\n90.26\n91.34\n93.32\n92.76\n93.2\n93.94\n95.12\n96.37\n97.78\n98.58\n99.87\n97.24\n98.74\n100.38\n100\n99.87\n99.2\n96.18\n95.65\n\n\n\n\n\nCode\nimport_df_joinable <- imf_df_transform(import_df, \"imp_pi\")\nhtmlTable(head(import_df_joinable, n=10))\n\n\n\n\n\n\ncc3\nyear\nimp_pi\n\n\n\n\n1\nDZA\n1965-01-01\n99.56\n\n\n2\nAGO\n1965-01-01\n99.53\n\n\n3\nCAF\n1965-01-01\n98.57\n\n\n4\nCIV\n1965-01-01\n92.85\n\n\n5\nEGY\n1965-01-01\n97.98\n\n\n6\nKEN\n1965-01-01\n93.97\n\n\n7\nMUS\n1965-01-01\n91.84\n\n\n8\nMAR\n1965-01-01\n90.21\n\n\n9\nNGA\n1965-01-01\n99.52\n\n\n10\nZAF\n1965-01-01\n94.08\n\n\n\n\n\nAttaching import price data to the main data frame.\n\n\nCode\nac_imf <- inner_join(ac_imf, import_df_joinable, by = c(\"cc3\", \"year\"))\nsample_n(ac_imf, 10) %>% htmlTable()\n\n\n\n\n\n\ncc3\ncountry\nyear\nsystemic_crisis\nexch_usd\ndomestic_debt_in_default\nsovereign_external_debt_default\ngdp_weighted_default\ninflation_rate\nindependence\ncurrency_crisis\ninflation_crisis\nbanking_crisis\nexp_pi\nimp_pi\n\n\n\n\n1\nEGY\nEgypt\n1987-01-01\n0\n0.7\n0\n0\n0\n25.185\n1\n0\n1\n0\n95.83\n95.82\n\n\n2\nCIV\nIvory Coast\n1984-01-01\n0\n486.5\n0\n1\n0\n4.281\n1\n0\n0\n0\n101.51\n96.88\n\n\n3\nDZA\nAlgeria\n1982-01-01\n0\n4.6355\n0\n0\n0\n6.593\n1\n0\n0\n0\n87.6\n98.99\n\n\n4\nZMB\nZambia\n1975-01-01\n0\n0.0006429\n0\n0\n0\n10.16949153\n1\n0\n0\n0\n83.13\n95.84\n\n\n5\nCAF\nCentral African Republic\n2009-01-01\n0\n455.3359711\n0\n1\n0\n3.522\n1\n0\n0\n0\n99.26\n99.4\n\n\n6\nKEN\nKenya\n1995-01-01\n1\n55.9389\n0\n1\n0\n1.554\n1\n0\n0\n1\n98.09\n94.42\n\n\n7\nNGA\nNigeria\n1996-01-01\n0\n21.8861\n0\n0\n0\n29.292\n1\n0\n1\n0\n74.6\n99\n\n\n8\nMUS\nMauritius\n1966-01-01\n0\n4.77862\n0\n0\n0\n2.982014118\n1\n0\n0\n0\n95.56\n91.73\n\n\n9\nNGA\nNigeria\n2007-01-01\n0\n117.968\n0\n0\n0\n5.413\n1\n0\n0\n0\n94.98\n99.73\n\n\n10\nEGY\nEgypt\n1999-01-01\n0\n3.405\n0\n0\n0\n3.745\n1\n0\n0\n0\n94.97\n95.02"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#creating-new-variables",
    "href": "posts/Final_Project- Said Arslan.html#creating-new-variables",
    "title": "Final Project",
    "section": "Creating new variables",
    "text": "Creating new variables\nI have now export price index and import price index data included in the data set. I will also create yearly change rates for export&import indices so that I could compare them with inflation rate for my analysis. I will use lag function to calculate change rates.\n\n\nCode\nac_imf <- ac_imf %>% group_by(cc3) %>% arrange(year) %>%\n  mutate(change_epi= round((exp_pi-lag(exp_pi))/lag(exp_pi)*100, digits=2)) %>%\n  mutate(change_ipi=round((imp_pi-lag(imp_pi))/lag(imp_pi)*100, digits=2)) %>% \n  ungroup()\n\n\nac_imf %>% group_by(country) %>% sample_n(2) %>% arrange(year) %>% htmlTable()\n\n\n\n\n\n\ncc3\ncountry\nyear\nsystemic_crisis\nexch_usd\ndomestic_debt_in_default\nsovereign_external_debt_default\ngdp_weighted_default\ninflation_rate\nindependence\ncurrency_crisis\ninflation_crisis\nbanking_crisis\nexp_pi\nimp_pi\nchange_epi\nchange_ipi\n\n\n\n\n1\nMAR\nMorocco\n1967-01-01\n0\n5.0276\n0\n0\n0\n-1.126519599\n1\n0\n0\n0\n100.51\n89.98\n0.07\n-0.12\n\n\n2\nTUN\nTunisia\n1969-01-01\n0\n0.52\n0\n0\n0\n2.804694188\n1\n0\n0\n0\n90.56\n91.41\n0.01\n0.09\n\n\n3\nZWE\nZimbabwe\n1969-01-01\n0\n0.00000000000000000000000000714\n1\n1\n0\n0.364644862\n1\n0\n0\n0\n99.6\n91.39\n0.43\n0.15\n\n\n4\nMUS\nMauritius\n1971-01-01\n0\n5.22362\n0\n0\n0\n1.886196037\n1\n0\n0\n0\n101\n90.77\n0.08\n-0.46\n\n\n5\nMAR\nMorocco\n1973-01-01\n0\n3.8851\n0\n0\n0\n7.193348119\n1\n0\n0\n0\n101.5\n91.14\n0.82\n1.54\n\n\n6\nKEN\nKenya\n1974-01-01\n0\n7.1429\n0\n0\n0\n16.41376144\n1\n0\n0\n0\n100.01\n97.49\n0.65\n3.45\n\n\n7\nKEN\nKenya\n1976-01-01\n0\n8.27\n0\n0\n0\n9.527607326\n1\n1\n0\n0\n100.36\n96.6\n0.8\n0.09\n\n\n8\nDZA\nAlgeria\n1979-01-01\n0\n3.7555\n0\n0\n0\n14.60957788\n1\n0\n0\n0\n85.38\n100.05\n13.4\n0.35\n\n\n9\nCAF\nCentral African Republic\n1979-01-01\n1\n200.9998578\n0\n0\n0\n9.210526316\n1\n0\n0\n1\n101.13\n99.81\n0.59\n0.59\n\n\n10\nCIV\nIvory Coast\n1980-01-01\n0\n221\n0\n0\n0\n8.81\n1\n0\n0\n0\n104.6\n99.09\n-3.34\n0.49\n\n\n11\nNGA\nNigeria\n1980-01-01\n0\n0.544454729\n0\n0\n0\n9.97\n1\n0\n0\n0\n90.94\n100.31\n1.65\n0.15\n\n\n12\nCIV\nIvory Coast\n1984-01-01\n0\n486.5\n0\n1\n0\n4.281\n1\n0\n0\n0\n101.51\n96.88\n0.92\n-0.41\n\n\n13\nCAF\nCentral African Republic\n1986-01-01\n0\n322.7497717\n0\n1\n0\n2.411\n1\n0\n0\n0\n99.45\n98.1\n0.15\n-0.67\n\n\n14\nNGA\nNigeria\n1986-01-01\n0\n3.316749585\n0\n0\n0\n6.25\n1\n1\n0\n0\n74.55\n99.12\n-13.04\n-0.37\n\n\n15\nZMB\nZambia\n1987-01-01\n0\n0.0077\n0\n1\n0\n47.028\n1\n0\n1\n0\n77.5\n94.37\n3.67\n0.52\n\n\n16\nZWE\nZimbabwe\n1989-01-01\n0\n0.0000000000000000000000000222\n0\n0\n0\n12.8\n1\n1\n0\n0\n99.35\n92.52\n0.15\n0.73\n\n\n17\nDZA\nAlgeria\n1991-01-01\n1\n21.3919\n0\n1\n0.23\n25.9\n1\n1\n1\n1\n69.81\n97.27\n-4.33\n-0.52\n\n\n18\nEGY\nEgypt\n1992-01-01\n0\n3.33861\n0\n0\n0\n21.142\n1\n0\n1\n1\n95.1\n95.79\n-0.2\n0.21\n\n\n19\nZMB\nZambia\n1994-01-01\n0\n0.699\n0\n1\n0\n54.614\n1\n1\n1\n0\n78.83\n93.64\n4.19\n0.13\n\n\n20\nZAF\nSouth Africa\n1996-01-01\n0\n4.6825\n0\n0\n0\n7.32\n1\n1\n0\n0\n96.03\n95.16\n-0.1\n0.42\n\n\n21\nAGO\nAngola\n2000-01-01\n0\n16.81784\n1\n1\n0\n325.029\n1\n1\n1\n0\n62.81\n98.36\n21.91\n0.17\n\n\n22\nZAF\nSouth Africa\n2005-01-01\n0\n6.325\n0\n0\n0\n3.393\n1\n0\n0\n0\n97.73\n98.08\n0.57\n0.95\n\n\n23\nAGO\nAngola\n2007-01-01\n0\n75.023\n0\n0\n0\n12.249\n1\n0\n0\n0\n89.05\n99.44\n3.1\n0.18\n\n\n24\nMUS\nMauritius\n2007-01-01\n0\n28.2162\n0\n0\n0\n8.827\n1\n0\n0\n0\n94.24\n97.91\n-4.07\n0.77\n\n\n25\nTUN\nTunisia\n2008-01-01\n0\n1.3099\n0\n0\n0\n4.913\n1\n0\n0\n0\n99.96\n100.14\n1.23\n1.73\n\n\n26\nEGY\nEgypt\n2013-01-01\n0\n6.94\n0\n0\n0\n6.914\n1\n0\n0\n0\n100.06\n99.94\n0.06\n-0.06"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#summary-statistics-of-the-columns",
    "href": "posts/Final_Project- Said Arslan.html#summary-statistics-of-the-columns",
    "title": "Final Project",
    "section": "Summary statistics of the columns",
    "text": "Summary statistics of the columns\n\n\nCode\nprint(dfSummary(ac_imf, \n                varnumbers= FALSE, \n                plain.ascii= FALSE, \n                style= \"grid\", \n                graph.magnif= 0.80, \n                valid.col= TRUE),\n      method= 'render', \n      table.classes= 'table-condensed')\n\n\n\n\nData Frame Summary\nac_imf\nDimensions: 629 x 17\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Valid\n      Missing\n    \n  \n  \n    \n      cc3\n[factor]\n      1. DZA2. AGO3. CAF4. CIV5. EGY6. KEN7. MUS8. MAR9. NGA10. ZAF[ 3 others ]\n      47(7.5%)35(5.6%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)49(7.8%)49(7.8%)149(23.7%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      country\n[character]\n      1. Central African Republic2. Egypt3. Ivory Coast4. Kenya5. Mauritius6. Morocco7. Tunisia8. Zambia9. Nigeria10. South Africa[ 3 others ]\n      50(7.9%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)50(7.9%)49(7.8%)49(7.8%)131(20.8%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      year\n[Date]\n      min : 1965-01-01med : 1990-01-01max : 2014-01-01range : 49y 0m 0d\n      50 distinct values\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      systemic_crisis\n[integer]\n      Min  : 0Mean : 0.1Max  : 1\n      0:550(87.4%)1:79(12.6%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      exch_usd\n[numeric]\n      Mean (sd) : 63 (132.7)min ≤ med ≤ max:0 ≤ 5.2 ≤ 744.3IQR (CV) : 33.7 (2.1)\n      420 distinct values\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      domestic_debt_in_default\n[integer]\n      Min  : 0Mean : 0.1Max  : 1\n      0:587(93.3%)1:42(6.7%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      sovereign_external_debt_default\n[integer]\n      Min  : 0Mean : 0.2Max  : 1\n      0:475(75.5%)1:154(24.5%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      gdp_weighted_default\n[numeric]\n      Mean (sd) : 0 (0)min ≤ med ≤ max:0 ≤ 0 ≤ 0.4IQR (CV) : 0 (6)\n      0.00:607(96.5%)0.06:4(0.6%)0.13:6(1.0%)0.23:6(1.0%)0.36:5(0.8%)0.40:1(0.2%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      inflation_rate\n[numeric]\n      Mean (sd) : 35098 (876785.1)min ≤ med ≤ max:-8.8 ≤ 8 ≤ 21989695IQR (CV) : 10.1 (25)\n      561 distinct values\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      independence\n[integer]\n      Min  : 0Mean : 1Max  : 1\n      0:5(0.8%)1:624(99.2%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      currency_crisis\n[integer]\n      Mean (sd) : 0.2 (0.4)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 0 (2.2)\n      0:516(82.0%)1:110(17.5%)2:3(0.5%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      inflation_crisis\n[integer]\n      Min  : 0Mean : 0.2Max  : 1\n      0:531(84.4%)1:98(15.6%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      banking_crisis\n[integer]\n      Min  : 0Mean : 0.1Max  : 1\n      0:538(85.5%)1:91(14.5%)\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      exp_pi\n[numeric]\n      Mean (sd) : 92.3 (12.1)min ≤ med ≤ max:39.6 ≤ 97.1 ≤ 116.8IQR (CV) : 9 (0.1)\n      512 distinct values\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      imp_pi\n[numeric]\n      Mean (sd) : 96.4 (3)min ≤ med ≤ max:88.8 ≤ 97.1 ≤ 101.6IQR (CV) : 4.9 (0)\n      460 distinct values\n      \n      629\n(100.0%)\n      0\n(0.0%)\n    \n    \n      change_epi\n[numeric]\n      Mean (sd) : 0.4 (4.5)min ≤ med ≤ max:-35.4 ≤ 0 ≤ 51.1IQR (CV) : 1.4 (10.2)\n      375 distinct values\n      \n      616\n(97.9%)\n      13\n(2.1%)\n    \n    \n      change_ipi\n[numeric]\n      Mean (sd) : 0.1 (1)min ≤ med ≤ max:-4.1 ≤ 0 ≤ 5.2IQR (CV) : 0.8 (9.4)\n      265 distinct values\n      \n      616\n(97.9%)\n      13\n(2.1%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\nMy last form of the dataset is composed from 629 observations and 17 variables. Each observation includes country-year economic indicators. 17 variables consists of: - 2 categorical variable - 1 date variable - 7 numeric variable - 7 binary variable"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#univariate-analysis",
    "href": "posts/Final_Project- Said Arslan.html#univariate-analysis",
    "title": "Final Project",
    "section": "Univariate analysis",
    "text": "Univariate analysis\n\nYear\nDataset covers 50 years from 1965 to 2014. Since we have 13 countries, each year should have frequency of 13.\n\n\nCode\nac_imf %>% mutate(Year= format(year, format=\"%Y\")) %>% \n  group_by(Year) %>% \n  summarise(number_of_obs=n()) %>% \n  arrange(number_of_obs) %>% \n  filter(number_of_obs < 13) %>% \n  htmlTable()\n\n\n\n\n\n\nYear\nnumber_of_obs\n\n\n\n\n1\n1965\n11\n\n\n2\n1966\n11\n\n\n3\n1967\n11\n\n\n4\n2014\n11\n\n\n5\n1968\n12\n\n\n6\n1969\n12\n\n\n7\n1981\n12\n\n\n8\n1982\n12\n\n\n9\n1983\n12\n\n\n10\n1984\n12\n\n\n11\n1985\n12\n\n\n12\n1986\n12\n\n\n13\n1987\n12\n\n\n14\n1988\n12\n\n\n15\n1989\n12\n\n\n16\n1990\n12\n\n\n17\n1991\n12\n\n\n\n\n\nSo, only for 4 years there is no observation for 2 countries and 13 years have missing information for only 1 country. All other 37 years include data from all countries, which is good.\n\n\nCountry\nAccording to the summary table, 8 of 13 countries have frequency of 50 which is as expected, however 5 countries have less than 50 frequency, which means that there is no observation for those countries for some years.\n\n\nCode\nac_imf %>% ggplot(aes(x= fct_rev(fct_infreq(country)))) + \n  geom_bar(stat= 'count', alpha= 0.6, width= 0.5) + \n  xlab(\"Country\") +\n  ylab(\"Number of observations\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\nIt looks that Angola has significant amount of missing data with no observation for 15 years. Other countries look good.\n\n\nExchange Rate\nLet me compare and analyze exchange rate of the countries with box plots.\n\n\nCode\nac_imf %>% ggplot() + \n  geom_boxplot(aes(x= reorder(country, exch_usd, FUN= median, decreasing= F), \n                   y= exch_usd, fill= country), show.legend= F) +\n  xlab(\"Country\") +\n  ylab(\"Exchange Rate\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\nMedian exchange rate of Central African Republic and Ivory Coast are significantly higher than exchange rates of other countries. HOwever, their box plots are not highly right skewed, they disperse evenly over their values. On the other hand, Nigeria and Angola have significantly right skewed box plots, which might be a sign that these countries experienced more currency crises than other countries. Similarly. Zimbabwe have the lowest median exchange rate but it has huge outlier points, which poses a bad evidence for Zimbabwe currency.\n\n\nCurrency Crises\ncurrency_crisis variable should only consist of 0 and 1 values but When we look at the summary table, we see that there are three cells with value of 2. I think they are wrong entries and their true value is 1. So let me fix them first.\n\n\nCode\nac_imf$currency_crisis[ac_imf$currency_crisis > 1] <- 1L\n\n\nNow I will create a table that shows how many currency crisis observed by each country over 50 years.\n\n\nCode\nac_imf %>% group_by(country) %>% \n  summarise(total_currency_crises= sum(currency_crisis)) %>% \n  arrange(desc(total_currency_crises)) %>% \n  htmlTable()\n\n\n\n\n\n\ncountry\ntotal_currency_crises\n\n\n\n\n1\nZimbabwe\n21\n\n\n2\nAngola\n19\n\n\n3\nZambia\n18\n\n\n4\nSouth Africa\n12\n\n\n5\nNigeria\n10\n\n\n6\nKenya\n9\n\n\n7\nAlgeria\n6\n\n\n8\nEgypt\n6\n\n\n9\nMauritius\n5\n\n\n10\nTunisia\n4\n\n\n11\nCentral African Republic\n1\n\n\n12\nIvory Coast\n1\n\n\n13\nMorocco\n1\n\n\n\n\n\nAs I expected in previous part, Zimbabwe and Angola had currency crises more often than other countries. Even though Central African Republic and Ivory Coast have higher exchange rate generally, they experienced a currency crisis only once.\n\n\nGDP weighted default\nAccording to the summary table, gdp weighted default is 0 majority of the time. When it is not 0, it is between 0 and 1 because it gives debt default and gdp ratio of the country. I will rescale gdp weighted default values so it shows percentage of gdp and then visualize the distribution by histogram.\n\n\nCode\nac_imf <- ac_imf %>% \n  mutate(gdp_weighted_default= gdp_weighted_default*100)\n\n\nInstead of distribution for single gdp weighted default values, I would like to know distribution of aggregate values of 13 countries for each year.\n\n\nCode\nac_imf %>% group_by(year) %>% \n  summarise(cum_gdp_wght_def=sum(gdp_weighted_default)) %>% \n  ggplot(aes(x=cum_gdp_wght_def, y = after_stat(density))) + \n  geom_histogram(bins = 10, fill=\"blue\", alpha=0.6) + \n  scale_x_continuous(breaks=c(0,10,20,30,40,50,60)) +\n  xlab(\"Yearly cumulative gdp weighted default\") +\n  theme_bw()\n\n\n\n\n\nAlso let me find out which countries and how many times had positive gdp weighted default as well as their average.\n\n\nCode\nac_imf %>% filter(gdp_weighted_default > 0) %>% \n  group_by(country) %>% \n  summarise(\n    number_of_years=n(), \n    mean_percentage=mean(gdp_weighted_default)) %>% \n  arrange(desc(number_of_years), desc(mean_percentage)) %>% \n  htmlTable()\n\n\n\n\n\n\ncountry\nnumber_of_years\nmean_percentage\n\n\n\n\n1\nAlgeria\n6\n23\n\n\n2\nMorocco\n6\n13\n\n\n3\nSouth Africa\n5\n36\n\n\n4\nTunisia\n4\n6\n\n\n5\nEgypt\n1\n40\n\n\n\n\n\nThe worst case is obviously for South Africa with 5 years and with average default of 36% of their gdp.\n\n\nIndependence\n\n\nCode\nac_imf %>% filter(independence < 1) %>% \n  select(country, year, independence) %>% \n  mutate(year=format(year, format=\"%Y\"))\n\n\n# A tibble: 5 × 3\n  country year  independence\n  <chr>   <chr>        <int>\n1 Angola  1970             0\n2 Angola  1971             0\n3 Angola  1972             0\n4 Angola  1973             0\n5 Angola  1974             0\n\n\nOver the years that covered by transformed dataset, only Angola was not independent from 1965 to 1975. So, it doesn’t enable us to evaluate the effect of independence on economic crisis.\n\n\nInflation Rate\nAccording to the summary table, inflation rate have some extreme values. Let me first find them.\n\n\nCode\nac_imf %>% select(country, inflation_rate) %>% \n  group_by(country) %>% \n  summarise_at(.vars=\"inflation_rate\", \n               .funs= list(minimum=min, \n                           median=median, \n                           maximum=max)) %>% \n  htmlTable()\n\n\n\n\n\n\ncountry\nminimum\nmedian\nmaximum\n\n\n\n\n1\nAlgeria\n0.3\n6.31\n31.7\n\n\n2\nAngola\n5.78\n46.71\n4146.01\n\n\n3\nCentral African Republic\n-8.82\n3.725\n27.7\n\n\n4\nEgypt\n0\n10\n25.18\n\n\n5\nIvory Coast\n-1.07\n4.25\n25.96\n\n\n6\nKenya\n-0.13\n8.62\n45.98\n\n\n7\nMauritius\n0.01\n6.47\n33.04\n\n\n8\nMorocco\n-1.13\n3.065\n15.98\n\n\n9\nNigeria\n-4.55\n11.77\n72.73\n\n\n10\nSouth Africa\n1.39\n8.71\n18.75\n\n\n11\nTunisia\n1.44\n4.66\n13.67\n\n\n12\nZambia\n2.52\n17.08\n183.26\n\n\n13\nZimbabwe\n-7.67\n12.55\n21989695.22\n\n\n\n\n\nAs we can see from the table that all countries’ median inflation rate is around normal/acceptable values, however, Angola has a very extreme maximum value (4146%). In addition to Angola, Zimbabwe has an astronomic maximum inflation rate with 21989695%. It is either a recording error or there is a very very exceptional thing happened in Zimbabwe for a year because Zimbabwe has median inflation rate of only 12%.\n\n\nCode\nac_imf %>% select(year, country, inflation_rate) %>% \n  filter(inflation_rate>1000) %>% \n  htmlTable()\n\n\n\n\n\n\nyear\ncountry\ninflation_rate\n\n\n\n\n1\n1993-01-01\nAngola\n1379.48\n\n\n2\n1995-01-01\nAngola\n2672.23\n\n\n3\n1996-01-01\nAngola\n4146.01\n\n\n4\n2006-01-01\nZimbabwe\n1281.11\n\n\n5\n2007-01-01\nZimbabwe\n66279.89\n\n\n6\n2008-01-01\nZimbabwe\n21989695.22\n\n\n\n\n\nSo there are 6 exceptional observations with higher than 1000% inflation rate. To deal with these cases, I will fix them to 1000% for health of my further analysis.\n\n\nCode\nac_imf$inflation_rate[ac_imf$inflation_rate > 1000] <- 1000\n\n\nNow I can plot the distribution of inflation rates over countries.\n\n\nCode\nac_imf %>% filter(inflation_rate < 100) %>% ggplot() + \n  geom_violin(aes(x= reorder(country, inflation_rate, FUN= median, decreasing= F), \n                   y= inflation_rate, fill= country), show.legend= F, alpha=0.8) +\n  xlab(\"Country\") +\n  ylab(\"Inflation Rate\") +\n  coord_flip() +\n  theme_bw()"
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#bivariate-and-multivarite-analysis",
    "href": "posts/Final_Project- Said Arslan.html#bivariate-and-multivarite-analysis",
    "title": "Final Project",
    "section": "Bivariate and multivarite analysis",
    "text": "Bivariate and multivarite analysis\n\nEconomic crises\nMy main focus of analysis is systemic crisis. According to the summary table, 79 times systemic crisis were observed over 50 years. In this perspective, the first thing I would like to know is how the distribution of systemic crises is between countries in terms of percentage of occurrence. Also I expect that the countries that had systemic crises also experienced banking crises. According to the summary table, 91 times banking crises observed. To verify my hypothesis I will visualise countries’ systemic crises and banking crises experiences over pie charts\n\n\nCode\nsystemic_pie <- ac_imf %>% filter(systemic_crisis==1) %>% select(country)\nPieChart(country, \n         hole = 0, \n         values = \"%\", \n         data = systemic_pie, \n         fill = brewer.pal(n = 12, name = \"Paired\"), \n         main = \"Systemic Crises\", \n         quiet = TRUE)\n\n\n\n\n\n\n\nCode\nbanking_pie <- ac_imf %>% filter(banking_crisis==1) %>% select(country)\nPieChart(country, \n         hole = 0, \n         values = \"%\", \n         data = banking_pie, \n         fill = \"rainbow\", \n         main = \"Banking Crises\", \n         quiet = TRUE)\n\n\n\n\n\nAs I expected, countries which had systemic crises more often experienced banking crises more often as well such as Central African Republic, Zimbabwe and Nigeria, These pie charts above also revealed another thing that all of the 13 countries observed banking crises at least once whereas only 10 countries observed systemic crises. So, there are 3 countries that had banking crisis but no systemic crisis.\nLet me find out which countries they are.\n\n\nCode\nac_imf %>% \n  group_by(country) %>% \n  summarize(total_sys_crs=sum(systemic_crisis)) %>% \n  arrange(total_sys_crs) %>% \n  ungroup() %>% \n  slice_head(n=3) %>% \n  htmlTable()\n\n\n\n\n\n\ncountry\ntotal_sys_crs\n\n\n\n\n1\nAngola\n0\n\n\n2\nMauritius\n0\n\n\n3\nSouth Africa\n0\n\n\n\n\n\nThese countries did not have systemic crises and also they had banking crises much less frequent than other countries, which makes me think that banking crises are related to systemic crises. To verify that, I will plot timelines for systemic crises and banking crises for each country.\n\n\nCode\nac_imf %>% select(country, year, systemic_crisis, banking_crisis) %>% \n  pivot_longer(cols= ends_with(\"crisis\"), \n               names_to= \"Type_of_crisis\", \n               values_to= \"occurance\") %>% \n  ggplot() +\n  geom_jitter(aes(year, occurance, color= Type_of_crisis, shape=Type_of_crisis), width = 0.2, height = 0.2) + \n  geom_line(aes(year, occurance, color= Type_of_crisis)) +\n  scale_y_continuous(name = \"Occurance of Crisis\", breaks = c(0,1)) +\n  facet_wrap(vars(country), ncol = 2) +\n  theme_minimal()\n\n\n\n\n\nPlots for Central African Republic, Zimbabwe, Kenya, Algeria, Ivory Coast, Tunusia, Nigeria, Zambia and Morocca pose big evidence that there is very close relationship between banking crises and systemic crises. Indeed, it looks that whenever a country had systemic crisis it had banking crisis as well. The reverse of that not be true though. From the plots above, we see that Angola and South Africa had banking crises for few times but they did not experience systemic crises meanwhile. However,\nBesides that, the plots above show that banking and sytemic crises became more frequent after 1980s. I will now investigate it and also include currency and inflation crises to my analysis.\n\n\nCode\nac_imf %>% select(year, systemic_crisis, banking_crisis, inflation_crisis, currency_crisis) %>% \n  pivot_longer(cols= ends_with(\"crisis\"), \n               names_to= \"Type_of_crisis\", \n               values_to= \"occurance\") %>% \n  group_by(year, Type_of_crisis) %>%\n  summarize(Number_of_Occurance=sum(occurance), .groups= \"drop\") %>%\n  ggplot(aes(year, Number_of_Occurance)) +\n  geom_col(aes(fill=Type_of_crisis), color=\"white\", width= 700, alpha=0.8)\n\n\n\n\n\nSo, the graph tells us that beginning with late 1980s more African countries started to experience crises more often. This trend lasted until 2000s. 1995 is the peak year in terms of overall number of economics crises. Also, systemic crises took place more often during this period.\n\n\nDomestic & Sovereign external debt in defaults\nNow I would like to visualize domestic debt in default and sovereign external debt in default cases varied over 50 year in African countries.\n\n\nCode\nac_imf %>% mutate(Year= as.numeric(format(year, format=\"%Y\"))) %>% \n  select(Year, contains(\"debt\")) %>% \n  pivot_longer(cols= contains(\"debt\"), \n               names_to= \"Debts_in_Deafult\", \n               values_to= \"occurance\") %>% \n  filter(occurance==1) %>% ggplot(aes(x= Year, fill= Debts_in_Deafult)) + \n  geom_histogram(position= \"dodge2\", alpha = 0.8, bins=15) +\n  geom_freqpoly(aes(Year, color=Debts_in_Deafult), size=1, binwidth = 3.5) +\n  scale_x_continuous(breaks=c(1960,1970,1980,1990,2000,2010)) +\n  theme_bw()\n\n\n\n\n\nThe graph shows that domestic debt in default cases have almost the same trend as economic crises we analyzed in previous part. They started to become more often after mid 80s but they lasted in 2000s as well. On the other hand, there is no specific trend for domestic debt in default cases. So, we can result that in years of systemic crises, debts in default are prevalent as well.\n\n\nExport & Import Price Index and Inflation Rate\nLet me first visualize how export and import price indices change in African countries over 50 years.\n\n\nCode\nac_imf %>%  dplyr::rename(\"Export_Price_Index\"=\"exp_pi\", \"Import_Price_Index\"=\"imp_pi\") %>% \n  pivot_longer(cols= c(\"Export_Price_Index\", \"Import_Price_Index\"), \n                        names_to= \"Price_Index\",\n                        values_to= \"Rate\") %>% \n  ggplot(aes(x=year, y=Rate)) +\n  geom_point(aes(color=Price_Index)) +\n  geom_line(aes(color=Price_Index)) +\n  guides(color= guide_legend(title = NULL, override.aes= list(size= 3))) +\n  scale_y_continuous(name= \"Export & Import Price Indices\") +\n  facet_wrap(vars(country), ncol = 2) +\n  theme_minimal()\n\n\n\n\n\nFrom the plots, we can see that Algeria, Angola, Nigeria, Zambia had significant drop, Ivory Coast and Tunusia had moderate drop in their export price indices during 80s and 90s, which might cause trade deficits in these countries and trigger other macroeconomic issues.\nI also expect that when the gap between Import and Export price indices of a country gets larger, it is more probable that the inflation rate in that country goes up. I would like to test my hypothesis in these African countries. To get better a fit and visualization, I will put an upper limit on inflation rate.\n\n\nCode\nac_imf %>% filter(inflation_rate <500) %>% mutate(Import_Export= imp_pi/exp_pi*100) %>% filter(Import_Export <200) %>% \n  ggplot(aes(x=Import_Export, y=inflation_rate)) +\n  xlab(\"Import and Export Price Index Ratio\") +\n  ylab(\"Inflation Rate\") +\n  geom_point() +\n  geom_smooth(se= FALSE) + \n  theme_bw()\n\n\n\n\n\n\n\nCorrelation Between Economic Indicators\nLastly I would like to explore the correlation between the economic indicators of African countries I got in the dataset.\n\n\nCode\nac_imf %>% select(systemic_crisis, \n                  banking_crisis, \n                  inflation_crisis, \n                  currency_crisis, \n                  inflation_rate, \n                  exch_usd, \n                  domestic_debt_in_default, \n                  sovereign_external_debt_default, \n                  exp_pi, \n                  imp_pi,\n                  gdp_weighted_default) %>% \n  dplyr::rename(\"Systemic Crisis\"=\"systemic_crisis\",\n                \"Banking Crisis\"=\"banking_crisis\",\n                \"Inflation Crisis\"=\"inflation_crisis\",\n                \"Currency Crisis\"=\"currency_crisis\",\n                \"Inflation Rate\"=\"inflation_rate\",\n                \"Exchange Rate\"=\"exch_usd\",\n                \"Domestic Debt\"=\"domestic_debt_in_default\",\n                \"External Debt\"=\"sovereign_external_debt_default\",\n                \"GDP Weighted Default\"=\"gdp_weighted_default\",\n                \"Export Price Index\"=\"exp_pi\",\n                \"Import Price Index\"=\"imp_pi\"\n                ) %>% \n  cor() %>% \n  corrplot(method=\"circle\", tl.srt= 60)\n\n\n\n\n\nThus, the biggest positive correlation is between the banking crises and systemic crises , which is I already pointed in my previous analysis. There is significant positive relationship between inflation rate and domestic debt in default. A new exploration I got form the correlation plot is that there is significant negative correlation between export price index and inflation crisis. It makes sense because these African countries depend on their exports in terms of revenue. If export price index gets lower, they face trade deficits and, as a result of that, their economies get into trouble."
  },
  {
    "objectID": "posts/Final_Project- Said Arslan.html#references",
    "href": "posts/Final_Project- Said Arslan.html#references",
    "title": "Final Project",
    "section": "References:",
    "text": "References:\n\nKaggle dataset: https://www.kaggle.com/datasets/chirin/africa-economic-banking-and-systemic-crisis-data\nCommodity of Terms of Trade: Commodity Import/Export Price Index, Individual Commodities Weighted by Ratio of Imports to GDP Historical, Annual (1965-present), Fixed Weights, Index (2012=100): https://data.imf.org/?sk=2CDDCCB8-0B59-43E9-B6A0-59210D5605D2&sId=1434492942851\nR for Data Science: https://r4ds.had.co.nz/index.html\nhttps://r-graph-gallery.com/index.html\nggplot2: Elegant Graphics for Data Analysis: https://ggplot2-book.org/\nDACSS 601 Class resources: https://github.com/DACSS/601_Fall_2022"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html",
    "href": "posts/Final_Project_ErikaNagai.html",
    "title": "Final Project Erika Nagai",
    "section": "",
    "text": "Even though half of the moviegoers in the United States are women, their story is not represented as much as that of men. Only 35% of the main characters in the best movies of 2021 were female, and male characters outnumbered female characters in 85% of the films. (Women and Hollywood, 2022) Also lacking are women behind the scenes in the film business. In 2021, just 12% of the top 100 films are directed by women. (Lauzen, n.d.)\nIn this project, I focused on analyzing female representation in movie stories rather than in their production. I used the data from three major movie databases, The Movie Database (TMDb), MovieLens, Open Movie Database (OMDb), and the Bechdel Test, one of the most common criteria to measure female presentation.\nI would like to answer the following questions in this analysis:\n\nIs female representation in movie stories improving over time?\nWhat genres are doing better than others in terms of female representation?\nAre movies with good female representation more popular?\nDo movies with good female representation more money?\n\nThis analysis intends to document the data cleaning process, which is quite long. If you are curious only about the analysis results, please skip to Data Analysis and Visualization part.\n\n\nCode\n# install libraries\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(summarytools)\n\n\nWarning: package 'summarytools' was built under R version 4.2.2\n\n\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\nlibrary(lubridate)\n\n\nWarning: package 'lubridate' was built under R version 4.2.2\n\n\nLoading required package: timechange\n\n\nWarning: package 'timechange' was built under R version 4.2.2\n\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(ggridges)"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#read-in-data",
    "href": "posts/Final_Project_ErikaNagai.html#read-in-data",
    "title": "Final Project Erika Nagai",
    "section": "Read in data",
    "text": "Read in data\nFor this analysis, I used the following different data sets.\n\n(1) “The Movie Data set” from Kaggle :\nThis dataset was obtained from Kaggle “The Movie Data set”(https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?resource=download&select=movies_metadata.csv).\n\n\nCode\n#(1) movies_metadata.csv obtained from Kaggle\nmovie = read_csv(\"_data/movies_metadata.csv\")\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 45466 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (14): belongs_to_collection, genres, homepage, imdb_id, original_langua...\ndbl   (7): budget, id, popularity, revenue, runtime, vote_average, vote_count\nlgl   (2): adult, video\ndate  (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n(2) Bechdel test API:\nI generated the data by using the bechdel test API https://bechdeltest.com/api/v1/doc.\n\n\nCode\n#(2) bechdel test obtained by using bechdel API\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nWarning: package 'jsonlite' was built under R version 4.2.2\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nCode\n#json_file <- \"http://bechdeltest.com/api/v1/getAllMovies\"\n#bechdel <- read_json(path = json_file, simplifyVector = TRUE)\n#bechdel$titleId <- paste(\"tt\",bechdel$imdbid, sep = \"\")\n\n#write.csv(bechdel, file = \"_data/bechdel.csv\")\nbechdel <- read_csv(\"_data/bechdel.csv\")\n\n\nNew names:\n• `` -> `...1`\n\n\nRows: 9802 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): imdbid, title, titleId\ndbl (4): ...1, year, rating, id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n(3) Open Movie Database (OMDb):\nThis dataset was generated by using Open Movie Database (OMDb) API https://www.omdbapi.com/. OMDb provides detailed information such as director, writer, nominated award, the number/score of reviews on IMDb, etc… if you provide the name or IMDb id of movies. I decided to use this database because it gives me data related to movies’ popularity and financial success of the films, which “The Movie Data set” doesn’t include.\nOMDb doesn’t give you a list of all movies registered on it. Instead, you need to provide the exact movie title or the IMDb id to get the list of information. Thus, I will use this API once I have the data that join movies_metadata.csv and the Bechdel test. (Please refer to “Read in data / Describe data (OMDb)” for this process.)"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#describe-data",
    "href": "posts/Final_Project_ErikaNagai.html#describe-data",
    "title": "Final Project Erika Nagai",
    "section": "Describe data",
    "text": "Describe data\n\n(1) “The Movie Data set”\nThis data was originally created from The Movie Database (https://www.themoviedb.org/) and MovieLens (https://movielens.org/).This movie dataset was generated by Movielens, a (non-profit) movie review website (https://movielens.org/), and was obtained from the following Kaggle link. (https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?resource=download&select=movies_metadata.csv)\nThe movie dataset contains 45,466 movies with release date between December 9th, 1874 and December 16th, 2020.\nThe data includes the general information of movies, such as genres, revenue, run time, languages, status (released/in production etc…).\n\n\nCode\nprint(summarytools::dfSummary(movie),\n      varnumbers = FALSE,\n      plain.ascii  = FALSE,\n      style        = \"grid\",\n      graph.magnif = 0.80,\n      valid.col    = FALSE,\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nmovie\nDimensions: 45466 x 24\n  Duplicates: 17\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      adult\n[logical]\n      1. FALSE2. TRUE\n      45454(100.0%)9(0.0%)\n      \n      3\n(0.0%)\n    \n    \n      belongs_to_collection\n[character]\n      1. {'id': 415931, 'name': 'T2. {'id': 421566, 'name': 'T3. {'id': 645, 'name': 'Jame4. {'id': 96887, 'name': 'Za5. {'id': 37261, 'name': 'Th6. {'id': 34055, 'name': 'Po7. {'id': 413661, 'name': 'C8. {'id': 374509, 'name': 'G9. {'id': 148324, 'name': 'U10. {'id': 38451, 'name': 'Ch[ 1688 others ]\n      29(0.6%)27(0.6%)26(0.6%)26(0.6%)25(0.6%)22(0.5%)21(0.5%)16(0.4%)15(0.3%)15(0.3%)4272(95.1%)\n      \n      40972\n(90.1%)\n    \n    \n      budget\n[numeric]\n      Mean (sd) : 4224579 (17424133)min ≤ med ≤ max:0 ≤ 0 ≤ 3.8e+08IQR (CV) : 0 (4.1)\n      1223 distinct values\n      \n      3\n(0.0%)\n    \n    \n      genres\n[character]\n      1. [{'id': 18, 'name': 'Dram2. [{'id': 35, 'name': 'Come3. [{'id': 99, 'name': 'Docu4. []5. [{'id': 18, 'name': 'Dram6. [{'id': 35, 'name': 'Come7. [{'id': 27, 'name': 'Horr8. [{'id': 35, 'name': 'Come9. [{'id': 35, 'name': 'Come10. [{'id': 18, 'name': 'Dram[ 4059 others ]\n      5000(11.0%)3621(8.0%)2723(6.0%)2442(5.4%)1301(2.9%)1135(2.5%)974(2.1%)930(2.0%)593(1.3%)532(1.2%)26215(57.7%)\n      \n      0\n(0.0%)\n    \n    \n      homepage\n[character]\n      1. http://www.georgecarlin.c2. http://www.wernerherzog.c3. http://breakblade.jp/4. http://phantasm.com5. http://www.crownintlpictu6. http://www.crownintlpictu7. http://www.crownintlpictu8. http://www.kungfupanda.co9. http://www.missionimpossi10. http://www.thehungergames[ 7663 others ]\n      12(0.2%)7(0.1%)6(0.1%)4(0.1%)4(0.1%)4(0.1%)4(0.1%)4(0.1%)4(0.1%)4(0.1%)7729(99.3%)\n      \n      37684\n(82.9%)\n    \n    \n      id\n[numeric]\n      Mean (sd) : 108359.9 (112460.7)min ≤ med ≤ max:2 ≤ 60003 ≤ 469172IQR (CV) : 130878.5 (1)\n      45433 distinct values\n      \n      3\n(0.0%)\n    \n    \n      imdb_id\n[character]\n      1. 02. tt11803333. tt00225374. tt00228795. tt00464686. tt00622297. tt00673068. tt00800009. tt008299210. tt0084387[ 45407 others ]\n      3(0.0%)3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)45427(100.0%)\n      \n      17\n(0.0%)\n    \n    \n      original_language\n[character]\n      1. en2. fr3. it4. ja5. de6. es7. ru8. hi9. ko10. zh[ 82 others ]\n      32269(71.0%)2438(5.4%)1529(3.4%)1350(3.0%)1080(2.4%)994(2.2%)826(1.8%)508(1.1%)444(1.0%)409(0.9%)3608(7.9%)\n      \n      11\n(0.0%)\n    \n    \n      original_title\n[character]\n      1. Alice in Wonderland2. Hamlet3. A Christmas Carol4. Cinderella5. Les Misérables6. Macbeth7. The Three Musketeers8. Blackout9. Frankenstein10. Heidi[ 43363 others ]\n      8(0.0%)8(0.0%)7(0.0%)7(0.0%)7(0.0%)7(0.0%)7(0.0%)6(0.0%)6(0.0%)6(0.0%)45397(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      overview\n[character]\n      1. No overview found.2. No Overview3. A few funny little novels4. Adaptation of the Jane Au5. King Lear, old and tired,6. No movie overview availab7. Recovering from a nail gu8. Released9. A film by Jem Cohen10. A former aristocrat Ippol[ 44296 others ]\n      133(0.3%)7(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)2(0.0%)2(0.0%)44345(99.6%)\n      \n      959\n(2.1%)\n    \n    \n      popularity\n[numeric]\n      Mean (sd) : 2.9 (6)min ≤ med ≤ max:0 ≤ 1.1 ≤ 547.5IQR (CV) : 3.3 (2.1)\n      43757 distinct values\n      \n      6\n(0.0%)\n    \n    \n      poster_path\n[character]\n      1. /5D7UBSEgdyONE6Lql6xS7s6O2. /2kslZXOaW0HmnGuVPCnQlCdX3. /qW1oQlOHizRHXZQrpkimYr0o4. /8VSZ9coCzxOCW2wE2Qene1H15. /cdwVC18URfEdQjjxqJyRMoGD6. /2ngnUQX9Abkesfq72uvBF3uj7. /2trk2fape4S0CYnRg38kNVe88. /4J6Ai4C5YRgfRUTlirrJ7Qsm9. /5GasjPRAy5rlEyDOH7MeOyxy10. /5ILjS6XB5deiHop8SXPsYxXW[ 45014 others ]\n      5(0.0%)4(0.0%)4(0.0%)3(0.0%)3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)45051(99.9%)\n      \n      386\n(0.8%)\n    \n    \n      production_companies\n[character]\n      1. []2. [{'name': 'Metro-Goldwyn-3. [{'name': 'Warner Bros.',4. [{'name': 'Paramount Pict5. [{'name': 'Twentieth Cent6. [{'name': 'Universal Pict7. [{'name': 'RKO Radio Pict8. [{'name': 'Columbia Pictu9. [{'name': 'Columbia Pictu10. [{'name': 'Mosfilm', 'id'[ 22698 others ]\n      11875(26.1%)742(1.6%)540(1.2%)505(1.1%)439(1.0%)320(0.7%)247(0.5%)207(0.5%)146(0.3%)145(0.3%)30297(66.6%)\n      \n      3\n(0.0%)\n    \n    \n      production_countries\n[character]\n      1. [{'iso_3166_1': 'US', 'na2. []3. [{'iso_3166_1': 'GB', 'na4. [{'iso_3166_1': 'FR', 'na5. [{'iso_3166_1': 'JP', 'na6. [{'iso_3166_1': 'IT', 'na7. [{'iso_3166_1': 'CA', 'na8. [{'iso_3166_1': 'DE', 'na9. [{'iso_3166_1': 'IN', 'na10. [{'iso_3166_1': 'RU', 'na[ 2383 others ]\n      17851(39.3%)6282(13.8%)2238(4.9%)1654(3.6%)1356(3.0%)1030(2.3%)840(1.8%)749(1.6%)735(1.6%)735(1.6%)11993(26.4%)\n      \n      3\n(0.0%)\n    \n    \n      release_date\n[Date]\n      min : 1874-12-09med : 2001-08-30max : 2020-12-16range : 146y 0m 7d\n      17333 distinct values\n      \n      90\n(0.2%)\n    \n    \n      revenue\n[numeric]\n      Mean (sd) : 11209349 (64332247)min ≤ med ≤ max:0 ≤ 0 ≤ 2787965087IQR (CV) : 0 (5.7)\n      6863 distinct values\n      \n      6\n(0.0%)\n    \n    \n      runtime\n[numeric]\n      Mean (sd) : 94.1 (38.4)min ≤ med ≤ max:0 ≤ 95 ≤ 1256IQR (CV) : 22 (0.4)\n      353 distinct values\n      \n      263\n(0.6%)\n    \n    \n      spoken_languages\n[character]\n      1. [{'iso_639_1': 'en', 'nam2. []3. [{'iso_639_1': 'fr', 'nam4. [{'iso_639_1': 'ja', 'nam5. [{'iso_639_1': 'it', 'nam6. [{'iso_639_1': 'es', 'nam7. [{'iso_639_1': 'ru', 'nam8. [{'iso_639_1': 'de', 'nam9. [{'iso_639_1': 'en', 'nam10. [{'iso_639_1': 'en', 'nam[ 1921 others ]\n      22395(49.3%)3829(8.4%)1853(4.1%)1289(2.8%)1218(2.7%)902(2.0%)807(1.8%)762(1.7%)681(1.5%)572(1.3%)11152(24.5%)\n      \n      6\n(0.0%)\n    \n    \n      status\n[character]\n      1. Canceled2. In Production3. Planned4. Post Production5. Released6. Rumored\n      2(0.0%)20(0.0%)15(0.0%)98(0.2%)45014(99.2%)230(0.5%)\n      \n      87\n(0.2%)\n    \n    \n      tagline\n[character]\n      1. Based on a true story.2. -3. Be careful what you wish 4. Trust no one.5. A Love Story6. Classic Albums7. Documentary8. Drama9. How far would you go?10. Know Your Enemy[ 20272 others ]\n      7(0.0%)4(0.0%)4(0.0%)4(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)20374(99.8%)\n      \n      25055\n(55.1%)\n    \n    \n      title\n[character]\n      1. Cinderella2. Alice in Wonderland3. Hamlet4. Beauty and the Beast5. Les Misérables6. A Christmas Carol7. Blackout8. The Three Musketeers9. Treasure Island10. Aftermath[ 42267 others ]\n      11(0.0%)9(0.0%)9(0.0%)8(0.0%)8(0.0%)7(0.0%)7(0.0%)7(0.0%)7(0.0%)6(0.0%)45381(99.8%)\n      \n      6\n(0.0%)\n    \n    \n      video\n[logical]\n      1. FALSE2. TRUE\n      45367(99.8%)93(0.2%)\n      \n      6\n(0.0%)\n    \n    \n      vote_average\n[numeric]\n      Mean (sd) : 5.6 (1.9)min ≤ med ≤ max:0 ≤ 6 ≤ 10IQR (CV) : 1.8 (0.3)\n      92 distinct values\n      \n      6\n(0.0%)\n    \n    \n      vote_count\n[numeric]\n      Mean (sd) : 109.9 (491.3)min ≤ med ≤ max:0 ≤ 10 ≤ 14075IQR (CV) : 31 (4.5)\n      1820 distinct values\n      \n      6\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\n\n\n(2) Bechdel test\nThis data (bechdel) dataset documents the Bechdel test rating of 9630 movies released between 1874 and 2022. Each row represents a movie.\nAccording to “Merriam-Webster”, the Bechdel test is “a set of criteria used as a test to evaluate a work of fiction (such as a film) based on its inclusion and representation of female characters” (https://www.merriam-webster.com/dictionary/Bechdel%20Test)\nA movie passes the Bechdel test when it has at least two female featured characters that talk to each other about other than a man (men).\nIn bechdel, the Bechdel test rating is registered in the following manner.\n0 ~ No two female characters\n1 ~ Two female characters who don’t talk to each other\n2 ~ Two female characters talk to each other about a man (men)\n3 ~ Passes Bechdel test: Two female characters talk to each other about other than a man (men)\nApart from Bechdel ratings, this dataset (bechdel) contains the released year, title of movies, and id. id that starts with “tt” followed by 7 digits is the IMDb id and can work as a foreign key when joining this data with the movie dataset.\n\n\nCode\nprint(summarytools::dfSummary(bechdel),\n      varnumbers = FALSE,\n      plain.ascii  = FALSE,\n      style        = \"grid\",\n      graph.magnif = 0.80,\n      valid.col    = FALSE,\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbechdel\nDimensions: 9802 x 7\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      ...1\n[numeric]\n      Mean (sd) : 4901.5 (2829.7)min ≤ med ≤ max:1 ≤ 4901.5 ≤ 9802IQR (CV) : 4900.5 (0.6)\n      9802 distinct values\n      \n      0\n(0.0%)\n    \n    \n      year\n[numeric]\n      Mean (sd) : 1996.2 (27)min ≤ med ≤ max:1010 ≤ 2006 ≤ 2022IQR (CV) : 25 (0)\n      142 distinct values\n      \n      0\n(0.0%)\n    \n    \n      rating\n[numeric]\n      Mean (sd) : 2.1 (1.1)min ≤ med ≤ max:0 ≤ 3 ≤ 3IQR (CV) : 2 (0.5)\n      0:1084(11.1%)1:2124(21.7%)2:1000(10.2%)3:5594(57.1%)\n      \n      0\n(0.0%)\n    \n    \n      id\n[numeric]\n      Mean (sd) : 5224.9 (3052.9)min ≤ med ≤ max:1 ≤ 5211.5 ≤ 10641IQR (CV) : 5233.5 (0.6)\n      9802 distinct values\n      \n      0\n(0.0%)\n    \n    \n      imdbid\n[character]\n      1. 00352792. 00864253. 01170564. 20439005. 24572826. 00000017. 00000028. 00000039. 000000410. 0000005[ 9784 others ]\n      2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)9784(99.8%)\n      \n      3\n(0.0%)\n    \n    \n      title\n[character]\n      1. Cinderella2. Dracula3. Little Women4. Pride and Prejudice5. Robin Hood6. Shelter7. A Star Is Born8. Alice in Wonderland9. Anna Karenina10. Annie[ 9547 others ]\n      5(0.1%)4(0.0%)4(0.0%)4(0.0%)4(0.0%)4(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)9765(99.6%)\n      \n      0\n(0.0%)\n    \n    \n      titleId\n[character]\n      1. tt2. tt00352793. tt00864254. tt01170565. tt20439006. tt24572827. tt00000018. tt00000029. tt000000310. tt0000004[ 9785 others ]\n      3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)9785(99.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#tidy-data",
    "href": "posts/Final_Project_ErikaNagai.html#tidy-data",
    "title": "Final Project Erika Nagai",
    "section": "Tidy data",
    "text": "Tidy data\n\n(1) TMDb data\n\n\nChecking missing values\nFirst of all, I took a look at missing values (NA values) in the data.\n\n\nCode\nmovie %>% select(everything()) %>%\n  summarise_all(funs(sum(is.na(.)))) %>%\n  t()\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n                       [,1]\nadult                     3\nbelongs_to_collection 40972\nbudget                    3\ngenres                    0\nhomepage              37684\nid                        3\nimdb_id                  17\noriginal_language        11\noriginal_title            0\noverview                959\npopularity                6\nposter_path             386\nproduction_companies      3\nproduction_countries      3\nrelease_date             90\nrevenue                   6\nruntime                 263\nspoken_languages          6\nstatus                   87\ntagline               25055\ntitle                     6\nvideo                     6\nvote_average              6\nvote_count                6\n\n\n\n\nDeleting unnecessary columns\nI deleted the following columns, which seem irrelevant for this analysis or have too many missing values.\n- adult: This information doesn’t add any significant meaning as almost all of the observations have FALSE value for this column\n- belong_to_collection: This column has 40000+ missing values\n- homepage: Unnecessary for this analysis\n- overview: Unnecessary for this analysis\n- poster_path: Unnecessary for this analysis\n- tagline: Unnecessary for this analysis\n- video: This information doesn’t add any significant meaning as almost all of observations have FALSE value for this column\n- popularity: This information may be interesting, however, it is not clear how this popularity is measured or where it was generated. Therefore, I decided to use popularity data from OMDb.\n- vote_average: Same as popularity\n-vote_count: Same as popularity\nThen, I changed the order of the columns.\n\n\nCode\nmovie <- movie %>% select(-c(\"adult\", \"homepage\", \"overview\", \"poster_path\", \"tagline\", \"belongs_to_collection\", \"poster_path\", \"video\", \"popularity\", \"vote_average\", \"vote_count\"))\n\ncol_order <- c(\"title\", \"original_title\", \"imdb_id\", \"id\", \"production_companies\", \"production_countries\", \"status\", \"release_date\", \"runtime\", \"revenue\", \"budget\", \"original_language\", \"spoken_languages\", \"genres\")\nmovie <- movie[, col_order]\n\ncolnames(movie)[4] <- \"movielens_id\"\n\n\n\n\nCleaning data in JSON nested list format\nThe values in some certain columns such as “genres”, “production_companies”, “production_countries”,“spoken_languages” are in a JSON list format for example:\n\n” [{‘id’: XXXX, ‘content(name/genre/title)’: XXX}, {‘id’: XXXX, ’content(name/genre/title): XXX}] ”\n\n\n\nCode\nmovie %>% select(c(\"genres\", \"production_companies\", \"production_countries\", \"spoken_languages\"))\n\n\n# A tibble: 45,466 × 4\n   genres                                                produ…¹ produ…² spoke…³\n   <chr>                                                 <chr>   <chr>   <chr>  \n 1 [{'id': 16, 'name': 'Animation'}, {'id': 35, 'name':… [{'nam… [{'iso… [{'iso…\n 2 [{'id': 12, 'name': 'Adventure'}, {'id': 14, 'name':… [{'nam… [{'iso… [{'iso…\n 3 [{'id': 10749, 'name': 'Romance'}, {'id': 35, 'name'… [{'nam… [{'iso… [{'iso…\n 4 [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'D… [{'nam… [{'iso… [{'iso…\n 5 [{'id': 35, 'name': 'Comedy'}]                        [{'nam… [{'iso… [{'iso…\n 6 [{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'C… [{'nam… [{'iso… [{'iso…\n 7 [{'id': 35, 'name': 'Comedy'}, {'id': 10749, 'name':… [{'nam… [{'iso… [{'iso…\n 8 [{'id': 28, 'name': 'Action'}, {'id': 12, 'name': 'A… [{'nam… [{'iso… [{'iso…\n 9 [{'id': 28, 'name': 'Action'}, {'id': 12, 'name': 'A… [{'nam… [{'iso… [{'iso…\n10 [{'id': 12, 'name': 'Adventure'}, {'id': 28, 'name':… [{'nam… [{'iso… [{'iso…\n# … with 45,456 more rows, and abbreviated variable names\n#   ¹​production_companies, ²​production_countries, ³​spoken_languages\n\n\nCode\n# These columns include [] {} and ' in their values so I made a function remove_simbols that removes these unnecessary symbols.\nremove_symbols <- function(x) {\n  removed_x <- str_remove_all(x, \"\\\\{|\\\\}|\\\\[|\\\\}|\\\\]|'|,|id|name|:| \")\n  return(removed_x)\n}\n\n\n\n(1). Tidying Genres\nEach movie contains information on one or more (up to 8) genres in a single column. To organize the information, the multiple genres contained in genre are split by “],” so that each genre is contained in eight new columns, genre1-genre8. Then, I removed unnecessary symbols such as ,.\n\n\nCode\n# Separating the `genre` by \"},\"\nmovie <- movie %>% \n  separate(genres, c(\"genre1\", \"genre2\", \"genre3\", \"genre4\", \"genre5\", \"genre6\", \"genre7\", \"genre8\"), \"\\\\},\", remove = FALSE)\n\n\nWarning: Expected 8 pieces. Missing pieces filled with `NA` in 45463 rows [1, 2,\n3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\n\nCode\n# Remove unnecessary symbols from each columns\n\nmovie$genre1 <- remove_symbols(movie$genre1)\nmovie$genre2 <- remove_symbols(movie$genre2)\nmovie$genre3 <- remove_symbols(movie$genre3)\nmovie$genre4 <- remove_symbols(movie$genre4)\nmovie$genre5 <- remove_symbols(movie$genre5)\nmovie$genre6 <- remove_symbols(movie$genre6)\nmovie$genre7 <- remove_symbols(movie$genre7)\nmovie$genre8 <- remove_symbols(movie$genre8)\n\nmovie %>% \n  select(matches(\"[1-9]\"))\n\n\n# A tibble: 45,466 × 8\n   genre1       genre2       genre3       genre4     genre5 genre6 genre7 genre8\n   <chr>        <chr>        <chr>        <chr>      <chr>  <chr>  <chr>  <chr> \n 1 16Animation  35Comedy     10751Family  <NA>       <NA>   <NA>   <NA>   <NA>  \n 2 12Adventure  14Fantasy    10751Family  <NA>       <NA>   <NA>   <NA>   <NA>  \n 3 10749Romance 35Comedy     <NA>         <NA>       <NA>   <NA>   <NA>   <NA>  \n 4 35Comedy     18Drama      10749Romance <NA>       <NA>   <NA>   <NA>   <NA>  \n 5 35Comedy     <NA>         <NA>         <NA>       <NA>   <NA>   <NA>   <NA>  \n 6 28Action     80Crime      18Drama      53Thriller <NA>   <NA>   <NA>   <NA>  \n 7 35Comedy     10749Romance <NA>         <NA>       <NA>   <NA>   <NA>   <NA>  \n 8 28Action     12Adventure  18Drama      10751Fami… <NA>   <NA>   <NA>   <NA>  \n 9 28Action     12Adventure  53Thriller   <NA>       <NA>   <NA>   <NA>   <NA>  \n10 12Adventure  28Action     53Thriller   <NA>       <NA>   <NA>   <NA>   <NA>  \n# … with 45,456 more rows\n\n\nI managed to separate the genre information above. Now all values in genre 1 - 8 has a value in the format of “number + name of genre” (example: 35Comedy)\nHowever, there are 2,445 observations whose genre value doesn’t fit the above format. Most of them don’t have any genre assigned so their genre1 values are empty or blank, which means they don’t have any genre assigned to them.\n\n\nCode\nmovie %>% filter(!str_detect(genre1, \"^[0-9]\")) %>% select(starts_with(\"genre\"))\n\n\n# A tibble: 2,445 × 9\n   genres genre1 genre2 genre3 genre4 genre5 genre6 genre7 genre8\n   <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n 1 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 2 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 3 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 4 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 5 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 6 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 7 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 8 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n 9 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n10 []     \"\"     <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n# … with 2,435 more rows\n\n\nHowever, looking at them closely, there are three observations whose genre value that is NOT blank but have strange strings.\nThe values in these observations such as “Carousel Production” and “Aniplex” don’t look like a name of genres but that of production studios. Also their original_title values don’t look like a title of movies but information of languages. It seems like these observations were not correctly read in because their values in other columns look weird.\nSince these observations are not reliable, I decided to delete them from this dataset.\n\n\nCode\nmovie %>% filter(!str_detect(genre1, \"^[0-9]\") &!str_detect(genre1, \"^[ \\t\\n]*$\")) %>% #^[ \\t\\n]*$ is a regular expression for blank.\n  select(c(original_title, production_countries, genre1:genre3))\n\n\n# A tibble: 3 × 5\n  original_title                           production_cou…¹ genre1 genre2 genre3\n  <chr>                                    <chr>            <chr>  <chr>  <chr> \n1 [{'iso_639_1': 'en', 'name': 'English'}] 6.0              Carou… Visio… Teles…\n2 [{'iso_639_1': 'ja', 'name': '日本語'}]  7.0              Anipl… GoHan… BROST…\n3 [{'iso_639_1': 'en', 'name': 'English'}] 4.3              Odyss… Pulse… Rogue…\n# … with abbreviated variable name ¹​production_countries\n\n\nCode\nmovie_clean1 <- movie %>% filter(!str_detect(genre1, \"^[A-Z]\"))\n\n\nGenre information is more organized but is still not easy to be analyzed. Thus, I decided to make dummy variables of each genre.\nFor example, if movie A is categorized as “comedy” and “adventure”, the line of movie A should have 1 in the “comedy” column and the “adventure” respectively and 0 in columns of other genres.\n\n\nCode\n#https://community.rstudio.com/t/creating-dummy-columns-based-on-multiple-columns/58145/3\nmovie_clean1 <- movie_clean1 %>% \n  pivot_longer(cols = matches(\"genre[1-9]\")) %>%\n  add_column(count = 1) %>%\n  arrange(value) %>%\n  filter(str_detect(value, \"^[0-9]\")) %>%\n  mutate(value1 = str_replace_all(value, \"[0-9]+\", \"\")) %>%\n  select(-c(value, name)) %>%\n  arrange(title) %>%\n  pivot_wider(\n    names_from = value1, \n    values_from = count, \n    values_fill = list(count=0),\n    values_fn = list(count = mean)) %>%\n  arrange(title)\n\n# I also deleted genre \"TVMovie\" and \"Foreign\" because they're more format or origin country information rather than genre.\nmovie_clean1 <- movie_clean1 %>% select(-c(\"TVMovie\",\"Foreign\"))\n\n\n\n\n(2). Tyding Production_countries\nThe values in production_countries are written in the following way.\n\n[{‘iso_3166_1’: ‘abbreviation of country’,‘name’: ‘full country name’}]\n\nWhen there is more than one country in the value, I took the first country in account for this analysis.\n\n\nCode\nmovie_clean1$production_countries <- remove_symbols(movie_clean1$production_countries)\nmovie_clean1$production_countries <- str_extract(movie_clean1$production_countries,\"(?<=_1)\\\\w{2}\")\n\nmovie_clean1 %>% select(production_countries)\n\n\n# A tibble: 42,991 × 1\n   production_countries\n   <chr>               \n 1 GB                  \n 2 IN                  \n 3 US                  \n 4 <NA>                \n 5 US                  \n 6 US                  \n 7 FR                  \n 8 JP                  \n 9 <NA>                \n10 US                  \n# … with 42,981 more rows\n\n\n\n\n(3). Tyding production_companies & spoken_languages\nThe values in both of production_companies and spoken_languages are in the following format.\n\n[{‘name’: ‘XXXX’, ‘id’: —}] > [{‘iso_639_1’:‘XXXX’, ‘name’:“—-}]\n\nI only need the information that is written as XXXX in this format.\n\n\nCode\nmovie_clean1$production_companies <-str_remove_all(movie_clean1$production_companies, \"'id': [0-9]*|'name':|\\\\[|\\\\]|'|\\\\{|,\") %>%\n  str_replace_all(\"\\\\},\", \",\") %>%\n  str_replace_all(\" \\\\} \", \",\")%>%\n  str_remove(\" \\\\}\")\n\nmovie_clean1$spoken_languages <-str_remove_all(movie_clean1$spoken_languages, \"'iso_639_1':|'name': '\\\\w+'|\\\\[|\\\\]|'|\\\\{|,\") %>%\n  str_replace_all(\" \\\\} \", \",\") %>%\n  str_remove_all(\" \\\\}\")\n\nmovie_clean1 %>% select(production_companies, spoken_languages)\n\n\n# A tibble: 42,991 × 2\n   production_companies                                                  spoke…¹\n   <chr>                                                                 <chr>  \n 1 \" Screen Yorkshire, British Film Institute (BFI), Creative Scotland,… \" en\"  \n 2 \"\"                                                                    \" hi\"  \n 3 \" Sebastian Films Limited\"                                            \" en\"  \n 4 \" Lorimar Pictures\"                                                   \" en\"  \n 5 \" Lone Star Production, Monogram Pictures\"                            \" en, …\n 6 \"\"                                                                    \" en\"  \n 7 \" Barnholtz Entertainment\"                                            \" en, …\n 8 \"\"                                                                    \" zh\"  \n 9 \"\"                                                                    \" fr\"  \n10 \" Disney Channel\"                                                     \" en\"  \n# … with 42,981 more rows, and abbreviated variable name ¹​spoken_languages\n\n\n\n\n\nAdding a new column\nA new column years was created to group years by decade.\n\n\nCode\nmovie_clean1 <- movie_clean1 %>% \n  mutate(\n    years = case_when(\n    lubridate::year(release_date) < 1920 ~ \"Before 1920\",\n    lubridate::year(release_date) >= 1920 & lubridate::year(release_date) < 1930 ~ \"1920s\",\n    lubridate::year(release_date) >= 1930 & lubridate::year(release_date) < 1940 ~ \"1930s\",\n    lubridate::year(release_date) >= 1940 & lubridate::year(release_date) < 1950 ~ \"1940s\",\n    lubridate::year(release_date) >= 1950 & lubridate::year(release_date) < 1960 ~ \"1950s\",\n    lubridate::year(release_date) >= 1960 & lubridate::year(release_date) < 1970 ~ \"1960s\",\n    lubridate::year(release_date) >= 1970 & lubridate::year(release_date) < 1980 ~ \"1970s\",\n    lubridate::year(release_date) >= 1980 & lubridate::year(release_date) < 1990 ~ \"1990s\",\n    lubridate::year(release_date) >= 1990 & lubridate::year(release_date) < 2000 ~ \"1990s\",\n    lubridate::year(release_date) >= 2000 & lubridate::year(release_date) < 2010 ~ \"2000s\",\n    TRUE ~ \"2010s\"\n  )\n  )\n\n\nNow, the dataset is cleaner. Look at the summary again.\n\n\nCode\nprint(summarytools::dfSummary(movie_clean1),\n      varnumbers = FALSE,\n      plain.ascii  = FALSE,\n      style        = \"grid\",\n      graph.magnif = 0.80,\n      valid.col    = FALSE,\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nmovie_clean1\nDimensions: 42991 x 33\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      title\n[character]\n      1. Cinderella2. Alice in Wonderland3. Hamlet4. Les Misérables5. A Christmas Carol6. Beauty and the Beast7. The Three Musketeers8. Treasure Island9. Aftermath10. Countdown[ 40025 others ]\n      10(0.0%)9(0.0%)9(0.0%)8(0.0%)7(0.0%)7(0.0%)7(0.0%)7(0.0%)6(0.0%)6(0.0%)42912(99.8%)\n      \n      3\n(0.0%)\n    \n    \n      original_title\n[character]\n      1. Alice in Wonderland2. Hamlet3. A Christmas Carol4. Les Misérables5. Macbeth6. The Three Musketeers7. Cinderella8. Frankenstein9. Heidi10. The Hound of the Baskervi[ 41043 others ]\n      8(0.0%)8(0.0%)7(0.0%)7(0.0%)7(0.0%)7(0.0%)6(0.0%)6(0.0%)6(0.0%)6(0.0%)42923(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      imdb_id\n[character]\n      1. tt00000012. tt00000033. tt00000054. tt00000085. tt00000106. tt00000127. tt00000148. tt00000239. tt000002910. tt0000070[ 42969 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)42969(100.0%)\n      \n      12\n(0.0%)\n    \n    \n      movielens_id\n[numeric]\n      Mean (sd) : 104553.2 (111978.6)min ≤ med ≤ max:2 ≤ 55734 ≤ 469172IQR (CV) : 118823.5 (1.1)\n      42991 distinct values\n      \n      0\n(0.0%)\n    \n    \n      production_companies\n[character]\n      1. (Empty string)2.  ·Metro-Goldwyn-Mayer (MGM3.  ·Warner Bros.4.  ·Paramount Pictures5.  ·Twentieth Century Fox Fi6.  ·Universal Pictures7.  ·RKO Radio Pictures8.  ·Columbia Pictures Corpor9.  ·Columbia Pictures10.  ·Mosfilm[ 22474 others ]\n      9770(22.7%)737(1.7%)538(1.3%)500(1.2%)434(1.0%)320(0.7%)247(0.6%)207(0.5%)146(0.3%)144(0.3%)29945(69.7%)\n      \n      3\n(0.0%)\n    \n    \n      production_countries\n[character]\n      1. US2. GB3. FR4. CA5. JP6. DE7. IT8. RU9. IN10. ES[ 132 others ]\n      18202(47.5%)3031(7.9%)2639(6.9%)1469(3.8%)1430(3.7%)1389(3.6%)1382(3.6%)777(2.0%)754(2.0%)576(1.5%)6653(17.4%)\n      \n      4689\n(10.9%)\n    \n    \n      status\n[character]\n      1. In Production2. Planned3. Post Production4. Released5. Rumored\n      20(0.0%)15(0.0%)96(0.2%)42601(99.2%)205(0.5%)\n      \n      54\n(0.1%)\n    \n    \n      release_date\n[Date]\n      min : 1874-12-09med : 2001-10-29max : 2020-12-16range : 146y 0m 7d\n      16860 distinct values\n      \n      29\n(0.1%)\n    \n    \n      runtime\n[numeric]\n      Mean (sd) : 95.5 (36.6)min ≤ med ≤ max:0 ≤ 95 ≤ 1256IQR (CV) : 21 (0.4)\n      343 distinct values\n      \n      182\n(0.4%)\n    \n    \n      revenue\n[numeric]\n      Mean (sd) : 11845510 (66095743)min ≤ med ≤ max:0 ≤ 0 ≤ 2787965087IQR (CV) : 0 (5.6)\n      6844 distinct values\n      \n      3\n(0.0%)\n    \n    \n      budget\n[numeric]\n      Mean (sd) : 4461834 (17886326)min ≤ med ≤ max:0 ≤ 0 ≤ 3.8e+08IQR (CV) : 0 (4)\n      1215 distinct values\n      \n      0\n(0.0%)\n    \n    \n      original_language\n[character]\n      1. en2. fr3. it4. ja5. de6. es7. ru8. hi9. ko10. zh[ 79 others ]\n      30566(71.1%)2348(5.5%)1359(3.2%)1287(3.0%)1029(2.4%)927(2.2%)786(1.8%)478(1.1%)431(1.0%)391(0.9%)3379(7.9%)\n      \n      10\n(0.0%)\n    \n    \n      spoken_languages\n[character]\n      1.  ·en2. (Empty string)3.  ·fr4.  ·ja5.  ·it6.  ·es7.  ·ru8.  ·de9.  ·en, fr10.  ·en, es[ 1896 others ]\n      21943(51.0%)2660(6.2%)1785(4.2%)1220(2.8%)1097(2.6%)826(1.9%)769(1.8%)722(1.7%)679(1.6%)568(1.3%)10719(24.9%)\n      \n      3\n(0.0%)\n    \n    \n      genres\n[character]\n      1. [{'id': 18, 'name': 'Dram2. [{'id': 35, 'name': 'Come3. [{'id': 99, 'name': 'Docu4. [{'id': 18, 'name': 'Dram5. [{'id': 35, 'name': 'Come6. [{'id': 27, 'name': 'Horr7. [{'id': 35, 'name': 'Come8. [{'id': 35, 'name': 'Come9. [{'id': 18, 'name': 'Dram10. [{'id': 27, 'name': 'Horr[ 4055 others ]\n      4996(11.6%)3620(8.4%)2721(6.3%)1300(3.0%)1133(2.6%)974(2.3%)930(2.2%)593(1.4%)531(1.2%)528(1.2%)25665(59.7%)\n      \n      0\n(0.0%)\n    \n    \n      War\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:41669(96.9%)1:1322(3.1%)\n      \n      0\n(0.0%)\n    \n    \n      Drama\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:22747(52.9%)1:20244(47.1%)\n      \n      0\n(0.0%)\n    \n    \n      Action\n[numeric]\n      Min  : 0Mean : 0.2Max  : 1\n      0:36399(84.7%)1:6592(15.3%)\n      \n      0\n(0.0%)\n    \n    \n      Thriller\n[numeric]\n      Min  : 0Mean : 0.2Max  : 1\n      0:35372(82.3%)1:7619(17.7%)\n      \n      0\n(0.0%)\n    \n    \n      Crime\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:38687(90.0%)1:4304(10.0%)\n      \n      0\n(0.0%)\n    \n    \n      Western\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:41949(97.6%)1:1042(2.4%)\n      \n      0\n(0.0%)\n    \n    \n      Documentary\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:39061(90.9%)1:3930(9.1%)\n      \n      0\n(0.0%)\n    \n    \n      Romance\n[numeric]\n      Min  : 0Mean : 0.2Max  : 1\n      0:36261(84.3%)1:6730(15.7%)\n      \n      0\n(0.0%)\n    \n    \n      Family\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:40224(93.6%)1:2767(6.4%)\n      \n      0\n(0.0%)\n    \n    \n      Animation\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:41060(95.5%)1:1931(4.5%)\n      \n      0\n(0.0%)\n    \n    \n      Comedy\n[numeric]\n      Min  : 0Mean : 0.3Max  : 1\n      0:29815(69.4%)1:13176(30.6%)\n      \n      0\n(0.0%)\n    \n    \n      Horror\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:38320(89.1%)1:4671(10.9%)\n      \n      0\n(0.0%)\n    \n    \n      Mystery\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:40527(94.3%)1:2464(5.7%)\n      \n      0\n(0.0%)\n    \n    \n      Adventure\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:39501(91.9%)1:3490(8.1%)\n      \n      0\n(0.0%)\n    \n    \n      History\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:41593(96.7%)1:1398(3.3%)\n      \n      0\n(0.0%)\n    \n    \n      ScienceFiction\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:39947(92.9%)1:3044(7.1%)\n      \n      0\n(0.0%)\n    \n    \n      Fantasy\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:40682(94.6%)1:2309(5.4%)\n      \n      0\n(0.0%)\n    \n    \n      Music\n[numeric]\n      Min  : 0Mean : 0Max  : 1\n      0:41394(96.3%)1:1597(3.7%)\n      \n      0\n(0.0%)\n    \n    \n      years\n[character]\n      1. 1920s2. 1930s3. 1940s4. 1950s5. 1960s6. 1970s7. 1990s8. 2000s9. 2010s10. Before 1920\n      398(0.9%)1264(2.9%)1444(3.4%)1997(4.6%)2474(5.8%)3270(7.6%)8965(20.9%)10605(24.7%)12291(28.6%)283(0.7%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\n\n\n(2) Bechdel data\n\nRemoving and Mutating Columns\nimdbid and titleId are duplicated because titleId is “tt+imdbid”. movie_clean1 data frame has IDs in the same format (starting with tt) as titleId of bechdel data frame so I deleted imdbid column.\nAlso, year and title information is available in movie data frame, thus I deleted them.\nI made a new column bechdel_pass where 1 means the movie passes bechdel test and 0 means otherwise.\n\n\nCode\n#\n\nbechdel <- bechdel %>% select(-c(\"imdbid\",\"...1\",\"title\"))\ncolnames(bechdel) <- c(\"year\", \"bechdel_rating\", \"id\", \"titleid\")\n\nsummary(bechdel)\n\n\n      year      bechdel_rating        id          titleid         \n Min.   :1010   Min.   :0.000   Min.   :    1   Length:9802       \n 1st Qu.:1988   1st Qu.:1.000   1st Qu.: 2558   Class :character  \n Median :2006   Median :3.000   Median : 5212   Mode  :character  \n Mean   :1996   Mean   :2.133   Mean   : 5225                     \n 3rd Qu.:2013   3rd Qu.:3.000   3rd Qu.: 7792                     \n Max.   :2022   Max.   :3.000   Max.   :10641                     \n\n\nCode\n# Changing the column order\ncol_order <- c(\"id\", \"titleid\",\"year\", \"bechdel_rating\")\nbechdel <- bechdel[, col_order]\n\n# Mutate a new column bechdel_pass\nbechdel <- bechdel %>% mutate(\n  bechdel_pass = case_when(\n    bechdel_rating == 3 ~ 1,\n    TRUE ~ 0\n  )\n)\n\n# Change data types of columns\nbechdel$bechdel_rating <- factor(bechdel$bechdel_rating, levels = c(\"0\", \"1\", \"2\", \"3\"))\nbechdel$bechdel_pass <- factor(bechdel$bechdel_pass, levels = c(\"0\", \"1\"))\n\n\n\n\n\nSense Check\nThis data looks quite clean, however, I realized that the minimum number of year is 1010, which is weird. “Inazuma Eleven The Movie” was released in 2010, but it seems to have been mistakenly recorded as 1010. So I manually corrected the data.\n\n\nCode\nsummary(bechdel)\n\n\n       id          titleid               year      bechdel_rating bechdel_pass\n Min.   :    1   Length:9802        Min.   :1010   0:1084         0:4208      \n 1st Qu.: 2558   Class :character   1st Qu.:1988   1:2124         1:5594      \n Median : 5212   Mode  :character   Median :2006   2:1000                     \n Mean   : 5225                      Mean   :1996   3:5594                     \n 3rd Qu.: 7792                      3rd Qu.:2013                              \n Max.   :10641                      Max.   :2022                              \n\n\nCode\nbechdel %>% \n  filter(year < 1800)\n\n\n# A tibble: 1 × 5\n     id titleid    year bechdel_rating bechdel_pass\n  <dbl> <chr>     <dbl> <fct>          <fct>       \n1 10556 tt1794796  1010 3              1           \n\n\nCode\nbechdel$year[bechdel$year == 1010] <- 2010"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#join-data-sets",
    "href": "posts/Final_Project_ErikaNagai.html#join-data-sets",
    "title": "Final Project Erika Nagai",
    "section": "Join data sets",
    "text": "Join data sets\nWe have two data frames movie and bechdel , which have imdb id as a foreign key.\n\n\nCode\nmovie_bechdel_join <- inner_join(movie_clean1, bechdel, \n                                 by=c(\"imdb_id\" = \"titleid\"),\n                                 copy = TRUE)"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#read-in-data-describe-data-omdb",
    "href": "posts/Final_Project_ErikaNagai.html#read-in-data-describe-data-omdb",
    "title": "Final Project Erika Nagai",
    "section": "Read in data / Describe data (OMDb)",
    "text": "Read in data / Describe data (OMDb)\nAs we have a joined data, I read in the movie popularity data from OMDb using OMDb API.\nomdb_df has 5 columns\n\nimdbID: can be used as a foreign key when joining with other data frames\nDirector: Director’s name\nMetascore: Review scores from 0 to 100 on the metacritic website (https://www.metacritic.com/movie). They are weighted averages. and reviews by certain reviewers are given more weight, however the detail is not revealed.\nimdbRating: Review scores from 0 to 10 on IMDb website (https://www.imdb.com/). They are weighted averages.\nimdbVotes: Number of votes on IMDb website.\n\n\n\nCode\n# Generating OMDb dataframe\n# imdb_id_list <- c()\n# \n# for (i in c(1:length(movie_bechdel_join$imdb_id))) {\n#   imdb_id_list[[i]] <- movie_bechdel_join$imdb_id[i]\n# }\n# \n# omdb_list <- lapply(imdb_id_list, function(movie_id) {\n#   actor_vector <- find_by_id(movie_id)\n#   actor_vector\n# })\n# \n# omdb_df <- tibble(place = omdb_list) %>%\n#   unnest_wider(place) %>%\n#   select(c(\"imdbID\", \"Director\", \"Metascore\", \"imdbRating\", \"imdbVotes\", \"BoxOffice\")) %>% unnest() %>% distinct()\n\n#write.csv(omdb_df, \"_data/omdb.csv\")\nomdb_df <- read_csv(\"_data/omdb.csv\")\n\n\nNew names:\nRows: 7729 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): imdbID, Director, Metascore, BoxOffice dbl (3): ...1, imdbRating,\nimdbVotes\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nCode\nomdb_df$Metascore <- as.numeric(omdb_df$Metascore)\n\n\nWarning: NAs introduced by coercion\n\n\nCode\nomdb_df$BoxOffice <- omdb_df$BoxOffice %>%\n  str_replace_all(\",\", \"\") %>% \n  str_extract(\"[0-9]*(?=$)\") %>% \n  as.numeric()\n\nomdb_df <- omdb_df %>% select(-...1)\nglimpse(omdb_df)\n\n\nRows: 7,729\nColumns: 6\n$ imdbID     <chr> \"tt2614684\", \"tt0074080\", \"tt0090556\", \"tt1022603\", \"tt1132…\n$ Director   <chr> \"Yann Demange\", \"Beverly Sebastian, Ferd Sebastian\", \"Tom M…\n$ Metascore  <dbl> 83, NA, NA, 76, 58, NA, 71, 52, 45, 52, 60, 76, NA, NA, 70,…\n$ imdbRating <dbl> 7.2, 5.4, 7.6, 7.7, 6.3, 6.3, 7.4, 6.5, 5.0, 6.5, 5.5, 7.2,…\n$ imdbVotes  <dbl> 56942, 1278, 2371, 516486, 2230, 8603, 181857, 74038, 35911…\n$ BoxOffice  <dbl> 1270847, NA, 441863, 32391374, 230600, NA, NA, 27766, 9600,…\n\n\nNow I have the data from OMDb as omdb_df. Let’s join movie_bechdel_join and omdb_df.\n\n\nCode\ndata <- inner_join(movie_bechdel_join, omdb_df, \n                                 by=c(\"imdb_id\" = \"imdbID\"),\n                                 copy = TRUE)"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#data-analysis-and-visualization",
    "href": "posts/Final_Project_ErikaNagai.html#data-analysis-and-visualization",
    "title": "Final Project Erika Nagai",
    "section": "Data Analysis and Visualization",
    "text": "Data Analysis and Visualization\n\nIs female representation in movie stories improving over time?\nTo find out the answer for this questions, I looked at the result of the Bechdel test.\nThe number of movies that pass the Bechdel test increases over time, especially after around 1980 as the total number of released movies increased.\n\n\nCode\nmovie_bechdel_join %>%\n  filter(lubridate::year(release_date) > 1920) %>%\n  group_by(year = lubridate::year(release_date), bechdel_rating) %>%\n  dplyr::summarize(n_total = n()) %>%\n  ggplot(aes(x=year, y= n_total, fill = bechdel_rating)) + geom_area(stat = \"identity\") +\n  scale_fill_manual(values = c(\"gray0\", \"gray40\", \"gray80\", \"orange\"),\n                    labels=c(\"0.No two women\", \n                             \"1.Two women\", \n                             \"2.Two women talk to each other about men\", \n                             \"3.Two women talk to each other about other than men (Pass)\")) +\n  labs(x= \"year\", y = \"Number\", title = \"The number of movies by Bechdel test ratings\", fill = \"Bechdel rating\")+\n  scale_x_continuous(n.breaks=14) +\n  theme(legend.position=\"bottom\") +\n  guides(fill = guide_legend(nrow = 2))\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\n\nHowever, the total number of released movies is increasing as well. Is the percentage of films that pass the Bechdel test increasing?\nIn the 1920s, which is 100 years ago, less than 20% of films passed the Bechdel test. There was a big increase in the percentage of Bechdel test-passing movies in the 1930s, however, the percentage of films passing the Bechdel test was stagnant at a bit lower than 50% from the 1930s to the 1950s. From 1950s to 1970s, the representation of women in the film somehow went backward, with less than 45% of films meeting the requirements of the Bechdel test; from the 1970s to the present, the percentage of films that pass the Bechdel test has continued to increase, reaching approximately 70% now. Although the representation of women seems to have improved since 1970, it has not improved dramatically compared to the 1930s and 1940s.\n\n\nCode\ndata_with_p <- data %>%\n  filter(lubridate::year(release_date) > 1920) %>%\n  group_by(years, bechdel_rating) %>%\n  dplyr::summarize(n = n()) %>%\n  mutate(percentage = n/sum(n)*100)\n\n\n`summarise()` has grouped output by 'years'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\ndata_with_p %>%\n  ggplot(aes(x=years, y= percentage, fill = bechdel_rating)) + \n  geom_col() +\n  geom_text(aes(label=paste0(round(percentage), \"%\")), color=\"white\", font=\"bold\", position = position_stack(vjust = 0.5)) +\n  \n  scale_fill_manual(values = c(\"gray0\", \"gray40\", \"gray80\", \"orange\"),\n                    labels=c(\"0.No two women\", \n                             \"1.Two women\", \n                             \"2.Two women talk to each other about men\", \n                             \"3.Two women talk to each other about other than men (Pass)\")) +\n  labs(x=\"year\", y=\"percent\", title = \"% of movies that pass/don't pass Bechdel Test\", fill=\"Bechdel Rating\") +\n  scale_y_continuous(n.breaks=10) +\n  theme(legend.position=\"bottom\") +\n  guides(fill = guide_legend(ncol = 2))\n\n\nWarning in geom_text(aes(label = paste0(round(percentage), \"%\")), color =\n\"white\", : Ignoring unknown parameters: `font`\n\n\n\n\n\nPerhaps the percentage of films that pass the Bechdel test has not changed too dramatically compared to 70 years ago. However, if we focus on the movies that do NOT pass the Bechdel test, we can see a different trend happening.\nIn the 1920s, the majority of films did not feature two significant women; this trend changed greatly in the 1930s, with the percentage of films that did not feature two women decreasing significantly, peaking in the 1960s to about 20%, and then declining.\nThe percentage of films in which the only conversation between female characters is about men also decreased from 1930 to the 2010s.\nInterestingly, however, the percentage of films in which female characters do not speak to each other (dark gray in the figure) has changed very little from the 1930s to the 2010s.\n\n\n\nCode\ndata_with_p %>%\n  filter(bechdel_rating!=\"3\") %>%\n  ggplot(aes(x=years, y= percentage, fill = bechdel_rating)) + \n  geom_col()+\n  geom_text(aes(label=paste0(round(percentage), \"%\")), color=\"white\", font=\"bold\", position = position_stack(vjust = 0.5)) +\n  scale_fill_manual(values = c(\"gray0\", \"gray40\", \"gray80\", \"orange\"),\n                    labels=c(\"0.No two women\", \n                             \"1.Two women\", \n                             \"2.Two women talk to each other about men\")) +\n  labs(x=\"year\", y=\"percent\", title = \"% of movies that do NOT pass Bechdel test by Bechdel test criteria\", fill = \"Bechdel Rating\", subtitle = \"% doesn't add up to 100% because it is missing the % of the movies that pass Bechdel test\") +\n  scale_y_continuous(n.breaks=10) +\n  theme(legend.position=\"bottom\") +\n  guides(fill = guide_legend(nrow = 2))\n\n\nWarning in geom_text(aes(label = paste0(round(percentage), \"%\")), color =\n\"white\", : Ignoring unknown parameters: `font`\n\n\n\n\n\n\n\nWhat category represents women better?\nI focused on 4428 movies released between 2000 and 2020 because I want to do category-based analysis of a recent situation, and it’s better to focus on the period where more movies were released to observe the trend.\nThe degree to which women are portrayed in a film’s story varies widely from genre to genre.\nThe genre with the highest percentage of films passing the bechdel test is Romance (72%) and the lowest is Western (22%), showing a large difference.\nInterestingly, not only the percentage of films that pass the Bechdel test, but also the percentage that meet each of the three criteria for passing the Bechdel test varies widely by genre.\nFor example, Western, War, and Documentary struggle more than other genres to feature two female characters. (To be precise, the Bechdel Test is for evaluating fictional stories, so it may not be appropriate to evaluate Documentary.)\n\n\nCode\nviz_by_genre <- data %>%\n  pivot_longer(cols = c(War:Music),names_to = \"genre_name\", values_drop_na = TRUE) %>%\n  filter(value == 1) \n\n\nviz_by_genre_with_p <- viz_by_genre %>%\n  group_by(genre_name, years, bechdel_rating) %>%\n  dplyr::summarize(n=n()) %>%\n  mutate(proportion = n/sum(n),\n         percentage=proportion*100)\n\n\n`summarise()` has grouped output by 'genre_name', 'years'. You can override\nusing the `.groups` argument.\n\n\nCode\nviz_by_genre_with_p %>%\n  filter(years %in% c(\"2000s\", \"2010s\")) %>%\n  group_by(genre_name, bechdel_rating) %>%\n  summarise(per=mean(percentage)) %>%\n  arrange(desc(bechdel_rating), desc(per)) -> order_genre\n\n\n`summarise()` has grouped output by 'genre_name'. You can override using the\n`.groups` argument.\n\n\nCode\nviz_by_genre_with_p$genre_name <-factor(viz_by_genre_with_p$genre_name, levels = order_genre$genre_name[1:18])\n\nviz_by_genre_with_p %>%\n  filter(years==\"2000s\"|years==\"2010s\") %>%\n  group_by(genre_name, bechdel_rating) %>%\n  summarize(n=sum(n)) %>%\n  mutate(p=n/sum(n),\n         percentage = 100*p) %>%\n  ggplot(aes(x=genre_name, y=percentage, fill= bechdel_rating)) +\n  geom_col() +\n  geom_text(aes(label=paste0(round(percentage), \"%\")), color=\"white\", position=position_stack(vjust=0.5), size=3)+\n  scale_fill_manual(values = c(\"gray0\", \"gray40\", \"gray80\", \"orange\"),\n                    labels=c(\"0.No two women\", \n                             \"1.Two women\", \n                             \"2.Two women talk to each other about men\", \n                             \"3.Two women talk to each other about other than men (Pass)\")) +\n  labs(x=\"Genre\", y=\"%\", fill=\"Bechdel rating\")+\n  coord_flip()  +\n  theme(legend.position=\"bottom\") +\n  guides(fill = guide_legend(nrow = 2))\n\n\n`summarise()` has grouped output by 'genre_name'. You can override using the\n`.groups` argument.\n\n\n\n\n\nAbove we looked at trends over the past 20 years, but what if we look at the historical trends over past 100 years?\nSome genres continue to improve their representation of women compared to the past, while others remain the same.\nFor example, Horror, Adventure, Science Fiction, Animation, and Fantasy have gradually increased the percentage of films that pass the Bechdel test.\nWestern, History, War, and Crime, Comedy on the other hand, have not changed much in terms of female representation compared to 100 years ago.\n\n\n\nCode\nviz_by_genre_with_p %>%\n  filter(years!=\"Before 1920\") %>%\n  ggplot(aes(x=years, y= percentage, fill = bechdel_rating)) + \n  geom_col() +\n  facet_wrap(~genre_name, ncol = 5)+\n  scale_fill_manual(values = c(\"gray0\", \"gray40\", \"gray80\", \"orange\"),\n                    labels=c(\"No two female characters\", \"Two female characters\", \"That talk each other\", \"Other than a man (Passes Bechdel Test)\")) +\n  labs(x = \"year\", y = \"percent\", title = \"Proportion of movies that pass / don't pass Bechdel test by categories\")+\n  theme(legend.position=\"bottom\") +\n  guides(fill = guide_legend(nrow = 2)) +\n  scale_x_discrete(breaks = c('1920s', '1960s', '2010s'),\n                   labels = c('1920', '1960', '2010'))\n\n\n\n\n\n\n\nDo movies with good female representation succeed in gaining more popularity on online review site?\nAgain, I focused on the movies released after 2000 to see the recent trends.\nI compared Metascore and IMDb ratings of the films that pass and don’t pass the Bechdel test.\nThe movies that don’t pass the Bechdel test seem to score higher on Metascore, however, the difference is quite small, and there seems to be no relationship between whether a film passes the Bechdel test and its reputation (Metascore).\n\n\nCode\ndata %>%\n  filter(lubridate::year(release_date) > 2000) %>%\n  ggplot(aes(x=bechdel_pass, y=Metascore)) + \n  geom_boxplot() +\n  scale_x_discrete(labels=c(\"0: Don't pass Bechel\", \"1: Pass Bechdel\")) +\n  labs(x=\"Bechdel Test\")\n\n\nWarning: Removed 915 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nT-test shows that there is no statistically significant difference in Metascores of the movies that pass and that don’t pass Bechdel test.\n\n\nCode\nttest_pass <- data %>% \n  filter(bechdel_pass == \"1\" & lubridate::year(release_date) > 2000) %>%\n  select(c(\"bechdel_pass\", \"imdbRating\", \"imdbVotes\", \"Metascore\", \"BoxOffice\"))\n\nttest_not_pass <- data %>%\n  filter(bechdel_pass == \"0\" & lubridate::year(release_date) > 2000) %>%\n  select(c(\"bechdel_pass\", \"imdbRating\", \"imdbVotes\", \"Metascore\", \"BoxOffice\"))\n\nt.test(ttest_pass$Metascore, ttest_not_pass$Metascore)\n\n\n\n    Welch Two Sample t-test\n\ndata:  ttest_pass$Metascore and ttest_not_pass$Metascore\nt = 0.37842, df = 2936, p-value = 0.7051\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9448254  1.3967391\nsample estimates:\nmean of x mean of y \n 57.81359  57.58764 \n\n\nThen, I compared the IMDb scores of the movies that pass and don’t pass Bechdel test. Those that don’t pass Bechdel test seem to score a bit higher.\n\n\nCode\ndata %>%\n  filter(lubridate::year(release_date) > 2000) %>%\n  ggplot(aes(x=bechdel_pass, y=imdbRating)) + \n  geom_boxplot() +\n  scale_x_discrete(labels=c(\"0: Don't pass Bechel\", \"1: Pass Bechdel\")) +\n  labs(x=\"Bechdel Test\", y=\"IMDb Rating\")\n\n\nWarning: Removed 8 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nT-test shows that the movies that do not meet Bechdel test criteria score statistically higher than those that do.\n\n\nCode\nt.test(ttest_pass$imdbRating, ttest_not_pass$imdbRating)\n\n\n\n    Welch Two Sample t-test\n\ndata:  ttest_pass$imdbRating and ttest_not_pass$imdbRating\nt = -5.6621, df = 3804.6, p-value = 1.605e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2230059 -0.1082905\nsample estimates:\nmean of x mean of y \n 6.383584  6.549233 \n\n\n\n\nDo movies with good female representation succeed in making more money?\n\n\nCode\ndata %>%\n  filter(lubridate::year(release_date) > 2000) %>%\n  ggplot(aes(x=bechdel_pass, y=BoxOffice)) + \n  geom_boxplot() +\n  scale_x_discrete(labels=c(\"0: Don't pass Bechel\", \"1: Pass Bechdel\"))\n\n\nWarning: Removed 1115 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nEven though the average box office amount of not-Bechdel-passing movies is higher than that of Bechdel-passing movies, T-test shows that the difference is not significant.\n\n\nCode\nt.test(ttest_pass$BoxOffice, ttest_not_pass$BoxOffice)\n\n\n\n    Welch Two Sample t-test\n\ndata:  ttest_pass$BoxOffice and ttest_not_pass$BoxOffice\nt = -1.5212, df = 3042.1, p-value = 0.1283\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9207124  1162341\nsample estimates:\nmean of x mean of y \n 43773430  47795821"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#conclusion",
    "href": "posts/Final_Project_ErikaNagai.html#conclusion",
    "title": "Final Project Erika Nagai",
    "section": "Conclusion",
    "text": "Conclusion\nThe representation of women in film stories stagnated from the 1930s to the 1970s, but has gradually improved, especially since the 1970s.\nHowever, this is not true for all genres: the percentage of films that pass the Bechdel test varies widely from the 20% range (Western) to the 70% range (Romance), depending on the genre. Some genres (Horror, Adventure, Science Fiction, Fantasy) have consistently increased the percentage of films that pass the Bechdel test, while others (War, Crime, History, Western) have remained stagnant. The percentage of films that pass the test is increasing constantly.\n\nIt turns out that whether or not a film passes the Bechdel test has little impact on its success (i.e., popularity and revenue). However, the reviews on the imdb website are higher for films that do not pass the Bechdel Test.\nThis analysis has so far shown how women’s representation has (or has not) improved, and which genres have been particularly well represented by women, but it has not explained why this has happened. I would like to conduct an analysis that can answer the question “Why?”. For example, I would like to test the hypothesis that the increase in female representation in fantasy and horror is due to the increase in female audiences.\nAlso, I would like to further investigate the relationship between female representation and film success (popularity and revenue). In particular, I am interested in how the gender of the audience affects their evaluation of films in which women are represented and those in which they are not."
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#reflection",
    "href": "posts/Final_Project_ErikaNagai.html#reflection",
    "title": "Final Project Erika Nagai",
    "section": "Reflection",
    "text": "Reflection\nOne of the most difficult parts of this analysis was finding the right data. Even though there are many movie database, the information was not complete or not clear enough to be used. Thus, I ended up using two different movie databases and one Bechdel test database because one movie database (The Movie Dataset) didn’t have a clear definition of review ratings and popularity.\nAnother difficulty that I faced is that I could not find a good dataset of the gender of directors/writers of movies. Even though I found “credit” dataset that included the name and the gender of people who were involved in the movie production, the information on director was limited and only a few thousand movies had director information. Originally, I wanted to visualize the relationship between the gender of movie directors and the female representation, however, I gave up doing that for this project due to the lack of an appropriate dataset."
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#contact",
    "href": "posts/Final_Project_ErikaNagai.html#contact",
    "title": "Final Project Erika Nagai",
    "section": "Contact",
    "text": "Contact\nThe dataset that I used for this analysis can be found on my GitHub (https://github.com/Enagai-nagai/601_Fall_2022/tree/template/posts/_data).\nI’m interested in analyzing and visualizing data related to the entertainment industry, behavior on digital platforms, and gender issues.\nIf you have any comments or questions, please contact me through Email.\n\nenagai ★ umass.edu (please replace ★ with @)\nnagainagai.e ★ gmail.com (please replace ★ with @)"
  },
  {
    "objectID": "posts/Final_Project_ErikaNagai.html#bibliography-and-references",
    "href": "posts/Final_Project_ErikaNagai.html#bibliography-and-references",
    "title": "Final Project Erika Nagai",
    "section": "Bibliography and References",
    "text": "Bibliography and References\nI used R and RStudio to realize this analysis.\nThe below is the source of data sets:\n\nBechdel API: https://bechdeltest.com/api/v1/doc\nThe Movie Dataset from Kaggle: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\nOMDb API: https://www.omdbapi.com/\n\nThe below is the source of information:\n\nWomen and Hollywood. (2022, March 15) Study: Women made up 34% of speaking roles in 2021’s top films, majority of those characters were white. . Retrieved December 13, 2022, from https://womenandhollywood.com/study-women-made-up-34-of-speaking-roles-in-2021s-top-films-majority-of-those-characters-were-white/#:~:text=In%202021’s%20top%20films%2C%20females,and%2037%20percent%20in%202019.\nLauzen, M. M. (2021). (rep.). It’s a Man’s (Celluloid) World, Even in a Pandemic Year: Portrayals of Female Characters in the Top U.S. Films of 2021. San Diego, California: San Diego State University and The Center for the Study of Women in Television and Film."
  },
  {
    "objectID": "posts/final_project_Guanhua_Tan.html",
    "href": "posts/final_project_Guanhua_Tan.html",
    "title": "DACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan",
    "section": "",
    "text": "Code\n# import package\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(tmap)\n\n\nWarning: package 'tmap' was built under R version 4.2.2\n\n\nCode\nlibrary(sf)\n\n\nWarning: package 'sf' was built under R version 4.2.2\n\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/final_project_Guanhua_Tan.html#briefly-describe-the-dataset.",
    "href": "posts/final_project_Guanhua_Tan.html#briefly-describe-the-dataset.",
    "title": "DACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan",
    "section": "Briefly Describe the Dataset.",
    "text": "Briefly Describe the Dataset.\nThis original OECD PISA 2018 School Questionnaire Dataset is one part of PISA 2018 dataset with a focus on schools. It covers 80 countries and regions all over the world. The dataset documents 21,903 schools’ responses regarding 187 questions. Some key identifiers include CNT (Country Name), STRATUM (Region Name) and OECD (belongs to or not OECD). I have narrowed my focus on a set of questions that start with “SC115.” “SC115” is a series of questions about the evaluation on digital devices in different countries. Some questions measure the availability of digital devices in these countries. Some questions interrogate if there are enough supports for teachers in terms of training program, technical support, and time (See PISA2018 “School Questionnaire,” p. 9). So I reorganize the dataset. I first use the “select’’ function to create a dataset covering all”SC155” questions. Then, I create another dataset containing all identifiers information (11 columns). Eventually, I use the “cbind” function to combine these two ones into a new dataset “pisa2018_joint” that contains SC155Q (11 questions) and all identifiers information (11 columns). After preliminarily tidying data, the clean dataset now records 29,903 schools’ responses concerning eleven questions along with their eleven identifiers."
  },
  {
    "objectID": "posts/final_project_Guanhua_Tan.html#painting-a-global-map",
    "href": "posts/final_project_Guanhua_Tan.html#painting-a-global-map",
    "title": "DACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan",
    "section": "Painting a Global Map",
    "text": "Painting a Global Map\n\n\nCode\n#tidy data for the world map\npisa2018_joint$Accessibility=rowMeans(pisa2018_joint[,c(\"SC155Q01HA\",\"SC155Q02HA\", \n                                                        \"SC155Q03HA\",\"SC155Q04HA\")])\npisa2018_joint$Human_Resource_Support=rowMeans(pisa2018_joint[\n  ,c(\"SC155Q05HA\",\"SC155Q06HA\", \"SC155Q07HA\",\"SC155Q08HA\",\"SC155Q09HA\", \"SC155Q10HA\", \"SC155Q11HA\")])\npisa2018_joint_clean <-pisa2018_joint %>%\n  select(CNT, STRATUM, OECD, Accessibility, Human_Resource_Support) %>%\n  group_by(CNT) %>%\n  mutate(Accessibility_Country_Ave=mean(Accessibility, na.rm=T)) %>%\n  mutate(Human_Resource_ave=mean(Human_Resource_Support, na.rm=T)) %>%\n  select(CNT,OECD, Accessibility_Country_Ave, Human_Resource_ave) %>%\n  distinct() %>%\n  arrange(desc(Accessibility_Country_Ave))%>%\n  mutate(OECD_or_Non_OECD =case_when(\n    OECD==0~\"Non-OECD\",\n    OECD==1~\"OECD\")) %>%\n  select(-OECD)\npisa2018_joint_clean\n\n\nThe set of questions “SC155” surveys the accessibility to digital devices and its related training as well as assistance. I further found that the questions from SC155Q01HA to SC155Q04HA focus on the accessibility to digital devices while the questions from SC155Q05HA to SC155Q11HA stress on if the schools offer enough training, support, and incentives. I mutate two new variables–“Accessibility” and “Human_Resource_Support” that respectively calculate the average of the former set of questions and the latter by applying the “rowMeans” function. Besides, I create another two new variables—“Accessibility_Country_Ave” and “Human_Resource_Support”– to calculate every country averages in terms of “Accessibility” and “Human_Resource_Support”. Furthermore, in order to delete the duplicated averages for every country, I apply the “distinct” function. After that, the dataset has 80 rows that present 80 countries and regions. Finally, I mutate “OECD_or_Non_OECD” as well in order to create graphics more easily later. After tidying data, the dataset “pisa2018_joint_clean” has four columns—CNT (Country Name), Accessibility_Country_Ave, Human_Resource_ave, and OECD_or_Non_OECD–and 80 rows.\n\n\nCode\n# joint data--merge pisa2018 into the world map data\ndata(\"World\")\nworld2<-World %>%\n  mutate(CNT=iso_a3) %>%\n  select(-iso_a3)\nworld2\n\nworld_pisa <-merge(x=world2, y=pisa2018_joint_clean, by=\"CNT\", all.x=T)\nworld_pisa\n\n\nIn order to create a map, I need to use the “merge” function to merge the dataset “pisa2018_joint_clean” to the map dataset “World.” I found that “iso_a3” in “World” shares the same code with “CNT” in my dataset. I merge my dataset into “World” dataset by “CNT” and I keep all the map dataset information. After that, “World” dataset includes the information about “Accessibility_Country_Ave” and “Human_Resource_ave.”\n\n\nCode\n#draw world map show regional differences\n# draw a world map about the accessibly to digital devices and human resource supports\n\nbbox_new <- st_bbox(world_pisa)\nxrange <- bbox_new$xmax - bbox_new$xmin \nyrange <- bbox_new$ymax - bbox_new$ymin\nbbox_new[3] <- bbox_new[3] + (0.4 * xrange)\nbbox_new[4] <- bbox_new[4] + (0.35 * yrange)\nbbox_new <- bbox_new %>%\n  st_as_sfc()\n\ntm_shape(world_pisa, bbox=bbox_new) +\n  tm_polygons(col=\"Accessibility_Country_Ave\",  palette = \"Set1\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Averages of Every Country's Accessiblity to Digtial Devices\",  \n            title.position = c('left', 'bottom'))\n\n\n\n\n\nCode\ntm_shape(world_pisa,bbox=bbox_new) +\n  tm_polygons(col=\"Human_Resource_ave\",  palette = \"Set1\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Averages of Every Country's Evaluation on Humman Support for Digtial Devices\",  \n            title.position = c('right', 'bottom'))\n\n\n\n\n\nFinally, I create two world maps reflecting regional differences in terms of accessibility to digital devices and human resources supports for them all over the world. Apparently, just a few countries have the grades higher than 3 points, which reflects that these countries’ agreement of enjoying the good accessibility to digital devices. However, even some developed countries still express their limited accessibility to digital devices such as United Kingdom and France. The phenomenon, in fact, opens more questions and their answers will lie in further investigations on these countries. I will take France and Unite Kingdom as case studies later. With respect to “Human Resource Support,” most countries have reported the grades under 3 points. This may not disclose that these countries (whatever OECD countries or Non-OECD ones) share the similar scenarios because there is no objective measurements for them to measure their human resources supports for digital devices. Therefore, instead of reflecting that OECD countries lack human resources supports, the data may show OECD and non-OECD countries have distinct expectations on human resources supports. So self-report can only demonstrate the gap between their expectations and current experiences."
  },
  {
    "objectID": "posts/final_project_Guanhua_Tan.html#painting-world-in-graphics",
    "href": "posts/final_project_Guanhua_Tan.html#painting-world-in-graphics",
    "title": "DACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan",
    "section": "Painting World in Graphics",
    "text": "Painting World in Graphics\n\n\nCode\n# further tidy data and pivot_longer\n# world difference in general\n\npisa2018_clean_pivot<-pisa2018_joint_clean %>%\n  pivot_longer(cols=c(Accessibility_Country_Ave, Human_Resource_ave), \n               names_to = \"Group\", values_to = \"Evaluation\")\npisa2018_clean_pivot %>%\n   ggplot(aes(Evaluation, fill=Group))+\n  stat_boxplot(geom = \"errorbar\", # Error bars\n               width = 0.1)+\n  geom_boxplot()+\n  facet_wrap(~Group+OECD_or_Non_OECD)+\n  labs(title=\"Pisa2018 Digital Diveces Evaluation\")+\n  coord_flip()\n\n\n\n\n\nIn order to further tidy data and create comparative graphics, I first used pivot_longer to create two new variables “Group” and “Evaluation”.I put the original variables–“Accessibility_Country_Ave” and “Human_Resource_ave” names to “Group” and values to “Evaluation.” After cleaning and rearranging the dataset, I create four graphics. The boxplot graphics reveal that generally speaking, OECD countries enjoy the higher access to digital devices than non-OECD countries. Because the median of OECD group is significantly higher than that of Non-OECD.\nBut these two groups have reported that their human resource evaluations are in the similar range. The median of OECD countries is similar to that of Non-OECD countries. Besides, the variation of OECD countries is smaller.\nThe result of the graphics concerning “human resource supports” reflects the limits to this survey’s methodology that the world map already has showed. Because this survey is reliance on schools’ self-report. There is no objective measurements for them to measure their access to and human resources supports for digital devices. Therefore, instead of reflecting that OECD countries lack human resources support, the data may show OECD and non-OECD countries have distinct expectations on human resources support. So self-report can only demonstrate the gap between their expectations and current situations."
  },
  {
    "objectID": "posts/final_project_Guanhua_Tan.html#relation-between-accessibility-to-digital-devices-and-human-resource-support-for-digital-human-resource",
    "href": "posts/final_project_Guanhua_Tan.html#relation-between-accessibility-to-digital-devices-and-human-resource-support-for-digital-human-resource",
    "title": "DACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan",
    "section": "Relation between “Accessibility to Digital Devices” and “Human Resource Support for Digital Human Resource”",
    "text": "Relation between “Accessibility to Digital Devices” and “Human Resource Support for Digital Human Resource”\n\n\nCode\n# Bivariate Visualization--point plot\n# differentiate OECD and Non-OECD\npisa2018_joint_clean %>%\n  ggplot(aes(x=Accessibility_Country_Ave, y=Human_Resource_ave, color=OECD_or_Non_OECD))+\n  geom_point(size=2)+\n  scale_colour_manual(values=c(\"#800000\",\"#004080\"))+\n  xlab(\"Accessiblity to Digital Devices for Countires\")+\n  ylab(\"Huamn Resourse Support for Countries\")+\n  geom_smooth(method=lm , color=\"black\", fill=\"#69b3a2\", se=TRUE)+\n  facet_wrap(~OECD_or_Non_OECD)\n\n\n\n\n\nThe point plot is good at showing the relationship between two groups of numbers. I use it to capture the correlation between “Accessibility to Digital Devices” and”Human Resource Support” in each country. The smooth line has demonstrated that the positive correlation between “Accessibility to Digital Devices” and”Human Resource Support” is true for the both groups.\nThe point plot shows that many points locate in a longer distance to the line in Non-OECD countries. On the contrary, points in OECD countries are much closer to the line. This situation also uncovers the finding, as I have discussed above, that the variation of OECD countries is relatively smaller than that of Non-OECD countries."
  },
  {
    "objectID": "posts/final_project_Guanhua_Tan.html#a-further-case-study-of-six-countries-and-region",
    "href": "posts/final_project_Guanhua_Tan.html#a-further-case-study-of-six-countries-and-region",
    "title": "DACSS601: Dtat Science Fundamentals Fianl Paper Guanhua Tan",
    "section": "A further Case Study of Six Countries and Region",
    "text": "A further Case Study of Six Countries and Region\nI have already deployed a world map and several boxplots to demonstrate regional differences all over the world and arrive at some primitive conclusions. What follows is I’d like to dig into 5 countries and one region (Hong Kong, Philippines, Argentina and Brazil belong to non-OECD, Britain and France belong to non-OECD).\n\n\nCode\n# tidy data about the case study\n# choose six cases to look at differences between OCED group and Non-OECD group\n# United Kingdom (GBR), Hong Kong (HKG), Philippines (PHL), Argentina (ARG), Brazil(BRA), France(FRA)\npisa2018_joint_case_study <- pisa2018_joint  %>%\n  select(CNT, STRATUM, OECD, SC155Q01HA,SC155Q02HA, SC155Q03HA, SC155Q04HA, SC155Q05HA,\n         SC155Q06HA, SC155Q07HA, SC155Q08HA, SC155Q09HA, SC155Q10HA, SC155Q11HA) %>%\n  group_by(STRATUM) %>%\n  arrange(STRATUM) %>%\n  filter(CNT== \"GBR\" | CNT== \"HKG\" | CNT==\"PHL\"| CNT==\"ARG\" | CNT==\"BRA\" |CNT==\"FRA\" )\npisa2018_joint_case_study\n\n\n\n\nCode\n#further tidy data\n\npisa2018_joint_case_study$Accessibility=rowMeans(pisa2018_joint_case_study[,c(\"SC155Q01HA\",\"SC155Q02HA\", \"SC155Q03HA\",\"SC155Q04HA\")])\npisa2018_joint_case_study$Human_Resource_Support=rowMeans(pisa2018_joint_case_study[,c(\"SC155Q05HA\",\"SC155Q06HA\", \"SC155Q07HA\",\"SC155Q08HA\",\"SC155Q09HA\", \"SC155Q10HA\", \"SC155Q11HA\")])\npisa2018_case_study_clean <- pisa2018_joint_case_study %>%\n  select(CNT,STRATUM,OECD, Accessibility, Human_Resource_Support) %>% \n  pivot_longer(cols=c(Accessibility, Human_Resource_Support), \n               names_to = \"Group\", values_to = \"Evaluation\")%>%\n   mutate(OECD_or_Non_OECD =case_when(\n    OECD==0~\"Non-OECD\",\n    OECD==1~\"OECD\")) %>%\n  select(-OECD)\npisa2018_case_study_clean\n\n\nFollowing the above-mentioned finding that the questions from SC155Q01HA to SC155Q04HA focus on the accessibility to digital devices while the questions from SC155Q05HA to SC155Q11HA stress on if the schools offer enough training, support, and incentives. I apply the same tidy data approach to the case study’s dataset. I mutate two new variables—“Accessibility” and “Human_Resource_Support” that respectively calculate the average of the former and the latter. I use pivot_longer to rearrange the dataset in order to show these two categories in one graphic. The result is the dataset “pisa2018_case_study_clean.”\n\n\nCode\n# violin graphic\npisa2018_case_study_clean %>%\n  ggplot(aes(CNT,Evaluation, fill=CNT), na.rm=T) +\n  geom_violin()+\n  facet_wrap(~Group)+\n  xlab(\"Country\")+\n  ylab(\"Evaluation\")+\n  ggtitle(\"Schools' Evaluation on Digtial Device\")+\n  labs(fill=\"Country\")\n\n\n\n\n\nI use the violin graphics to disclose the evaluation of digital devices in six cases with the dimension of “Accessibility” and “Human Resource Support”. The graphic has showed, whatever accessibility and human resource supports, Hong Kong, France and Britain report higher grades than ones in Argentina, Brazil, and the Philippines reply. Most schools’ in Hong Kong, France and Britain responses are around the range of “agree” while most schools’ in Argentina, Brazil, the Philippines replies are around the range of “disagree”, which reflects that schools in Argentina, Brazil, the Philippines are not satisfied by their current situations.\n\n\nCode\npisa2018_case_study_clean %>%\n  ggplot(aes(CNT, Evaluation,fill=CNT), na.rm=T)+\n  geom_violin()+\n  xlab(\"Country\")+\n  ylab(\"Evaluaiton\")+\n  ggtitle(\"Schools' Evaluation on Digtial Device\")+\n  facet_wrap(~Group+OECD_or_Non_OECD)+\n  labs(fill=\"Country\")\n\n\n\n\n\nI add one more dimension–OECD or Non-OECD to this violin plot. In fact, the plot demonstrates the variation of the evaluation on digital devices in non-OECD countries. In terms of “Accessibility,” Hong Kong has a better access to digital devices than Argentina, Brazil, the Philippines. the similar situation happens to the evaluation of “Human Resources Support” as well. The fact may be interpreted that Hong Kong has more enough fiscal resources to support their schools. Despite of not good as Hong Kong, more percentage of schools in the Philippines has reported that they are satisfied by the access to and human resource support for digital devices than ones in Argentina and Brazil.\nBy contrast, two OECD countries–France and Brain share the similar shapes in terms of accessibility and human resource support. It reflects that most schools in these two countries believe that they enjoy an good access to digital device and receive enough human resources support. Also, it reflects that these is no a big variation between them.\n\n\nCode\n# using SC155Q01HA as a case study to show part-whole relationships\n\ndf<-pisa2018_joint %>%\n  group_by(CNT) %>%\n  filter(CNT== \"GBR\" | CNT== \"HKG\" | CNT==\"PHL\"| CNT==\"ARG\" | CNT==\"BRA\" |CNT==\"FRA\" ) %>%\n  drop_na() %>%\n  count(Q1=factor(SC155Q01HA))%>%\n  mutate(pct=paste0(sprintf(\"%4.1f\",n/sum(n)*100), \"%\"))\n\nggplot(df, aes(x=CNT, y=n ,fill=Q1), na.rm=T)+\n  geom_bar(stat=\"identity\", position=\"dodge\")+\n  geom_text(aes(label=pct), \n            position = position_dodge(width = 1),\n            vjust=-0.5, size=2)+\n  ylab(\"Schools' Response Count\")+\n  xlab(\"Country\")+\n  labs(fill=\"Digital Devices Evaluation\")+\n  scale_fill_discrete(labels=c(\"Strongly Disagree\", \n                               \"Disagree\", \"Agree\", \"Strongly Agree\"))\n\n\n\n\n\nEventually, I’d like to take schools’ responses to SC155Q01HA as a case study to disclose part-whole relationships. The bar chart reflects schools’ responses to SC155Q01HA –“The number of digital devices connected to the Internet is sufficient.” (PISA 2018 “School Questionnaire,” p. 9). The bar plot shows the homogeneous tendency to the above-mentioned violin graphics. It has demonstrated that most schools in Argentina and Brazil are not happy with their accessibility to digital devices–more 70% of them report “strongly disagree” and “disagree.” Schools in the Philippines enjoyed the better accessibility to digital devices–less than 60% of them return the native responses. By sharp contrast, 60-70% of schools in France, the United Kingdom and Hong Kong are happy with their accessibility.More responses from France, Britain and Hong Kong reported “disagree” that that from Argentina, Brazil, and the Philippines.\nBut it reveals an additional funding that the number of responses from France is relatively fewer in terms of its size. In other words, the sample of France is very tiny, which may not reflect the situation there, compared with the samples from the rest five countries and area. Unfortunately, the OECD does offer explanation in its Code Book."
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html",
    "href": "posts/Final_Project_JackSniezek.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#introduction",
    "href": "posts/Final_Project_JackSniezek.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nThe Super Bowl is easily the biggest betting event in the United States, with an estimated $7.61 billion in bets for the Super Bowl LVI. This figure is up 78%, or $3.33B from the year before. The huge surge in betting figures can be explained by the Supreme Court overturning a federal ban on sports betting in 2018, which had been in place since 1992. The law previously only allowed a select few states to operate. Today, 39 states and Washington DC either have legal sports betting or are considering legislation to implement it. Of the 39 states with legal sports betting or legislation, 22 allow online and mobile sports betting, which has led to the rise of many online sportsbooks such as DraftKings, Caesar’s Sportsbook, and BetMGM.\nThe purpose of this paper is to explore the trends in NFL betting in the Super Bowl Era. The key points that I intend to cover in my research are:\n• Stadiums where spread is covered the most how it changes over time • How weather affects who has covered the spread • Who covers the spread in the Super Bowl/playoff games\nI would like to also examine the over/under for these, as well as how individual teams fare against weather, stadium, and opponent, but I am afraid this will result in me taking on too much to handle. So, for the purposes of this paper, I will stick to these research topics.\nThe dataset I will be working with was available on Kaggle.com and consists of NFL game results, betting odds information, stadium information, and weather information. The data is made up of game results and weather information 1966 to present, and betting odds beginning in 1979. The betting odds included are the spread and the over/under. Weather data included is temperature, wind speed, and humidity for each game. Stadium data includes years active, type, capacity, and location. I will have to do significant data clean up, as the weather data is incomplete in some spots, there is a lot of extra stadium data, and many teams have changed names/cities. There are also some data points that are unique that have no info such as stadiums used for one or two games and have no information associated with it(for example, several baseball stadiums were venues in the early days of the NFL)."
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#read-in-the-data",
    "href": "posts/Final_Project_JackSniezek.html#read-in-the-data",
    "title": "Final Project",
    "section": "Read in the Data",
    "text": "Read in the Data\nFirst I read in the data. There are 3 separate csv files that contain the data I will be working with; spreadspoke_scores, nfl_stadiums, and nfl_teams.\n\n\nCode\n#change directory\nspreadspoke_scores <- read_csv(\"_data/nfl_data/spreadspoke_scores.csv\")\n\n\nRows: 13504 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): schedule_date, schedule_week, team_home, team_away, team_favorite_i...\ndbl (8): schedule_season, score_home, score_away, spread_favorite, over_unde...\nlgl (2): schedule_playoff, stadium_neutral\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nspreadspoke_scores\n\n\n# A tibble: 13,504 × 17\n   schedule_date sched…¹ sched…² sched…³ team_…⁴ score…⁵ score…⁶ team_…⁷ team_…⁸\n   <chr>           <dbl> <chr>   <lgl>   <chr>     <dbl>   <dbl> <chr>   <chr>  \n 1 9/2/1966         1966 1       FALSE   Miami …      14      23 Oaklan… <NA>   \n 2 9/3/1966         1966 1       FALSE   Housto…      45       7 Denver… <NA>   \n 3 9/4/1966         1966 1       FALSE   San Di…      27       7 Buffal… <NA>   \n 4 9/9/1966         1966 2       FALSE   Miami …      14      19 New Yo… <NA>   \n 5 9/10/1966        1966 1       FALSE   Green …      24       3 Baltim… <NA>   \n 6 9/10/1966        1966 2       FALSE   Housto…      31       0 Oaklan… <NA>   \n 7 9/10/1966        1966 2       FALSE   San Di…      24       0 New En… <NA>   \n 8 9/11/1966        1966 1       FALSE   Atlant…      14      19 Los An… <NA>   \n 9 9/11/1966        1966 2       FALSE   Buffal…      20      42 Kansas… <NA>   \n10 9/11/1966        1966 1       FALSE   Detroi…      14       3 Chicag… <NA>   \n# … with 13,494 more rows, 8 more variables: spread_favorite <dbl>,\n#   over_under_line <dbl>, stadium <chr>, stadium_neutral <lgl>,\n#   weather_temperature <dbl>, weather_wind_mph <dbl>, weather_humidity <dbl>,\n#   weather_detail <chr>, and abbreviated variable names ¹​schedule_season,\n#   ²​schedule_week, ³​schedule_playoff, ⁴​team_home, ⁵​score_home, ⁶​score_away,\n#   ⁷​team_away, ⁸​team_favorite_id\n\n\nCode\nstadiums <- read_csv(\"_data/nfl_data/nfl_stadiums.csv\")\n\n\nRows: 118 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): stadium_name, stadium_location, stadium_type, stadium_address, stad...\ndbl (5): stadium_open, stadium_close, LATITUDE, LONGITUDE, ELEVATION\nnum (1): stadium_capacity\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nstadiums\n\n\n# A tibble: 118 × 15\n   stadium_name  stadi…¹ stadi…² stadi…³ stadi…⁴ stadi…⁵ stadi…⁶ stadi…⁷ stadi…⁸\n   <chr>         <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>     <dbl>\n 1 Acrisure Sta… Pittsb…    2001      NA outdoor 100 Ar… 15212   cold      65500\n 2 Alamo Dome    San An…      NA      NA indoor  100 Mo… 78203   dome      72000\n 3 Allegiant St… Paradi…    2020      NA indoor  <NA>    <NA>    dome      65000\n 4 Allianz Arena Munich…      NA      NA outdoor <NA>    <NA>    modera…   75024\n 5 Alltel Stadi… Jackso…      NA      NA <NA>    <NA>    <NA>    <NA>         NA\n 6 Alumni Stadi… Chestn…      NA      NA outdoor Perime… 2467    cold         NA\n 7 Anaheim Stad… Anahei…    1980    1994 outdoor 2000 E… 92806   warm         NA\n 8 Arrowhead St… Kansas…    1972      NA outdoor 1 Arro… 64129   cold      76416\n 9 AT&T Stadium  Arling…    2009      NA retrac… 1 AT&T… 76011   dome      80000\n10 Atlanta-Fult… Atlant…    1966    1991 outdoor 521 Ca… 30312   warm         NA\n# … with 108 more rows, 6 more variables: stadium_surface <chr>, STATION <chr>,\n#   NAME <chr>, LATITUDE <dbl>, LONGITUDE <dbl>, ELEVATION <dbl>, and\n#   abbreviated variable names ¹​stadium_location, ²​stadium_open,\n#   ³​stadium_close, ⁴​stadium_type, ⁵​stadium_address,\n#   ⁶​stadium_weather_station_code, ⁷​stadium_weather_type, ⁸​stadium_capacity\n\n\nCode\nteams <- read_csv(\"_data/nfl_data/nfl_teams.csv\")\n\n\nRows: 44 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): team_name, team_name_short, team_id, team_id_pfr, team_conference, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nteams\n\n\n# A tibble: 44 × 8\n   team_name          team_nam…¹ team_id team_…² team_…³ team_…⁴ team_…⁵ team_…⁶\n   <chr>              <chr>      <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 Arizona Cardinals  Cardinals  ARI     CRD     NFC     NFC We… NFC     NFC We…\n 2 Atlanta Falcons    Falcons    ATL     ATL     NFC     NFC So… NFC     NFC We…\n 3 Baltimore Colts    Colts      IND     CLT     AFC     <NA>    AFC     AFC Ea…\n 4 Baltimore Ravens   Ravens     BAL     RAV     AFC     AFC No… AFC     AFC Ce…\n 5 Boston Patriots    Patriots   NE      NWE     AFC     <NA>    AFC     <NA>   \n 6 Buffalo Bills      Bills      BUF     BUF     AFC     AFC Ea… AFC     AFC Ea…\n 7 Carolina Panthers  Panthers   CAR     CAR     NFC     NFC So… NFC     NFC We…\n 8 Chicago Bears      Bears      CHI     CHI     NFC     NFC No… NFC     NFC Ce…\n 9 Cincinnati Bengals Bengals    CIN     CIN     AFC     AFC No… AFC     AFC Ce…\n10 Cleveland Browns   Browns     CLE     CLE     AFC     AFC No… AFC     AFC Ce…\n# … with 34 more rows, and abbreviated variable names ¹​team_name_short,\n#   ²​team_id_pfr, ³​team_conference, ⁴​team_division, ⁵​team_conference_pre2002,\n#   ⁶​team_division_pre2002"
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#combine-the-data",
    "href": "posts/Final_Project_JackSniezek.html#combine-the-data",
    "title": "Final Project",
    "section": "Combine the Data",
    "text": "Combine the Data\nThe datasets will have to be merged using joins before I can start manipulating the data. The stadium data was easy to join because it only required one variable name change. I had issues joining the teams data as there was not a clear variable that was exactly the same on both datasets. Initially I tried to use team_id and team_favorite_id, but some of the teams had name changes but still had the same id. This created duplicate data that could not be removed because multiple ids were connected to one data point. Ultimately, I found success by using team_name and team_home for the join.\n\n\nCode\n#Rename stadium so I can join to stadiums data, create new variable for join with teams data\nscores <- spreadspoke_scores%>%\n  rename(stadium_name = stadium)\n\n#new variable to join scores data\nnew_teams <- teams%>%\n  mutate(team_home = team_name)\n  #mutate(team_name_id = str_c(team_name, team_id, sep = \"_\"), .before = team_name)\n\n#select only data that I can use\n#team_id had to be removed because the duplicates made redundant data, i.e. some teams changed names/location but id was the same\nnew_teams <- new_teams%>%\n  select(team_home,team_name,team_id,team_name_short,team_conference,team_division,team_division_pre2002)\n\n#in case I messed up the data\nfull_scores <- scores\n\n#joins\n\nfull_scores <- full_join(x = scores, y = new_teams, by = \"team_home\")\nfull_scores <- full_join(x = full_scores,y = stadiums, by = \"stadium_name\")\n\nfull_scores\n\n\n# A tibble: 13,505 × 37\n   schedule_date sched…¹ sched…² sched…³ team_…⁴ score…⁵ score…⁶ team_…⁷ team_…⁸\n   <chr>           <dbl> <chr>   <lgl>   <chr>     <dbl>   <dbl> <chr>   <chr>  \n 1 9/2/1966         1966 1       FALSE   Miami …      14      23 Oaklan… <NA>   \n 2 9/3/1966         1966 1       FALSE   Housto…      45       7 Denver… <NA>   \n 3 9/4/1966         1966 1       FALSE   San Di…      27       7 Buffal… <NA>   \n 4 9/9/1966         1966 2       FALSE   Miami …      14      19 New Yo… <NA>   \n 5 9/10/1966        1966 1       FALSE   Green …      24       3 Baltim… <NA>   \n 6 9/10/1966        1966 2       FALSE   Housto…      31       0 Oaklan… <NA>   \n 7 9/10/1966        1966 2       FALSE   San Di…      24       0 New En… <NA>   \n 8 9/11/1966        1966 1       FALSE   Atlant…      14      19 Los An… <NA>   \n 9 9/11/1966        1966 2       FALSE   Buffal…      20      42 Kansas… <NA>   \n10 9/11/1966        1966 1       FALSE   Detroi…      14       3 Chicag… <NA>   \n# … with 13,495 more rows, 28 more variables: spread_favorite <dbl>,\n#   over_under_line <dbl>, stadium_name <chr>, stadium_neutral <lgl>,\n#   weather_temperature <dbl>, weather_wind_mph <dbl>, weather_humidity <dbl>,\n#   weather_detail <chr>, team_name <chr>, team_id <chr>,\n#   team_name_short <chr>, team_conference <chr>, team_division <chr>,\n#   team_division_pre2002 <chr>, stadium_location <chr>, stadium_open <dbl>,\n#   stadium_close <dbl>, stadium_type <chr>, stadium_address <chr>, …\n\n\nThe fully merged data contains 13,505 rows(games), and 37 columns."
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#clean-and-mutate",
    "href": "posts/Final_Project_JackSniezek.html#clean-and-mutate",
    "title": "Final Project",
    "section": "Clean and Mutate",
    "text": "Clean and Mutate\nMy next big challenge was trying to create a variable that tells whether or not a spread was covered. I mutated the favorite variable to show the team name instead of the team id, and then I mutated the spread from a negative number to a positive in order for it to mathematically make sense to compare to the scores. Then I could use a case_when() to compare the two and determine if the spread was covered. The data was then ready to be selected and filtered. I took 13 variables and filtered out data from before 1979, except Super Bowls, since that was when the spread data was available. Also, I noticed that some spreads were 0 and the favorite was “PICK”, so I filtered by spreads > 0.\n\n\nCode\n#rename \nfull_scores <- full_scores%>%\n  rename(favorite = team_favorite_id,\n         spread = spread_favorite)\n\n#adjust favorite so data is team name and not id\n#make spread positive number to keep calculations accurate\n#new variable to show if a spread was covered, uses case_when\nfull_scores <- full_scores%>%\n  mutate(favorite = ifelse(favorite == team_id,team_home,team_away))\n\n\nfull_scores <- full_scores%>%\n  mutate(spread = spread * -1,\n         favorite_covers = case_when(team_home == favorite & (score_home - score_away) > spread ~ \"Cover\",\n                                     team_home == favorite & (score_home - score_away) < spread ~ \"Not Cover\",\n                                     team_away == favorite & (score_away - score_home) > spread ~ \"Cover\",\n                                     team_away == favorite & (score_away - score_home) < spread ~ \"Not Cover\"))\n\n\n#select and filter the data  \nfull_scores <- full_scores%>%\n  select(schedule_season,schedule_week,team_home,score_home,score_away,team_away,favorite,spread,favorite_covers,stadium_name,stadium_type,weather_temperature,weather_wind_mph)%>%\n  filter(schedule_season > 1978 | schedule_week == \"Superbowl\",\n         spread > 0)\n\nfull_scores\n\n\n# A tibble: 10,781 × 13\n   schedule_sea…¹ sched…² team_…³ score…⁴ score…⁵ team_…⁶ favor…⁷ spread favor…⁸\n            <dbl> <chr>   <chr>     <dbl>   <dbl> <chr>   <chr>    <dbl> <chr>  \n 1           1966 Superb… Green …      35      10 Kansas… Green …   14   Cover  \n 2           1967 Superb… Green …      33      14 Oaklan… Green …   13.5 Cover  \n 3           1968 Superb… Baltim…       7      16 New Yo… Baltim…   18   Not Co…\n 4           1969 Superb… Kansas…      23       7 Minnes… Minnes…   12   Not Co…\n 5           1970 Superb… Baltim…      16      13 Dallas… Baltim…    2.5 Cover  \n 6           1971 Superb… Dallas…      24       3 Miami … Dallas…    6   Cover  \n 7           1972 Superb… Miami …      14       7 Washin… Miami …    1   Cover  \n 8           1973 Superb… Miami …      24       7 Minnes… Miami …    6.5 Cover  \n 9           1974 Superb… Minnes…       6      16 Pittsb… Pittsb…    3   Cover  \n10           1975 Superb… Dallas…      17      21 Pittsb… Pittsb…    7   Not Co…\n# … with 10,771 more rows, 4 more variables: stadium_name <chr>,\n#   stadium_type <chr>, weather_temperature <dbl>, weather_wind_mph <dbl>, and\n#   abbreviated variable names ¹​schedule_season, ²​schedule_week, ³​team_home,\n#   ⁴​score_home, ⁵​score_away, ⁶​team_away, ⁷​favorite, ⁸​favorite_covers"
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#visualize",
    "href": "posts/Final_Project_JackSniezek.html#visualize",
    "title": "Final Project",
    "section": "Visualize",
    "text": "Visualize\nHere is where I struggled. My intent was to show spread coverage at different stadiums on a bar graph, but I could not figure out how to graph a count. I also tried a similar plot with wind mph and Super Bowls, but also could not figure out if I could plot the count of the spreads. However, I was able to manipulate the data into showing what I was looking for in the graph. I was able to get a count of the number of covers at each stadium by week. For example, in Giants Stadium, favorites have covered 17 times in week 16. Thats the most out of any team. I also did an overall count of stadium covers, with Giants Stadium having the most covers at 223. Giants Stadium is a unique situation, as both the Giants and Jets both play their home games there. Because tof this, it gets twice the amount of games as other stadiums. Next, I checked the Super Bowl covers at each stadium. The Rose Bowl has the most covers at 4, followed by the Louisiana Superdome and the Orange Bowl both at 3. Covers at different wind speeds is interesting. The most covers by more than double is at 0 mph, which makes sense. Rounding out the top 10 wind speeds for covers is 5 mph through 12 mph. The highest wind speed to have a cover is 32 mph.\nIn addition to using the counts to show my data, I made some tables to represent stadium covers, wind speed covers, and temperature covers. The stadium table breaks down stadium covers by week. The wind speed table breaks it down wind speed covers by week, with a filter to show the covers when the wind speed is more than 10 mph. The temperature covers table represents temperature covers by week. The coldest temperature for game that the spread was covered was -3 degrees.\n\n\nCode\nstadium_covers <- full_scores%>%\n  select(schedule_season, schedule_week,stadium_name, favorite_covers)%>%\n  filter(schedule_season > 1978,favorite_covers == \"Cover\")%>%\n  count(stadium_name, favorite_covers, schedule_week,sort = TRUE)\n\n  #ggplot(aes(y = stadium_name, x = schedule_week, fill = favorite_covers), geom_bar())\n\nstadium_covers\n\n\n# A tibble: 1,232 × 4\n   stadium_name     favorite_covers schedule_week     n\n   <chr>            <chr>           <chr>         <int>\n 1 Giants Stadium   Cover           16               17\n 2 Giants Stadium   Cover           1                16\n 3 Giants Stadium   Cover           3                16\n 4 Lambeau Field    Cover           2                16\n 5 Texas Stadium    Cover           12               16\n 6 Giants Stadium   Cover           10               15\n 7 Giants Stadium   Cover           7                15\n 8 Lambeau Field    Cover           11               15\n 9 Lambeau Field    Cover           13               15\n10 Qualcomm Stadium Cover           14               15\n# … with 1,222 more rows\n\n\nCode\nstadium_covers_total <- full_scores%>%\n  select(schedule_season, schedule_week,stadium_name, favorite_covers)%>%\n  filter(schedule_season > 1978,favorite_covers == \"Cover\")%>%\n  count(stadium_name, favorite_covers,sort = TRUE)\n\nstadium_covers_total\n\n\n# A tibble: 94 × 3\n   stadium_name         favorite_covers     n\n   <chr>                <chr>           <int>\n 1 Giants Stadium       Cover             223\n 2 Lambeau Field        Cover             191\n 3 Soldier Field        Cover             164\n 4 Arrowhead Stadium    Cover             150\n 5 Ralph Wilson Stadium Cover             148\n 6 Candlestick Park     Cover             146\n 7 Qualcomm Stadium     Cover             146\n 8 Louisiana Superdome  Cover             132\n 9 Texas Stadium        Cover             125\n10 Oakland Coliseum     Cover             107\n# … with 84 more rows\n\n\nCode\nsuperbowl_covers <- full_scores%>%\n  filter(schedule_week == \"Superbowl\",favorite_covers == \"Cover\")%>%\n  count(stadium_name, favorite_covers, schedule_week,sort = TRUE)\n  \n  #ggplot(aes(x = favorite_covers), geom_bar())\n\nsuperbowl_covers\n\n\n# A tibble: 18 × 4\n   stadium_name                  favorite_covers schedule_week     n\n   <chr>                         <chr>           <chr>         <int>\n 1 Rose Bowl                     Cover           Superbowl         4\n 2 Louisiana Superdome           Cover           Superbowl         3\n 3 Orange Bowl                   Cover           Superbowl         3\n 4 Los Angeles Memorial Coliseum Cover           Superbowl         2\n 5 Tulane Stadium                Cover           Superbowl         2\n 6 Cowboys Stadium               Cover           Superbowl         1\n 7 Dolphin Stadium               Cover           Superbowl         1\n 8 Ford Field                    Cover           Superbowl         1\n 9 Georgia Dome                  Cover           Superbowl         1\n10 Hard Rock Stadium             Cover           Superbowl         1\n11 Hubert H. Humphrey Metrodome  Cover           Superbowl         1\n12 Joe Robbie Stadium            Cover           Superbowl         1\n13 Mercedes-Benz Stadium         Cover           Superbowl         1\n14 Pontiac Silverdome            Cover           Superbowl         1\n15 Pro Player Stadium            Cover           Superbowl         1\n16 Raymond James Stadium         Cover           Superbowl         1\n17 Rice Stadium                  Cover           Superbowl         1\n18 Stanford Stadium              Cover           Superbowl         1\n\n\nCode\nwind_covers <- full_scores%>%\n  filter(schedule_season > 1978,favorite_covers == \"Cover\")%>%\n  count(weather_wind_mph, favorite_covers,sort = TRUE)\n\n  #ggplot(aes(x= schedule_week, y = weather_wind_mph, fill = mean(weather_wind_mph)),geom_histogram())\n\nwind_covers\n\n\n# A tibble: 31 × 3\n   weather_wind_mph favorite_covers     n\n              <dbl> <chr>           <int>\n 1                0 Cover            1129\n 2               NA Cover             451\n 3                8 Cover             369\n 4                6 Cover             325\n 5                9 Cover             325\n 6                7 Cover             294\n 7               10 Cover             294\n 8               11 Cover             268\n 9               12 Cover             232\n10                5 Cover             209\n# … with 21 more rows\n\n\nCode\ntbl_stadium <- with(full_scores, table(stadium_name,schedule_week, favorite_covers))\ntbl_stadium\n\n\n, , favorite_covers = Cover\n\n                                     schedule_week\nstadium_name                           1 10 11 12 13 14 15 16 17 18  2  3  4  5\n  Acrisure Stadium                     0  1  1  0  0  0  0  0  0  0  0  0  0  0\n  Alamo Dome                           0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Allegiant Stadium                    0  2  1  0  0  1  0  0  0  0  0  0  2  0\n  Allianz Arena                        0  1  0  0  0  0  0  0  0  0  0  0  0  0\n  Alltel Stadium                       0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Anaheim Stadium                      0  4  3  7  6  6  6  2  2  0  2  3  1  8\n  Arrowhead Stadium                   11  6 11  8  9 10  7 11 10  1  6 10  9  7\n  AT&T Stadium                         3  1  1  2  0  1  0  1  1  0  1  2  2  1\n  Atlanta-Fulton County Stadium        3  4  3  3  5  3  3  5  1  0  2  2  1  3\n  Bank of America Stadium              5  7  9  3  5  9  5 10  6  0  4  5  2  6\n  Busch Memorial Stadium               1  3  2  1  1  3  2  3  0  0  5  2  3  1\n  Caesars Superdome                    0  0  1  1  1  0  0  1  1  0  1  0  0  1\n  Candlestick Park                     6  4 10 13  7 11 10  9  4  0  9  5  8 13\n  CenturyLink Field                    4  4  8  3  6  7  4  6  3  0  4 11  2  1\n  Cinergy Field                        6  2  7  7  9  5  4  6  2  0  2  3  5  2\n  Cleveland Municipal Stadium          4  2  4  6  7  3  0  4  1  0  5  4  2  2\n  Cowboys Stadium                      2  1  2  2  0  0  0  3  1  0  0  3  0  1\n  Dolphin Stadium                      0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Edward Jones Dome                    4  2  2  6  9  1  6  8  4  0  1  4  6  4\n  Empower Field at Mile High           0  0  0  0  0  1  0  0  0  0  0  1  0  0\n  Estadio Azteca                       0  0  4  0  0  0  0  0  0  0  0  0  1  0\n  EverBank Field                       8  5  9  4  5 12  5  5  4  0  6  3  5  5\n  FedEx Field                          7  3  7  5  4  9  5  6  8  0  5  5  3  9\n  FirstEnergy Stadium                  7  4 10  5  4  7  5  6  4  0  2  6  1  8\n  Ford Field                           5  4  7 14  8  2  3  5  4  0  5  5  2  5\n  Foxboro Stadium                      9  6  2  4  7  5  7  1  1  0  6  4  4  5\n  GEHA Field at Arrowhead Stadium      0  1  0  1  0  0  0  0  0  0  0  0  0  0\n  Georgia Dome                         2  8  3  8  6  5  4  5  6  0  9  7  5  6\n  Giants Stadium                      16 15 14 12 10 13 14 17 12  0 12 16  7  9\n  Gillette Stadium                     5  3  5  5  4  5  5  6 10  0  3  8  2  8\n  Hard Rock Stadium                    2  2  2  1  3  0  0  0  1  0  2  1  2  0\n  Heinz Field                          4  8  2  1  6  6  4  5  6  0  6  3  5  6\n  Highmark Stadium                     0  0  0  0  0  0  1  0  0  1  1  1  1  1\n  Houlihan's Stadium                   7  4  3  4  2  1  6  4  5  0  4  3  4  4\n  Houston Astrodome                    2  3  5  3  1  8  3  3  0  1  3  5  4  3\n  Hubert H. Humphrey Metrodome         8  7  4  7  7 10  3  7  4  0  8  5  4  4\n  Husky Stadium                        0  0  0  0  1  1  0  0  0  0  1  1  0  0\n  Jack Murphy Stadium                  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Joe Robbie Stadium                   0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Lambeau Field                       12 12 15  7 15 13  7 12 11  0 16  7  8  9\n  Levi's Stadium                       1  0  1  4  0  0  2  1  1  0  1  0  4  3\n  Liberty Bowl Memorial Stadium        0  1  1  0  1  0  0  0  0  0  0  0  0  0\n  Lincoln Financial Field              6  4  7  8  5  5  5  5  6  1  5  3  4  6\n  Los Angeles Memorial Coliseum        3  1  7  5  3  2  3  4  3  0  5  6  2  2\n  Louisiana Superdome                 10  8  8  9  8  6  6 11  7  1  4  7  8  6\n  LP Stadium                           0  0  0  0  1  0  0  0  1  0  0  1  0  0\n  Lucas Oil Stadium                    2  1  4  7  4  2  5  1  3  0  3  3  2  4\n  Lumen Field                          0  0  0  0  0  0  0  0  1  0  0  0  0  1\n  M&T Bank Stadium                     9  5  5  9  6  7 10  3  5  0  8  3  8  3\n  Mall of America Field                0  0  2  1  2  1  1  0  1  0  0  1  1  2\n  Memorial Stadium (Baltimore)         2  2  1  1  0  2  1  1  1  0  2  0  1  1\n  Memorial Stadium (Champaign)         0  0  0  0  0  0  0  0  2  0  0  1  0  0\n  Memorial Stadium (Clemson)           0  0  0  0  0  0  1  0  0  0  0  1  0  1\n  Mercedes-Benz Stadium                1  0  2  0  3  1  1  1  0  1  1  0  1  0\n  Mercedes-Benz Superdome              1  1  2  1  1  0  2  2  0  0  1  0  0  2\n  MetLife Stadium                      7  3  3  8  4  7  8  3  6  1  7  5  6  6\n  Metropolitan Stadium                 0  2  2  1  0  0  1  0  0  0  1  1  1  0\n  Mile High Stadium                    9  6  7  6  3  2  4  1  3  0  8  3  4  4\n  New Era Field                        1  1  0  2  1  4  2  0  2  0  1  1  0  0\n  Nissan Stadium                       5  4  1  6  2 10  7  3  7  0  2  4  4  3\n  NRG Stadium                          0  0  2  1  3  1  0  1  2  0  1  1  1  3\n  Oakland Coliseum                     5 10  3  4  8  7  9  5  7  0  6  6 10  2\n  Orange Bowl                          0  1  1  1  2  2  0  5  0  0  5  1  3  0\n  Paul Brown Stadium                   2  6  4  7  6  5  5  3  5  0  6  3  5  5\n  Paycor Stadium                       0  0  0  0  0  0  0  0  0  0  0  0  1  0\n  Pontiac Silverdome                   7  4  2  2  8  8  6  9  0  0  5  4  6  2\n  Pro Player Stadium                   0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Qualcomm Stadium                     6  4  8  7 14 15 12  8  5  0 13  7  7  8\n  Ralph Wilson Stadium                15 11  8 10  6  9  6  5  5  0  9  6 13  5\n  Raymond James Stadium                2  6  8  3  4 10  6  4  4  1  7  7  4  3\n  RCA Dome                             4  4  7  5  5  3  6  6  3  1  6  4  6  6\n  Reliant Stadium                      7  0  2  6  3  3  2  2  6  0  2  4  6  1\n  RFK Memorial Stadium                 5  3  5  3  6  2  3  1  2  0  2  5  2  4\n  Rice Stadium                         0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Rogers Centre                        0  0  0  0  1  0  1  0  0  0  0  0  0  0\n  Rose Bowl                            0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Seattle Kingdome                     4  5  4  1  4  6  2  4  3  0  9  3  3  3\n  SoFi Stadium                         3  1  0  0  1  3  2  1  2  0  0  0  1  1\n  Soldier Field                       10 12  8  9 10 10  9 11  9  0 10  5 14  8\n  Sports Authority Field at Mile High  5  1  4  1  2  3  4  4  7  0  3  6  5  5\n  Stanford Stadium                     0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  State Farm Stadium                   1  0  0  0  0  0  0  0  0  0  0  1  0  1\n  StubHub Center                       0  0  1  1  0  1  1  0  1  0  0  1  0  1\n  Sun Devil Stadium                    0  1  2  1  3  2  2  0  1  0  1  1  1  1\n  Sun Life Stadium                     7  6 11  5  5  6  8  2  5  0  5  7  5  2\n  Tampa Stadium                        0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  TCF Bank Stadium                     0  0  0  0  2  1  2  1  0  0  2  1  0  0\n  Texas Stadium                        2  9  5 16 11  4  6  7  4  0  7  4  4  6\n  Three Rivers Stadium                 7  5  5  7  6  4  4  9  4  1  4  7  4  6\n  TIAA Bank Field                      1  0  2  1  1  2  0  1  0  0  1  1  1  1\n  Tiger Stadium (LSU)                  0  0  0  0  1  0  1  0  0  0  0  0  0  0\n  Tottenham Hotspur Stadium            0  0  0  0  0  0  0  0  0  0  0  0  0  1\n  Tottenham Stadium                    0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Tulane Stadium                       0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Twickenham Stadium                   0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  U.S. Bank Stadium                    4  0  3  2  0  2  2  1  1  1  0  1  2  1\n  University of Phoenix Stadium        1  6  3  7  6  6  5  8  5  0  8  6  2  6\n  Vanderbilt Stadium                   0  0  1  0  0  0  0  0  1  0  0  0  1  0\n  Veterans Stadium                     3  5  4  2  2  7  5  6  4  0  4  7  4  5\n  Wembley Stadium                      0  1  0  0  0  0  0  0  0  0  0  0  3  0\n                                     schedule_week\nstadium_name                           6  7  8  9 Conference Division Superbowl\n  Acrisure Stadium                     0  0  0  0          0        0         0\n  Alamo Dome                           0  0  0  0          0        0         0\n  Allegiant Stadium                    0  3  0  0          0        0         0\n  Allianz Arena                        0  0  0  0          0        0         0\n  Alltel Stadium                       0  0  0  0          0        0         0\n  Anaheim Stadium                      0  3  6  4          0        1         0\n  Arrowhead Stadium                    4 10 11  2          2        3         0\n  AT&T Stadium                         1  2  1  1          0        0         0\n  Atlanta-Fulton County Stadium        3  1  1  4          0        0         0\n  Bank of America Stadium              2  6  5  9          1        2         0\n  Busch Memorial Stadium               2  1  1  1          0        0         0\n  Caesars Superdome                    1  0  0  1          0        0         0\n  Candlestick Park                     6  5  7  6          4        8         0\n  CenturyLink Field                    2  2  2  8          2        2         0\n  Cinergy Field                        5  2  5  6          2        2         0\n  Cleveland Municipal Stadium          4  3  3  5          0        2         0\n  Cowboys Stadium                      1  3  3  1          0        0         1\n  Dolphin Stadium                      0  0  0  0          0        0         1\n  Edward Jones Dome                    4  7  5  4          0        2         0\n  Empower Field at Mile High           0  1  1  0          0        0         0\n  Estadio Azteca                       0  0  0  0          0        0         0\n  EverBank Field                       5  3  3  4          0        1         0\n  FedEx Field                          4  4  4  1          0        0         0\n  FirstEnergy Stadium                  5  4  4  7          0        0         0\n  Ford Field                           2  3  7  2          0        0         1\n  Foxboro Stadium                      4  3  3  3          1        1         0\n  GEHA Field at Arrowhead Stadium      1  0  0  0          0        0         0\n  Georgia Dome                         5  8  2  8          1        2         1\n  Giants Stadium                      14 15 11  8          1        4         0\n  Gillette Stadium                     7  3  8  5          3        8         0\n  Hard Rock Stadium                    2  2  0  2          0        0         1\n  Heinz Field                          3  7  2  3          3        3         0\n  Highmark Stadium                     0  0  0  0          0        0         0\n  Houlihan's Stadium                   2  0  5  5          1        0         0\n  Houston Astrodome                    5  3  4  3          0        0         0\n  Hubert H. Humphrey Metrodome         4 11  3  3          0        2         1\n  Husky Stadium                        0  2  0  1          0        0         0\n  Jack Murphy Stadium                  0  0  0  0          0        0         0\n  Joe Robbie Stadium                   0  0  0  0          0        0         1\n  Lambeau Field                        6 12  9  9          1        5         0\n  Levi's Stadium                       0  4  1  3          1        1         0\n  Liberty Bowl Memorial Stadium        0  1  0  0          0        0         0\n  Lincoln Financial Field              5  1  6  4          1        1         0\n  Los Angeles Memorial Coliseum        6  0  3  1          1        3         2\n  Louisiana Superdome                  9  6  8  6          0        1         3\n  LP Stadium                           1  0  0  0          0        0         0\n  Lucas Oil Stadium                    3  5  2  3          1        1         0\n  Lumen Field                          0  0  2  0          0        0         0\n  M&T Bank Stadium                     4  3  5  5          0        0         0\n  Mall of America Field                1  1  1  0          0        1         0\n  Memorial Stadium (Baltimore)         2  3  2  0          0        0         0\n  Memorial Stadium (Champaign)         0  0  0  0          0        0         0\n  Memorial Stadium (Clemson)           0  0  0  0          0        0         0\n  Mercedes-Benz Stadium                1  1  0  2          0        0         1\n  Mercedes-Benz Superdome              1  0  1  1          0        0         0\n  MetLife Stadium                      9  6  3  4          0        0         0\n  Metropolitan Stadium                 2  0  1  0          0        0         0\n  Mile High Stadium                    3  3  4  6          3        3         0\n  New Era Field                        2  0  3  2          0        1         0\n  Nissan Stadium                       4  5 10  4          0        0         0\n  NRG Stadium                          1  1  2  0          0        0         0\n  Oakland Coliseum                     4  6  7  4          1        2         0\n  Orange Bowl                          1  3  0  5          2        2         3\n  Paul Brown Stadium                   2  4  3  3          0        0         0\n  Paycor Stadium                       0  1  0  1          0        0         0\n  Pontiac Silverdome                   4  5  2  2          0        0         1\n  Pro Player Stadium                   0  0  0  0          0        0         1\n  Qualcomm Stadium                     7  6 10  7          0        0         0\n  Ralph Wilson Stadium                 9  6  8 10          2        3         0\n  Raymond James Stadium                9  4  2  5          0        1         1\n  RCA Dome                             6  2  3  6          1        0         0\n  Reliant Stadium                      1  3  5  5          0        0         0\n  RFK Memorial Stadium                 3  7  2  2          2        3         0\n  Rice Stadium                         0  0  0  0          0        0         1\n  Rogers Centre                        0  0  1  0          0        0         0\n  Rose Bowl                            0  0  0  0          0        0         4\n  Seattle Kingdome                     0  4  2  5          0        0         0\n  SoFi Stadium                         1  2  0  1          0        0         0\n  Soldier Field                        9  9 10  3          4        3         0\n  Sports Authority Field at Mile High  3  5  6  3          1        1         0\n  Stanford Stadium                     0  0  0  0          0        0         1\n  State Farm Stadium                   0  2  0  0          0        0         0\n  StubHub Center                       0  0  0  0          0        0         0\n  Sun Devil Stadium                    3  2  3  2          0        0         0\n  Sun Life Stadium                     2  7  5  9          1        1         0\n  Tampa Stadium                        0  0  0  0          0        0         0\n  TCF Bank Stadium                     1  0  0  1          0        0         0\n  Texas Stadium                       10  9  8  3          2        5         0\n  Three Rivers Stadium                 3  7  5  6          1        3         0\n  TIAA Bank Field                      1  0  1  0          0        0         0\n  Tiger Stadium (LSU)                  0  0  0  1          0        0         0\n  Tottenham Hotspur Stadium            1  0  0  0          0        0         0\n  Tottenham Stadium                    1  0  0  0          0        0         0\n  Tulane Stadium                       0  0  0  0          0        0         2\n  Twickenham Stadium                   0  2  1  0          0        0         0\n  U.S. Bank Stadium                    1  1  1  2          0        0         0\n  University of Phoenix Stadium        6  6  7  7          0        0         0\n  Vanderbilt Stadium                   0  1  0  0          0        0         0\n  Veterans Stadium                     4  5  6  8          0        2         0\n  Wembley Stadium                      0  2  5  0          0        0         0\n                                     schedule_week\nstadium_name                          SuperBowl Wildcard WildCard\n  Acrisure Stadium                            0        0        0\n  Alamo Dome                                  0        0        0\n  Allegiant Stadium                           0        0        0\n  Allianz Arena                               0        0        0\n  Alltel Stadium                              0        0        0\n  Anaheim Stadium                             0        0        0\n  Arrowhead Stadium                           0        2        0\n  AT&T Stadium                                0        0        0\n  Atlanta-Fulton County Stadium               0        0        0\n  Bank of America Stadium                     0        2        0\n  Busch Memorial Stadium                      0        0        0\n  Caesars Superdome                           0        0        0\n  Candlestick Park                            0        1        0\n  CenturyLink Field                           0        1        1\n  Cinergy Field                               0        1        0\n  Cleveland Municipal Stadium                 0        1        0\n  Cowboys Stadium                             0        1        0\n  Dolphin Stadium                             0        0        0\n  Edward Jones Dome                           0        0        0\n  Empower Field at Mile High                  0        0        0\n  Estadio Azteca                              0        0        0\n  EverBank Field                              0        1        0\n  FedEx Field                                 0        2        0\n  FirstEnergy Stadium                         0        0        0\n  Ford Field                                  0        0        0\n  Foxboro Stadium                             0        1        0\n  GEHA Field at Arrowhead Stadium             0        0        0\n  Georgia Dome                                0        0        0\n  Giants Stadium                              0        3        0\n  Gillette Stadium                            0        2        0\n  Hard Rock Stadium                           0        0        0\n  Heinz Field                                 0        0        1\n  Highmark Stadium                            0        1        0\n  Houlihan's Stadium                          0        1        0\n  Houston Astrodome                           0        0        0\n  Hubert H. Humphrey Metrodome                0        4        0\n  Husky Stadium                               0        0        0\n  Jack Murphy Stadium                         0        0        0\n  Joe Robbie Stadium                          0        0        0\n  Lambeau Field                               0        4        1\n  Levi's Stadium                              0        0        0\n  Liberty Bowl Memorial Stadium               0        0        0\n  Lincoln Financial Field                     0        0        0\n  Los Angeles Memorial Coliseum               0        2        0\n  Louisiana Superdome                         0        1        0\n  LP Stadium                                  0        0        0\n  Lucas Oil Stadium                           0        1        0\n  Lumen Field                                 0        0        0\n  M&T Bank Stadium                            0        3        0\n  Mall of America Field                       0        0        0\n  Memorial Stadium (Baltimore)                0        0        0\n  Memorial Stadium (Champaign)                0        0        0\n  Memorial Stadium (Clemson)                  0        0        0\n  Mercedes-Benz Stadium                       0        0        0\n  Mercedes-Benz Superdome                     0        1        0\n  MetLife Stadium                             0        1        0\n  Metropolitan Stadium                        0        0        0\n  Mile High Stadium                           0        1        0\n  New Era Field                               0        0        0\n  Nissan Stadium                              0        2        0\n  NRG Stadium                                 0        1        0\n  Oakland Coliseum                            0        1        0\n  Orange Bowl                                 0        1        0\n  Paul Brown Stadium                          0        2        0\n  Paycor Stadium                              0        0        0\n  Pontiac Silverdome                          0        0        0\n  Pro Player Stadium                          0        0        0\n  Qualcomm Stadium                            0        2        0\n  Ralph Wilson Stadium                        0        2        0\n  Raymond James Stadium                       0        1        0\n  RCA Dome                                    0        3        0\n  Reliant Stadium                             0        3        1\n  RFK Memorial Stadium                        0        2        0\n  Rice Stadium                                0        0        0\n  Rogers Centre                               0        0        0\n  Rose Bowl                                   0        0        0\n  Seattle Kingdome                            0        1        0\n  SoFi Stadium                                0        1        0\n  Soldier Field                               0        1        0\n  Sports Authority Field at Mile High         0        0        0\n  Stanford Stadium                            0        0        0\n  State Farm Stadium                          0        0        0\n  StubHub Center                              0        0        0\n  Sun Devil Stadium                           0        0        0\n  Sun Life Stadium                            0        3        0\n  Tampa Stadium                               0        0        0\n  TCF Bank Stadium                            0        0        0\n  Texas Stadium                               0        3        0\n  Three Rivers Stadium                        0        1        0\n  TIAA Bank Field                             0        0        0\n  Tiger Stadium (LSU)                         0        0        0\n  Tottenham Hotspur Stadium                   0        0        0\n  Tottenham Stadium                           0        0        0\n  Tulane Stadium                              0        0        0\n  Twickenham Stadium                          0        0        0\n  U.S. Bank Stadium                           0        0        0\n  University of Phoenix Stadium               0        1        0\n  Vanderbilt Stadium                          0        0        0\n  Veterans Stadium                            0        2        0\n  Wembley Stadium                             0        0        0\n\n, , favorite_covers = Not Cover\n\n                                     schedule_week\nstadium_name                           1 10 11 12 13 14 15 16 17 18  2  3  4  5\n  Acrisure Stadium                     0  0  0  0  0  0  0  0  0  0  0  0  1  0\n  Alamo Dome                           0  0  0  0  0  0  0  1  0  0  0  0  1  0\n  Allegiant Stadium                    1  1  1  0  1  0  1  2  0  1  2  1  0  1\n  Allianz Arena                        0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Alltel Stadium                       0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Anaheim Stadium                      6  4  4  3  0  4  6  2  1  1  6  3  1  4\n  Arrowhead Stadium                    9  8 12 15  7 12 15 11  4  0 15  5 11 11\n  AT&T Stadium                         0  1  0  3  3  0  2  3  1  0  2  0  3  3\n  Atlanta-Fulton County Stadium        1  1  7  4  1  3  2  4  0  0  6  1  1  5\n  Bank of America Stadium              8  7  8  3  2  8  6  7  3  0  9  8  7 11\n  Busch Memorial Stadium               2  2  4  3  2  3  1  2  0  0  0  1  2  2\n  Caesars Superdome                    0  0  0  0  0  0  0  0  0  0  0  0  1  0\n  Candlestick Park                     6 11  9  8  5  9  6 10  6  1  7  7  5  8\n  CenturyLink Field                    3  3  2  4  5  1  2  9  6  0  4  5  2  4\n  Cinergy Field                        3  4  6  3  3  6  3  3  6  0  7  4  4  4\n  Cleveland Municipal Stadium          2  3  4  2  5  4  4  2  0  0  6  5  5  3\n  Cowboys Stadium                      1  0  2  4  3  3  5  1  2  0  2  2  3  4\n  Dolphin Stadium                      0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Edward Jones Dome                    8  5  8  5  5  1  9  5  3  0  5  5  5  4\n  Empower Field at Mile High           0  1  1  1  0  0  1  0  0  1  1  1  1  1\n  Estadio Azteca                       0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  EverBank Field                       5  5  5  3  6  7  5  7  2  0  8  2  6  4\n  FedEx Field                          8  6  5  8  8  6  4  5  7  0  6  8  6  2\n  FirstEnergy Stadium                 10  4  3  8  4  5  5  5  4  1  9  4  8  3\n  Ford Field                           6  1  3  3  5  8  6  3  5  1  3  5  5  5\n  Foxboro Stadium                      2  8  5  8  4  6  4  5  5  0  5 10  3  3\n  GEHA Field at Arrowhead Stadium      0  0  0  0  0  0  0  0  0  0  1  0  0  1\n  Georgia Dome                         7  5  7  9  5  3  7  4 11  1  7  2 10  6\n  Giants Stadium                      13 12 17 12 18 13 19 13  9  1 22 11 16 16\n  Gillette Stadium                     6  4  3  5  4  4  3  5  4  0  2  8  4  1\n  Hard Rock Stadium                    1  1  0  1  2  3  2  2  0  1  1  1  1  1\n  Heinz Field                          5  9  2  5  8  5  7  4  4  0  5  3  6  4\n  Highmark Stadium                     1  1  1  0  1  0  0  0  0  0  0  0  0  0\n  Houlihan's Stadium                   3  2  4  6  5  7  8  1  2  0  5  3  5  5\n  Houston Astrodome                    5  3  3  6  4  5  7  6  4  0  3  6  3  3\n  Hubert H. Humphrey Metrodome         5  9  8  7  6  4  6 12  6  0  4  9  6  5\n  Husky Stadium                        0  1  0  0  0  0  0  1  2  0  1  1  2  1\n  Jack Murphy Stadium                  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Joe Robbie Stadium                   0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Lambeau Field                       15 11  5  7  7 14  5  6  5  0 11 10 10  9\n  Levi's Stadium                       3  5  0  2  1  3  3  3  4  0  2  2  2  2\n  Liberty Bowl Memorial Stadium        1  0  0  0  0  0  0  0  1  0  0  0  1  0\n  Lincoln Financial Field              2  6  2  4  3  3  2  6  3  0  7  9  2  3\n  Los Angeles Memorial Coliseum        3  2  2  5  4  4  7  5  3  0  4  3  5  3\n  Louisiana Superdome                 10  9  7  7 12  7 10 13  6  0  9  8  7 12\n  LP Stadium                           1  1  0  0  0  0  0  1  0  0  0  0  0  0\n  Lucas Oil Stadium                    4  6  3  4  1  1  4  3  6  0  6  5  4  3\n  Lumen Field                          1  0  1  1  1  0  0  1  0  0  1  1  0  0\n  M&T Bank Stadium                     2  4  4  3  9  3  6  6  9  1  4  7  6  5\n  Mall of America Field                1  2  1  0  1  1  1  0  3  0  2  3  0  0\n  Memorial Stadium (Baltimore)         1  2  0  1  4  1  2  2  1  0  2  0  1  1\n  Memorial Stadium (Champaign)         1  1  0  1  0  0  1  0  0  0  0  0  0  1\n  Memorial Stadium (Clemson)           0  0  0  0  0  1  0  1  0  0  0  0  0  0\n  Mercedes-Benz Stadium                2  0  1  2  1  0  1  1  0  0  1  2  3  1\n  Mercedes-Benz Superdome              2  1  1  1  0  1  1  1  1  0  1  1  1  1\n  MetLife Stadium                      6  9  5  7  8  3  4  7  6  0  8  7  4  6\n  Metropolitan Stadium                 2  0  0  1  1  0  1  1  0  0  1  1  0  2\n  Mile High Stadium                    7  5  6  3  6  7  8  4  4  0  2  6  5  4\n  New Era Field                        1  0  0  2  0  1  1  1  1  0  0  3  1  1\n  Nissan Stadium                       6 11  8  3  4  5  5  8  4  0 10  6  2  5\n  NRG Stadium                          3  0  1  2  1  3  0  2  1  1  1  1  3  2\n  Oakland Coliseum                     6  5  5  9  7  4 10  6  6  0  6  5  3  8\n  Orange Bowl                          0  1  3  2  4  2  2  3  0  0  2  0  2  3\n  Paul Brown Stadium                   4  4  5  6  6  7  2 10  6  0  7  4  5  4\n  Paycor Stadium                       1  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Pontiac Silverdome                   2  4  8  6 11  2  7  6  3  1  6  4  9  4\n  Pro Player Stadium                   0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Qualcomm Stadium                     7 11  8 12  9  8 10  6  9  0  7  9  9  6\n  Ralph Wilson Stadium                11  4  5  8  6  8  5  8  6  0  6  9  9 11\n  Raymond James Stadium                9  8  3  5  5  3  4  7  8  0  5  5  7  2\n  RCA Dome                             9  7  5  3  7  4  6  8  6  0  4  5  2  5\n  Reliant Stadium                      4  1  6  4  2  3  5  5  5  0  1  3  3  6\n  RFK Memorial Stadium                 8  5  5  3  6  6  3  4  3  1  6  3  1  5\n  Rice Stadium                         0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Rogers Centre                        0  0  0  0  1  1  0  0  0  0  0  0  0  0\n  Rose Bowl                            0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Seattle Kingdome                     4  6  4  6 10  8  6  7  3  0  2  4  5  5\n  SoFi Stadium                         1  2  3  1  1  0  1  0  0  1  3  3  2  1\n  Soldier Field                       16 13  5  9 13  7 13  7  4  0 12 11  8  5\n  Sports Authority Field at Mile High  6  4  5 10  2  7  4  4  8  0  7  5  4  5\n  Stanford Stadium                     0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  State Farm Stadium                   0  1  0  1  1  1  0  1  0  1  0  0  0  1\n  StubHub Center                       1  0  1  0  1  1  0  2  0  0  1  1  2  1\n  Sun Devil Stadium                    0  2  1  2  0  0  3  2  3  0  1  0  3  1\n  Sun Life Stadium                     5  6  7  8  8  7 14 14  7  0  7 11  3  7\n  Tampa Stadium                        0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  TCF Bank Stadium                     0  0  0  1  0  0  0  0  1  0  0  0  1  0\n  Texas Stadium                        8  4 10  5 10  6  7 10  4  0  5  9  6  6\n  Three Rivers Stadium                 5  3  4  4  4  6  3  6  0  0  4  3  5  3\n  TIAA Bank Field                      2  0  0  2  1  0  2  0  1  1  1  3  0  1\n  Tiger Stadium (LSU)                  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Tottenham Hotspur Stadium            0  0  0  0  0  0  0  0  0  0  0  0  1  2\n  Tottenham Stadium                    0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Tulane Stadium                       0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  Twickenham Stadium                   0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  U.S. Bank Stadium                    1  0  3  1  2  0  2  1  3  0  1  5  1  2\n  University of Phoenix Stadium        7 10  4  5  7 10  9  7  6  0  7  9  9  5\n  Vanderbilt Stadium                   0  0  0  1  0  1  0  0  0  0  1  0  0  0\n  Veterans Stadium                     6  9  9  8  4  8  6  4  4  0  8  3  6  4\n  Wembley Stadium                      0  0  0  0  0  0  0  0  0  0  0  1  2  0\n                                     schedule_week\nstadium_name                           6  7  8  9 Conference Division Superbowl\n  Acrisure Stadium                     1  0  0  0          0        0         0\n  Alamo Dome                           1  0  0  0          0        0         0\n  Allegiant Stadium                    0  0  0  0          0        0         0\n  Allianz Arena                        0  0  0  0          0        0         0\n  Alltel Stadium                       0  0  0  0          0        0         1\n  Anaheim Stadium                      1  4  0  1          0        0         0\n  Arrowhead Stadium                   10  9 14 13          2        4         0\n  AT&T Stadium                         1  0  0  3          0        0         0\n  Atlanta-Fulton County Stadium        4  2  4  1          0        1         0\n  Bank of America Stadium              2  6  9  3          0        2         0\n  Busch Memorial Stadium               0  2  5  5          0        0         0\n  Caesars Superdome                    0  0  2  1          0        0         0\n  Candlestick Park                     7  5  7 10          4        6         0\n  CenturyLink Field                    9  3  2  5          1        1         0\n  Cinergy Field                        6  4  2  5          0        0         0\n  Cleveland Municipal Stadium          5  6  5  2          1        2         0\n  Cowboys Stadium                      0  1  3  3          0        1         0\n  Dolphin Stadium                      0  0  0  0          0        0         0\n  Edward Jones Dome                    4  4  3  2          2        1         0\n  Empower Field at Mile High           1  0  0  0          0        0         0\n  Estadio Azteca                       0  0  0  0          0        0         0\n  EverBank Field                       8  5  2  2          1        0         0\n  FedEx Field                          8 10  1  9          0        0         0\n  FirstEnergy Stadium                  8  4  4  5          0        0         0\n  Ford Field                           4  6  5  5          0        0         0\n  Foxboro Stadium                      7  8  3  8          0        0         0\n  GEHA Field at Arrowhead Stadium      0  0  0  1          0        0         0\n  Georgia Dome                         7  3  4  6          1        3         0\n  Giants Stadium                      15 12 17 16          1        2         0\n  Gillette Stadium                     4  4  3  4          4        4         0\n  Hard Rock Stadium                    2  1  1  1          0        0         0\n  Heinz Field                          5  0  8  5          1        2         0\n  Highmark Stadium                     0  0  1  0          0        0         0\n  Houlihan's Stadium                   6  7  3  3          0        1         0\n  Houston Astrodome                    3  2  4  5          0        1         0\n  Hubert H. Humphrey Metrodome         5  2  7  9          1        0         0\n  Husky Stadium                        1  0  0  1          0        0         0\n  Jack Murphy Stadium                  0  0  0  0          0        0         1\n  Joe Robbie Stadium                   0  0  0  0          0        0         1\n  Lambeau Field                       12  4  9  8          2        3         0\n  Levi's Stadium                       2  2  0  4          0        0         1\n  Liberty Bowl Memorial Stadium        0  0  1  0          0        0         0\n  Lincoln Financial Field              3  6  5  3          2        2         0\n  Los Angeles Memorial Coliseum        5  2  3  6          0        2         0\n  Louisiana Superdome                  5  7 13  6          1        1         2\n  LP Stadium                           0  0  0  0          0        0         0\n  Lucas Oil Stadium                    2  2  4  4          0        0         1\n  Lumen Field                          1  1  0  0          0        0         0\n  M&T Bank Stadium                     5  6  5  5          0        3         0\n  Mall of America Field                2  1  1  1          0        0         0\n  Memorial Stadium (Baltimore)         2  1  1  1          0        0         0\n  Memorial Stadium (Champaign)         0  0  0  1          0        1         0\n  Memorial Stadium (Clemson)           0  1  1  0          0        0         0\n  Mercedes-Benz Stadium                1  2  3  0          0        0         0\n  Mercedes-Benz Superdome              0  1  1  1          1        2         1\n  MetLife Stadium                      5  6  6  7          0        0         1\n  Metropolitan Stadium                 0  1  0  0          0        0         0\n  Mile High Stadium                    5  7  4  1          0        4         0\n  New Era Field                        0  1  2  1          0        0         0\n  Nissan Stadium                       7  5  3  2          0        4         0\n  NRG Stadium                          1  0  2  2          0        0         0\n  Oakland Coliseum                     7 10  6  7          1        0         0\n  Orange Bowl                          1  3  0  0          1        3         2\n  Paul Brown Stadium                   5  5  8  3          0        0         0\n  Paycor Stadium                       0  0  0  0          0        0         0\n  Pontiac Silverdome                   6  4  4  9          0        0         0\n  Pro Player Stadium                   0  0  0  0          0        0         0\n  Qualcomm Stadium                     8  5  5 10          1        4         2\n  Ralph Wilson Stadium                 8  9  6 10          1        1         0\n  Raymond James Stadium                4  6  6  6          0        2         2\n  RCA Dome                             6  7  9  8          0        3         0\n  Reliant Stadium                      4  3  0  2          0        0         1\n  RFK Memorial Stadium                 1  3  4  7          2        1         0\n  Rice Stadium                         0  0  0  0          0        0         0\n  Rogers Centre                        0  0  0  0          0        0         0\n  Rose Bowl                            0  0  0  0          0        0         1\n  Seattle Kingdome                     4  6  3  5          0        0         0\n  SoFi Stadium                         1  2  2  1          1        0         1\n  Soldier Field                       12  8 11  8          0        4         0\n  Sports Authority Field at Mile High  7  2  5  4          2        4         0\n  Stanford Stadium                     0  0  0  0          0        0         0\n  State Farm Stadium                   0  0  1  1          0        0         0\n  StubHub Center                       1  1  0  1          0        0         0\n  Sun Devil Stadium                    1  1  1  1          0        0         1\n  Sun Life Stadium                     5 14  3  5          0        0         1\n  Tampa Stadium                        0  0  0  0          0        0         2\n  TCF Bank Stadium                     1  0  0  1          0        0         0\n  Texas Stadium                        5  3  9  2          0        3         0\n  Three Rivers Stadium                 1  7  4  6          3        2         0\n  TIAA Bank Field                      1  2  0  3          0        0         0\n  Tiger Stadium (LSU)                  0  0  1  0          0        0         0\n  Tottenham Hotspur Stadium            1  0  0  0          0        0         0\n  Tottenham Stadium                    0  0  0  0          0        0         0\n  Tulane Stadium                       0  0  0  0          0        0         1\n  Twickenham Stadium                   0  0  0  0          0        0         0\n  U.S. Bank Stadium                    2  0  3  1          0        1         1\n  University of Phoenix Stadium        7  6  4  5          1        1         2\n  Vanderbilt Stadium                   0  0  1  0          0        0         0\n  Veterans Stadium                     5  4  8  5          2        0         0\n  Wembley Stadium                      0  2  6  1          0        0         0\n                                     schedule_week\nstadium_name                          SuperBowl Wildcard WildCard\n  Acrisure Stadium                            0        0        0\n  Alamo Dome                                  0        0        0\n  Allegiant Stadium                           0        0        0\n  Allianz Arena                               0        0        0\n  Alltel Stadium                              0        0        0\n  Anaheim Stadium                             0        1        0\n  Arrowhead Stadium                           0        3        0\n  AT&T Stadium                                0        2        0\n  Atlanta-Fulton County Stadium               0        0        0\n  Bank of America Stadium                     0        0        0\n  Busch Memorial Stadium                      0        0        0\n  Caesars Superdome                           0        0        0\n  Candlestick Park                            0        1        0\n  CenturyLink Field                           0        4        0\n  Cinergy Field                               0        1        0\n  Cleveland Municipal Stadium                 0        1        0\n  Cowboys Stadium                             0        1        0\n  Dolphin Stadium                             0        0        0\n  Edward Jones Dome                           0        0        0\n  Empower Field at Mile High                  0        0        0\n  Estadio Azteca                              0        0        0\n  EverBank Field                              0        1        0\n  FedEx Field                                 0        2        0\n  FirstEnergy Stadium                         0        0        0\n  Ford Field                                  0        0        0\n  Foxboro Stadium                             0        0        0\n  GEHA Field at Arrowhead Stadium             0        0        0\n  Georgia Dome                                0        0        0\n  Giants Stadium                              0        5        0\n  Gillette Stadium                            0        2        0\n  Hard Rock Stadium                           0        0        0\n  Heinz Field                                 0        4        0\n  Highmark Stadium                            0        0        0\n  Houlihan's Stadium                          0        0        0\n  Houston Astrodome                           0        4        0\n  Hubert H. Humphrey Metrodome                0        2        0\n  Husky Stadium                               0        0        0\n  Jack Murphy Stadium                         0        0        0\n  Joe Robbie Stadium                          0        0        0\n  Lambeau Field                               0        4        0\n  Levi's Stadium                              0        0        0\n  Liberty Bowl Memorial Stadium               0        0        0\n  Lincoln Financial Field                     0        4        0\n  Los Angeles Memorial Coliseum               0        1        0\n  Louisiana Superdome                         0        4        0\n  LP Stadium                                  0        0        0\n  Lucas Oil Stadium                           0        2        0\n  Lumen Field                                 0        0        0\n  M&T Bank Stadium                            0        1        0\n  Mall of America Field                       0        0        0\n  Memorial Stadium (Baltimore)                0        0        0\n  Memorial Stadium (Champaign)                0        0        0\n  Memorial Stadium (Clemson)                  0        0        0\n  Mercedes-Benz Stadium                       0        0        0\n  Mercedes-Benz Superdome                     0        2        0\n  MetLife Stadium                             0        0        0\n  Metropolitan Stadium                        0        0        0\n  Mile High Stadium                           0        0        0\n  New Era Field                               0        1        0\n  Nissan Stadium                              0        0        0\n  NRG Stadium                                 1        1        0\n  Oakland Coliseum                            0        1        0\n  Orange Bowl                                 0        0        0\n  Paul Brown Stadium                          0        3        0\n  Paycor Stadium                              0        0        0\n  Pontiac Silverdome                          0        1        0\n  Pro Player Stadium                          0        0        0\n  Qualcomm Stadium                            0        3        0\n  Ralph Wilson Stadium                        0        1        0\n  Raymond James Stadium                       0        2        0\n  RCA Dome                                    0        0        0\n  Reliant Stadium                             0        0        0\n  RFK Memorial Stadium                        0        0        0\n  Rice Stadium                                0        0        0\n  Rogers Centre                               0        0        0\n  Rose Bowl                                   0        0        0\n  Seattle Kingdome                            0        2        0\n  SoFi Stadium                                0        0        0\n  Soldier Field                               0        2        0\n  Sports Authority Field at Mile High         0        1        0\n  Stanford Stadium                            0        0        0\n  State Farm Stadium                          0        0        0\n  StubHub Center                              0        0        0\n  Sun Devil Stadium                           0        0        0\n  Sun Life Stadium                            0        3        0\n  Tampa Stadium                               0        0        0\n  TCF Bank Stadium                            0        1        0\n  Texas Stadium                               0        2        0\n  Three Rivers Stadium                        0        1        0\n  TIAA Bank Field                             0        0        0\n  Tiger Stadium (LSU)                         0        0        0\n  Tottenham Hotspur Stadium                   0        0        0\n  Tottenham Stadium                           0        0        0\n  Tulane Stadium                              0        0        0\n  Twickenham Stadium                          0        0        0\n  U.S. Bank Stadium                           0        0        0\n  University of Phoenix Stadium               0        1        0\n  Vanderbilt Stadium                          0        0        0\n  Veterans Stadium                            0        5        0\n  Wembley Stadium                             0        0        0\n\n\nCode\ntbl_wind <- with(full_scores, table(weather_wind_mph>10, schedule_week, favorite_covers))\ntbl_wind \n\n\n, , favorite_covers = Cover\n\n       schedule_week\n          1  10  11  12  13  14  15  16  17  18   2   3   4   5   6   7   8   9\n  FALSE 227 160 207 198 209 216 183 198 169   9 218 193 193 175 162 191 186 176\n  TRUE   58  93  85  93  92  93  93  81  67   2  57  61  51  67  70  72  72  67\n       schedule_week\n        Conference Division Superbowl SuperBowl Wildcard WildCard\n  FALSE          5       16        13         0       18        3\n  TRUE           1        2         6         0        2        1\n\n, , favorite_covers = Not Cover\n\n       schedule_week\n          1  10  11  12  13  14  15  16  17  18   2   3   4   5   6   7   8   9\n  FALSE 251 199 201 210 205 193 222 216 184  13 244 226 223 191 190 182 183 209\n  TRUE   49  85  73  87  94  93 102 110  55   3  68  62  57  75  80  77  89  66\n       schedule_week\n        Conference Division Superbowl SuperBowl Wildcard WildCard\n  FALSE         12       20        11         1       26        0\n  TRUE           2        4         6         0        5        0\n\n\nCode\n#tbl_heat <- with(full_scores, table(weather_temperature,schedule_week, favorite_covers))\n#tbl_heat \n#decided not to include the temperature table because its huge"
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#reflection",
    "href": "posts/Final_Project_JackSniezek.html#reflection",
    "title": "Final Project",
    "section": "Reflection",
    "text": "Reflection\nI am disappointed that I could not get any graphs to work. That being said, I enjoyed working with this dataset, and am happy with what I was able to produce with the data. This was challenging data to work with, and it took me many long nights to work through some of the mutations and getting the joins to work right. I did learn quite a bit about Rstudio and found it very rewarding when my code started working correctly. I even found some satisfaction working on the graphs, when I at least got the counts working as I had hoped. I thought it was a fascinating dataset, with so many variables I could’ve used for analysis. It probably could have been done 100 different ways. In the end, the only question I had left was if the over/under data would have worked out better than the spreads."
  },
  {
    "objectID": "posts/Final_Project_JackSniezek.html#references",
    "href": "posts/Final_Project_JackSniezek.html#references",
    "title": "Final Project",
    "section": "References",
    "text": "References\nCourse textbook:\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media.\nLink: https://r4ds.had.co.nz/explore-intro.html\nR programming language:\nR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nR Packages:\nTidyverse: Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686.\nggplot2: H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nSports Betting Laws by State https://www.investopedia.com/sports-betting-laws-by-state-5219064\nRecord 31.4 Million Americans to Wager $7.61B on Super Bowl LVI https://www.americangaming.org/new/record-31-4-million-americans-to-wager-on-super-bowl-lvi/"
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html",
    "href": "posts/Final_project_NiyatiSharma.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(gganimate)\nlibrary(hrbrthemes)\nlibrary(gender)\nlibrary(tidyr)\nlibrary(viridis)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nlibrary(ggthemes)\nlibrary(RColorBrewer)\n#install.packages(\"remotes\")\n# library(genderdata)\nlibrary(ggjoy)\nlibrary(readxlsb)\n# load(\"Final_project_NiyatiSharma.RData\")\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#introduction",
    "href": "posts/Final_project_NiyatiSharma.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nPayroll data analytics is the process where we are using payroll data and implementing data analytics to it that allows organizations to understand economic trends and patterns so they can make better decisions.It extends the capability of traditional payroll services by offering a different view into payroll information.It can also be used to eliminate errors and inefficiencies in payroll processes and to drive greater pay equality and fairness of compensation across companies.\nFor my research I collected data of NYC to visualize how the City’s budget is being spent on salary and overtime pay for all municipal employees. Data is input into the City’s Personnel Management System (“PMS”) by the respective user Agencies. Each record represents the following statistics for every city employee: Agency, Last Name, First Name, Middle Initial, Agency Start Date, Work Location Borough, Job Title Description, Leave Status as of the close of the FY (June 30th), Base Salary, Pay Basis, Regular Hours Paid, Regular Gross Paid, Overtime Hours worked, Total Overtime Paid,and Total Other Compensation (i.e. lump sum and/or retro payments). This data can be used to analyze how the City’s financial resources are allocated and how much of the City’s budget is being devoted to overtime. This data should be increments of salary increases received over the course of any one fiscal year will not be reflected. All that is captured, is the employee’s final base and gross salary at the end of the fiscal year."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#reaserch-question",
    "href": "posts/Final_project_NiyatiSharma.html#reaserch-question",
    "title": "Final Project",
    "section": "Reaserch question",
    "text": "Reaserch question\nIn this project I’m going to visualize the working hours, the hourly paid, total paid and how these dimensions change over different fiscal years in my final project. I will also find the following questions :\n\nPay over the years - Distribution Basis of Pay - Annual, Daily, Hourly\nHighest & the Lowest Paying Agencies - Annually, Daily & Hourly Highest\nThe Lowest Paying Job Titles - Annually, Daily & Hourly\nPay by Location Overtime Compensation"
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#data-description",
    "href": "posts/Final_project_NiyatiSharma.html#data-description",
    "title": "Final Project",
    "section": "Data description",
    "text": "Data description\nNYC Open Data website giving the public access to thousands of public datasets, which helps to learn about New York. https://data.cityofnewyork.us/City-Government/Citywide-Payroll-Data-Fiscal-Year-/k397-673e\nCitywide Payroll Data (Fiscal Year) dataset is about salary and overtime payment of municipal employees. It aims to analyze how the city budgets have been used for payment. It could be also helpful for me to visualize the job market condition.\nThere are 2.36 million rows and 17 columns in the dataset. The dataset contains payroll records from 2019 through 2022,with the working location of New York City and other cities. It also includes the employees who are ceased as of June 30.\n1.Fiscal Year : Financial year 2.Payroll Number : Unique number for every agency/employer 3.Agency Name : The Payroll agency that the employee works for\n4.Last Name : Last name of employee 5.First Name : First name of employee 6.Middle Initial : Middle initial of employee 7.Agency Start Date : Date which employee began working for their current agency 8.Work Location Borough : Borough of employee’s primary work location 9.Title Description : Civil service title description of the employee 10.Leave Status as of Jun 30 : Status of employee as of the close of the relevant fiscal year: Active, Ceased,or On Leave 11.Base Salary : Base Salary assigned to the employee 12.Pay Basis : Lists whether the employee is paid on an hourly, per diem or annual basis 13.Regular Hours : Number of regular hours employee worked in the fiscal year 14.Regular Gross : Paid The amount paid to the employee for base salary during the fiscal year 15.OT Hours : Overtime Hours worked by employee in the fiscal year 16.Total OT Paid : Total overtime pay paid to the employee in the fiscal year 17.Total Other Pay : Includes any compensation in addition to gross salary and overtime pay, ie Differentials, lump sums, uniform allowance,meal allowance, retroactive pay increases, settlement amounts, and bonus pay, if applicable."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#step-1-reading-data-from-csv-file",
    "href": "posts/Final_project_NiyatiSharma.html#step-1-reading-data-from-csv-file",
    "title": "Final Project",
    "section": "Step 1 : Reading data from csv file",
    "text": "Step 1 : Reading data from csv file\nNOTE : I deleted unused columns from the dataset due to large size of datafile as I am not able to push the code on Git.\n\n\nCode\nNY_payroll <- read_csv(\"_data/Citywide_Payroll_Data__Fiscal_Year_ 2019-22.csv\")\nstr(NY_payroll)\n\n\nspc_tbl_ [1,048,575 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Fiscal Year          : num [1:1048575] 2019 2019 2019 2019 2019 ...\n $ Agency Name          : chr [1:1048575] \"OFFICE OF THE MAYOR\" \"OFFICE OF THE MAYOR\" \"OFFICE OF THE MAYOR\" \"OFFICE OF THE MAYOR\" ...\n $ Agency Start Date    : chr [1:1048575] \"12/31/2017\" \"1/1/2014\" \"1/25/2016\" \"1/3/2017\" ...\n $ Work Location Borough: chr [1:1048575] \"MANHATTAN\" \"MANHATTAN\" \"MANHATTAN\" \"MANHATTAN\" ...\n $ Title Description    : chr [1:1048575] \"FIRST DEPUTY MAYOR\" \"MAYOR\" \"DEPUTY MAYOR\" \"DEPUTY MAYOR\" ...\n $ Base Salary          : num [1:1048575] 282659 258750 244643 244643 244643 ...\n $ Pay Basis            : chr [1:1048575] \"per Annum\" \"per Annum\" \"per Annum\" \"per Annum\" ...\n $ Regular Hours        : num [1:1048575] 1825 1825 1825 1825 1825 ...\n $ Regular Gross Paid   : num [1:1048575] 278980 258041 246125 246036 241461 ...\n $ OT Hours             : num [1:1048575] 0 0 0 0 0 0 0 0 0 0 ...\n $ Total OT Paid        : num [1:1048575] 0 0 0 0 0 0 0 0 0 0 ...\n $ Total Other Pay      : num [1:1048575] 0 500 0 0 0 0 0 1000 0 1000 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Fiscal Year` = col_double(),\n  ..   `Agency Name` = col_character(),\n  ..   `Agency Start Date` = col_character(),\n  ..   `Work Location Borough` = col_character(),\n  ..   `Title Description` = col_character(),\n  ..   `Base Salary` = col_double(),\n  ..   `Pay Basis` = col_character(),\n  ..   `Regular Hours` = col_double(),\n  ..   `Regular Gross Paid` = col_double(),\n  ..   `OT Hours` = col_double(),\n  ..   `Total OT Paid` = col_double(),\n  ..   `Total Other Pay` = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n##Step 2: Data prepration\n1.Convert character to lower case in the data for following columns (Agency Name,Last Name,First Name,Mid Init,Work Location Borough,Title Description,Leave Status as of June 30,Pay Basis) 2.Replace white spaces with underscore for column name 3.Convert Regular_Gross_Paid,Base_Salary,OT_Hours and Total_OT_Paid to numerical data 4.Calculate the start of employment Start_Date based on Agency_Start_Date 5.Create new column Total_pay which is addition of Total_OT_Paid + Regular_Gross_Paid 6.Remove duplicate rows\n\n\nCode\n#tolower, whitespaces removal\n\nNY_payroll[c(3,4,5,6,8,9,10,12)]<-map(NY_payroll[c(3,4,5,6,8,9,10,12)], tolower) # convert into lower case\nnames(NY_payroll) <- gsub(\" \", \"_\", names(NY_payroll)) # remove spaces from column names\n\n#convert to numeric\nNY_payroll[c(11,14,16,17)] <- lapply(NY_payroll[c(11,14,16,17)], function(x) as.numeric(gsub(\"\\\\$\",\"\",x)))\n\n\nError in `NY_payroll[c(11, 14, 16, 17)]`:\n! Can't subset columns past the end.\nℹ Locations 14, 16, and 17 don't exist.\nℹ There are only 12 columns.\n\n\nCode\n#create Data columns\nNY_payroll <- NY_payroll %>%  add_column(region = \"new york\")\nNY_payroll$Start_Date<- as.Date(NY_payroll$Agency_Start_Date,format=\"%m/%d/%Y\")\nNY_payroll$Start_Year<- as.numeric(format(NY_payroll$Start_Date,'%Y'))\nNY_payroll$Total_Pay<-NY_payroll$Total_OT_Paid + NY_payroll$Regular_Gross_Paid\n\n\nError in NY_payroll$Total_OT_Paid + NY_payroll$Regular_Gross_Paid: non-numeric argument to binary operator\n\n\nCode\nNY_payroll<-NY_payroll[!duplicated(NY_payroll),]\nhead(NY_payroll)\n\n\n# A tibble: 6 × 15\n  Fiscal_Year Agency_N…¹ Agenc…² Work_…³ Title…⁴ Base_…⁵ Pay_B…⁶ Regul…⁷ Regul…⁸\n        <dbl> <chr>      <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n1        2019 OFFICE OF… 12/31/… manhat… first … 282659  per An… 1825    278980…\n2        2019 OFFICE OF… 1/1/20… manhat… mayor   258750  per An… 1825    258041…\n3        2019 OFFICE OF… 1/25/2… manhat… deputy… 244643  per An… 1825    246124…\n4        2019 OFFICE OF… 1/3/20… manhat… deputy… 244643  per An… 1825    246035…\n5        2019 OFFICE OF… 3/12/2… manhat… deputy… 244643  per An… 1825    241461…\n6        2019 OFFICE OF… 1/1/20… manhat… chief … 236750  per An… 1825    238149…\n# … with 6 more variables: OT_Hours <chr>, Total_OT_Paid <dbl>,\n#   Total_Other_Pay <chr>, region <chr>, Start_Date <date>, Start_Year <dbl>,\n#   and abbreviated variable names ¹​Agency_Name, ²​Agency_Start_Date,\n#   ³​Work_Location_Borough, ⁴​Title_Description, ⁵​Base_Salary, ⁶​Pay_Basis,\n#   ⁷​Regular_Hours, ⁸​Regular_Gross_Paid\n\n\nWhen looking at the salary’s change (Regular_Gross_Paid) over time (or based on another feature), the mean and median over a given group will be used. However, this distribution might be skewed because of some other categories.\n\n\nCode\nNY_payroll %>% select(Pay_Basis,Regular_Hours,Regular_Gross_Paid,Total_Other_Pay) %>% head(10)\n\n\n# A tibble: 10 × 4\n   Pay_Basis Regular_Hours Regular_Gross_Paid Total_Other_Pay\n   <chr>     <chr>         <chr>              <chr>          \n 1 per Annum 1825          278980.28          0              \n 2 per Annum 1825          258041.16          500            \n 3 per Annum 1825          246124.64          0              \n 4 per Annum 1825          246035.81          0              \n 5 per Annum 1825          241461.21          0              \n 6 per Annum 1825          238149.78          0              \n 7 per Annum 1825          237517.73          0              \n 8 per Annum 1825          231522.96          1000           \n 9 per Annum 1825          230755.36          0              \n10 per Annum 1825          222490.58          1000           \n\n\nThere are 4 types of Pay Basis :\n\n\nCode\nNY_payroll %>% dplyr::group_by(Pay_Basis) %>% summarize(count=n())\n\n\n# A tibble: 4 × 2\n  Pay_Basis        count\n  <chr>            <int>\n1 per Annum       620443\n2 per Day         226495\n3 per Hour        114760\n4 Prorated Annual   4306\n\n\nCode\np2 <- NY_payroll %>% \n  group_by(Pay_Basis) %>% # Variable to be transformed\n  count() %>% \n  ungroup() %>% \n  mutate(perc = `n` / sum(`n`)) %>% \n  arrange(perc) %>%\n  mutate(labels = scales::percent(perc)) %>%\n  ggplot(aes(x = \"\", y = perc, fill = Pay_Basis)) +\n  ggtitle(\"Districution according to pay basis\") +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = labels),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"pay basis\")) +\n  scale_fill_viridis_d() +\n  coord_polar(theta = \"y\") + \n  theme_void()\np2\n\n\n\n\n\nFrom the following bar plot, we see a distribution in terms of number of employee wrt to pay basis.\nAs we could see from the visualisation that, 59% employee is employed on per annum basis, 28% on per day basis and remaining on per day and prorated annual. Hence, we understand that 59% employee are permanent employees and remaining work as part time.\nAlso there are 2 types of salaries :\nBase_Salary Regular_Gross_Paid\nThe difference between both is that Regular Gross Pay is the total amount of money you get before taxes or other deductions are subtracted from your salary, whereas Base.Salary is a fixed amount of money paid to an employee by an employer in return for work performed. Base salary does not include benefits, bonuses or any other potential compensation from an employer.\n\n\nCode\nNY_payroll$Fiscal_Year <- as.factor(NY_payroll$Fiscal_Year)\nfunc <- function(x) sprintf(\"%0.5f\", x)\nplot <- ggplot(data=NY_payroll, aes(x=Total_Pay, group=Fiscal_Year, fill=Fiscal_Year)) +\ngeom_density(adjust=1.5, alpha=.4) +\ntheme_ipsum()\n\nplot + scale_y_continuous(labels = func) + scale_x_continuous(labels = func)\n\n\nError in `geom_density()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'Total_Pay' not found\n\n\nAs we know that total pay is sum of Regular Pay and Total overtime pay. Now, we want to see how the total pay is distributed wrt to no. Of years using a density plot.\nFrom the density plot, we observe that total pay is distributed from 0 to 200000, where the maximum or most frequent payment resides in first 12.5% of the data. The density plot looks right skewed. We see similar behaviour for all four years."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#working-hours",
    "href": "posts/Final_project_NiyatiSharma.html#working-hours",
    "title": "Final Project",
    "section": "Working hours",
    "text": "Working hours\n\n\nCode\nfunc <- function(x) sprintf(\"%d\", x)\nNY_payroll$Regular_Hours_categories <- cut(as.numeric(NY_payroll$Regular_Hours), c(min(NY_payroll$Regular_Hours),0,500,1000,1500,2000,2500,max(NY_payroll$Regular_Hours)), labels = c(1:7), include.lowest = TRUE)\n#NY_payroll$Regular_Hours_categories <- as.factor(NY_payroll$Regular_Hours)\np <- ggplot(data=NY_payroll, aes(x=Regular_Hours_categories, fill=Regular_Hours_categories)) +\ngeom_bar() +\ngeom_text(stat='count', aes(label=..count..), vjust=-1,size=7)+\ntheme(text = element_text(size=17))\np\n\n\n\n\n\nWe already saw distribution plot and bar plot for the Total Pay and Pay Basis distribution. Now, if we want to see how many employee distributed across different pay ranges, we can use countplot function or frequency plot. The pay ranges are divided into 7 categories- (minm,0), (0,500), (500,1000), (1000,1500), (1500,2000), (2000,2500), (2500, maxm). We could see from following plot that maximum employees lies in (minm,0) amount. We see similar behavior in density plot where we asserted that maximum employees lies between 0 to 12.5% of total pay."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#average-total-pay-for-every-fiscal-year-for-different-pay-basis",
    "href": "posts/Final_project_NiyatiSharma.html#average-total-pay-for-every-fiscal-year-for-different-pay-basis",
    "title": "Final Project",
    "section": "Average total pay for every fiscal year for different pay basis",
    "text": "Average total pay for every fiscal year for different pay basis\n\n\nCode\ndat1 = NY_payroll%>% na.omit() %>%\n  filter( Pay_Basis=='per annum') %>%\n    group_by(Fiscal_Year) %>% summarise(mean_total_pay = mean(Total_Pay))\n\n\nError in `summarise()`:\n! Problem while computing `mean_total_pay = mean(Total_Pay)`.\nCaused by error in `mean()`:\n! object 'Total_Pay' not found\n\n\nCode\np <- ggplot(\n  dat1,\n  aes(y =  as.numeric(mean_total_pay), x = as.factor(Fiscal_Year), color = factor(Fiscal_Year), group = 1)) +\n  geom_point(size=1.5) +\n  scale_color_viridis_d() +\n  scale_linetype_manual() +\n  labs(x = \"Fiscal Year\", y = \"Average Total Pay\", title = \"Average Total Pay Vs Year (per annum basis)\") +\n  theme(legend.position = \"top\")\n\n\nError in ggplot(dat1, aes(y = as.numeric(mean_total_pay), x = as.factor(Fiscal_Year), : object 'dat1' not found\n\n\nCode\np +  geom_line(color='blue')\n\n\nError in `geom_line()`:\n! Problem while setting up geom.\nℹ Error occurred in the 3rd layer.\nCaused by error in `compute_geom_1()`:\n! `geom_line()` requires the following missing aesthetics: y\n\n\n\n\nCode\ndat1 = NY_payroll%>% na.omit() %>%\n  filter( Pay_Basis=='per hour' | Pay_Basis=='per day') %>%\n    group_by(Fiscal_Year) %>% summarise(mean_total_pay = mean(Total_Pay))\n\n\nError in `summarise()`:\n! Problem while computing `mean_total_pay = mean(Total_Pay)`.\nCaused by error in `mean()`:\n! object 'Total_Pay' not found\n\n\nCode\np <- ggplot(\n  dat1,\n  aes(y =  as.numeric(mean_total_pay), x = as.factor(Fiscal_Year), color = factor(Fiscal_Year), group = 1)) +\n  geom_point(size=1.5) +\n  scale_color_viridis_d() +\n  scale_linetype_manual() +\n  labs(x = \"Fiscal Year\", y = \"Average Total Pay\", title = \"Average Total Pay Vs Year (per day & per hour basis)\") +\n  theme(legend.position = \"top\")\n\n\nError in ggplot(dat1, aes(y = as.numeric(mean_total_pay), x = as.factor(Fiscal_Year), : object 'dat1' not found\n\n\nCode\np +  geom_line(color='blue')\n\n\nError in `geom_line()`:\n! Problem while setting up geom.\nℹ Error occurred in the 3rd layer.\nCaused by error in `compute_geom_1()`:\n! `geom_line()` requires the following missing aesthetics: y\n\n\nFrom the dataset, we could see that we have two different types of pay basis - permanent work pay and temporary work pay. For this, we can find average total pay vs year plot for different pay basis. We can see from the above two plots that Average Total Pay was maximum on 2019 for ‘per annum’ basis and for ‘per day & per hour’ basis on 2022. Similarly, we see that Average Total Pay was minimum on 2020 for ‘per annum’ basis and for ‘per day & per hour’ basis on 2021. This information could useful if we want to find more information on pay average per year for different types of job."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#average-working-hours-for-different-years",
    "href": "posts/Final_project_NiyatiSharma.html#average-working-hours-for-different-years",
    "title": "Final Project",
    "section": "Average working hours for different years :",
    "text": "Average working hours for different years :\nIf we want to see how average working hours for different years looks like, we could use a scatter plot to deduce a relationship between average working hours and fiscal year. From below plot, we could see that, the average working hour increased from 2019 to 2020 and then remain constant for 2021. However, it declined during 2022. There could be multiple unwarranted reasons for this trend which could be deduced from the given plot.\n\n\nCode\ndat1 = NY_payroll%>% na.omit() %>%\n    group_by(Fiscal_Year) %>% summarise(Mean_Regular_Hours = mean(Regular_Hours))\np <- ggplot(\n  dat1,\n  aes(y =  as.numeric(Mean_Regular_Hours), x = as.factor(Fiscal_Year), color = factor(Fiscal_Year), group = 1)) +\n  geom_point(size=1.5) +\n  scale_color_viridis_d() +\n  scale_linetype_manual() +\n  labs(x = \"Fiscal Year\", y = \"Average Regular Hours\") +\n  theme(legend.position = \"top\")\np +  geom_line(color='blue')\n\n\n\n\n\n\n\nCode\nNY_payroll %>% dplyr::filter(Pay_Basis=='per annum') %>% group_by(Fiscal_Year) %>% \n  summarise(meanPaid = mean(Regular_Gross_Paid),\n            meanBaseSalary = mean(Base_Salary),\n            medianPaid = median(Regular_Gross_Paid),\n            medianBaseSalary = median(Base_Salary)) %>% reshape2::melt(id=c('Fiscal_Year')) %>% \n  ggplot(aes(x=factor(Fiscal_Year),y=value,group=variable)) + \n  geom_line(aes(color=variable),size=2,alpha=.5) + geom_point(size=1.5) + \n  scale_color_manual(name=\"\",values = c(\"green\",\"orange\",\"darkred\",\"red\")) + theme_fivethirtyeight() + ggtitle('Mean, Median of Regular Gross Paid and Base Salary($US) per annum over years') +\ntheme(plot.title = element_text(size = 12))\n\n\n\n\n\n\n\nCode\nNY_payroll %>% dplyr::filter(Pay_Basis=='per hour') %>% group_by(Fiscal_Year) %>% \n  summarise(meanPaid = mean(Regular_Gross_Paid),\n            meanBaseSalary = mean(Base_Salary),\n            medianPaid = median(Regular_Gross_Paid),\n            medianBaseSalary = median(Base_Salary)) %>% reshape2::melt(id=c('Fiscal_Year')) %>% \n  ggplot(aes(x=factor(Fiscal_Year),y=value,group=variable)) + \n  geom_line(aes(color=variable),size=2,alpha=.5) + geom_point(size=1.5) + \n  scale_color_manual(name=\"\",values =  c(\"green\",\"orange\",\"blue\",\"red\")) + theme_fivethirtyeight() + ggtitle('Mean, Median of Regular Gross Paid and Base Salary($US) per day over years') +\n  theme(plot.title = element_text(size = 12) )\n\n\n\n\n\nTo see the statistical analysis of our salary data we could see the mean and median distribution. We know from statistics that if the mean is greater than median than we have right skewed that else we have a left skewed data in vice versa. From the below plot we see, that for all years, the mean is higher than the median suggesting the distribution is right skewed. Now, if we see our distribution plot, we mentioned that the distribution is right skewed. Hence, we explain the distribution plot using statistics.\nNow, if we see the trend for the different years from 2019-2022. The mean is increasing from 2019 to 2022 suggesting that the salary of the employee has been increased. However, if we see the median which first decreased for year 2020 and then increased for 2021 and finally decreased for 2022, suggesting that salary increment is not consistent over the years. Now, comparing both mean and median together we could assert that for year 2020, the salary is increased only for few population on the higher end of the distribution, leading to lower median and higher mean. Now for year 2021, the result suggests that salary has been increased for more larger population as compared to previous year, concluding increase in median along with mean.We see that the median is greater than mean for 2021 suggested that data is left skewed. Finally for year 2022, again we see the similar trend as 2020, that salary has been increased but only for few population leading to lesser median and higher mean.\nFor the per hour median and mean of Regular Gross Pay and Base Salary, the gross pay is the highest in the mean and larger than the median suggesting that there is non-uniformity in the data. Few employees have high hourly pay (possible outliers) causing the difference with median gross pay. The base salary suggests that its not applicable to 50% of the employees as median is tending towards zero and mean pay pretty large than median suggesting few population have large hourly base salary causing higher base salary."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#highest-the-lowest-paying-agencies",
    "href": "posts/Final_project_NiyatiSharma.html#highest-the-lowest-paying-agencies",
    "title": "Final Project",
    "section": "Highest & the Lowest Paying Agencies",
    "text": "Highest & the Lowest Paying Agencies"
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#jobs-title-that-pay-the-most-least-money-in-nyc",
    "href": "posts/Final_project_NiyatiSharma.html#jobs-title-that-pay-the-most-least-money-in-nyc",
    "title": "Final Project",
    "section": "Jobs Title that pay the Most & Least money in NYC",
    "text": "Jobs Title that pay the Most & Least money in NYC\nNow, we do same analysis for ‘Job Title’ which we did in the previous section. We want to find which job title has maximum or minimum average total pay for different pay basis. We used bar plot for descending or ascending average total pay for our analysis."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#average-pay-by-location",
    "href": "posts/Final_project_NiyatiSharma.html#average-pay-by-location",
    "title": "Final Project",
    "section": "Average pay by Location",
    "text": "Average pay by Location\nWork Location Borough tells us the area that a particular agency in NYC belongs to so it is interesting to know which one of these areas have the highest pay!\n\n\nCode\n# Using number of observation per group\nfunc <- function(x) sprintf(\"%d\", x)\nplot <- NY_payroll %>% na.omit() %>% ggplot(aes(x = Work_Location_Borough, y = Total_Pay)) +\n       stat_boxplot(geom = \"errorbar\", # Boxplot with error bars \n                    width = 0.2) +\n       geom_boxplot(fill = 'green', colour = \"blue\", # Colors\n                    alpha = 0.9, outlier.colour = \"red\") +\n       scale_y_continuous(name = \"Total Pay\", labels = func) +  # Continuous variable label\n       scale_x_discrete(name = \"Work Location Borough\") +      # Group label\n       ggtitle(\"Boxplot by Work Location Borough\") + # Plot title\n       theme(axis.line = element_line(colour = \"black\", # Theme customization\n                                      size = 0.25), axis.text.y = element_text(size = 8 ))\n\nplot\n\n\nError in `stat_boxplot()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'Total_Pay' not found\n\n\nIn order to find out the total pay distribution wrt location, we could use box plot distribution for Total Pay VS Location plot. Now, if we the box plot, then for most of the locations we see there are outliers in the data suggesting that total pay have extreme values I.e few employees have relevantly more total pay than remaining. Moreover, if we compare among different locations, then we could see that Washington have higher median than others suggesting that we have more uniformity in the total pay as compared to other locations. However, if we want to find out which location has higher pay than we see that Manhattan has higher pay as compared to because of outliers. Also, if we see that most of the box plot are either right or left skewed with outliers suggesting non uniformity in the total pay distribution. However, for few cities like Ulster, Westchester, Duchess etc looks to have normal distribution suggesting uniform pay distribution. Therefore, we can comment on the uniformity of pay distribution or average pay distribution or city with higher or lower pay distribution using box plot."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#agency-name-which-has-highest-overtime-hours-and-over-time-salary",
    "href": "posts/Final_project_NiyatiSharma.html#agency-name-which-has-highest-overtime-hours-and-over-time-salary",
    "title": "Final Project",
    "section": "Agency name which has highest overtime hours and over time salary :",
    "text": "Agency name which has highest overtime hours and over time salary :\nWhen we deal with payroll data, our one of the primary point of interest is overtime payment. Therefore, we want to deduce a relationship between ‘Agency Name’ and ‘overtime’ hours or payment.\n\n\nCode\ndat1 = NY_payroll %>% na.omit() %>% group_by(Agency_Name) %>% summarise(Mean_OT_Hours = mean(OT_Hours))\nnewdata <- dat1[order(dat1$Mean_OT_Hours, decreasing = TRUE),]\nnewdata <- newdata[0:10,]\n\nnewdata %>% mutate(Agency_Name = fct_reorder(Agency_Name, Mean_OT_Hours)) %>% ggplot( aes(y =  as.numeric(Mean_OT_Hours), x = as.factor(Agency_Name),fill=Agency_Name)) +\ngeom_bar(stat='identity',width=0.5,size=.3,color='blue') +\ncoord_flip() + \ntheme_fivethirtyeight() + \ntheme(axis.text.y= element_text(size=6.6), axis.text.x= element_text(size=8), legend.text = element_text(size=5))\n\n\n\n\n\nFrom the above graph, we see that the ‘fire department’ has the most overtime working hours followed by ‘board of election’ than ‘department of correction’ as top 3 agencies with overtime working hours.\n\n\nCode\ndat1 = NY_payroll %>% na.omit() %>% group_by(Agency_Name) %>% summarise(Mean_OT_paid = mean(Total_OT_Paid))\nnewdata <- dat1[order(dat1$Mean_OT_paid, decreasing = TRUE),]\nnewdata <- newdata[0:10,]\n\nnewdata %>% mutate(Agency_Name = fct_reorder(Agency_Name, desc(Mean_OT_paid))) %>% ggplot( aes(y =  as.numeric(Mean_OT_paid), x = as.factor(Agency_Name),fill=Agency_Name)) +\ngeom_bar(stat='identity',width=0.3,size=.2,color='blue') +\ntheme_fivethirtyeight() + \ntheme(axis.text.y= element_text(size=6.6), axis.text.x= element_text(angle=90,size=6), legend.text = element_text(size=5)) +\ntheme(panel.grid = element_line(size = 0.5))\n\n\n\n\n\nFrom the above plot, average overtime payment is displayed against different agencies. We have sorted our data in descending order to capture the maximum overtime payment. We see that ‘fire department’ followed by ‘department of sanitation’ than ‘department of correction’."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#distribution-of-average-total-payment-based-on-new-york-counties-for-different-years",
    "href": "posts/Final_project_NiyatiSharma.html#distribution-of-average-total-payment-based-on-new-york-counties-for-different-years",
    "title": "Final Project",
    "section": "Distribution of average total payment based on new york counties for different years",
    "text": "Distribution of average total payment based on new york counties for different years\nFinally, we could use ‘New York’ map to show the distribution of average total pay across all counties. We used theme_map() for ‘new york’ state and used light shade to display lower total pay and darker shade for higher total pay for all fiscal years. This map plot gives visual idea of maximum and minimum average pay for a location which more descriptive on comparison to bar that could be used in place of bar plot for same relationship.\nWe see that the same county has darker shade and light shade across all fiscal years. Thus, suggesting that these are high paying areas with better jobs. However, we might need more evidence to corroborate our statement.\n\n\nCode\nny_counties <- map_data('county') %>%\n    as.data.frame() %>%\n    filter(region == 'new york')\n\nNY_payroll %>%\n      filter(Fiscal_Year==2019 & region == \"new york\") %>%\n      group_by(region, subregion = tolower(Work_Location_Borough)) %>%\n      dplyr::summarise(mean_total_pay = mean(Total_Pay, na.rm = TRUE)) %>%\n      right_join(ny_counties, by = c('region', 'subregion')) %>%\n      ggplot(aes(x = long, y = lat, group = group, fill = mean_total_pay)) + \n      geom_polygon() + \n      geom_path(color = 'white', size = 0.1) + \n      scale_fill_continuous(low = \"orange\", \n                            high = \"darkred\",\n                            name = 'Total Pay (dollar)') + \n      theme_map() + \n      theme() + \n      coord_map('albers', lat0=30, lat1=40) + \n      ggtitle(paste0(\"Average Total Pay for NY Employees for 2019\")) + \n      theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `dplyr::summarise()`:\n! Problem while computing `mean_total_pay = mean(Total_Pay, na.rm =\n  TRUE)`.\nℹ The error occurred in group 1: region = \"new york\", subregion = \"albany\".\nCaused by error in `mean()`:\n! object 'Total_Pay' not found\n\n\n\n\nCode\nny_counties <- map_data('county') %>%\n    as.data.frame() %>%\n    filter(region == 'new york')\n\n NY_payroll %>%\n      filter(Fiscal_Year==2020 & region == \"new york\") %>%\n      group_by(region, subregion = tolower(Work_Location_Borough)) %>%\n      summarize(mean_total_pay = mean(Total_Pay, na.rm = TRUE)) %>%\n      right_join(ny_counties, by = c('region', 'subregion')) %>%\n      ggplot(aes(x = long, y = lat, group = group, fill = mean_total_pay)) + \n      geom_polygon() + \n      geom_path(color = 'white', size = 0.1) + \n      scale_fill_continuous(low = \"blue\", \n                            high = \"darkred\",\n                            name = 'Total Pay (dollar)') + \n      theme_map() + \n      theme() + \n      coord_map('albers', lat0=30, lat1=40) + \n      ggtitle(paste0(\"Average Total Pay for NY Employees for 2020\")) + \n      theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `summarize()`:\n! Problem while computing `mean_total_pay = mean(Total_Pay, na.rm =\n  TRUE)`.\nℹ The error occurred in group 1: region = \"new york\", subregion = \"albany\".\nCaused by error in `mean()`:\n! object 'Total_Pay' not found\n\n\n\n\nCode\nny_counties <- map_data('county') %>%\n    as.data.frame() %>%\n    filter(region == 'new york')\n\n NY_payroll %>%\n      filter(Fiscal_Year==2021 & region == \"new york\") %>%\n      group_by(region, subregion = tolower(Work_Location_Borough)) %>%\n      summarize(mean_total_pay = mean(Total_Pay, na.rm = TRUE)) %>%\n      right_join(ny_counties, by = c('region', 'subregion')) %>%\n      ggplot(aes(x = long, y = lat, group = group, fill = mean_total_pay)) + \n      geom_polygon() + \n      geom_path(color = 'white', size = 0.1) + \n      scale_fill_continuous(low = \"green\", \n                            high = \"red\",\n                            name = 'Total Pay (dollar)') + \n      theme_map() + \n      theme() + \n      coord_map('albers', lat0=30, lat1=40) + \n      ggtitle(paste0(\"Average Total Pay for NY Employees for 2021\")) + \n      theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `summarize()`:\n! Problem while computing `mean_total_pay = mean(Total_Pay, na.rm =\n  TRUE)`.\nCaused by error in `mean()`:\n! object 'Total_Pay' not found\n\n\n\n\nCode\nny_counties <- map_data('county') %>%\n    as.data.frame() %>%\n    filter(region == 'new york')\n\n NY_payroll %>%\n      filter(Fiscal_Year==2022 & region == \"new york\") %>%\n      group_by(region, subregion = tolower(Work_Location_Borough)) %>%\n      summarize(mean_total_pay = mean(Total_Pay, na.rm = TRUE)) %>%\n      right_join(ny_counties, by = c('region', 'subregion')) %>%\n      ggplot(aes(x = long, y = lat, group = group, fill = mean_total_pay)) + \n      geom_polygon() + \n      geom_path(color = 'white', size = 0.1) + \n      scale_fill_continuous(low = \"pink\", \n                            high = \"purple\",\n                            name = 'Total Pay (dollar)') + \n      theme_map() + \n      theme() + \n      coord_map('albers', lat0=30, lat1=40) + \n      ggtitle(paste0(\"Average Total Pay for NY Employees for 2022\")) + \n      theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `summarize()`:\n! Problem while computing `mean_total_pay = mean(Total_Pay, na.rm =\n  TRUE)`.\nCaused by error in `mean()`:\n! object 'Total_Pay' not found"
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#conclusions",
    "href": "posts/Final_project_NiyatiSharma.html#conclusions",
    "title": "Final Project",
    "section": "Conclusions :",
    "text": "Conclusions :\nThroughout this project, we used different visualizations techniques using ggplot library to demonstrate the different data behaviors. We used ‘NYC Payroll’ dataset from which we explained overall pay distribution across different agencies, regions and pay basis types. We used pie chart, bar plot, box plot, distribution curve, line plot, theme map etc, for our overall data analysis. The choice of plot was picked based on the nature of the data and the analysis we wanted to deduce.\nFrom the above analysis, we saw how the working hours, per basis type, total payment and average total pay varies with fiscal year or agencies type. We also saw statistical analysis using mean and median of salaries wrt fiscal year to see the trend on the increment of total pay over the years. Furthermore, we saw the agency which pays most and least overtime salary and the agency which have maximum overtime hours. These are important aspects when we see payment data. Lastly, we used ‘New York’ state map to display the Total Payment distribution across different counties. Therefore, we explained how using different data visualizations we can explain the data and its behavior."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#further-thoughts",
    "href": "posts/Final_project_NiyatiSharma.html#further-thoughts",
    "title": "Final Project",
    "section": "Further thoughts :",
    "text": "Further thoughts :\nIn this project, I added all possible visualizations which could be interpreted from the dataset. The data contains information about NYC payroll. Hence, columns like total pay, gross pay, special allowance, counties, job types, job title, payment basis, overtime hours etc. create meaningful significance. I tried to use different plots which I felt explains the data property more closely than any other plot. The map plot where I showed the distribution between average total pay and counites could be improved. We could add more information like county name and data set could include all county information for diverse results. Overall I feel that I explained the dataset with loads of data visualisation tools to explain the trend, behavior and relationship between two or multiple variables."
  },
  {
    "objectID": "posts/Final_project_NiyatiSharma.html#references",
    "href": "posts/Final_project_NiyatiSharma.html#references",
    "title": "Final Project",
    "section": "References",
    "text": "References\n\nhttps://data.cityofnewyork.us/City-Government/Citywide-Payroll-Data-Fiscal-Year-/k397-673e\nhttps://studentwork.prattsi.org/infovis/projects/visualizing-the-citywide-payroll/\nhttps://www.kaggle.com/code/jonathanbouchet/nyc-payroll-data/report\n\n\n\nCode\n#save.image(\"Final_project_NiyatiSharma.RData\")"
  },
  {
    "objectID": "posts/Final_Project_Tejaswini_Ketineni.html",
    "href": "posts/Final_Project_Tejaswini_Ketineni.html",
    "title": "Final_Project",
    "section": "",
    "text": "What is Customer Segmentation and it’s prospective use ?\nCustomer segmentation is the process in which we divide the groups based on the common characteristics.Our final goal is to find the customers who has the greatest potential to let the firm grow and retrieve maximum gains.The insights that we draw from the customer segmentation helps us design a proper segmentation strategy.Customer segmentation turns out beneficial for the following reasons :\n1.We can devise a highly effective marketing strategy. 2.We can improve customer retention. 3.We can improve the metrics of conversion. 4.Enhanced product development.\n\n\nReading the Data\nLoading required packages and data.\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(naniar)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(corpus)\nlibrary(tm)\nlibrary(tmap)\nlibrary(treemapify)\nlibrary(wordcloud)\nlibrary(dlookr)\nlibrary(highcharter)\nlibrary(countrycode)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nread the data extracted from kaggle which is from UCI machine learning repository.\n\n#reading in the dataframe and checking the answers\nOnline_retail<-read.csv(\"_data/Online_Retail_v2.csv\", header=TRUE)\n\n\nhead(Online_retail)\n\n  Invoice    StockCode                Description Quantity    InvoiceDate\n1 A563186            B            Adjust bad debt        1 12/08/11 14:51\n2 A563187            B            Adjust bad debt        1 12/08/11 14:52\n3  550193         PADS PADS TO MATCH ALL CUSHIONS        1  15/04/11 9:27\n4  561226         PADS PADS TO MATCH ALL CUSHIONS        1 26/07/11 10:13\n5  568200         PADS PADS TO MATCH ALL CUSHIONS        1 25/09/11 14:58\n6  568375 BANK CHARGES               Bank Charges        1 26/09/11 17:01\n   UnitPrice CustomerID        Country\n1 -11062.060         NA United Kingdom\n2 -11062.060         NA United Kingdom\n3      0.001      13952 United Kingdom\n4      0.001      15618 United Kingdom\n5      0.001      16198 United Kingdom\n6      0.001      13405 United Kingdom\n\n\n\ndf<-Online_retail\n\n\ncolnames(Online_retail)\n\n[1] \"Invoice\"     \"StockCode\"   \"Description\" \"Quantity\"    \"InvoiceDate\"\n[6] \"UnitPrice\"   \"CustomerID\"  \"Country\"    \n\n\n\ndim(Online_retail)\n\n[1] 539394      8\n\n\nThe data set contains of about 539394 rows and 8 columns.\n\n\nCleaning the data and performing Feature Engineering.\n\nmiss_var_summary(Online_retail,order=TRUE)\n\n# A tibble: 8 × 3\n  variable    n_miss pct_miss\n  <chr>        <int>    <dbl>\n1 CustomerID  132605     24.6\n2 Invoice          0      0  \n3 StockCode        0      0  \n4 Description      0      0  \n5 Quantity         0      0  \n6 InvoiceDate      0      0  \n7 UnitPrice        0      0  \n8 Country          0      0  \n\ngg_miss_var(Online_retail)\n\n\n\n\nWe see that the customer ID’s are missing we now see the structure of the whole data\n\nsummary(Online_retail)\n\n   Invoice           StockCode         Description           Quantity        \n Length:539394      Length:539394      Length:539394      Min.   :-80995.00  \n Class :character   Class :character   Class :character   1st Qu.:     1.00  \n Mode  :character   Mode  :character   Mode  :character   Median :     3.00  \n                                                          Mean   :     9.85  \n                                                          3rd Qu.:    10.00  \n                                                          Max.   : 80995.00  \n                                                                             \n InvoiceDate          UnitPrice           CustomerID       Country         \n Length:539394      Min.   :-11062.06   Min.   :12346    Length:539394     \n Class :character   1st Qu.:     1.25   1st Qu.:13954    Class :character  \n Mode  :character   Median :     2.08   Median :15152    Mode  :character  \n                    Mean   :     4.63   Mean   :15288                      \n                    3rd Qu.:     4.13   3rd Qu.:16791                      \n                    Max.   : 38970.00   Max.   :18287                      \n                                        NA's   :132605                     \n\n\nOne notable observation we can see here is that the Quantity has negative value as minimum. we will look into this in further steps.\n\nn_distinct(Online_retail$CustomerID)\n\n[1] 4372\n\n\n\nn_distinct(Online_retail$Description)\n\n[1] 4042\n\n\n\nsapply(Online_retail,function(x)sum(is.null(x)))\n\n    Invoice   StockCode Description    Quantity InvoiceDate   UnitPrice \n          0           0           0           0           0           0 \n CustomerID     Country \n          0           0 \n\n\n\nsapply(Online_retail,function(x)sum(is.na(x)))\n\n    Invoice   StockCode Description    Quantity InvoiceDate   UnitPrice \n          0           0           0           0           0           0 \n CustomerID     Country \n     132605           0 \n\n\nwe see that there are 132605 missing values in CustomerID\n\nOnline_retail<-Online_retail%>%na.omit(Online_retail$CustomerID)\nOnline_retail$Description<-replace_na(Online_retail$Description, \"No-info\")\n\n\nprint(summarytools::dfSummary(Online_retail,varnumbers = FALSE,plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\nData Frame Summary\nOnline_retail\nDimensions: 406789 x 8\n  Duplicates: 5225\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Invoice\n[character]\n      1. 5763392. 5791963. 5807274. 5782705. 5735766. 5676567. 5671838. 5756079. 57144110. 570488[ 22176 others ]\n      542(0.1%)533(0.1%)529(0.1%)442(0.1%)435(0.1%)421(0.1%)399(0.1%)377(0.1%)364(0.1%)353(0.1%)402394(98.9%)\n      \n      0\n(0.0%)\n    \n    \n      StockCode\n[character]\n      1. 85123A2. 224233. 85099B4. 848795. 475666. 207257. 227208. POST9. 2072710. 22197[ 3674 others ]\n      2077(0.5%)1904(0.5%)1662(0.4%)1418(0.3%)1415(0.3%)1359(0.3%)1232(0.3%)1196(0.3%)1126(0.3%)1118(0.3%)392282(96.4%)\n      \n      0\n(0.0%)\n    \n    \n      Description\n[character]\n      1. WHITE HANGING HEART T-LIG2. REGENCY CAKESTAND 3 TIER3. JUMBO BAG RED RETROSPOT4. ASSORTED COLOUR BIRD ORNA5. PARTY BUNTING6. LUNCH BAG RED RETROSPOT7. SET OF 3 CAKE TINS PANTRY8. POSTAGE9. LUNCH BAG  BLACK SKULL.10. PACK OF 72 RETROSPOT CAKE[ 3886 others ]\n      2070(0.5%)1904(0.5%)1662(0.4%)1418(0.3%)1415(0.3%)1358(0.3%)1232(0.3%)1196(0.3%)1126(0.3%)1080(0.3%)392328(96.4%)\n      \n      0\n(0.0%)\n    \n    \n      Quantity\n[integer]\n      Mean (sd) : 12 (247.9)min ≤ med ≤ max:-80995 ≤ 5 ≤ 80995IQR (CV) : 10 (20.6)\n      435 distinct values\n      \n      0\n(0.0%)\n    \n    \n      InvoiceDate\n[character]\n      1. 14/11/11 15:272. 28/11/11 15:543. 05/12/11 17:174. 23/11/11 13:395. 31/10/11 14:096. 21/09/11 14:407. 10/11/11 12:378. 17/10/11 13:319. 10/10/11 17:1210. 24/10/11 17:07[ 20446 others ]\n      543(0.1%)534(0.1%)530(0.1%)444(0.1%)436(0.1%)422(0.1%)378(0.1%)365(0.1%)354(0.1%)353(0.1%)402430(98.9%)\n      \n      0\n(0.0%)\n    \n    \n      UnitPrice\n[numeric]\n      Mean (sd) : 3.5 (69.3)min ≤ med ≤ max:0 ≤ 2 ≤ 38970IQR (CV) : 2.5 (20)\n      619 distinct values\n      \n      0\n(0.0%)\n    \n    \n      CustomerID\n[integer]\n      Mean (sd) : 15287.8 (1713.6)min ≤ med ≤ max:12346 ≤ 15152 ≤ 18287IQR (CV) : 2837 (0.1)\n      4371 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Country\n[character]\n      1. United Kingdom2. Germany3. France4. EIRE5. Spain6. Netherlands7. Belgium8. Switzerland9. Portugal10. Australia[ 27 others ]\n      361854(89.0%)9493(2.3%)8490(2.1%)7483(1.8%)2532(0.6%)2367(0.6%)2069(0.5%)1876(0.5%)1480(0.4%)1256(0.3%)7889(1.9%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\n\nplot_outlier(Online_retail,UnitPrice,col=\"#AE4371\")\n\n\n\n\n\nplot_outlier(Online_retail,Quantity,col=\"#E6AB02\")\n\n\n\n\nThere are missing values present in CustomerID’s and if the description also has null values we replace the null values with “No info” and we simply omit the customerID.Data is highly skewed with outliers and when we remove them, it looks like normally distributed.\nFor quantity as we have seen above while calculating the summary, we see that there are negative values in the data. Instead of directly removing them as the outlier plot showed those values, there could be a possibility that negative and positive values of quantity could be inter-connected.To be elaborate, there is a high possibility that those outliers could be cancelled or reverted orders and hence got negative values.Let’s analyse in detail.\n\ncheck_quantity<-Online_retail%>%filter(Quantity<0)\n\nSo we see a invoice charecter C , which could represent cancellation. Let’s sort this and understand the range.\n\ncheck_quantity<-check_quantity%>%arrange(Quantity)\n\nLet’s try to understand about the first listed entry to dig deep as it can be the huge outlier.We will study the orders made by the consumer with ID 16446.\n\nOnline_retail%>%filter(CustomerID==16446)\n\n  Invoice StockCode                 Description Quantity   InvoiceDate\n1  553573     22982         PANTRY PASTRY BRUSH        1 18/05/11 9:52\n2  553573     22980      PANTRY SCRUBBING BRUSH        1 18/05/11 9:52\n3 C581484     23843 PAPER CRAFT , LITTLE BIRDIE   -80995 09/12/11 9:27\n4  581483     23843 PAPER CRAFT , LITTLE BIRDIE    80995 09/12/11 9:15\n  UnitPrice CustomerID        Country\n1      1.25      16446 United Kingdom\n2      1.65      16446 United Kingdom\n3      2.08      16446 United Kingdom\n4      2.08      16446 United Kingdom\n\n\nSo we see that from the time stamps , PAPER CRAFT , LITTLE BIRDIE the order is cancelled in 12 mins after it is placed.So we remove all the cancelled orders.\n\nOnline_retail_no_outliers<-Online_retail%>%filter(Quantity>0)%>%filter(UnitPrice>0)\n\nSo, Now we have removed the outliers successfully.\n\n\nFeatureEngineering.\nIn order to find out how much money is being spent, we come up with an idea of creating a variable called spent - which is a product of quantity and unit price.\n\nOnline_retail_new<-Online_retail_no_outliers%>%\n  mutate(Online_retail_no_outliers,Expenditure=Quantity*UnitPrice)\n\n\ncolnames(Online_retail_new)\n\n[1] \"Invoice\"     \"StockCode\"   \"Description\" \"Quantity\"    \"InvoiceDate\"\n[6] \"UnitPrice\"   \"CustomerID\"  \"Country\"     \"Expenditure\"\n\n\nWe observe that the invoice date is in charecter variable type, So we would want to convert it to a date column.\n\nlibrary(lubridate)\nOnline_retail_new$InvoiceDate<-dmy_hm(Online_retail_new$InvoiceDate)\nOnline_retail_new$year<-year(Online_retail_new$InvoiceDate)\nOnline_retail_new$month<-month(Online_retail_new$InvoiceDate)\nOnline_retail_new$hour<-hour(Online_retail_new$InvoiceDate)\nOnline_retail_new$wday<-wday(Online_retail_new$InvoiceDate)\n\n\n\nExploratory Data Analysis Visualizations.\nWe first intend to understand the revenue generated by the countries to the firm.\n\ndsub1<-Online_retail_new%>%group_by(Country) %>%dplyr::summarise(Total_Income = sum(Expenditure))\nplot1 <- ggplot(dsub1,aes(x = reorder(Country,-Total_Income),Total_Income)) + geom_bar(stat = \"identity\",fill=\"dodgerblue2\") + coord_flip()+  labs( x = 'Income_Earned',y = \"Country\", title = \"Total Income by Country\") +theme(axis.text.x = element_text(angle=90))\nplot1\n\n\n\n\n\ndsub2<-Online_retail_new %>%group_by(Country)%>%filter(Country!=\"United Kingdom\")%>% \ndplyr::summarise(Total_Income = sum(Expenditure))\n\nplot2<-ggplot(dsub2,aes(x = reorder(Country,-Total_Income),Total_Income)) + geom_bar(stat = \"identity\",fill=\"red\") +  coord_flip()+ labs( x = 'Income_Earned',y = \"Country\", title = \"Total Income by Country\")+theme(axis.text.x = element_text(angle=90))\nplot2\n\n\n\n\nWe see that there is huge revenue generated by united kingdom when compared to the other countries, and if we exclude UK, Netherlands has the highest share of Revenue.\n\nlibrary(ggplot2)\nRevenue_by_month<-Online_retail_new%>% \n  group_by(month)%>%\n  dplyr::summarise(Total_Income = sum(Expenditure))\nplot_by_month<-ggplot(Revenue_by_month, mapping = aes(x = reorder(month, -Total_Income), Total_Income)) + \n  geom_bar(stat = \"identity\",fill=\"orchid4\") +   labs( x = 'Revenue_generated',y = \"Month\", title = \"Total Revenue by Month\") +theme(axis.text.x = element_text(angle=90))\n\nRevenue_by_year <- Online_retail_new %>% \n  group_by(year) %>% \n  dplyr::summarise(Total_Income = sum(Expenditure))\nplot_by_year <- ggplot(Revenue_by_year, aes(x = reorder(year, -Total_Income), Total_Income)) + geom_bar(stat = \"identity\",fill=\"lightblue\") +   labs( x = 'Revenue_generated',y = \"year\", title = \"Total Revenue by year\") +theme(axis.text.x = element_text(angle=90))\n\nplot_by_month\n\n\n\n\n\nplot_by_year\n\n\n\n\nWe observe that the month of november has the highest sales and the revenue generated has an exponential increase from 2010 to 2011. Now let us understand what are the items those are being frequently ordered and the items being frequently cancelled.\n\ntop_orders<-Online_retail_new %>% group_by(Description) %>% dplyr::summarise(Total_order_expense = sum(Expenditure)) %>% arrange(-Total_order_expense)\nggplot(data = head(top_orders,10),aes(x = reorder((Description), Total_order_expense), Total_order_expense)) + geom_bar(stat = \"identity\",fill=\"orange1\")+  labs( y = 'Revenue',x = \"Products\", title = \"Top 10 Products\") +theme(axis.text.x = element_text(angle=90)) + coord_flip()\n\n\n\n\nTree graph representation of the same\n\ntop_orders<-Online_retail_new%>% group_by(Description)%>%dplyr::summarise(Total_order_expense = sum(Expenditure)) %>% arrange(-Total_order_expense)\nggplot(data = head(top_orders,10), aes(area = round(Total_order_expense,2), fill = Description,label=round(Total_order_expense,2))) +\n  geom_treemap()+geom_treemap_text(colour = \"black\",place = \"centre\",size = 15)+scale_fill_brewer(palette = \"Blues\")\n\n\n\n\nNow the cancelled metrics are understood and it is understood that paper craft, little birdie is cancelled as well along with being ordered more.\n\ncancel_df<-Online_retail%>%filter(Quantity < 0)\ncancel_df<-cancel_df%>%\n  mutate(cancel_df,Expenditure=Quantity*UnitPrice)\ncancelled_data <- cancel_df %>% group_by(Description) %>% dplyr::summarise(Total_order_expense = sum(Expenditure)) %>% arrange(Total_order_expense)\nggplot(head(cancelled_data,10),  aes(x = reorder((Description), -Total_order_expense), Total_order_expense)) + geom_bar(stat = \"identity\",fill=\"turquoise4\")+  labs( y = 'Income',x = \"Products\", title = \"Top 10 Products- cancelled\") +theme(axis.text.x = element_text(angle=90)) + coord_flip()\n\n\n\n\nWe now are going to understand the unique words in the description, which are observed in the orders.\n\nproducts_list <- unique(Online_retail_new$Description)\np_list <- Corpus(VectorSource(products_list))\ntoSpace <- content_transformer(function (x , pattern) gsub(pattern, \" \", x))\np_list<-tm_map(p_list, toSpace, \"/\")\np_list<-tm_map(p_list, toSpace, \"@\")\np_list<-tm_map(p_list, toSpace, \"\\\\|\")\np_list<-tm_map(p_list, content_transformer(tolower))\np_list<-tm_map(p_list, removeNumbers)\np_list<-tm_map(p_list, removeWords, stopwords(\"english\"))\np_list<-tm_map(p_list, removeWords, c( \"blue\",\"white\",\"metal\",\"small\", \"large\",\"red\",\"black\",\"design\",\"pink\",\"glass\",\"set\"))\np_list<-tm_map(p_list, removePunctuation)\np_list<-tm_map(p_list, stripWhitespace)\ndtm<-TermDocumentMatrix(p_list)\nm<-as.matrix(dtm)\nv<-sort(rowSums(m),decreasing=TRUE)\nd<-data.frame(word = names(v),freq=v)\n\nset.seed(123)\nwordcloud(words=d$word,freq=d$freq,min.freq=1,\n          max.words=20, random.order=FALSE, rot.per=0.35, \n          colors=brewer.pal(8,\"Dark2\"))\n\n\n\n\nA bigger font size usually represents higher repetition of that particular word in description.The stop words are chosen iteratively because the colors would not give any useful insights while studying the frequently ordered item.\n\nworld_map<-Online_retail_new%>%group_by(Country)%>% dplyr::summarise(revenue=sum(Expenditure))\n  \nhighchart(type = \"map\") %>%hc_add_series_map(worldgeojson,world_map%>%bind_cols(as_tibble(world_map$revenue)) %>% group_by(world_map$Country) %>% dplyr::summarise(revenue = log1p(sum(value))) %>% ungroup() %>% mutate(iso2 = countrycode(sourcevar = world_map$Country,origin=\"country.name\", destination=\"iso2c\")),value = \"revenue\", joinBy = \"iso2\") %>%\nhc_title(text = \"Revenue by country (log)\") %>%\nhc_tooltip(useHTML = TRUE, headerFormat = \"\",pointFormat = \"{point.map_info$Country}\") %>% \n  hc_colorAxis(stops = color_stops(colors = viridisLite::inferno(10, begin = 0.1)))\n\n\n\n\n\n\n\nsummary(Online_retail_new)\n\n   Invoice           StockCode         Description           Quantity       \n Length:397884      Length:397884      Length:397884      Min.   :    1.00  \n Class :character   Class :character   Class :character   1st Qu.:    2.00  \n Mode  :character   Mode  :character   Mode  :character   Median :    6.00  \n                                                          Mean   :   12.99  \n                                                          3rd Qu.:   12.00  \n                                                          Max.   :80995.00  \n  InvoiceDate                       UnitPrice          CustomerID   \n Min.   :2010-12-01 08:26:00.00   Min.   :   0.001   Min.   :12346  \n 1st Qu.:2011-04-07 11:12:00.00   1st Qu.:   1.250   1st Qu.:13969  \n Median :2011-07-31 14:39:00.00   Median :   1.950   Median :15159  \n Mean   :2011-07-10 23:41:23.50   Mean   :   3.116   Mean   :15294  \n 3rd Qu.:2011-10-20 14:33:00.00   3rd Qu.:   3.750   3rd Qu.:16795  \n Max.   :2011-12-09 12:50:00.00   Max.   :8142.750   Max.   :18287  \n   Country           Expenditure             year          month       \n Length:397884      Min.   :     0.00   Min.   :2010   Min.   : 1.000  \n Class :character   1st Qu.:     4.68   1st Qu.:2011   1st Qu.: 5.000  \n Mode  :character   Median :    11.80   Median :2011   Median : 8.000  \n                    Mean   :    22.40   Mean   :2011   Mean   : 7.612  \n                    3rd Qu.:    19.80   3rd Qu.:2011   3rd Qu.:11.000  \n                    Max.   :168469.60   Max.   :2011   Max.   :12.000  \n      hour            wday     \n Min.   : 6.00   Min.   :1.00  \n 1st Qu.:11.00   1st Qu.:2.00  \n Median :13.00   Median :4.00  \n Mean   :12.73   Mean   :3.51  \n 3rd Qu.:14.00   3rd Qu.:5.00  \n Max.   :20.00   Max.   :6.00  \n\n\nAs we know, the clustering is grouping homogenous data points together.The motive of clustering is to make the Distance between data points in the cluster should be made minimal.we adopt a variety of clustering algorithms and RFM analysis is one of the most pivotal step. RFM Analysis gives score to each of the customers on three elements of recency, frequency and monetary. The lesser value of recency depicts that the customer visits the store more.Frequency depicts the gap between two purchases and higher value indicates more frequent purchases.Monetary refers to the amount of money spent.\n\nrecency<-Online_retail_new%>%dplyr::select(CustomerID,InvoiceDate)%>%mutate(recency= as.Date(\"2011-12-09\")-as.Date(InvoiceDate))  \nrecency<-recency%>%dplyr::select(CustomerID,recency)%>%group_by(CustomerID)%>% slice(which.min(recency))\n\n#frequency\namount_products<-Online_retail_new%>%dplyr::select(CustomerID,InvoiceDate)%>%group_by(CustomerID, InvoiceDate)%>%dplyr::summarise(n_prod=n())\ndf_frequency<-amount_products %>% dplyr::select(CustomerID) %>%group_by(CustomerID) %>% dplyr::summarise(frequency=n())\n\n#monetary\ncustomer<-summarise_at(group_by(Online_retail_new,CustomerID,Country), vars(Expenditure,Quantity), funs(sum(.,na.rm = TRUE)))\nmonetary<-select(customer, c(\"CustomerID\", \"Expenditure\"))\n\n#RFM DF\n# inner join the three RFM data frames by CustomerID\nrfm<-recency%>%dplyr::inner_join(., df_frequency, by = \"CustomerID\") %>% dplyr::inner_join(., monetary, by = \"CustomerID\")\n# drop the days from recency column and transform it into numeric data type\nrfm<-rfm %>% mutate(recency=str_replace(recency, \" days\", \"\")) %>% mutate(recency = as.numeric(recency)) %>% ungroup()\nhead(rfm, 3)\n\n# A tibble: 3 × 4\n  CustomerID recency frequency Expenditure\n       <int>   <dbl>     <int>       <dbl>\n1      12346     325         1      77184.\n2      12347       2         7       4310 \n3      12348      75         4       1797.\n\n\nwe now scale the data frame.\n\nrfm1<-select(rfm,-CustomerID)\ndf_scale<-scale(rfm1)\n\nNow we use silhouette method to find out optimal number of clusters.\n\nlibrary(factoextra)\nfviz_nbclust(df_scale, kmeans,method=\"silhouette\")\n\n\n\n\nWe now perform kmeans clustering with optimal clusters\n\nset.seed(123)\nk_means_clustering<- kmeans(df_scale, 2, nstart = 25)\nfviz_cluster(k_means_clustering,df_scale)\n\n\n\n\n\nprint(k_means_clustering)\n\nK-means clustering with 2 clusters of sizes 4320, 26\n\nCluster means:\n       recency frequency Expenditure\n1  0.005210046 -0.048307 -0.05619323\n2 -0.865669143  8.026394  9.33672099\n\nClustering vector:\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1555] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1592] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1629] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1666] 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n[1703] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1740] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1777] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1814] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1851] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1888] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1925] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1962] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1\n[1999] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2036] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2073] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2110] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2147] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2184] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2221] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2258] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2295] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2332] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2369] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2406] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2443] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2480] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2517] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2554] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2591] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2628] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2665] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n[2702] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2739] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2776] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2813] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2850] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2887] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2924] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2961] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[2998] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3035] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3072] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3109] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3146] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3183] 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3220] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3257] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3294] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3331] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3368] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3405] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3442] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3479] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3516] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3553] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3590] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3627] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3664] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3701] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n[3738] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3775] 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3812] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3849] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3886] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3923] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3960] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[3997] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4034] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4071] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n[4108] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4145] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4182] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n[4219] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4256] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4293] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[4330] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 6448.496 2601.650\n (between_SS / total_SS =  30.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\nConclusion and Scope.\nFrom this we can say that cluster 2 is the most valuable customers as the money spent and the frequency of purchases are more.The advertising and marketing patterns might vary based on type of product being launched, motive behind the launch and so on.., knowing about all the charecteristics of the two clusters would save a lot of toil during the marketing process and provide us with the best possible outcomes.\n\n\nReferences\nhttps://rfm.rsquaredacademy.com/index.html https://rstudio-pubs-static.s3.amazonaws.com/375287_5021917f670c435bb0458af333716136.html https://www.kaggle.com/analytical-customer-segmentation-analysis-r"
  },
  {
    "objectID": "posts/final_project_XiaoyanHu.html#introduction",
    "href": "posts/final_project_XiaoyanHu.html#introduction",
    "title": "Final project: Calculating and visualizing texture profile analysis",
    "section": "1 Introduction",
    "text": "1 Introduction\nThere is a growing demand on plant-based meat analogs due to ethical, environmental and health concerns associated associated with the production of real meat and seafood. Meat and seafood analogs should mimic the desirable appearance, texture, and flavor of the real versions. In this study, we investigated on the texture of animal meat and lab made analogs, especially on their texture profile of adipose tissue(know as fat meat).\nIn this data, I have 5 samples. Control is beef fat.Sample 1-4 composed of different plant-based ingredients such as soybean oil, coconut oil, soybean protein and agar to combine them into a emulsion or gel that looks similar as fat meat under microscope. However, they still have huge differences on the macro texture.\nTexture profile analysis mimics motion and force in human oral cavity. the measurement including two times of compression and raw data including time and corresponding time. From this data set, many other parameters can be calculated from the graph. The automatic calculation sometimes can be confusing and messy data have to be processed by excels and individual calculations.\nParameter including hardness, adhesiveness, resilience, cohesion and chewiness. The definition and calculation will be introduced in below section.\nIn this report, the raw data of texture profile analysis will be tidied and calculated. The 5 parameter data will be calculated either from raw data or graph and will be visualized. Interpret of comparison in different samples will be performed. Using R may provide an efficient way to process the raw data with better visualization compare to traditional excel plotting and calculation."
  },
  {
    "objectID": "posts/final_project_XiaoyanHu.html#data",
    "href": "posts/final_project_XiaoyanHu.html#data",
    "title": "Final project: Calculating and visualizing texture profile analysis",
    "section": "2 Data",
    "text": "2 Data\nRaw TPA data is presented as time versus force. By plotting this graph, further texture parameters can be done by calculation.\n\n2.1 tidy data\nIn this section, the raw data was reded in, pivoted and transformed into a new table as previewed below.\n\n\nCode\n#read in data.\n# data1 = Texture Profile analysis of 5 samples including data of each time and force\n# the cell_limits c=(upper left cell), c=(lower right cell)\ndata1<- read_excel(\"_data/final_data.xlsx\",\n                   sheet=\"TPAraw\",\n                  range = cell_limits(c(3,1),c(NA,10)),\n              col_names = c(\"control0time\",\"control0force\",\"S10time\",\"S10force\",\"S20time\",\"S20force\",\"S30time\",\"S30force\",\"S40time\",\"S40force\"))\n\n#generate a row number which will help locate data below\ndata1<-mutate(data1, row_num = seq.int(nrow(data1)))\n                               \n#converting character data into numeric\ninvisible(lapply(data1, as.numeric))\n\n#pivot data into 4 variables\ndata2<-data1%>% pivot_longer(cols = c(control0force,S10force, S20force, S30force,S40force),\n                             names_to = \"sampleforce\",\n                             values_to = \"force1\")%>%\n  separate(sampleforce, into = c(\"sample\", \"type\"), sep = \"0\")\n\n#extract columns needed and form a clean data frame\nTDdata<- data.frame(time = data2$control0time,\n                    force1 = data2$force1,\n                    sample = data2$sample,\n                    row_num = data2$row_num)\n#preview of tidy data\nhead(TDdata)\n\n\n   time force1  sample row_num\n1 0.000   -0.1 control       1\n2 0.000   -1.0      S1       1\n3 0.000   -1.2      S2       1\n4 0.000    1.0      S3       1\n5 0.000    0.7      S4       1\n6 0.005   -5.1 control       2\n\n\nCode\n#result_df = TDdata%>%\n # group_by(sample)%>%\n # arrange(time + sample)%>%\n # mutate(force_T1 = lead(force1))%>%\n  \n#filter(force1>0)%>%\n #filter(force_T1 <0)%>%print()\n#df_1 = result_df %>% filter(sample == \"control\")\n#df_2 = result_df %>% filter(sample == \"S1\")\n#df_3 = result_df %>% filter(sample == \"S2\")\n#df_4 = result_df %>% filter(sample == \"S3\")\n#df_5 = result_df %>% filter(sample == \"S4\")\n\n\n\n\n\n2.2 Calculation\nBelow is a sample graph of how raw data plotted out as time VS force looks like.\n\n\nCode\n# In this section, other parameter described above will be calculated\n# Below includes a sample graph\ninclude_graphics(\"_data/TPA1.png\")\n\n\n\n\n\nIn the texture profile analysis, the highest point of first peak represents the hardness of the sample.The F1 force is defined as hardness of sample. In below block, a function is written to filter out the maximum force of one sample. Below table in force column present the highest peak in each sample as the hardness.\n\n2.2.1 find hardness F1\n\n\nCode\n# Hardness = F1\n# group by sample and filter the sample you want to calculate\n#calculate the cumulative area by time and force in each row, this data will used in later section\n#slice out the maximum force\n#control F1#S1 F1\nf1func<-function(x) {\n  return( TDdata%>%group_by(sample)%>%\n            filter(sample == x)%>%\n  mutate(area1=cumtrapz(time,force1))%>%\n  slice_max(force1))\n}\n\ndf1<-f1func('control')\ndf2<-f1func('S1')\ndf3<-f1func('S2')\ndf4<-f1func('S3')\ndf5<-f1func('S4')\n\n#merge sliced samples into a table\nmergefunc<-function(a,b,c,d,e){\n  return(a %>%full_join(b,by = c(\"time\", \"force1\", \"sample\", \"row_num\", \"area1\"))%>%\n  full_join(c,by = c(\"time\", \"force1\", \"sample\", \"row_num\", \"area1\"))%>% \n  full_join(d,by = c(\"time\", \"force1\", \"sample\", \"row_num\", \"area1\"))%>% \n  full_join(e,by = c(\"time\", \"force1\", \"sample\", \"row_num\", \"area1\")))\n}\nF1<-mergefunc(df1,df2,df3,df4,df5)%>%print()\n\n\n# A tibble: 5 × 5\n# Groups:   sample [5]\n   time  force1 sample  row_num area1[,1]\n  <dbl>   <dbl> <chr>     <int>     <dbl>\n1  2.35 24080.  control     470   12334. \n2  1.13    82.9 S1          227      45.7\n3  1.14  1178.  S2          230     505. \n4  2.12  1032.  S3          424     624. \n5  1.06  2189.  S4          212     720. \n\n\nCode\n#TRY <- function(control,max_time,time){\n#  return(TDdata%>%group_by(sample)%>%\n#  filter(sample = 'control')%>%\n#  mutate(area2 = cumtrapz(time,force1))%>%\n # mutate(force_t1 =lead(force1)))%>%\n  #  F1%>%select(force1, sample ,time) %>%\n# rename(max_time = time) %>%\n #right_join(TDdata)%>%\n  # filter(time>max_time)%>%\n  #filter(force1 > 0)%>%\n  #filter(force_t1<0)\n  \n#} \n#TRY(control, max_time, time)\n\n\nResilience is defined as area b/area and can be understand as how much sample regain after first compression. And calculated as area b/area a. Therefore, the next trunk finds out the endpoint of first peak. the cumulative area from end of first peak minus cumulative area of the maximum force(calculated above) will be the area b.\n\n\n2.2.2 find area b\n\n\nCode\n#function to find the endpoint of peak 1 is wroted as finding the first time point after F1 that cooresponding force cross 0. \n#lead function align the force in next time point, therefore, the first point where force+1 is negative, and force is positive, implies the first point cross 0, which is the first peak end point. \n\n#endpoint of first peak\nendfunc<-function(x, y){\n  return(TDdata%>%group_by(sample)%>%\n  filter(sample == x)%>%\n  mutate(area2 = cumtrapz(time,force1))%>%\n  filter(row_num > y)%>%\n  mutate(force_t1 =lead(force1))%>%\n  filter(force1 > 0)%>%\n  filter(force_t1<0)%>%\n  slice(1))\n}\n\ndf6<-endfunc('control', 470)\n\ndf7<-endfunc('S1', 227)\n\ndf8<-endfunc('S2', 230)\n  \ndf9<-endfunc('S3',424)\n\ndf10<-endfunc('S4', 212)\n\n#merge sliced samples into a table\nmergefunc2<-function(a,b,c,d,e){\n  return(a %>%full_join(b,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area2\",\"force_t1\"))%>%\n  full_join(c,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area2\",\"force_t1\"))%>% \n  full_join(d,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area2\",\"force_t1\"))%>% \n  full_join(e,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area2\",\"force_t1\")))\n}\nEND1<-mergefunc2(df6,df7,df8,df9, df10)\n\n#area b is calculated by cumulative area at end point minus cumulative area at F1\nareab = F1 %>% full_join(END1,by = c( \"sample\"))%>%mutate(areab =  area2 -area1)\n\n\nThe cohesion stands for How well the product withstands a second deformation relative to its resistance under the first deformation an calculated as area (d+e)/(a+b). Therefore, we still need to find the start and end point of peak 2.\nIn this section, similar functions were wrote to finding the function. The data was cutted after endpoint 1 to find the maximum which is the F2.\n\n\n2.2.3 find F2\n\n\nCode\n#finding F2 by filtering the sample after the first peak and then slice tha maximum\nF2func<-function(x,y){\n  return(TDdata%>%group_by(sample)%>%\n  filter(sample == x)%>%\n   mutate(area1 = cumtrapz(time,force1))%>%\n  filter(row_num > y)%>%\n  slice_max(force1))\n}\ndf11<-F2func('control', 551)\n\ndf12<-F2func('S1', 259)\n\ndf13<-F2func('S2', 268)\n   \ndf14<-F2func('S3', 587)\n  \ndf15<-F2func('S4', 320)\n  \nF2<- mergefunc(df11, df12, df13, df14, df15)\n\n\nThe start point of second peak was filtered back to find the cross 0 point. And the end point is also selected by the cross 0 point after F2.\n\n\n2.2.4 find the start point of area d\n\n\nCode\n#find the start point of area d\nstart2<-function(x,y){\n  return(TDdata%>%group_by(sample)%>%\n  filter(sample == x)%>%\n  mutate(area3 = cumtrapz(time,force1))%>%\n  filter(row_num < y)%>%\n  arrange(desc(row_num))%>%\n   mutate(force_t1 =lead(force1))%>%\n  filter(force1 > 0)%>%\n  filter(force_t1<0)%>%\n  slice(1))\n}\ndfd1<-start2('control', 3408)\ndfd2<-start2('S1', 2681)\ndfd3<-start2('S2', 2688)\ndfd4<-start2('S3', 3270)\ndfd5<-start2('S4', 2631)\n\nmergefunc3<-function(a,b,c,d,e){\n  return(a %>%full_join(b,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area3\",\"force_t1\"))%>%\n  full_join(c,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area3\",\"force_t1\"))%>% \n  full_join(d,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area3\",\"force_t1\"))%>% \n  full_join(e,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area3\",\"force_t1\")))\n}\n\nstart2<-mergefunc3(dfd1, dfd2,dfd3,dfd4,dfd5)\n\n\n\n\n2.2.5 find the ending point of area e\n\n\nCode\n#find the ending point of area e\nEND2func<-function(x, y){\n  return(TDdata%>%group_by(sample)%>%\n  filter(sample == x)%>%\n  mutate(area4 = cumtrapz(time,force1))%>%  \n  filter(row_num >y)%>%\n  mutate(force_t1 =lead(force1))%>%\n  filter(force1 > 0)%>%\n  filter(force_t1<0)%>%\n  slice(1))\n}\ndfd6<-END2func('control',3408)\ndfd7<-END2func('S1',2681)\ndfd8<-END2func('S2',2688)\ndfd9<-END2func('S3',3270)\ndfd10<-END2func('S4',2631)\n\nmergefunc4<-function(a,b,c,d,e){\n  return(a %>%full_join(b,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area4\",\"force_t1\"))%>%\n  full_join(c,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area4\",\"force_t1\"))%>% \n  full_join(d,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area4\",\"force_t1\"))%>% \n  full_join(e,by = c(\"time\", \"force1\", \"sample\", \"row_num\",\"area4\",\"force_t1\")))\n}\n\nEND2<-mergefunc4(dfd6,dfd7,dfd8,dfd9,dfd10)\n\nareae = F2 %>% full_join(END2,by = c( \"sample\"))%>%mutate(areae =  area4 -area1)\naread = F2 %>% full_join(start2,by = c( \"sample\"))%>%mutate(aread =  area1 -area3)\n\n\nSpringiness is defined as how a product physically springs back at the second compression.\nTherefore, the critical point we need to find out from the data is the coordinate of F1, the time where area b ends, F2, and where the second peak starts and ends. After obtaining these data, the calculation can be performed.\n\n\n2.2.6 combine all calculated value into a data frame for visualization\n\n\nCode\n# all value calculated above are sorted into a new table\n# Gumminess = Hardness*cohesion\n# Chewiness = Gumminess * Springiness\n\nparameters<- data.frame(sample_name = F1$sample,\n                        hardness = F1$force1,\n                       resilience = c(areab$area2-areab$area1),\n                       cohesion = c((aread$aread +areae$areae) / (F1$area1 +areab$areab)),\n                        springiness = c((F2$time-start2$time)/F1$time))\nparameters<-parameters%>%mutate(chewiness = parameters$hardness * parameters$cohesion)%>%print()\n\n\n  sample_name hardness resilience  cohesion springiness   chewiness\n1     control  24079.7  3800.8282 0.5426024   0.8848614 13065.70323\n2          S1     82.9     5.3125 0.8855980   0.8938053    73.41607\n3          S2   1177.8    60.7060 0.3251442   0.8995633   382.95482\n4          S3   1031.9   275.1400 0.8644005   0.7754137   891.97485\n5          S4   2189.4   208.8793 0.7663462   0.9289100  1677.83841"
  },
  {
    "objectID": "posts/final_project_XiaoyanHu.html#visualizition",
    "href": "posts/final_project_XiaoyanHu.html#visualizition",
    "title": "Final project: Calculating and visualizing texture profile analysis",
    "section": "3 Visualizition",
    "text": "3 Visualizition\nIn texture analysis, comparing the values sometimes can be difficult to interpret the data. By visualizing, we can easily compare the single parameters in each sample and analysis how the material brings the strength and weakness separately. We can also compare the overall similarity of each sample to the control ro select a good candidate to continue research on.\n\n3.1 overview of raw data\nBelow is an overview of all five samples. As we can see, all the parameters were calculated from this graph. * Put your cursor on graph to see data points\n\n\nCode\np_force = ggplot(data = TDdata) + \n  geom_line(mapping = aes(x = time, y = force1, color = sample)) +\n  labs(title = \"force v.s. time\")\np_force %>% ggplotly()\n\n\n\n\n\n\n\n\n3.2 hardness\nAll the parameters was plotted in bar chart first to compare each single parameter individually. This helps understand the specific strength of each material. Control which is the beef sample shows good hardness implies the inner structure of beef provide a good support to the over all material. The lab created samples we can see that emulsion samples(S1 and S3) are less hard than gelled samples(S2 and S4) while solid coconut oil under room temperature(S3 and S4) are better than liquid soybean oil(S1 and S2)\n\n\nCode\n#bar chart need to include stat='identity‘ which is basically telling ggplot2 you will provide the y-values for the barplot, rather than counting the aggregate number of rows for each x value, which is the default stat=count\nggplot(parameters, aes(x= sample_name, y= hardness, fill = hardness)) +\n    geom_bar(stat='identity',colour = 'black') + \n  scale_y_log10() +\n  labs(title = 'Hardness')+ \n  scale_color_gradient(low = \"royalblue\", high = \"steelblue\")\n\n\n\n\n\n\n\n3.3 resilience\nResilience is how much the sample bounce back from the first compression. Still, the control sample raise most. And the soybean gelled emulsion (S3) remains highest. This indicates the liquid oil gives the best recovery to the gelling agent.\n\n\nCode\nggplot(parameters, aes(x= sample_name, y= resilience,fill = resilience)) +\n    geom_bar(stat='identity',colour = 'black') + scale_y_log10()+ labs(title = 'Resilience')+ scale_fill_gradient(low = \"hotpink3\", high = \"lightpink\")\n\n\n\n\n\n\n\n3.4 cohesion\nThe cohesion measure how the sample stand at second compression, which can be understand as how much hardness remains in second compression. Some brittle sample may broke during this time. From the graph we can easily tell, emulsion samples (S1S3) have relative high cohesion under second compress.\n\n\nCode\nggplot(parameters, aes(x= sample_name, y= cohesion, fill = cohesion)) +\n    geom_bar(stat='identity',colour = 'black') + labs(title = 'Cohesion') + scale_fill_gradient(low = \"turquoise4\", high = \"steelblue1\") \n\n\n\n\n\n\n\n3.5 spinginess\nThe springiness indicate the relative time sample bounce back. There was no significant differenceb between each samples.\n\n\nCode\nggplot(parameters, aes(x= sample_name, y= springiness, fill = springiness)) +\n    geom_bar(stat='identity',colour = 'black')  +\n  labs(title = 'Springiness')+ \n  scale_fill_gradient(low = \"cadetblue4\", high = \"cadetblue1\")\n\n\n\n\n\n\n\n3.6 chewiness\nThe chewiness expresses the overall behavior of second compression. Here we can see, our control have the relatively high chewiness while samples are gradually gaining chewiness by adding factors such as solid oil and gelling agent.\n\n\nCode\nggplot(parameters, aes(x= sample_name, y= chewiness, fill = chewiness)) +\n    geom_bar(stat='identity',colour = 'black') +  scale_y_log10 ()+ labs(title = 'chewiness')+ scale_fill_gradient(low = \"mediumorchid4\", high = \"plum\")\n\n\n\n\n\n\n\n3.7 overall comparison\nThe radar chart gives a better idea of overall balance overall samples. From the radar chart, we can easily understandthe control still have strength over power all lab-made samples, especially when some parameters were plotted in log scale. However, between samples, we can also differenciate that overall S3 and S4 have a similar area than S1 and S2. This indicates the solid fat contributes more to the fat structure instead of the gelling agent (S3,S4 is overall better than S1,S2). Also looking at S1 and S2, gelling agent provide much improvement while did not work in S3 and S4. The solid oil somehow restricted the impovement of gelling agent. Overall, S3 and S4 performed best and overall close the character of control.\n\n\nCode\nmax<-c('max',25000, 4000, 1,1,15000)\nmin<-c('min',10,10,0,0.5,10)\nparameters1<-rbind(c = min, c=max, parameters, deparse.level = 0)\nparameter2<- transform(parameters1, \n          hardness = as.numeric(hardness),\n          resilience = as.numeric(resilience),\n          cohesion = as.numeric(cohesion),\n          springiness = as.numeric(springiness),\n          chewiness = as.numeric(chewiness))\n\nparameters3<-parameter2%>%\n  mutate(hardnesslog = log10(hardness))%>%\n  mutate(chewinesslog = log10(chewiness))%>%\n  mutate(resiliencelog = log10(resilience))\nparameters3<-parameters3[-c(1:3,6)]\n  radarchart(parameters3,maxmin = FALSE,cglwd = 0.5,\n             pcol  = c(\"white\",\"white\",\"blue\",\"pink4\",\"orchid\",\"turquoise3\",\"yellow\"),\n             plty = 1,palcex = 5,\n              pfcol =  scales::alpha(c(\"white\",\"white\",\"blue\",\"pink4\",\"orchid\",\"turquoise3\",\"yellow\"),0.2))\n            \n    legend(x = \"right\",\n                    legend = c(\"control\",\"S1\",\"S2\",\"S3\",\"S4\") ,\n           fill = c(\"blue\",\"pink4\",\"orchid\",\"turquoise3\",\"gold\")\n                    )"
  },
  {
    "objectID": "posts/final_project_XiaoyanHu.html#reflection",
    "href": "posts/final_project_XiaoyanHu.html#reflection",
    "title": "Final project: Calculating and visualizing texture profile analysis",
    "section": "4 Reflection",
    "text": "4 Reflection\n\n4.1 Why I choose this project\nIn this project, I selected a measurement data that we used daily in our lab. I want to chose this data because the machine can calculate the value, however sometime with mistakes. In this case, we have to import the data into excel, plot it, find value we need among thousands of data point and then calculate. This process is boring and redunant. By learning r, I found it might be a food idea that I can find a easier way to calculated all this value automatically to avoid human labor. And by the time this programming works, the future data can be imported, organized and visualized automatically which can save much time. Therefore, I decided to work on this project.\n\n\n4.2 Difficultes I met\nBy looking at others projects and examples in the class, I found most of them are aiming at visualization and analysis. However, in my case, the most heavy work was in calculation. The raw data was simple and easy to tidy, however, finding the significant values in this data requires a lot of work. The measurements were human tested samples and there are many interference data points. For example, I want to find the end point of the first peak, where force = 0. However, how would I select the point since the nears point to 0 may be 10, and the next wil be -10, where 0 doesn’t actually have a physically point. In this case, I have to using functions to filter and understand where the value I want exactly and not taking in those interference points.\nAlso, reducing the redundant steps has also been a difficulty for me to solve. By learning functions, I found that repeated steps can be easily archieved by some package or functions.\nAnother thing I have learnt from this class and this project is how to brainstorm the steps that you reach to your final goals. My data looks like a simple data set but considering the interference, it became very difficult to make it accurate. More and more steps were added when i wrote the code and repeat and reduced. So there are tons of effort which may not been present in the code. The learning process was really interesting but tiring, with understanding varies packages and functions. The accumulation will make the future coding much more easier.\n\n\n4.3 Next step of this project\nI haven’t optimized the calculation into a best, easiest and shortest steps to calculate all the values which can be done in the future. Another problem I left was indexing of table into tables. This code may be simplified without writing functions and creating much tables.\nAlso, there might be a way to cooperate the sample ingredients into data presenting. However, most of my work was done in descriptions. It may also be difficult to visualize or make those numeric to consider about the ingredients.This could also be innovative in presenting scientific research results to help other understand."
  },
  {
    "objectID": "posts/final_project_XiaoyanHu.html#conclusion",
    "href": "posts/final_project_XiaoyanHu.html#conclusion",
    "title": "Final project: Calculating and visualizing texture profile analysis",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nIn conclusion, this project calculated the value of texture analysis profile and visualized the data to compare the samples. This helps us to understant the single parameter and overall strenght and weakness of each sample and find the most close lab-made sample to our control.\nIn terms of R, this project help me helps me build up idea of how to approach data in a desired manner and how to make the coding easy and simple. Althought I haven’t make it perfect in this project, it is a really great process to learn the idea with processing data."
  },
  {
    "objectID": "posts/final_project_XiaoyanHu.html#bibliography",
    "href": "posts/final_project_XiaoyanHu.html#bibliography",
    "title": "Final project: Calculating and visualizing texture profile analysis",
    "section": "6 Bibliography",
    "text": "6 Bibliography\n\nHu, X. and D. J. McClements (2022). “Construction of plant-based adipose tissue using high internal phase emulsions and emulsion gels.” Innovative Food Science & Emerging Technologies 78: 103016.\nCentre for Indutrial, R. (2022). Texture Analysis And Texture Profile Analysis - Rheology Lab. 3)Wickham, H. and G. Grolemund (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, O’Reilly Media."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html",
    "href": "posts/JanhviJoshi_FinalProject.html",
    "title": "DACSS 601 Final Project",
    "section": "",
    "text": "Capital Punishment has been a debate in the USA for a long time, dating back to the colonial period. As of April 2022, capital punishment is legal penalty in 27 states, and within the federal government and military justice system. Many organisations have actively monitored this debate , often by asking citizens of America whether they are in the favor of death penalty for a person convicted in murder. Throughout the years, the response has been almost 50-50 for and against.\nThrough this project, I wanted to understand the historical trends of capital punishment and whether it was rooted in any kind of bias - especially racially or on the basis of sex of a person. I think these patterns and trends would provide historical and cultural context to this highly controversial debate. The hope of this project is also that the people reading this would be able to look at the data and graphs and form an opinion on this debate."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html#executions-by-state-and-regions",
    "href": "posts/JanhviJoshi_FinalProject.html#executions-by-state-and-regions",
    "title": "DACSS 601 Final Project",
    "section": "Executions by State and Regions",
    "text": "Executions by State and Regions\nI wanted to see how many executions took place in all states by regions across all years using a bar graph. This helps answer the question: how many states performed the most executions, and how many the least or none?\n\nper_state <- data %>%\n  group_by(State_of_Execution, Region) %>% tally()\nsummary(per_state)\n\n State_of_Execution    Region                n         \n Length:50          Length:50          Min.   :  1.00  \n Class :character   Class :character   1st Qu.: 19.75  \n Mode  :character   Mode  :character   Median : 44.00  \n                                       Mean   :118.74  \n                                       3rd Qu.:155.25  \n                                       Max.   :648.00  \n\nggplot(per_state, aes(x = n, y = reorder(State_of_Execution, n), fill = Region)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  theme(axis.title = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  ggtitle(\"Executions by State\",\n          subtitle = \"From 1608 - 2002, color coded by Regions\")\n\n\n\n\nThrough the above graph, we can see that the state with the most executions performed is Virginia, followed by New York and Texas. Additionally, the South Atlantic, Middle Atlantic, and West North Central regions have higher number of executions. Based on these observations, we could conclude that generally, the number of capital punishment decreases as one moves from east to west."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html#visualizations-on-race",
    "href": "posts/JanhviJoshi_FinalProject.html#visualizations-on-race",
    "title": "DACSS 601 Final Project",
    "section": "Visualizations on Race",
    "text": "Visualizations on Race\nNext, I have done a few visualizations on race in the hope of finding the answer to the questions:\n\nDoes the historical data reveal any racial bias?\nHave there been any changes to these biases across the years?\n\n\nTotal Number of Executions per Race\n\nper_race <- data %>% group_by(Year, Race, HalfCenturies) %>% \n  tally()\n\nggplot(per_race, aes(x=Race))+\n  geom_bar(stat=\"count\", width=0.7, fill=\"steelblue\")+\n  labs(title=\"Plot of Number of Executions per Race\", \n         x=\"Race\", y = \"Number of Executions\") +\n  theme_minimal()\n\n\n\n\nFrom this graph, we can see that the most executions being performed throughout the years are on White and Black inmates, followed by Hispanic and Native American people. Next, I will plot how these executions trend over the years.\n\n\nTotal Number of Executions per Race accross Years\n\nexecutions_race <-\n  ggplot(data = per_race, aes(x = Year, y = n)) +\n  geom_point(aes(color = Race)) +\n  labs(title = \"Number of Executions by Race and Year\", x=\"Year\", y=\"Number of Executions\")\n\nggplotly(executions_race)\n\n\n\n\n\nIt can be observed that executions of black and white inmates follows the same trend in general over time. However, in the early 1900s, the number of executions for black inmates seems to be a little higher. Similarly the late 1800s seems to be the time when more executions were for Hispanics - this could be due to the Dakota war of 1860. While Hispanic executions increased in the 2000s.\n\n\nTotal Number of Executions per Race across Regions\n\nper_race_region <- data %>% group_by(Year,Race, Region) %>% \n  tally()\n\n\nggplot(per_race_region, aes(Region, fill = Race)) +\n  geom_bar(position = \"stack\") +\n  labs(y=\"Count of Executions\") + \n  ggtitle(\"Number of Executions by Region and Race\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nThis graph could reveal if there are any racial biases across regions. We can see that the South Atlantic region, followed by South Central, has the most executions for Black individuals. Additionally, the western regions, like Mountain and Pacific, and South Central carried out more executions on Hispanic, Asian-Pacific Islander, and Native American people.\n\n\nExecutions on Race acorss Regions and Years\n\nrr_vars <- c(\"Race\", \"Region\", \"HalfCenturies\")\nrace_region <- data[rr_vars]\n\n# Base plot\np <- ggplot(race_region, aes(x = Region, y = HalfCenturies, fill = Race)) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title=\"Heatmap of Executions across Region and Years by Race\", \n         x=\"Region\", y = \"Years\")\np + geom_tile()\n\n\n\n\nMostly, white people have received the capital punishment in all regions across all the years, however, there are some exceptions. For example, it seems that South Atlantic executed Black people the most in the years 1650 to 1850. This same trend was seen in the graph above. Similarly, Native Americans were executed the most in year 1700 to 1850 in the Pacific region. Overall, these 3 races have been executed the most across all regions and years.\n\n\nTotal Number of Executions by Race and Sex\n\nper_race_sex <- data %>% group_by(Year,Race, Sex) %>% \n  tally()\n\nper_race_sex\n\n# A tibble: 744 × 4\n# Groups:   Year, Race [670]\n    Year Race  Sex        n\n   <int> <chr> <chr>  <int>\n 1  1608 White Male       1\n 2  1622 White Male       1\n 3  1624 White Male       1\n 4  1626 White Male       1\n 5  1637 White Male       1\n 6  1638 White Male       4\n 7  1641 Black Male       1\n 8  1641 White Male       1\n 9  1642 White Male       2\n10  1643 White Female     1\n# … with 734 more rows\n\nggplot(per_race_sex, aes(Sex, fill = Race)) +\n  geom_bar(position = position_dodge()) +\n    geom_text(aes(label=after_stat(count)), stat=\"count\", vjust=1.6, \n            color=\"black\", position = position_dodge(0.9), size=3.5)+\n  labs(y=\"Count of Executions\") +\n  ggtitle(\"Number of Executions by Sex and Race\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nI created the above graph to find out the number of executions by race and sex. It is observed that for both male and female sexual orientations, individuals identifying as White were executed the most. It should be noted that in this dataset, no females belonging to the Native American and Asian Pacific races were not executed."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html#visualizations-on-gender",
    "href": "posts/JanhviJoshi_FinalProject.html#visualizations-on-gender",
    "title": "DACSS 601 Final Project",
    "section": "Visualizations on Gender",
    "text": "Visualizations on Gender\nIn this section, I have done a few visualizations on race in the hope of finding the answer to the questions:\n\nAre men sentenced to death more often than women?\nHave there been any reversals in this trend?\n\n\nTotal Executions by Sex\n\nper_gender <- data %>% group_by(Year, Sex) %>% tally()\n\nggplot(per_gender, aes(x=Sex))+\n  geom_bar(stat=\"count\", width=0.7, fill=\"steelblue\")+\n  labs(title=\"Plot of Number of Executions by Sex\", \n         x=\"Sex\", y = \"Number of Executions\") +\n  theme_minimal()\n\n\n\n\nIt can be seen that more males were executed that females over the years. For further analysis, I visualized the graph below.\n\n\nTotal Executions by Sex across Years\n\nper_gender <- data %>% group_by(Year, Sex) %>% tally()\n\nexecutions_gender <-\n  ggplot(data = per_gender, aes(x = Year, y = n)) +\n  geom_point(aes(color = Sex)) +\n  labs(title = \"Number of Executions by Sex and Year\", x=\"Year\", y=\"Number of Executions\")\n\nggplotly(executions_gender)\n\n\n\n\n\nFrom this graph, it can be seen that males are in the majority of capital punishment executions. The only time this trend was reversed was during the Salem Witch trials in the 1690s. During this time, a total 0f 14 women were executed while no male was executed.\n\n\nTotal Executions per Sex across Regions\n\nper_sex_region <- data %>% group_by(Year,Sex, Region) %>% \n  tally()\n\n\nggplot(per_sex_region, aes(Region, fill = Sex)) +\n  geom_bar(position = position_dodge2()) +\n      geom_text(aes(label=after_stat(count)), stat=\"count\", vjust=1.6, \n            color=\"black\", position = position_dodge(0.9), size=3.5)+\n  labs(y=\"Count of Executions\")+\n  ggtitle(\"Executions by Region and Sex\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nFrom the above graph, it can be said that, in general, the ratio of men vs women executions remains the same throughout all regions, except for in the New England region where women seem to be executed more - this is possibly due to the Salem Witch trials. This ratio is also not followed in East and West North Central and Pacific regions where males seem to have been executed much more than females.\n\n\nTotal Executions by Method of Execution and Sex\n\ngender_method <- data %>% group_by(Sex, Method_of_Execution, Year, HalfCenturies) %>% tally()\n\nggplot(gender_method, aes(Method_of_Execution, fill = Sex)) +\n  geom_bar(position = \"stack\") +\n  labs(title = \"Number of Executions by Method of Execution and Sex\", x=\"Method of Execution\", y=\"Number of Executions\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nThis graph shows that mostly, both males and females were executed by hanging. The next method used for both the genders was Electrocutions. We can see that many males were also executed by a shot and asphyxiation-gas."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html#number-of-executions-by-method-of-execution-and-years",
    "href": "posts/JanhviJoshi_FinalProject.html#number-of-executions-by-method-of-execution-and-years",
    "title": "DACSS 601 Final Project",
    "section": "Number of Executions by Method of Execution and Years",
    "text": "Number of Executions by Method of Execution and Years\n\nmethod_years <-\n  ggplot(gender_method, aes(HalfCenturies, fill = Method_of_Execution)) +\n  geom_bar(position = position_stack())+\n      geom_text(aes(label=after_stat(count)), stat=\"count\", vjust=1.6, \n            color=\"black\", position = position_stack(0.9), size=3.5)+\n  labs(title = \"Number of Executions by Method of Execution and Years\", x=\"Years\", y=\"Number of Executions\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplotly(method_years)\n\n\n\n\n\nI visualized this graph to understand how the methods of executions have changed throughout the years 1600 to 2000. From 1600s to the 1900s, Hanging was the method that was used the most to carry out executions. For a brief period between 1900s to 2000s, Electrocution started becoming the method being used more, closely followed by asphyxiation-gas.In the 1950s, Injection seemed to have been used a lot too."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html#age-at-time-of-execution-by-race",
    "href": "posts/JanhviJoshi_FinalProject.html#age-at-time-of-execution-by-race",
    "title": "DACSS 601 Final Project",
    "section": "Age at Time of Execution by Race",
    "text": "Age at Time of Execution by Race\nThe following graph was plotted in an effort to answer the folliwing questions: * What is the average age of individuals at the time of execution? * Is this age consistent across races?\n\nper_age_race <- data[,c(\"Age\", \"Race\", \"Sex\")]\nper_age_race <- na.omit(per_age_race)\n\nggplot(per_age_race, aes(x=Race, y=Age)) +\n  geom_boxplot(aes(color = Race)) +\n  ggtitle(\"Age at Time of Execution by Race\")\n\n\n\n\nThrough the box plot, the median age of execution for Black, Hispanic, and Native American individuals is 25, while for White and Asian-Pacific Island individuals is is higher - 30. There are quite a few outliers for the Black and White box plots."
  },
  {
    "objectID": "posts/JanhviJoshi_FinalProject.html#crimes-leading-to-capital-punishment",
    "href": "posts/JanhviJoshi_FinalProject.html#crimes-leading-to-capital-punishment",
    "title": "DACSS 601 Final Project",
    "section": "Crimes leading to Capital Punishment",
    "text": "Crimes leading to Capital Punishment\nThis graph answers the question: Which crimes have led to the most executions?\n\ncrime_count <- data %>% group_by(Crime_Committed) %>%\n  tally()\ncrime_count <- na.omit(crime_count)\n\ncrime_count <- crime_count[order(-crime_count$n),] %>%\n  mutate(percent = n / sum(n))\n\nggplot(crime_count, aes(area = percent, fill = Crime_Committed, label=paste(Crime_Committed, n, sep = \"\\n\"))) +\n  geom_treemap()+\n  geom_treemap_text(place=\"centre\", size=10)\n\n\n\n\nThe majority executions were a result of murder, specifically related to robbery, followed by rape. This is followed by capital punishments related to slave revolts between 1600 and 2000, which are around 500, approximately 2% of the total punishments. These are indicative of racial bias during these years."
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html",
    "href": "posts/JerinJacob_finalproject.html",
    "title": "JerinJacob_final_projectpdf",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readr)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(hrbrthemes)\nlibrary(usmap)\nlibrary(tidycensus)"
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#introduction",
    "href": "posts/JerinJacob_finalproject.html#introduction",
    "title": "JerinJacob_final_projectpdf",
    "section": "Introduction",
    "text": "Introduction\nThis is an analysis of crime data (both violent and non-violent) of all the counties in the State of Massachusetts for the year 2021. The crime data of all 14 counties were available, but as separate files. Also, to make a better analysis, I decided to keep the number of crimes as a function of population. The population data was downloaded from US census website. The top 3 reasons for increased crime rates at a place were identified as poverty rate, unemployment rate and youth male population. These data are also taken from census data."
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#reading-the-data",
    "href": "posts/JerinJacob_finalproject.html#reading-the-data",
    "title": "JerinJacob_final_projectpdf",
    "section": "Reading the Data:",
    "text": "Reading the Data:\nReading crime data from 14 files, concatenating them to a single dataframe & cleaning. To read all the 14 files and concatenate it as a single dataframe, a function was written.\n\nfilepath <- \"_data/601_final_project_jerin_jacob/\"\ncsv_file_names <- list.files(path = filepath, pattern = \"_2021*\")\n#csv_file_names\n\n\nread_crimes<-function(file_name){\n  x<-unlist(str_split(file_name, pattern=\"[[:punct:]]\", n=3))\n  read_csv(paste0(filepath, file_name),\n           skip = 8, \n           col_names = c(\"Location\",\"6pm-9pm\",\"9pm-12am\",\"12am-3am\",\"3am-6am\",\"6am-9am\",\"9am-12pm\",\"12pm-3pm\",\"3pm-6pm\"), show_col_types = FALSE)%>%\n             mutate(County = x[1],\n                    Year = x[2])\n}\ncounties<-\n  purrr::map_dfr(csv_file_names, read_crimes) %>%\n  select(`Location`, `12am-3am`, `3am-6am`,  `6am-9am`, `9am-12pm`, `12pm-3pm`, `3pm-6pm`, `6pm-9pm`, `9pm-12am`, `County`, `Year`)\n#counties\n\nReading & cleaning data for population, unemployment, poverty rate, age & sex, median household income\n\nma_population <- read_csv('_data/601_final_project_jerin_jacob/MA_population.csv', col_names = c(\"Number\", \"County\", \"Population\"))\n\nma_population$County <- word(ma_population$County, 1)\n\nma_population <- ma_population[ -c(1) ]\n\nma_population\n\n# A tibble: 14 × 2\n   County     Population\n   <chr>           <dbl>\n 1 Middlesex     1605899\n 2 Worcester      826655\n 3 Suffolk        801162\n 4 Essex          787038\n 5 Norfolk        703740\n 6 Bristol        563301\n 7 Plymouth       518597\n 8 Hampden        466647\n 9 Barnstable     213505\n10 Hampshire      161361\n11 Berkshire      125927\n12 Franklin        70529\n13 Dukes           17430\n14 Nantucket       11212\n\n\n\nunemployment <- read_csv('_data/601_final_project_jerin_jacob/LURReport.csv', skip = 6) %>%\n  drop_na(Month) %>%\n  filter(Month == \"Annual\") %>%\n  filter(Year == \"2020\") %>%\n  mutate(County = str_remove_all(Area, \" COUNTY\")) %>%\n  select(County, `Area Rate`)%>%\n  rename(\"unemp_rate\" = \"Area Rate\") \nunemployment$County <- str_to_title(unemployment$County)\n#unemployment\n\n\npoverty <- read_excel('_data/601_final_project_jerin_jacob/PovertyReport.xlsx', skip = 4) %>%\n  mutate(County = Name, poverty_rate = Percent...7) %>%\n  select(County, poverty_rate) %>%\n  filter(!County == \"Massachusetts\")\n\n  \n#poverty\n\n\nage_sex <- read_csv('_data/601_final_project_jerin_jacob/age_sex.csv', show_col_types = FALSE)%>%\n  mutate(County = str_remove_all(CTYNAME, \" County\")) %>%\n  filter(YEAR == 12) %>%\n    mutate(male18_24 = round((AGE1824_MALE/POPESTIMATE)*100)) %>%\n    mutate(male25_29 = round((AGE2529_MALE/POPESTIMATE)*100)) %>%\n  select(County, male18_24, male25_29)\nage_sex$young_males_total <- as.numeric(apply(age_sex[, 2:3], 1, sum))\n  \n\nage_sex <- age_sex %>%\n  select(County, young_males_total)\n\n\nmedian_hh <- read_excel('_data/601_final_project_jerin_jacob/householdincome.xlsx', skip = 2) %>%\n select(Name, `Median Household Income (2020)`)\ncolnames(median_hh) <- c('County', 'median_hh_income')\nmedianhh <- filter(median_hh, grepl(\"County\", County, ignore.case = TRUE))\nmedianhh$County <- word(filtered_medianhh$County, 1) \n\nError in vctrs::vec_recycle_common(string = string, start = start, end = end): object 'filtered_medianhh' not found\n\n#scaled_medianhh$income_factor <- scaled_medianhh$median_hh_income / 8000\nmedianhh\n\n# A tibble: 14 × 2\n   County                    median_hh_income\n   <chr>                                <dbl>\n 1 Barnstable County, MA                76287\n 2 Berkshire County, MA                 65458\n 3 Bristol County, MA                   71998\n 4 Dukes County, MA                     80459\n 5 Essex County, MA                     88269\n 6 Franklin County, MA                  62920\n 7 Hampden County, MA                   61600\n 8 Hampshire County, MA                 73864\n 9 Middlesex County, MA                111158\n10 Nantucket County/town, MA            95713\n11 Norfolk County, MA                  106348\n12 Plymouth County, MA                  88420\n13 Suffolk County, MA                   85221\n14 Worcester County, MA                 77931"
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#main-reasons-for-increase-in-crime-rates",
    "href": "posts/JerinJacob_finalproject.html#main-reasons-for-increase-in-crime-rates",
    "title": "JerinJacob_final_projectpdf",
    "section": "4 main reasons for increase in crime rates",
    "text": "4 main reasons for increase in crime rates\nTo make the analysis easier, I joined (left join) the data of the four parameters that are thought to be the main factors causing increased crime rate, which are Poverty, Unemployment, Median Household Income and Youth male population.\n\ncrimerate_reasons <- left_join(unemployment, poverty, by = 'County') %>%\n  left_join(., age_sex, by = 'County') #%>%\n  #left_join(., scaled_medianhh, by = 'County')\n  \n# crimerate_reasons %>%\n#   select(!income_factor)\n\n\nCounty_crime_total <- filter(counties, Location == \"All Location Types\")\n#County_crime_total\n#head(ma_population)\n\nTo get the crime rate for each county, I joined the population data of the counties with the crime data and made the number of crimes a function of per 100000 people in the county so that we can compare crime rate by each county.\n\ndf_crime_rate <- County_crime_total %>%\n  left_join(ma_population,by= \"County\")%>%\n  mutate(across(c(2:9),\n           .fns = ~./(Population/100000)))%>%\n  pivot_longer(cols = (ends_with(\"am\") | ends_with(\"pm\")) | ends_with(\"noon\"), names_to = \"Time\", values_to = \"Crime_Rate\")\ndf_crime_rate$Crime_Rate <- round(df_crime_rate$Crime_Rate)\ndf_crime_rate$Time<- factor(df_crime_rate$Time,                 # Relevel group factor\n                         levels = c(\"12am-3am\", \"3am-6am\", \"6am-9am\", \"9am-12pm\", \"12pm-3pm\", \"3pm-6pm\", \"6pm-9pm\",\"9pm-12am\"))\ndf_crime_rate <- df_crime_rate %>%\n  group_by(County)\ndf_crime_rate\n\n# A tibble: 112 × 6\n# Groups:   County [14]\n   Location           County     Year  Population Time     Crime_Rate\n   <chr>              <chr>      <chr>      <dbl> <fct>         <dbl>\n 1 All Location Types Barnstable 2021      213505 12am-3am        201\n 2 All Location Types Barnstable 2021      213505 3am-6am          89\n 3 All Location Types Barnstable 2021      213505 6am-9am         284\n 4 All Location Types Barnstable 2021      213505 9pm-12am        310\n 5 All Location Types Barnstable 2021      213505 9am-12pm        615\n 6 All Location Types Barnstable 2021      213505 12pm-3pm        666\n 7 All Location Types Barnstable 2021      213505 3pm-6pm         592\n 8 All Location Types Barnstable 2021      213505 6pm-9pm         454\n 9 All Location Types Berkshire  2021      125927 12am-3am        195\n10 All Location Types Berkshire  2021      125927 3am-6am         137\n# … with 102 more rows"
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#analysis",
    "href": "posts/JerinJacob_finalproject.html#analysis",
    "title": "JerinJacob_final_projectpdf",
    "section": "Analysis",
    "text": "Analysis\n\nFinding which county has the highest crime rate\nWhen I analysed the county wise crime rate, Hampden (5783) came out to be the top county in crime rate followed by Suffolk (5511) and Nantucket (4066) in the year 2021. Norfolk had the least crime rate (2186)\n\nwhich_county <- df_crime_rate %>%\n  # select(County, Crime_Rate) %>%\n  group_by(County) %>%\n  summarise(Crime_Rate = sum(Crime_Rate)) %>%\n#test\n\n  ggplot(aes(x = County, y = Crime_Rate, fill = County)) +\n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  geom_text(aes(label = Crime_Rate), vjust = 0) +\n  ggtitle(\"Counties Having Highest Crime Rate\")\n\nwhich_county\n\n\n\n\n\n\nFinding which time of the day has the highest crime rate\nSince the data had time of the day as a variable, I was curious to know what time of the day has more probability for a crime to happen. It was surprising to me to find out that 12pm-3pm had the most number of crimes followed by 3pm-6pm and 9am-12-pm. It is an interesting finding that nights are safer in Massachusetts than daytime as far as the number of crimes are concerned.\n\ntime_graph <- County_crime_total %>%\n  pivot_longer(cols = (ends_with(\"am\") | ends_with(\"pm\")) | ends_with(\"noon\"), \n               names_to = \"Time\", values_to = \"Crimes\")\ntime_graph$Time<- factor(time_graph$Time,                 # Relevel group factor\n                         levels = c(\"12am-3am\", \"3am-6am\", \"6am-9am\", \"9am-12pm\", \"12pm-3pm\", \"3pm-6pm\", \"6pm-9pm\",\"9pm-12am\"))\n\ntime_graph <- time_graph%>%\n  group_by(Time) %>%\n  summarise(Crimes = sum(Crimes))\n\ntime_graph %>%\n  tail(10) %>%\n\n  ggplot(aes(x = Time, y = Crimes)) +\n  geom_point(shape = 21, color = \"black\", fill = \"red\", size = 4) +\n  geom_line(color = \"grey\") +\n  theme_ipsum() +\n  ggtitle(\"Time When Most Crimes Happen\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nThe counties having the most crime rate follow the same pattern in the time of the day in which more crimes happen.\n\ndf_crime_rate %>%\n  filter(County == \"Hampden\" | County == \"Suffolk\"| County ==  \"Nantucket\") %>%\n  group_by(County, Time) %>%\n  summarise(CrimeRate = sum(Crime_Rate)) %>%\n  ggplot(aes(x = County, y = CrimeRate, fill = Time)) + \n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  ggtitle(\"Time vs Counties with Top Crime Rates\")"
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#analysing-the-reasons-behind-the-crime-rate.",
    "href": "posts/JerinJacob_finalproject.html#analysing-the-reasons-behind-the-crime-rate.",
    "title": "JerinJacob_final_projectpdf",
    "section": "Analysing the reasons behind the crime rate.",
    "text": "Analysing the reasons behind the crime rate.\n\n# crimerate_reasons %>%\n#   select(!income_factor)\n\nAs expected, Hampden (14.3%) and Suffolk (16.5%) had a higher rate of poverty but surprisingly, Nantucket has the lowest poverty rate (5.3%) even though Nantucket is in the top three counties in crime rate. However, Nantucket had the highest unemployment rate (11.1%) followed by Hampden (11.0%). Suffolk too had a comparatively higher unemployment rate (10.3%). When it comes to the percentage of young males of age 18-29, Hampshire is the top county regardless of its lower crime rate. But Suffolk and Hampden has some correlation between the crime rate and the percentage of young male. Hampden and Suffolk are having their median household income at $61600 and $85221 respectively which are lower than that of median household income of the Massachusetts state($89,026).\n\ncrimerate_reasons %>%\n  pivot_longer(!County, names_to = \"Reason\", values_to = \"Percent\") %>%\n  ggplot(aes(x=County, y=Percent, group=Reason, color=Reason)) +\n  geom_line() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1 )) +\n  ggtitle(\"Causes of Increased Crime Rate\")\n\n\n\n\n\nmedianhh %>%\n  ggplot(aes(x = County, y = median_hh_income, fill = median_hh_income)) +\n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  geom_text(aes(label = median_hh_income), vjust = 0, size = 3) +\n  ggtitle(\"Countywise Median Household Income\")"
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#conclusion",
    "href": "posts/JerinJacob_finalproject.html#conclusion",
    "title": "JerinJacob_final_projectpdf",
    "section": "Conclusion",
    "text": "Conclusion\nAs expected, poverty, unemployment and median household income are showing some significant correlation with the crime rates in the counties of Massachusetts. Age and gender are moderating the other reasons to increase the crime rate."
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#limitation-of-study-and-future-scope",
    "href": "posts/JerinJacob_finalproject.html#limitation-of-study-and-future-scope",
    "title": "JerinJacob_final_projectpdf",
    "section": "Limitation of study and future scope",
    "text": "Limitation of study and future scope\nThe study was done only with the data of 2021 and there are other factors that affect the crime rate. Due to the time constraint and unavailability of data, I am concluding my analysis here. But this analysis can be done in future with data of more years and including other variables that affect the crime rate."
  },
  {
    "objectID": "posts/JerinJacob_finalproject.html#bibliography",
    "href": "posts/JerinJacob_finalproject.html#bibliography",
    "title": "JerinJacob_final_projectpdf",
    "section": "Bibliography",
    "text": "Bibliography\nSource of data:\n1, https://masscrime.chs.state.ma.us/public/View/dispview.aspx\n2, https://lmi.dua.eol.mass.gov/LMI/LaborForceAndUnemployment\n3, https://data.census.gov\nProgramming Language: R\nCourse book : R for Data Science by Hadley Wickham & Garrett Grolemund"
  },
  {
    "objectID": "posts/JerinJacob_final_project.html",
    "href": "posts/JerinJacob_final_project.html",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(hrbrthemes)\nlibrary(usmap)\nlibrary(tidycensus)\n\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#introduction",
    "href": "posts/JerinJacob_final_project.html#introduction",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Introduction",
    "text": "Introduction\nThis is an analysis of crime data (both violent and non-violent) of all the counties in the State of Massachusetts for the year 2021. The crime data of all 14 counties were available, but as separate files. Also, to make a better analysis, I decided to keep the number of crimes as a function of population. The population data was downloaded from US census website. The top 3 reasons for increased crime rates at a place were identified as poverty rate, unemployment rate and youth male population. These data are also taken from census data."
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#reading-the-data",
    "href": "posts/JerinJacob_final_project.html#reading-the-data",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Reading the Data:",
    "text": "Reading the Data:\nReading crime data from 14 files, concatenating them to a single dataframe & cleaning. To read all the 14 files and concatenate it as a single dataframe, a function was written.\n\n\nCode\nfilepath <- \"_data/601_final_project_jerin_jacob/\"\ncsv_file_names <- list.files(path = filepath, pattern = \"_2021*\")\n#csv_file_names\n\n\n\n\nCode\nread_crimes<-function(file_name){\n  x<-unlist(str_split(file_name, pattern=\"[[:punct:]]\", n=3))\n  read_csv(paste0(filepath, file_name),\n           skip = 8, \n           col_names = c(\"Location\",\"6pm-9pm\",\"9pm-12am\",\"12am-3am\",\"3am-6am\",\"6am-9am\",\"9am-12pm\",\"12pm-3pm\",\"3pm-6pm\"), show_col_types = FALSE)%>%\n             mutate(County = x[1],\n                    Year = x[2])\n}\ncounties<-\n  purrr::map_dfr(csv_file_names, read_crimes) %>%\n  select(`Location`, `12am-3am`, `3am-6am`,  `6am-9am`, `9am-12pm`, `12pm-3pm`, `3pm-6pm`, `6pm-9pm`, `9pm-12am`, `County`, `Year`)\n#counties\n\n\nReading & cleaning data for population, unemployment, poverty rate, age & sex, median household income\n\n\nCode\nma_population <- read_csv('_data/601_final_project_jerin_jacob/MA_population.csv', col_names = c(\"Number\", \"County\", \"Population\"))\n\nma_population$County <- word(ma_population$County, 1)\n\nma_population <- ma_population[ -c(1) ]\n\nma_population\n\n\n# A tibble: 14 × 2\n   County     Population\n   <chr>           <dbl>\n 1 Middlesex     1605899\n 2 Worcester      826655\n 3 Suffolk        801162\n 4 Essex          787038\n 5 Norfolk        703740\n 6 Bristol        563301\n 7 Plymouth       518597\n 8 Hampden        466647\n 9 Barnstable     213505\n10 Hampshire      161361\n11 Berkshire      125927\n12 Franklin        70529\n13 Dukes           17430\n14 Nantucket       11212\n\n\n\n\nCode\nunemployment <- read_csv('_data/601_final_project_jerin_jacob/LURReport.csv', skip = 6) %>%\n  drop_na(Month) %>%\n  filter(Month == \"Annual\") %>%\n  filter(Year == \"2020\") %>%\n  mutate(County = str_remove_all(Area, \" COUNTY\")) %>%\n  select(County, `Area Rate`)%>%\n  rename(\"unemp_rate\" = \"Area Rate\") \nunemployment$County <- str_to_title(unemployment$County)\n#unemployment\n\n\n\n\nCode\npoverty <- read_excel('_data/601_final_project_jerin_jacob/PovertyReport.xlsx', skip = 4) %>%\n  mutate(County = Name, poverty_rate = Percent...7) %>%\n  select(County, poverty_rate) %>%\n  filter(!County == \"Massachusetts\")\n\n  \n#poverty\n\n\n\n\nCode\nage_sex <- read_csv('_data/601_final_project_jerin_jacob/age_sex.csv', show_col_types = FALSE)%>%\n  mutate(County = str_remove_all(CTYNAME, \" County\")) %>%\n  filter(YEAR == 12) %>%\n    mutate(male18_24 = round((AGE1824_MALE/POPESTIMATE)*100)) %>%\n    mutate(male25_29 = round((AGE2529_MALE/POPESTIMATE)*100)) %>%\n  select(County, male18_24, male25_29)\nage_sex$young_males_total <- as.numeric(apply(age_sex[, 2:3], 1, sum))\n  \n\nage_sex <- age_sex %>%\n  select(County, young_males_total)\n\n\n\n\nCode\nmedian_hh <- read_excel('_data/601_final_project_jerin_jacob/householdincome.xlsx', skip = 2) %>%\n select(Name, `Median Household Income (2020)`)\ncolnames(median_hh) <- c('County', 'median_hh_income')\nmedianhh <- filter(median_hh, grepl(\"County\", County, ignore.case = TRUE))\nmedianhh$County <- word(filtered_medianhh$County, 1) \n\n\nError in vctrs::vec_recycle_common(string = string, start = start, end = end): object 'filtered_medianhh' not found\n\n\nCode\n#scaled_medianhh$income_factor <- scaled_medianhh$median_hh_income / 8000\nmedianhh\n\n\n# A tibble: 14 × 2\n   County                    median_hh_income\n   <chr>                                <dbl>\n 1 Barnstable County, MA                76287\n 2 Berkshire County, MA                 65458\n 3 Bristol County, MA                   71998\n 4 Dukes County, MA                     80459\n 5 Essex County, MA                     88269\n 6 Franklin County, MA                  62920\n 7 Hampden County, MA                   61600\n 8 Hampshire County, MA                 73864\n 9 Middlesex County, MA                111158\n10 Nantucket County/town, MA            95713\n11 Norfolk County, MA                  106348\n12 Plymouth County, MA                  88420\n13 Suffolk County, MA                   85221\n14 Worcester County, MA                 77931"
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#main-reasons-for-increase-in-crime-rates",
    "href": "posts/JerinJacob_final_project.html#main-reasons-for-increase-in-crime-rates",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "4 main reasons for increase in crime rates",
    "text": "4 main reasons for increase in crime rates\nTo make the analysis easier, I joined (left join) the data of the four parameters that are thought to be the main factors causing increased crime rate, which are Poverty, Unemployment, Median Household Income and Youth male population.\n\n\nCode\ncrimerate_reasons <- left_join(unemployment, poverty, by = 'County') %>%\n  left_join(., age_sex, by = 'County') #%>%\n  #left_join(., scaled_medianhh, by = 'County')\n  \n# crimerate_reasons %>%\n#   select(!income_factor)\n\n\n\n\nCode\nCounty_crime_total <- filter(counties, Location == \"All Location Types\")\n#County_crime_total\n#head(ma_population)\n\n\nTo get the crime rate for each county, I joined the population data of the counties with the crime data and made the number of crimes a function of per 100000 people in the county so that we can compare crime rate by each county.\n\n\nCode\ndf_crime_rate <- County_crime_total %>%\n  left_join(ma_population,by= \"County\")%>%\n  mutate(across(c(2:9),\n           .fns = ~./(Population/100000)))%>%\n  pivot_longer(cols = (ends_with(\"am\") | ends_with(\"pm\")) | ends_with(\"noon\"), names_to = \"Time\", values_to = \"Crime_Rate\")\ndf_crime_rate$Crime_Rate <- round(df_crime_rate$Crime_Rate)\ndf_crime_rate$Time<- factor(df_crime_rate$Time,                 # Relevel group factor\n                         levels = c(\"12am-3am\", \"3am-6am\", \"6am-9am\", \"9am-12pm\", \"12pm-3pm\", \"3pm-6pm\", \"6pm-9pm\",\"9pm-12am\"))\ndf_crime_rate <- df_crime_rate %>%\n  group_by(County)\ndf_crime_rate\n\n\n# A tibble: 112 × 6\n# Groups:   County [14]\n   Location           County     Year  Population Time     Crime_Rate\n   <chr>              <chr>      <chr>      <dbl> <fct>         <dbl>\n 1 All Location Types Barnstable 2021      213505 12am-3am        201\n 2 All Location Types Barnstable 2021      213505 3am-6am          89\n 3 All Location Types Barnstable 2021      213505 6am-9am         284\n 4 All Location Types Barnstable 2021      213505 9pm-12am        310\n 5 All Location Types Barnstable 2021      213505 9am-12pm        615\n 6 All Location Types Barnstable 2021      213505 12pm-3pm        666\n 7 All Location Types Barnstable 2021      213505 3pm-6pm         592\n 8 All Location Types Barnstable 2021      213505 6pm-9pm         454\n 9 All Location Types Berkshire  2021      125927 12am-3am        195\n10 All Location Types Berkshire  2021      125927 3am-6am         137\n# … with 102 more rows"
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#analysis",
    "href": "posts/JerinJacob_final_project.html#analysis",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Analysis",
    "text": "Analysis\n\nFinding which county has the highest crime rate\nWhen I analysed the county wise crime rate, Hampden (5783) came out to be the top county in crime rate followed by Suffolk (5511) and Nantucket (4066) in the year 2021. Norfolk had the least crime rate (2186)\n\n\nCode\nwhich_county <- df_crime_rate %>%\n  # select(County, Crime_Rate) %>%\n  group_by(County) %>%\n  summarise(Crime_Rate = sum(Crime_Rate)) %>%\n#test\n\n  ggplot(aes(x = County, y = Crime_Rate, fill = County)) +\n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  geom_text(aes(label = Crime_Rate), vjust = 0) +\n  ggtitle(\"Counties Having Highest Crime Rate\")\n\nwhich_county\n\n\n\n\n\n\n\nFinding which time of the day has the highest crime rate\nSince the data had time of the day as a variable, I was curious to know what time of the day has more probability for a crime to happen. It was surprising to me to find out that 12pm-3pm had the most number of crimes followed by 3pm-6pm and 9am-12-pm. It is an interesting finding that nights are safer in Massachusetts than daytime as far as the number of crimes are concerned.\n\n\nCode\ntime_graph <- County_crime_total %>%\n  pivot_longer(cols = (ends_with(\"am\") | ends_with(\"pm\")) | ends_with(\"noon\"), \n               names_to = \"Time\", values_to = \"Crimes\")\ntime_graph$Time<- factor(time_graph$Time,                 # Relevel group factor\n                         levels = c(\"12am-3am\", \"3am-6am\", \"6am-9am\", \"9am-12pm\", \"12pm-3pm\", \"3pm-6pm\", \"6pm-9pm\",\"9pm-12am\"))\n\ntime_graph <- time_graph%>%\n  group_by(Time) %>%\n  summarise(Crimes = sum(Crimes))\n\ntime_graph %>%\n  tail(10) %>%\n\n  ggplot(aes(x = Time, y = Crimes)) +\n  geom_point(shape = 21, color = \"black\", fill = \"red\", size = 4) +\n  geom_line(color = \"grey\") +\n  theme_ipsum() +\n  ggtitle(\"Time When Most Crimes Happen\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\nThe counties having the most crime rate follow the same pattern in the time of the day in which more crimes happen.\n\n\nCode\ndf_crime_rate %>%\n  filter(County == \"Hampden\" | County == \"Suffolk\"| County ==  \"Nantucket\") %>%\n  group_by(County, Time) %>%\n  summarise(CrimeRate = sum(Crime_Rate)) %>%\n  ggplot(aes(x = County, y = CrimeRate, fill = Time)) + \n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  ggtitle(\"Time vs Counties with Top Crime Rates\")"
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#analysing-the-reasons-behind-the-crime-rate.",
    "href": "posts/JerinJacob_final_project.html#analysing-the-reasons-behind-the-crime-rate.",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Analysing the reasons behind the crime rate.",
    "text": "Analysing the reasons behind the crime rate.\n\n\nCode\n# crimerate_reasons %>%\n#   select(!income_factor)\n\n\nAs expected, Hampden (14.3%) and Suffolk (16.5%) had a higher rate of poverty but surprisingly, Nantucket has the lowest poverty rate (5.3%) even though Nantucket is in the top three counties in crime rate. However, Nantucket had the highest unemployment rate (11.1%) followed by Hampden (11.0%). Suffolk too had a comparatively higher unemployment rate (10.3%). When it comes to the percentage of young males of age 18-29, Hampshire is the top county regardless of its lower crime rate. But Suffolk and Hampden has some correlation between the crime rate and the percentage of young male. Hampden and Suffolk are having their median household income at $61600 and $85221 respectively which are lower than that of median household income of the Massachusetts state($89,026).\n\n\nCode\ncrimerate_reasons %>%\n  pivot_longer(!County, names_to = \"Reason\", values_to = \"Percent\") %>%\n  ggplot(aes(x=County, y=Percent, group=Reason, color=Reason)) +\n  geom_line() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1 )) +\n  ggtitle(\"Causes of Increased Crime Rate\")\n\n\n\n\n\n\n\nCode\nmedianhh %>%\n  ggplot(aes(x = County, y = median_hh_income, fill = median_hh_income)) +\n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  geom_text(aes(label = median_hh_income), vjust = 0, size = 3) +\n  ggtitle(\"Countywise Median Household Income\")"
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#conclusion",
    "href": "posts/JerinJacob_final_project.html#conclusion",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Conclusion",
    "text": "Conclusion\nAs expected, poverty, unemployment and median household income are showing some significant correlation with the crime rates in the counties of Massachusetts. Age and gender are moderating the other reasons to increase the crime rate."
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#limitation-of-study-and-future-scope",
    "href": "posts/JerinJacob_final_project.html#limitation-of-study-and-future-scope",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Limitation of study and future scope",
    "text": "Limitation of study and future scope\nThe study was done only with the data of 2021 and there are other factors that affect the crime rate. Due to the time constraint and unavailability of data, I am concluding my analysis here. But this analysis can be done in future with data of more years and including other variables that affect the crime rate."
  },
  {
    "objectID": "posts/JerinJacob_final_project.html#bibliography",
    "href": "posts/JerinJacob_final_project.html#bibliography",
    "title": "601 Fall Final Project- A study on the crime data of Massachusetts state",
    "section": "Bibliography",
    "text": "Bibliography\nSource of data:\n1, https://masscrime.chs.state.ma.us/public/View/dispview.aspx\n2, https://lmi.dua.eol.mass.gov/LMI/LaborForceAndUnemployment\n3, https://data.census.gov\nProgramming Language: R\nCourse book : R for Data Science by Hadley Wickham & Garrett Grolemund"
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nload(\"KarlaBarrett_Final.RData\")\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#introduction",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nMaine is one of the top 10 least diverse states in the United States, although, it has seen an increase in immigrant and refugee resettlement rates over the past decade. This is something I consider quite a bit, as I manage a medical residency program and one of our goals is to increase the diversity of our faculty and residents to better reflect the demographics of the patient population we are caring for. This goal has yet to be achieved, so I thought it would be interesting to explore some data about migration patterns to and from Maine and ask, “is Maine becoming a more diverse state?”. This was the original impetus for my final paper, however, I also found it interesting to explore where people are moving to when they leave Maine and where they are moving from when they come to Maine."
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#data",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#data",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nThe data chosen for this project details migration patterns for people born between the years 1984 and 1992. The data was captured by US Census, tax and HUD reporting.The primary variables of interest are: origin city/state (where the person was living at age 16) and destination city/state (where the person was living at age 26), race, and parental income level. The data was downloaded from migrationpatterns.org, a collaborative research project between the U.S. Census Bureau and Harvard University.\nI tidied the data in a few ways:\n\nRead in the data and labeled the original data frame “MillenialMigration”\nRenamed the column names for ease of understanding (e.g. d_cz_name became Dest_City)\nThe race and parental income data was combined in one column in the original data set. I decided to separate out the two data points into different columns by using the mutate function to replace the original character format (e.g. AsianQ1) to a format that included an underscore (e.g. Asian_Q1) so I could then use the separate function, with the underscore as the separator to create the two distinct columns.I thought it would be interesting to be able to explore the race and income level data points on their own.\nFinally, after struggling with the lag time to process all the code to accomplish separating out the columns, I opted to save the new data frame as a csv file using write.csv so I could continue on with the project without waiting for the code to run from start to finish every time I revisited my work. I have included all the code used to get to this point as comments.\n\nAfter the initial tidying, I started zeroing in on the data for Maine, by creating new data frames for:\n\nMigration to Maine\nMigration from Maine\nBoth migration to and from Maine\n\nSome of the questions I wanted to explore at this point were:\n\nDid more people migrate to or from Maine?\nMigration to Maine\n\nWhere did people migrate from?\nWhere in Maine did they settle?\nIs there a clear race or parental income level pattern for migration into the state? Is there a difference between a popular urban location such as Portland, and a more rural city such as Augusta?\n\nMigration from Maine\n\nWhere did they go?\nRace or parental income level patterns?\n\n\nTo help me answer some of my questions, I did some setup by summarizing and joining of data such as migration patterns by Maine cities, states, race, and parental income. This setup was to aid in additional transformations and visualizations. A lot of this work ended up being unnecessary, but it was good practice!\nInterestingly, there are only four Maine cities reflected in the data set: Portland, Bangor, Presque Isle, and Calais, which certainly turned my focus away from spending too much time looking into where exactly in Maine people moved to and from as those four cities do not paint a full picture. Portland is by far the largest city by population so it would not be surprising to see the most migration in and out of Portland due to that fact alone. The four cities included in the original data set as they are vastly different, especially Presque Isle and Calais, are two of the most remote towns in the state.\n\n\nCode\n#| label: Data Transformations\n#| warning: false\n#| \n#Read in original data set\n# MillenialMigration <- read_csv(\"_data/od.csv\")\nhead(MillenialMigration)\n\n\n\n\n  \n\n\n\nCode\n#Changed column names for ease of understanding\nMillenialMigration <- MillenialMigration %>%\n  rename(Origin_Zone = o_cz,\n         Origin_City = o_cz_name,\n         Origin_State = o_state_name,\n         Dest_Zone = d_cz,\n         Dest_City = d_cz_name,\n         Dest_State = d_state_name,\n         Num_Migrators = n,\n         N_from_Origin = n_tot_o,\n         N_from_Dest = n_tot_d,\n         Race_ParentalIncome = pool,\n         Num_Migrators = n)\n\n\nError in `rename()`:\n! Can't rename columns that don't exist.\n✖ Column `o_cz` doesn't exist.\n\n\nCode\nMillenialMigration\n\n\n\n\n  \n\n\n\nCode\n#I wanted to separate the Race and Parental Income data, in order to analyze the data separately in future iterations. I could not figure out a way to separate the two  without putting a character in between. I used the following code to update the Race_ParentalIncome column to have an underscore in it. This was probably  not the most efficient way to accomplish this outcome and it took me quite a while to get the code right.In addition, I found it was taking a very long time to run this code every time I returned to work on the assignment, so I decided to export the new CSV file and am writing in all the code used to get to this step as comments to show my work and continuing with the new file to avoid the time issues I was having running the code. \n#MillenialMigration_ <- MillenialMigration %>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"AsianQ1\", \"Asian_Q1\"))%>%\n  # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"AsianQ2\", \"Asian_Q2\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"AsianQ3\", \"Asian_Q3\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"AsianQ4\", \"Asian_Q4\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"AsianQ5\", \"Asian_Q5\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"BlackQ1\", \"Black_Q1\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"BlackQ2\", \"Black_Q2\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"BlackQ3\", \"Black_Q3\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"BlackQ4\", \"Black_Q4\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"BlackQ5\", \"Black_Q5\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"HispanicQ1\", \"Hispanic_Q1\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"HispanicQ2\", \"Hispanic_Q2\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"HispanicQ3\", \"Hispanic_Q3\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"HispanicQ4\", \"Hispanic_Q4\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"HispanicQ5\", \"Hispanic_Q5\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"OtherQ1\", \"Other_Q1\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"OtherQ2\", \"Other_Q2\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"OtherQ3\", \"Other_Q3\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"OtherQ4\", \"Other_Q4\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"OtherQ5\", \"Other_Q5\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"WhiteQ1\", \"White_Q1\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"WhiteQ2\", \"White_Q2\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"WhiteQ3\", \"White_Q3\"))%>%\n # mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"WhiteQ4\", \"White_Q4\"))%>%\n #  mutate(Race_ParentalIncome = stringr::str_replace(Race_ParentalIncome, \"WhiteQ5\", \"White_Q5\"))\n\n#I used the following code to check my work\n#MigratorsByRace_Income <- MillenialMigration_ %>%\n # group_by(Race_ParentalIncome) %>%\n  #summarise(Freq = sum(Num_Migrators))\n#print(n=30, MigratorsByRace_Income)\n\n#The following code was used to separate the column and create two new columns, one for Race and one for Parental Income. \n#MillenialMigration_Sep <- separate(MillenialMigration_, Race_ParentalIncome, into = c(\"Race\", \"Parental_Income\"), sep = \"_\")\n#MillenialMigration_Sep\n\n#The following code was used to create a new CSV file with the separated columns.\n#write.csv(MillenialMigration_Sep, file = \"C:\\\\Users\\\\kbarr\\\\OneDrive\\\\Documents\\\\GitHub\\\\601_Fall_2022\\\\posts\\\\MillenialMigration_Sep.csv\", row.names = FALSE)\n\n#Read in updated data set with separate columns for race and parental income\n# MillenialMigration_Sep <- read_csv(\"_data/MillenialMigration_Sep.csv\")\nhead(MillenialMigration_Sep)\n\n\n\n\n  \n\n\n\nCode\n# save.image(\"KarlaBarrett_Final.RData\")\ncolnames(MillenialMigration_Sep)\n\n\n [1] \"Origin_Zone\"     \"Origin_City\"     \"Origin_State\"    \"Dest_Zone\"      \n [5] \"Dest_City\"       \"Dest_State\"      \"Num_Migrators\"   \"N_from_Origin\"  \n [9] \"N_from_Dest\"     \"Race\"            \"Parental_Income\" \"pr_d_o\"         \n[13] \"pr_o_d\"         \n\n\nCode\n#The following code was used to set up smaller data frames to aid in visualizations and analysis\n\n#Filter for Maine as destination state\nMillenialMigration_to_Maine <- MillenialMigration_Sep %>%\n  filter(Dest_State== \"Maine\")\n#Filter for Maine as origin state\nMillenialMigration_from_Maine <- MillenialMigration_Sep %>%\n  filter(Origin_State== \"Maine\")\n#Summarise total migrators to cities in Maine\nMillenialMigration_to_Maine_Sum <- MillenialMigration_to_Maine %>%\n  group_by(Dest_City) %>%\n  summarise(Num_Migrators_to_Maine = sum(Num_Migrators))\n#Rename City variable to prep for join\nMillenialMigration_to_Maine_Sum <- MillenialMigration_to_Maine_Sum %>%\n  rename(City = Dest_City)\n#Summarise total migrators from cities in Maine\nMillenialMigration_from_Maine_Sum <- MillenialMigration_from_Maine %>%\n  group_by(Origin_City) %>%\n  summarise(Num_Migrators_from_Maine = sum(Num_Migrators))\n#Rename City variable to prep for join\nMillenialMigration_from_Maine_Sum <- MillenialMigration_from_Maine_Sum %>%\n  rename(City = Origin_City)\n#Join totals to and from Maine\nMigration_In_And_Out_Maine <- inner_join(MillenialMigration_from_Maine_Sum, MillenialMigration_to_Maine_Sum, by=\"City\")\nMigration_In_And_Out_Maine\n\n\n\n\n  \n\n\n\nCode\n#Formula for data frame with both Maine as origin and destination (total migrators touching Maine)\nMillenialMigration_Maine_All <- MillenialMigration_Sep %>%\n  filter(Dest_State== \"Maine\" | Origin_State == \"Maine\")\nMillenialMigration_Maine_All\n\n\n\n\n  \n\n\n\nCode\n#Migrators by race/income - all data\nMigratorsByIncome <- MillenialMigration_Sep %>%\n  group_by(Parental_Income) %>%\n  summarise(Num_Migrators= sum(Num_Migrators))\n\nMigratorsByRace <- MillenialMigration_Sep %>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators))\n\n#Migrators by race/income - Maine data\nMigration_to_Maine_Race <- MillenialMigration_to_Maine %>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators))\n\nMigration_to_Maine_Income <- MillenialMigration_to_Maine %>%\n  group_by(Parental_Income) %>%\n  summarise(Num_Migrators= sum(Num_Migrators))\n\nMigration_from_Maine_Race <- MillenialMigration_to_Maine %>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators))\n\nMigration_from_Maine_Income <- MillenialMigration_to_Maine %>%\n  group_by(Parental_Income) %>%\n  summarise(Num_Migrators= sum(Num_Migrators))\n\n\n# Tidy to Maine by State\nTo_Maine_Sum_By_State <- MillenialMigration_to_Maine %>%\n  group_by(Origin_State) %>%\n  summarise(Num_Migrators = sum(Num_Migrators))\n  \nTo_Maine_Sum_By_State <- To_Maine_Sum_By_State %>% arrange(desc(Num_Migrators))\n\nTo_Maine_Sum_By_State <- To_Maine_Sum_By_State %>%\n  rename(Num_Migrators_to_Maine = Num_Migrators)\nTo_Maine_Sum_By_State <- To_Maine_Sum_By_State %>%\n  rename(State = Origin_State)\n\n\n# Tidy from Maine by State\nFrom_Maine_Sum_By_State <- MillenialMigration_from_Maine %>%\n  group_by(Dest_State) %>%\n  summarise(Num_Migrators = sum(Num_Migrators))\n\nFrom_Maine_Sum_By_State <- From_Maine_Sum_By_State %>% arrange(desc(Num_Migrators))\n\nFrom_Maine_Sum_By_State <- From_Maine_Sum_By_State %>%\n  rename(Num_Migrators_from_Maine = Num_Migrators)\nFrom_Maine_Sum_By_State <- From_Maine_Sum_By_State %>%\n  rename(State = Dest_State)\n\nTo_Maine_Sum_By_State\n\n\n\n\n  \n\n\n\nCode\nFrom_Maine_Sum_By_State"
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#visualizations-state-to-state-movement",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#visualizations-state-to-state-movement",
    "title": "Final Project",
    "section": "Visualizations: State to State Movement",
    "text": "Visualizations: State to State Movement\nThe first visualization I created was simply to compare the total number of migrators in and out of each city.Portland and Bangor saw the most migration, Calais and Presque Isle saw the least, the results were not surprising given the population size of each city as mentioned before. Each city experienced more migration out than in.\n\n\nCode\n#Table transformation and ggplot code for total number of migrators from each Maine city\nMigrationMaineCombined <- Migration_In_And_Out_Maine %>% gather(Total, Value, -City)\nMigrationMaineCombined \n\n\n\n\n  \n\n\n\nCode\nggplot(MigrationMaineCombined, aes(x = City, y = Value, fill = Total)) +\n  geom_col(position = \"dodge\") + labs(title = \"Maine Migration by Cities\", y = \"Total Migrators\", x = \"City in Maine\") + geom_text(aes(label = Value, hjust = \"center\"))\n\n\n\n\n\nThe next question I explored was where are people migrating to Maine from? It turns out, the vast most of the migration in Maine is occurring within Maine. After inter-state movement, the largest group of people to move to Maine were from other Northeastern states (VT, NY, CT, NH, etc.). I originally made the visualization to include all states, but decided to filter it down to the top 10. I also chose to put the states on the y axis for a better visual representation.\n\n\nCode\n#| label: Where did they come from?\n#| warning: false\n\n#Original code for plot with all states: ggplot(To_Maine_Sum_By_State, aes(Num_Migrators_to_Maine, State)) + geom_bar(stat = \"identity\", fill = \"cadetblue\")\n\nTo_Maine_Sum_By_State %>%\n  filter(Num_Migrators_to_Maine >= 637) %>%\n  ggplot(aes(Num_Migrators_to_Maine, State)) + geom_bar(stat = \"identity\", fill = \"cadetblue\") + labs(title = \"Where Did People Come to Maine From?\", x = \"Number of Migrators\")\n\n\n\n\n\nAfter, I used the same process to create a graph showing where people from Maine were moving to. Seven of the top 10 states showed up on both lists. The three states on the list of places people moved to Maine from but not on the list of states people moved to were: Vermont, Rhode Island, and Connecticut. Alternatively, the three states not in the top ten list for states people moved from are: Texas, North Carolina, and Colorado. Anecdotally, I am not surprised to see Colorado in the top ten list of states Mainers move to, skiers love to go out west!\n\n\nCode\n#| label: Where did they go to?\n#| warning: false\n\nFrom_Maine_Sum_By_State %>%\n  filter(Num_Migrators_from_Maine >= 988) %>%\n  ggplot(aes(Num_Migrators_from_Maine, State)) + geom_bar(stat = \"identity\", fill = \"cadetblue\") + labs(title = \"Where Did Mainers go?\", x = \"Number of Migrators\")\n\n\n\n\n\nFinally, I wanted to see the movement in and out of each state on the same graph. I created two versions of this, the second with a facet wrap after creating a new column with grouped values.The facet wrap helped spread the plot points out and I believe made it a better visual aid. Although, there is not much to take away from these charts other than viewing the difference between migration to and from Maine between different states.\n\n\nCode\n# Join to and from Maine\nTo_and_From_Maine_ByState <- inner_join(To_Maine_Sum_By_State, From_Maine_Sum_By_State, by=\"State\")\n\nTo_and_From_Maine_ByState_Combined <- To_and_From_Maine_ByState %>% gather(Total, Value, -State)\n\nggplot(To_and_From_Maine_ByState_Combined, aes(x = Value, y = State)) +\n  geom_point() + labs(title = \"Where Did People Move to and From\", y = \"State\", x = \"Value\")+\n  geom_point(aes(color = factor(Total)))\n\n\n\n\n\nCode\nToAndFrom <- To_and_From_Maine_ByState_Combined %>% mutate(Total = case_when(Value <100 ~ \"Under 100 Migrators\", \n                                     Value <500 ~ \"Under 500 Migrators\",\n                                     Value >501 ~ \"Over 500  Migrators\"))\n\nToAndFrom <- To_and_From_Maine_ByState_Combined %>%\n  select(State, Total, Value)%>%\n  mutate(\n    Value_Groups = case_when(Value <100 ~ \"Under 100 Migrators\", \n                                     Value <500 ~ \"Under 500 Migrators\",\n                                     Value >501 ~ \"Over 500  Migrators\"))\nToAndFrom\n\n\n\n\n  \n\n\n\nCode\nState_Movement_Facet <- ggplot(ToAndFrom, aes(x = Value, y = State)) +\n  geom_point() + labs(title = \"Where Did People Move to and From\", y = \"State\", x = \"Value\")+\n  geom_point(aes(color = factor(Total)))+\n  facet_wrap(~Value_Groups)\nState_Movement_Facet"
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#visualizations-race-and-income",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#visualizations-race-and-income",
    "title": "Final Project",
    "section": "Visualizations: Race and Income",
    "text": "Visualizations: Race and Income\nI moved on to exploring the relationship between migration patterns and race and income levels (specifically parental income levels, which I think is important to note as generational wealth is a significant factor in someone’s ability to move).\nI grouped the data by parental income or race for both Maine data, and the original data set as a whole, then summarized the number of migrators in each category. I renamed columns in order to join the the Maine data and the total data in one data frame that would allow me to compare the data in the same visualization.\nThe migration patterns by race were similar for both Maine and the entire data set. The income category however saw an oddity, the highest parental income quintile in Maine saw very little migration compared to the migration patterns for the whole data set.\nGiven how much more migration is seen among white populations, I wanted to focus in on non-white populations to help answer my question of “is Maine becoming more diverse”. I found the difference between the number of non-white migrators that came to Maine and left Maine and created a bar chart. There was a very slight increase in black and hispanic populations, but asian and “other” both dropped. Overall, based on this limited data set, it appears Maine is not becoming more diverse.\n\n\nCode\n#Migrators by Income Data Transformation\nMigratorsByIncome <- MillenialMigration_Sep %>%\n  group_by(Parental_Income) %>%\n  summarise(Num_Migrators= sum(Num_Migrators)) %>%\n  rename(Num_All_Income = Num_Migrators)\n\nMaine_Migration_Income <- MillenialMigration_Maine_All %>%\n  group_by(Parental_Income) %>%\n  summarise(Num_Migrators= sum(Num_Migrators)) %>%\n  rename(Num_Maine_Income = Num_Migrators)\n\nMaine_vs_All_Income <- inner_join(MigratorsByIncome, Maine_Migration_Income, by=\"Parental_Income\")\n\nMaine_vs_All_Income_Transform <- Maine_vs_All_Income %>% gather(AllOrMaine, Num_Migrators, -Parental_Income)\n\nMaine_vs_All_Income_Transform_Groups<- Maine_vs_All_Income_Transform %>%\n  select(Parental_Income, Num_Migrators, AllOrMaine)%>%\n  mutate(\n    AllorMaineGroups = case_when(AllOrMaine == \"Num_All_Income\" ~ \"All Data\", AllOrMaine == \"Num_Maine_Income\" ~ \"Maine Data\"))\n\nggplot(Maine_vs_All_Income_Transform_Groups, aes(x = Num_Migrators, y = Parental_Income)) +\n  geom_count()+ labs(title = \"Migration by Income Level - Maine vs All Data\")+\n  geom_count(aes(color = factor(Parental_Income)))+\n  facet_wrap(~AllorMaineGroups, scales = \"free\")\n\n\n\n\n\nCode\n#Migrators by Race Data Transformation\nMigratorsByRace <- MillenialMigration_Sep %>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators)) %>%\n  rename(Num_All_Race = Num_Migrators)\n\nMaine_Migration_Race <- MillenialMigration_Maine_All %>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators)) %>%\n  rename(Num_Maine_Race = Num_Migrators)\n\nMaine_vs_All_Race <- inner_join(MigratorsByRace, Maine_Migration_Race, by=\"Race\")\n\nMaine_vs_All_Race_Transform <- Maine_vs_All_Race %>% gather(AllOrMaine, Num_Migrators, -Race)\n\nMaine_vs_All_Race_Transform_Groups<- Maine_vs_All_Race_Transform %>%\n  select(Race, Num_Migrators, AllOrMaine)%>%\n  mutate(\n    AllorMaineGroups = case_when(AllOrMaine == \"Num_All_Race\" ~ \"All Data\", AllOrMaine == \"Num_Maine_Race\" ~ \"Maine Data\"))\n\nggplot(Maine_vs_All_Race_Transform_Groups, aes(x = Num_Migrators, y = Race)) +\n  geom_count()+ labs(title = \"Migration by Race - Maine vs All Data\")+\n  geom_count(aes(color = factor(AllorMaineGroups)))+\n  scale_color_manual(values = c(\"All Data\" = \"thistle4\", \"Maine Data\" = \"cadetblue\"))+\n  facet_wrap(~AllorMaineGroups, scales = \"free\")\n\n\n\n\n\nCode\n#Exploring non-white migration to and from Maine\nTo_Maine_nonWhite_Migration <-MillenialMigration_to_Maine%>%\n  filter(Race != \"White\")%>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators)) %>%\n  rename(Num_to_Maine_Race = Num_Migrators)\n\nFrom_Maine_nonWhite_Migration <- MillenialMigration_from_Maine%>%\n  filter(Race != \"White\")%>%\n  group_by(Race) %>%\n  summarise(Num_Migrators= sum(Num_Migrators)) %>%\n  rename(Num_from_Maine_Race = Num_Migrators)\n\nMaine_non_white_migration <- inner_join(To_Maine_nonWhite_Migration, From_Maine_nonWhite_Migration, by=\"Race\")\n\nMaine_non_white_migration_difference <- Maine_non_white_migration%>% \n  select(Race, Num_from_Maine_Race, Num_to_Maine_Race) %>%\n  mutate(Difference = Num_to_Maine_Race - Num_from_Maine_Race)\nMaine_non_white_migration_difference\n\n\n\n\n  \n\n\n\nCode\nggplot(Maine_non_white_migration_difference, aes(fill=Race, y=Difference, x=Race)) + \n  geom_bar(position=\"stack\", stat = \"identity\") + labs(title = \"Change in Non-white Population in Maine between 1984 and 1992\", y = \"Change\" )"
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#reflection",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#reflection",
    "title": "Final Project",
    "section": "Reflection",
    "text": "Reflection\nI did not read the full report available on migrationpatterns.org until after I finished my project, as I wanted to avoid the analysis of much more experienced researchers affecting my approach to the project, both the questions I wanted to investigate and the scope of my abilities. For example, after working with the data, I determined the best visualizations to use would be bar charts and point plots, due to the fact that I was only working with one numerical variable. Upon reviewing the report, I discovered there are so many more possibilities and I am both glad I did not see it beforehand (to keep my goal output realistic) and a bit disappointed I did not experiment more with histograms box plots, for example. Also, after reviewing the report, I would like to learn how to plot the migration data on a map of the U.S. I researched how to do it a bit and determined it was not feasible for this project but I would like to keep trying in the future.\nOne of the most challenging aspects of this project was the time spent trying to find tiny little mistakes that affected large portions of my work (a missed comma, using the wrong column name, using + when I needed to use %>%), but ultimately, I found the experience to be incredibly valuable as I am now much better at spotting mistakes and every problem solved, no matter how little felt like a huge win. Another challenge I had that I did not end up solving is changing the order of the facets in my charts. I searched for multiple hours for a solution to no avail and the order of the facets did not end up in ascending or descending order."
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#conclusion",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#conclusion",
    "title": "Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nOne of the questions I was looking to answer with this project was, “is Maine becoming more diverse?”. The conclusion I came to with this data set is, no. However, there are a number of factors that limited my ability to fully explore this question:\n\nThis data set is not capturing migration from other countries. Maine has seen an increase in immigrant and refugee resettlement, particularly in the larger cities (Portland, Lewiston, Augusta, Bangor) since this time frame in which this data was collected.\nMaine has also seen in influx in migration from other states due to the pandemic and the remote work revolution, data from the past few years would likely look different than this data from 1984-1992.\n\nOther questions I still have are:\n\nWhat incomes ranges are included in each quintile?\nHow much does college choice affect migration patterns? - I think this would be an interesting question to explore!\nHow was the data chosen?\nWhy are only four cities in Maine included in the data set?"
  },
  {
    "objectID": "posts/KarlaBarrett-Dexter.FinalProject.html#bibliography",
    "href": "posts/KarlaBarrett-Dexter.FinalProject.html#bibliography",
    "title": "Final Project",
    "section": "Bibliography",
    "text": "Bibliography\nTextbook: Grolemund, G., & Wickham, H. (2017). R for Data Science. O’Reilly Media.\nData source: U.S Census Bureau, Harvard University. (n.d.). Young Adult Migration. Migration Patterns. Retrieved December 18, 2022, from https://migrationpatterns.org/\nR: R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.\nSape Research Group. (n.d.) ggplot2 Quick Reference: colour (and fill). Software and programmer Efficiency Research Group. Retrieved December 18, 2022, from http://sape.inf.usi.ch/quick-reference/ggplot2/colour\nhttps://datatofish.com/export-dataframe-to-csv-in-r/\nData to Fish. (n.d.). How to Export DataFrame to CSV in R. Retrieved December 18, 2022, fromdatatofish.com/export-dataframe-to-csv-in-r/\n:::"
  },
  {
    "objectID": "posts/Manan_Patel_PostTemplate.html",
    "href": "posts/Manan_Patel_PostTemplate.html",
    "title": "Layoffs dataset",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(stats)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Manan_Patel_PostTemplate.html#research-questions-",
    "href": "posts/Manan_Patel_PostTemplate.html#research-questions-",
    "title": "Layoffs dataset",
    "section": "Research Questions-",
    "text": "Research Questions-\n\nA list of the top 10 companies that have laid off employees.\nThe top 3 companies that have laid off employees on a yearly basis.\nThe top 3 locations where the most layoffs have occurred on a yearly basis.\nA list of the top 20 companies that have laid off a specific percentage of their employees.\nThe top 10 countries where the most layoffs have occurred.\nThe top 10 locations in the United States where the most layoffs have occurred.\nThe locations in India where the most layoffs have occurred.\nThe relationship between funds received and layoffs.\nThe stage of a company’s development at which the most layoffs have occurred.\nThe industry that has experienced the most layoffs.\nThe total number of layoffs on a yearly basis.\nThe number of layoffs that have occurred on a yearly basis according to country.\n\nWe will try our best to answer each on of the questions mentioned above, let’s get started-"
  },
  {
    "objectID": "posts/Manan_Patel_PostTemplate.html#reading-the-dataset-",
    "href": "posts/Manan_Patel_PostTemplate.html#reading-the-dataset-",
    "title": "Layoffs dataset",
    "section": "Reading the dataset-",
    "text": "Reading the dataset-\n\n\nCode\n# data frame\ndf <- read.csv(\"_data/layoffs.csv\")\nhead(df)\n\n\n   company      location  industry total_laid_off percentage_laid_off\n1  Vedantu     Bengaluru Education            385                  NA\n2    Plaid   SF Bay Area   Finance            260                0.20\n3   Grover        Berlin    Retail             40                0.10\n4 CircleCI   SF Bay Area   Product             NA                0.17\n5     Doma   SF Bay Area   Finance            515                0.40\n6 BuzzFeed New York City     Media            180                0.12\n        date    stage       country funds_raised\n1 2022-12-07 Series E         India          292\n2 2022-12-07 Series D United States          734\n3 2022-12-07  Unknown United States         2300\n4 2022-12-07 Series F United States          315\n5 2022-12-06      IPO United States          679\n6 2022-12-06      IPO United States          696"
  },
  {
    "objectID": "posts/Manan_Patel_PostTemplate.html#data-description-",
    "href": "posts/Manan_Patel_PostTemplate.html#data-description-",
    "title": "Layoffs dataset",
    "section": "Data Description-",
    "text": "Data Description-\n\ncompany - This column refers to the name of the company that has experienced a layoff.\nlocation - This column refers to the location of the layoff, which could be a city, state, or region.\nindustry - This column refers to the industry in which the company operates, such as tech, healthcare, or finance.\ntotal_laid_off - This column refers to the total number of employees who were laid off by the company.\npercentage_laid_off - This column refers to the percentage of the company’s workforce that was laid off.\ndate - This column refers to the date on which the layoff occurred.\nstage - This column refers to the stage of funding that the company was in at the time of the layoff. This could be early-stage, late-stage, or publicly traded.\ncountry - This column refers to the name of the country in which the company is located.\nfunds_raised - This column refers to the amount of funds that the company had raised, in millions of dollars, prior to the layoff. This information could be relevant in understanding the financial situation of the company at the time of the layoff.\n\nLet’s have a look on the last few rows-\n\n\nCode\n# Get the last six rows of df\ntail(df)\n\n\n           company       location       industry total_laid_off\n1747       Service    Los Angeles         Travel             NA\n1748  HopSkipDrive    Los Angeles Transportation              8\n1749   Panda Squad    SF Bay Area       Consumer              6\n1750 Tamara Mellon    Los Angeles         Retail             20\n1751      EasyPost Salt Lake City      Logistics             75\n1752     Homebound    SF Bay Area    Real Estate             NA\n     percentage_laid_off       date    stage       country funds_raised\n1747                1.00 2020-03-16     Seed United States          5.1\n1748                0.10 2020-03-13  Unknown United States         45.0\n1749                0.75 2020-03-13     Seed United States          1.0\n1750                0.40 2020-03-12 Series C United States         90.0\n1751                  NA 2020-03-11 Series A United States         12.0\n1752                  NA             Unknown United States        128.0\n\n\nGet the class of each column in df-\n\n\nCode\n# sapply will get the class of each column\nsapply(df, class)\n\n\n            company            location            industry      total_laid_off \n        \"character\"         \"character\"         \"character\"           \"integer\" \npercentage_laid_off                date               stage             country \n          \"numeric\"         \"character\"         \"character\"         \"character\" \n       funds_raised \n          \"numeric\""
  },
  {
    "objectID": "posts/Manan_Patel_PostTemplate.html#checking-for-all-possible-na-and-cleaning-up-the-datasets-",
    "href": "posts/Manan_Patel_PostTemplate.html#checking-for-all-possible-na-and-cleaning-up-the-datasets-",
    "title": "Layoffs dataset",
    "section": "Checking for all possible NA and cleaning up the datasets-",
    "text": "Checking for all possible NA and cleaning up the datasets-\n\n\nCode\n# Use sapply to apply a lambda function to each column in the df data frame\nmissing_counts <- sapply(df, function(x) {\n  # Within the lambda function, we will use the sum and is.na functions to count the number of missing values in x\n  sum(is.na(x))\n})\n# then finally we will print the resulting vector of missing value counts\nmissing_counts\n\n\n            company            location            industry      total_laid_off \n                  0                   0                   0                 520 \npercentage_laid_off                date               stage             country \n                576                   0                   0                   0 \n       funds_raised \n                130 \n\n\n\n\nCode\n# Use the drop_na function to remove rows with missing values from the df\ndf <- df %>% drop_na()\n# Using sapply to apply a lambda function to each column in the df\nmissing_counts <- sapply(df, function(x) {\n# Within the lambda function, use the sum and is.na functions to count the number of missing values in x\n  sum(is.na(x))\n})\n# then finally we will print the resulting vector of missing value counts\nmissing_counts\n\n\n            company            location            industry      total_laid_off \n                  0                   0                   0                   0 \npercentage_laid_off                date               stage             country \n                  0                   0                   0                   0 \n       funds_raised \n                  0 \n\n\nLet’s convert character columns to factor columns in df-\n\n\nCode\n# converting character to factor\ndf <- df %>% mutate_if(is.character,as.factor)\nglimpse(df)\n\n\nRows: 859\nColumns: 9\n$ company             <fct> Plaid, Grover, Doma, BuzzFeed, Chipper Cash, Stash…\n$ location            <fct> SF Bay Area, Berlin, SF Bay Area, New York City, S…\n$ industry            <fct> Finance, Retail, Finance, Media, Finance, Finance,…\n$ total_laid_off      <int> 260, 40, 515, 180, 50, 32, 20, 65, 30, 47, 110, 10…\n$ percentage_laid_off <dbl> 0.200, 0.100, 0.400, 0.120, 0.125, 0.080, 0.080, 0…\n$ date                <fct> 2022-12-07, 2022-12-07, 2022-12-06, 2022-12-06, 20…\n$ stage               <fct> Series D, Unknown, IPO, IPO, Series C, Unknown, Se…\n$ country             <fct> United States, United States, United States, Unite…\n$ funds_raised        <dbl> 734, 2300, 679, 696, 302, 480, 165, 265, 103, 184,…\n\n\nNow we will count the number of unique values in the company column of df-\n\n\nCode\n#counting the number of unique values\nn_distinct(df$company)\n\n\n[1] 762\n\n\n\n\nCode\n# Convert the date column to a year column and a month column\ndf$yrs <- strftime(df$date,'%Y') %>% as.factor()  # extract year from date and convert to a factor\ndf$mnt <- strftime(df$date,'%m') %>% as.factor()  # extract month from date and convert to a factor\n\n\nNow let’s see the mean, median, and standard deviation of total_laid_off.\n\n\nCode\n#mean will calculates the average  \nmean(df$total_laid_off)\n\n\n[1] 218.8068\n\n\nCode\n#median will calculates the median\nmedian(df$total_laid_off)\n\n\n[1] 75\n\n\nCode\n#sd will calculates the standard deviation\nsd(df$total_laid_off)\n\n\n[1] 641.0096\n\n\nSetup figure size-\n\n\nCode\nfig <- function(width, height) {\n  # Set the width and height options for the plot\n  options(repr.plot.width = width, repr.plot.height = height)\n}\n# Call it\nfig(20, 10)\n\n\n\n\nCode\n# Get the number of unique values in the location\nprint(length(unique(df$location)))\n\n\n[1] 99\n\n\nCode\n# Get the unique values in the location\nunique(df$location)\n\n\n [1] SF Bay Area     Berlin          New York City   Tel Aviv       \n [5] Boston          Burlington      Singapore       Los Angeles    \n [9] Jakarta         Sacramento      Buenos Aires    Bengaluru      \n[13] London          Melbourne       Sao Paulo       Waterloo       \n[17] Lagos           Dubai           Gurugram        Phoenix        \n[21] Gothenburg      Toronto         Dublin          Seattle        \n[25] Nairobi         Dover           Hamburg         San Diego      \n[29] Logan           Tallin          Lehi            Columbus       \n[33] Nebraska City   Copenhagen      Vancouver       Oslo           \n[37] Stockholm       Pittsburgh      Montreal        San Luis Obispo\n[41] Jerusalem       Austin          New Delhi       Belo Horizonte \n[45] Chicago         Salt Lake City  Bangkok         Raleigh        \n[49] Portland        Bristol         Washington D.C. Indianapolis   \n[53] Stamford        Curitiba        Mumbai          Boulder        \n[57] Sydney          Detroit         Ottawa          Ferdericton    \n[61] Dakar           Florianópolis   Philadelphia    Hong Kong      \n[65] Beijing         Vienna          Atlanta         Dallas         \n[69] Spokane         Chennai         Reno            Helsinki       \n[73] Malmo           Kuala Lumpur    Bend            Mexico City    \n[77] Cincinnati      Miami           Moscow          Shanghai       \n[81] Non-U.S.        Nashville       Las Vegas       Edinburgh      \n[85] Madison         Amsterdam       Santa Fe        Denver         \n[89] Ahmedabad       Joinville       Zurich          Missoula       \n[93] Minneapolis     Guadalajara     Blumenau        Milwaukee      \n[97] Ann Arbor       Lisbon          Munich         \n99 Levels: Ahmedabad Amsterdam Ann Arbor Atlanta Austin Bangkok ... Zurich\n\n\n\n\nCode\n# Get the number of unique values in the industry\nprint(length(unique(df$industry)))\n\n\n[1] 28\n\n\nCode\n# Get the unique values in the industry\nunique(df$industry)\n\n\n [1] Finance        Retail         Media          Security                     \n [6] Marketing      Food           Crypto         Education      Other         \n[11] Consumer       Transportation Healthcare     Infrastructure Data          \n[16] Sales          Fitness        Real Estate    Support        Logistics     \n[21] Recruiting     Construction   HR             Product        Aerospace     \n[26] Legal          Travel         Energy        \n28 Levels:  Aerospace Construction Consumer Crypto Data Education ... Travel\n\n\n\n\nCode\n# Get the unique values in the stage\nunique(df$stage)\n\n\n [1] Series D       Unknown        IPO            Series C       Series E      \n [6] Series B       Private Equity Series A       Series J       Series F      \n[11] Acquired       Series H       Series G       Seed           Series I      \n15 Levels: Acquired IPO Private Equity Seed Series A Series B ... Unknown\n\n\n\n\nCode\n# Total how many unique countries are there in the dataset\nunique(df$country)\n\n\n [1] United States        Israel               Singapore           \n [4] Indonesia            Argentina            India               \n [7] United Kingdom       Australia            Germany             \n[10] Brazil               Canada               Nigeria             \n[13] Sweden               Ireland              Kenya               \n[16] Estonia              Norway               Denmark             \n[19] Thailand             Senegal              Hong Kong           \n[22] China                United Arab Emirates Austria             \n[25] Finland              Malaysia             Mexico              \n[28] Russia               Seychelles           Netherlands         \n[31] Switzerland          Portugal            \n32 Levels: Argentina Australia Austria Brazil Canada China Denmark ... United States\n\n\n\n\nCode\n# Total how many unique companies are there in the dataset\nlength(unique(df$company))\n\n\n[1] 762\n\n\n\n\nCode\n# Convert the 'date' column to a date data type\ndf$date <- as.Date(df$date)\n\n# Extract the month and year from the 'date' column\ndf$month <- month(df$date)\ndf$year <- year(df$date)\n\n\n\n\nCode\n# Select rows that are duplicates\nduplicate_rows <- df[duplicated(df),]\n\n# Remove duplicate rows from the data frame\ndf <- df[!duplicated(df),]\n\n\n\n\nCode\ncolSums(is.na(df))\n\n\n            company            location            industry      total_laid_off \n                  0                   0                   0                   0 \npercentage_laid_off                date               stage             country \n                  0                   0                   0                   0 \n       funds_raised                 yrs                 mnt               month \n                  0                   0                   0                   0 \n               year \n                  0 \n\n\n\n\nCode\nsummary(df)\n\n\n     company             location             industry   total_laid_off   \n Uber    :  4   SF Bay Area  :236   Finance       :124   Min.   :    3.0  \n Doma    :  3   New York City:101   Retail        : 72   1st Qu.:   35.0  \n Glossier:  3   Boston       : 40   Marketing     : 61   Median :   75.0  \n Intercom:  3   Los Angeles  : 34   Healthcare    : 60   Mean   :  218.2  \n Katerra :  3   Seattle      : 32   Transportation: 58   3rd Qu.:  160.0  \n Latch   :  3   Bengaluru    : 31   Food          : 50   Max.   :11000.0  \n (Other) :839   (Other)      :384   (Other)       :433                    \n percentage_laid_off      date                 stage              country   \n Min.   :0.0000      Min.   :2020-03-12   Series B:144   United States:561  \n 1st Qu.:0.1000      1st Qu.:2020-05-03   IPO     :128   India        : 55  \n Median :0.1700      Median :2022-06-02   Series C:121   Canada       : 48  \n Mean   :0.2377      Mean   :2021-09-16   Unknown :115   Brazil       : 34  \n 3rd Qu.:0.3000      3rd Qu.:2022-08-15   Series D:111   Israel       : 28  \n Max.   :1.0000      Max.   :2022-12-07   Series A: 64   Germany      : 27  \n                                          (Other) :175   (Other)      :105  \n  funds_raised         yrs           mnt          month             year     \n Min.   :     0.00   2020:331   04     :164   Min.   : 1.000   Min.   :2020  \n 1st Qu.:    51.25   2021: 13   06     :140   1st Qu.: 4.000   1st Qu.:2020  \n Median :   153.50   2022:514   05     :110   Median : 6.000   Median :2022  \n Mean   :   880.37              11     :101   Mean   : 6.501   Mean   :2021  \n 3rd Qu.:   425.25              07     : 84   3rd Qu.: 8.000   3rd Qu.:2022  \n Max.   :121900.00              08     : 73   Max.   :12.000   Max.   :2022  \n                                (Other):186                                  \n\n\n\n\nCode\n#creating a subset of df by filtering for rows where percentage_laid_off = 1\nsubset(df, percentage_laid_off == 1)\n\n\n                company      location       industry total_laid_off\n58  Deliveroo Australia     Melbourne           Food            120\n89             Planetly        Berlin          Other            200\n112        Fifth Season    Pittsburgh           Food            100\n137            Playdots New York City       Consumer             65\n163          Kitty Hawk   SF Bay Area      Aerospace            100\n181        Simple Feast    Copenhagen           Food            150\n199               Reali   SF Bay Area    Real Estate            140\n256              Metigy        Sydney      Marketing             75\n273              Soluto      Tel Aviv        Support            120\n306  Butler Hospitality New York City           Food           1000\n327         WanderJaunt   SF Bay Area         Travel             85\n331           Crejo.Fun     Bengaluru      Education            170\n375           SummerBio   SF Bay Area     Healthcare            101\n415         The Grommet        Boston         Retail             40\n436               Udayy      Gurugram      Education            100\n458         BeyondMinds      Tel Aviv           Data             65\n482                SEND        Sydney           Food            300\n493               Ahead   SF Bay Area     Healthcare             44\n521             Katerra   SF Bay Area   Construction           2434\n525               Hubba       Toronto         Retail             45\n527          Pocketmath     Singapore      Marketing             21\n531    Bridge Connector     Nashville     Healthcare            154\n543               Eatsy     Singapore           Food             20\n546   Buy.com / Rakuten   SF Bay Area         Retail             87\n562                Dark   SF Bay Area        Product              6\n594            Bluprint        Denver      Education            137\n600         Stay Alfred       Spokane         Travel            221\n615               Deliv   SF Bay Area         Retail            669\n625                Jump New York City Transportation            500\n670          TutorMundi     Sao Paulo      Education              4\n722                Atsu       Seattle Infrastructure              6\n816             Amplero       Seattle      Marketing             17\n820                HOOQ     Singapore       Consumer            250\n834         Consider.co   SF Bay Area          Other             13\n856            Help.com        Austin        Support             16\n    percentage_laid_off       date    stage       country funds_raised  yrs mnt\n58                    1 2022-11-15      IPO     Australia    1700.0000 2022  11\n89                    1 2022-11-04 Acquired       Germany       5.0000 2022  11\n112                   1 2022-10-28 Series B United States      35.0000 2022  10\n137                   1 2022-10-13 Acquired United States      10.0000 2022  10\n163                   1 2022-09-21  Unknown United States       1.0000 2022  09\n181                   1 2022-09-07  Unknown       Denmark     173.0000 2022  09\n199                   1 2022-08-24 Series B United States     117.0000 2022  08\n256                   1 2022-07-31 Series B     Australia      18.0000 2022  07\n273                   1 2022-07-24 Acquired        Israel      18.0000 2022  07\n306                   1 2022-07-08 Series B United States      50.0000 2022  07\n327                   1 2022-07-01 Series B United States      26.0000 2022  07\n331                   1 2022-06-30     Seed         India       3.0000 2022  06\n375                   1 2022-06-20  Unknown United States       7.0000 2022  06\n415                   1 2022-06-09 Acquired United States       5.0000 2022  06\n436                   1 2022-06-01     Seed         India       2.0000 2022  06\n458                   1 2022-05-23 Series A        Israel      16.0000 2022  05\n482                   1 2022-05-04     Seed     Australia       3.0000 2022  05\n493                   1 2022-04-14  Unknown United States       9.0000 2022  04\n521                   1 2021-06-01  Unknown United States    1600.0000 2021  06\n525                   1 2021-02-01 Series B        Canada      61.0000 2021  02\n527                   1 2021-01-20  Unknown     Singapore      20.0000 2021  01\n531                   1 2020-11-17 Series B United States      45.0000 2020  11\n543                   1 2020-08-08     Seed     Singapore       0.9755 2020  08\n546                   1 2020-07-30 Acquired United States      42.4000 2020  07\n562                   1 2020-06-23     Seed United States       3.0000 2020  06\n594                   1 2020-05-26 Acquired United States     108.0000 2020  05\n600                   1 2020-05-20 Series B United States      62.0000 2020  05\n615                   1 2020-05-13 Series C United States      80.0000 2020  05\n625                   1 2020-05-07 Acquired United States      11.0000 2020  05\n670                   1 2020-04-24 Series A        Brazil       2.0000 2020  04\n722                   1 2020-04-10  Unknown United States       1.0000 2020  04\n816                   1 2020-03-29 Series B United States      25.0000 2020  03\n820                   1 2020-03-27  Unknown     Singapore      95.0000 2020  03\n834                   1 2020-03-26     Seed United States       5.0000 2020  03\n856                   1 2020-03-16     Seed United States       6.0000 2020  03\n    month year\n58     11 2022\n89     11 2022\n112    10 2022\n137    10 2022\n163     9 2022\n181     9 2022\n199     8 2022\n256     7 2022\n273     7 2022\n306     7 2022\n327     7 2022\n331     6 2022\n375     6 2022\n415     6 2022\n436     6 2022\n458     5 2022\n482     5 2022\n493     4 2022\n521     6 2021\n525     2 2021\n527     1 2021\n531    11 2020\n543     8 2020\n546     7 2020\n562     6 2020\n594     5 2020\n600     5 2020\n615     5 2020\n625     5 2020\n670     4 2020\n722     4 2020\n816     3 2020\n820     3 2020\n834     3 2020\n856     3 2020\n\n\n\n\nCode\nhead(df, 2)\n\n\n  company    location industry total_laid_off percentage_laid_off       date\n1   Plaid SF Bay Area  Finance            260                 0.2 2022-12-07\n2  Grover      Berlin   Retail             40                 0.1 2022-12-07\n     stage       country funds_raised  yrs mnt month year\n1 Series D United States          734 2022  12    12 2022\n2  Unknown United States         2300 2022  12    12 2022\n\n\nCode\ncolSums(is.na(df))\n\n\n            company            location            industry      total_laid_off \n                  0                   0                   0                   0 \npercentage_laid_off                date               stage             country \n                  0                   0                   0                   0 \n       funds_raised                 yrs                 mnt               month \n                  0                   0                   0                   0 \n               year \n                  0 \n\n\n\n\nCode\ndim(df)\n\n\n[1] 858  13\n\n\n\n\nCode\n# subset the data by selecting rows where company = Lyft\ndf[df$company == \"Lyft\", ]\n\n\n    company    location       industry total_laid_off percentage_laid_off\n93     Lyft SF Bay Area Transportation            700                0.13\n283    Lyft SF Bay Area Transportation             60                0.02\n650    Lyft SF Bay Area Transportation            982                0.17\n          date stage       country funds_raised  yrs mnt month year\n93  2022-11-03   IPO United States         4900 2022  11    11 2022\n283 2022-07-20   IPO United States         4900 2022  07     7 2022\n650 2020-04-29   IPO United States         4900 2020  04     4 2020\n\n\nThis will show all the company names which are their multiple times.\nNow, we will check the data, we will do more cleaning and re-factoring.\n\n\nCode\nhead(df, n = 2)\n\n\n  company    location industry total_laid_off percentage_laid_off       date\n1   Plaid SF Bay Area  Finance            260                 0.2 2022-12-07\n2  Grover      Berlin   Retail             40                 0.1 2022-12-07\n     stage       country funds_raised  yrs mnt month year\n1 Series D United States          734 2022  12    12 2022\n2  Unknown United States         2300 2022  12    12 2022\n\n\nCode\ncolSums(is.na(df))\n\n\n            company            location            industry      total_laid_off \n                  0                   0                   0                   0 \npercentage_laid_off                date               stage             country \n                  0                   0                   0                   0 \n       funds_raised                 yrs                 mnt               month \n                  0                   0                   0                   0 \n               year \n                  0 \n\n\n\n\nCode\ndf <- df %>% drop_na()\ndf %>% sapply(function(x)sum(is.na(x)))\n\n\n            company            location            industry      total_laid_off \n                  0                   0                   0                   0 \npercentage_laid_off                date               stage             country \n                  0                   0                   0                   0 \n       funds_raised                 yrs                 mnt               month \n                  0                   0                   0                   0 \n               year \n                  0 \n\n\n\n\nCode\ndf <- df %>% mutate_if(is.character,as.factor)\nglimpse(df)\n\n\nRows: 858\nColumns: 13\n$ company             <fct> Plaid, Grover, Doma, BuzzFeed, Chipper Cash, Stash…\n$ location            <fct> SF Bay Area, Berlin, SF Bay Area, New York City, S…\n$ industry            <fct> Finance, Retail, Finance, Media, Finance, Finance,…\n$ total_laid_off      <int> 260, 40, 515, 180, 50, 32, 20, 65, 30, 47, 110, 10…\n$ percentage_laid_off <dbl> 0.200, 0.100, 0.400, 0.120, 0.125, 0.080, 0.080, 0…\n$ date                <date> 2022-12-07, 2022-12-07, 2022-12-06, 2022-12-06, 2…\n$ stage               <fct> Series D, Unknown, IPO, IPO, Series C, Unknown, Se…\n$ country             <fct> United States, United States, United States, Unite…\n$ funds_raised        <dbl> 734, 2300, 679, 696, 302, 480, 165, 265, 103, 184,…\n$ yrs                 <fct> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 20…\n$ mnt                 <fct> 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11…\n$ month               <dbl> 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11…\n$ year                <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 20…\n\n\n\n\nCode\ntable(df$year)\n\n\n\n2020 2021 2022 \n 331   13  514"
  },
  {
    "objectID": "posts/Manan_Patel_PostTemplate.html#total-layoffs-year-wise-",
    "href": "posts/Manan_Patel_PostTemplate.html#total-layoffs-year-wise-",
    "title": "Layoffs dataset",
    "section": "Total layoffs year wise-",
    "text": "Total layoffs year wise-\n\n\nCode\ntotal_laid_year <- df %>%\n  group_by(year) %>%\n  summarize(total_laid_off = sum(total_laid_off))\ntotal_laid_year\n\n\n# A tibble: 3 × 2\n   year total_laid_off\n  <dbl>          <int>\n1  2020          60960\n2  2021           6490\n3  2022         119755\n\n\n\n\nCode\nggplot(data = total_laid_year, aes(x = year, y = total_laid_off)) +\n  geom_bar(stat = \"identity\", color = \"red\") +\n  labs(x = \"year\", y = \"Layoffs\") +\n  ggtitle(\"layoffs year wise\") +\n  theme(plot.background = element_rect(fill = \"yellow\"))\n\n\n\n\n\n\n\nCode\ndf %>%\n  group_by(location) %>%\n  summarise(total_laid_off = n_distinct(company)) %>%\n  arrange(location)\n\n\n# A tibble: 99 × 2\n   location       total_laid_off\n   <fct>                   <int>\n 1 Ahmedabad                   1\n 2 Amsterdam                   1\n 3 Ann Arbor                   1\n 4 Atlanta                     3\n 5 Austin                     10\n 6 Bangkok                     1\n 7 Beijing                     1\n 8 Belo Horizonte              2\n 9 Bend                        1\n10 Bengaluru                  25\n# … with 89 more rows\n\n\nCode\ndf %>%\n  # Group the data by year and stage\n  group_by(year, stage) %>%\n  # Summarize the data by counting the number of distinct funds raised within each group\n  summarise(n = n_distinct(company), .groups = \"keep\") %>%\n  ggplot(mapping = aes(x = year, y = n, color = stage)) +\n  geom_point() + \n  geom_line() +\n  labs(y = \"Total laid off\", title = \"Total laid off in stages-wise over years\")\n\n\n\n\n\n\n\nCode\ndf %>%\n  group_by(year, stage) %>%\n  summarise(n = n_distinct(funds_raised), .groups = \"keep\") %>%\n  ggplot(mapping = aes(x = year, y = n, color = stage)) +\n  geom_point() + \n  geom_line() +\n  labs(y = \"Funds Raised\", title = \"Funds raised included in the stage over the years\")\n\n\n\n\n\n\n\nCode\ndf %>% count(company) %>% arrange(desc(n))\n\n\n                          company n\n1                            Uber 4\n2                            Doma 3\n3                        Glossier 3\n4                        Intercom 3\n5                         Katerra 3\n6                           Latch 3\n7                            Lyft 3\n8                             Oda 3\n9                         Peloton 3\n10                         Redfin 3\n11                        RenoRun 3\n12                           Trax 3\n13                      Unacademy 3\n14                    Zeus Living 3\n15                            2TM 2\n16                     Better.com 2\n17                           Bird 2\n18                        Bizzabo 2\n19                          Blend 2\n20                         Bonsai 2\n21                         Bounce 2\n22                           Brex 2\n23                        Carvana 2\n24                        Compass 2\n25                     Crypto.com 2\n26                     Cybereason 2\n27                          Ebanx 2\n28                     Ethos Life 2\n29                       Flyhomes 2\n30                         Food52 2\n31                       FrontRow 2\n32                        Fundbox 2\n33                         Gemini 2\n34                         Gopuff 2\n35                        Groupon 2\n36                          Homie 2\n37                      Hootsuite 2\n38                          Hopin 2\n39                         Infarm 2\n40                        IronNet 2\n41                           Juul 2\n42                         Knotel 2\n43                            Kry 2\n44                         Leafly 2\n45                Lighter Capital 2\n46                           Loft 2\n47                           Loom 2\n48                     MakeMyTrip 2\n49                         Mejuri 2\n50                      Metromile 2\n51                       MindBody 2\n52                        Netflix 2\n53                      New Relic 2\n54                        On Deck 2\n55                       OneTrust 2\n56                       Opendoor 2\n57                        Patreon 2\n58                         Pollen 2\n59                      Quantcast 2\n60                    QuintoAndar 2\n61                Rad Power Bikes 2\n62                       RealSelf 2\n63                         Ritual 2\n64                         Rivian 2\n65                      Robinhood 2\n66                     Salesforce 2\n67                        Salsify 2\n68                          Sendy 2\n69                          Smava 2\n70                         Sonder 2\n71                     Stitch Fix 2\n72                         StockX 2\n73                         Swiggy 2\n74                       Truepill 2\n75                         UiPath 2\n76                        Vedantu 2\n77                            Vee 2\n78                    WanderJaunt 2\n79                        Zilingo 2\n80                         Zillow 2\n81                         Zomato 2\n82                          &Open 1\n83                   10X Genomics 1\n84                        1stdibs 1\n85                         54gene 1\n86                         6sense 1\n87                             99 1\n88                           Abra 1\n89                          Acast 1\n90                           Acko 1\n91                            Ada 1\n92                    Ada Support 1\n93       Adaptive Biotechnologies 1\n94                         AdRoll 1\n95                          Ahead 1\n96                         Airbnb 1\n97                        Airtime 1\n98                          Ajaib 1\n99                      AlayaCare 1\n100                        Albert 1\n101             AliExpress Russia 1\n102                 Alto Pharmacy 1\n103                        Amazon 1\n104                      Amperity 1\n105                       Amplero 1\n106                        Andela 1\n107                        Anodot 1\n108               Antidote Health 1\n109                Apartment List 1\n110                     App Annie 1\n111                      AppLovin 1\n112                    ApplyBoard 1\n113                    Aqgromalin 1\n114                          Aqua 1\n115                 Aqua Security 1\n116                       Argo AI 1\n117                        Argyle 1\n118              Arrive Logistics 1\n119                         Asana 1\n120                         Astra 1\n121                          AtoB 1\n122                          Atsu 1\n123                          Aura 1\n124           Automation Anywhere 1\n125                     AvantStay 1\n126                           Avo 1\n127                          Away 1\n128                           Aya 1\n129                          B8ta 1\n130                         Banxa 1\n131                         Baton 1\n132                         Bench 1\n133           Berlin Brands Group 1\n134                        Bestow 1\n135                          Bevi 1\n136                   Beyond Meat 1\n137                   BeyondMinds 1\n138                      BioMarin 1\n139                        BitMEX 1\n140                      BitOasis 1\n141                      Bitpanda 1\n142                         Bitso 1\n143                      BitTitan 1\n144                Blockchain.com 1\n145                       BlockFi 1\n146                    Blueground 1\n147                      Bluprint 1\n148                          Bolt 1\n149                      BookClub 1\n150                    BookMyShow 1\n151                         Boozt 1\n152                     Borrowell 1\n153                       BounceX 1\n154                        Branch 1\n155                Branch Metrics 1\n156                    Brave Care 1\n157                       Breathe 1\n158                      Breather 1\n159              Bridge Connector 1\n160               Bright Machines 1\n161                  Bright Money 1\n162                       Brighte 1\n163                        Bringg 1\n164                        Bryter 1\n165                       Buenbit 1\n166                       Builder 1\n167                      Built In 1\n168                       Bullish 1\n169                        BusBud 1\n170          Bustle Digital Group 1\n171            Butler Hospitality 1\n172                        Button 1\n173             Buy.com / Rakuten 1\n174                      BuzzFeed 1\n175                        Byju's 1\n176                         Cadre 1\n177                     Calibrate 1\n178                          Calm 1\n179                         Cameo 1\n180                 Candy Digital 1\n181                         Canoo 1\n182                     Capitolis 1\n183                   CaptivateIQ 1\n184                 Carbon Health 1\n185                        Careem 1\n186                  Career Karma 1\n187                      CarGurus 1\n188                     Carousell 1\n189                        Cars24 1\n190                         Carta 1\n191                        Carwow 1\n192                        Casper 1\n193                         Cazoo 1\n194                       Celsius 1\n195                      Cerebral 1\n196                     Chargebee 1\n197                     Checkmarx 1\n198                  Checkout.com 1\n199                        Checkr 1\n200                         Chime 1\n201                  Chipper Cash 1\n202                         Cisco 1\n203                      CityMall 1\n204                Clarify Health 1\n205                     ClassPass 1\n206                         Clear 1\n207                     Clearbanc 1\n208                       Clearco 1\n209                     CleverTap 1\n210                       ClickUp 1\n211                         Clinc 1\n212                    Cloudinary 1\n213                        Clutch 1\n214                        Code42 1\n215                   Coding Dojo 1\n216                        Cogito 1\n217                      Coinbase 1\n218                       CoinJar 1\n219                    Coinsquare 1\n220                     Community 1\n221                       Compete 1\n222                     ConsenSys 1\n223                   Consider.co 1\n224                     ContaAzul 1\n225                    ContraFect 1\n226                       Convene 1\n227                        Convoy 1\n228             Coterie Insurance 1\n229                        Crayon 1\n230                 Credit Sesame 1\n231                     Crejo.Fun 1\n232                   CrowdStreet 1\n233                        Cruise 1\n234                        CTO.ai 1\n235                   Culture Amp 1\n236                  Culture Trip 1\n237                       CureFit 1\n238                         Curve 1\n239                         Cvent 1\n240                          D2iQ 1\n241                   Dapper Labs 1\n242                          Dark 1\n243                     DataRails 1\n244                     DataRobot 1\n245                         Deliv 1\n246                     Deliveroo 1\n247           Deliveroo Australia 1\n248                    Demandbase 1\n249                        Deputy 1\n250                Descartes Labs 1\n251                    DialSource 1\n252                  Divergent 3D 1\n253                   Divvy Homes 1\n254                          Dock 1\n255                         Docly 1\n256                      DocuSign 1\n257                     Domestika 1\n258                          Domo 1\n259                      DoorDash 1\n260                         Dover 1\n261                       Dropbox 1\n262                       Dutchie 1\n263                Dynamic Signal 1\n264                        Earnin 1\n265                      EasyPost 1\n266                         Eatsy 1\n267                        Ecobee 1\n268           Eden / Managed By Q 1\n269                          eGym 1\n270                    Element AI 1\n271                     Elementor 1\n272                       Elinvar 1\n273                      Embroker 1\n274                       Emotive 1\n275                         Enjoy 1\n276                         Envoy 1\n277                     Equitybee 1\n278                     EquityZen 1\n279                         eToro 1\n280                    Eucalyptus 1\n281                    Eventbrite 1\n282                        Exodus 1\n283                       ezCater 1\n284                     FabHotels 1\n285                        Fabric 1\n286                        Facily 1\n287                         Faire 1\n288                        FarEye 1\n289                  Fifth Season 1\n290                  Finite State 1\n291               Finleap Connect 1\n292                        Fiverr 1\n293                     Flipboard 1\n294                      Flockjay 1\n295                     Flytedesk 1\n296               Flywheel Sports 1\n297                       Flywire 1\n298                       Foodsby 1\n299                      Forma.ai 1\n300                       Forward 1\n301                     FourKites 1\n302                       Foxtrot 1\n303                     Freetrade 1\n304                    Freshbooks 1\n305                     Frontdesk 1\n306             Funding Societies 1\n307                            G2 1\n308                        Gather 1\n309                           Gem 1\n310                     Getaround 1\n311                     GetNinjas 1\n312                  GetYourGuide 1\n313                     Glassdoor 1\n314                        Glitch 1\n315                        GoBear 1\n316                      GoHealth 1\n317                         Gojek 1\n318                        GoodRx 1\n319                         GoPro 1\n320                      Gorillas 1\n321                   GoSpotCheck 1\n322                    GoTo Group 1\n323                          Grab 1\n324                      GrayMeta 1\n325           Greenhouse Software 1\n326                          Grin 1\n327              Group Nine Media 1\n328                        Grover 1\n329                        GumGum 1\n330                       Gympass 1\n331                   Happy Money 1\n332                          Hash 1\n333                   HealthMatch 1\n334                      Help.com 1\n335                        Heroes 1\n336                         Hibob 1\n337                  Highsnobiety 1\n338               Hippo Insurance 1\n339                     Hireology 1\n340                      Hologram 1\n341                          HOOQ 1\n342                  HopSkipDrive 1\n343                Horizn Studios 1\n344                       Hotmart 1\n345                         Houzz 1\n346                         Hubba 1\n347                        Hubilo 1\n348                         Huobi 1\n349                  Hyperscience 1\n350                        Ibotta 1\n351                          iFit 1\n352                         Iflix 1\n353                           Ike 1\n354                      Illumina 1\n355                Immersive Labs 1\n356                     Immutable 1\n357                        Impala 1\n358              Impossible Foods 1\n359             Incredible Health 1\n360                      InDebted 1\n361                   Industrious 1\n362                    InfluxData 1\n363                       InfoSum 1\n364                    Innovaccer 1\n365                     Inspirato 1\n366                     Instamojo 1\n367                   Instructure 1\n368           Integral Ad Science 1\n369                     Introhive 1\n370                        Intuit 1\n371                      Involves 1\n372                  iPrice Group 1\n373                     Iris Nova 1\n374                           IRL 1\n375                        iRobot 1\n376                       Iron Ox 1\n377                      Jam City 1\n378                          Jama 1\n379                    JetClosing 1\n380                         Jetty 1\n381                         Jimdo 1\n382                        Jiobit 1\n383                       Jobcase 1\n384                          JOKR 1\n385                          Jump 1\n386                          Juni 1\n387                         Kabam 1\n388             Kayak / OpenTable 1\n389                   KeepTruckin 1\n390                        Kenoby 1\n391                        Khoros 1\n392                         Kiavi 1\n393                   Kickstarter 1\n394                        Kitopi 1\n395                    Kitty Hawk 1\n396                        Klarna 1\n397                         Klook 1\n398                         Knock 1\n399               Kodiak Robotics 1\n400                          Koho 1\n401                     KoinWorks 1\n402                 Komodo Health 1\n403                       Kontist 1\n404                        Kraken 1\n405                          Kuda 1\n406                        Kueski 1\n407                      Lacework 1\n408                   Ladder Life 1\n409                      Lastline 1\n410                       Lattice 1\n411                       Lawgeex 1\n412                          LEAD 1\n413                       Legible 1\n414                         Lemon 1\n415                  Lending Club 1\n416                   Lendingkart 1\n417                        Lendis 1\n418                         Lever 1\n419                       Liftoff 1\n420                    Lightricks 1\n421                          Lime 1\n422                      LinkedIn 1\n423                      Linkfire 1\n424                      Linktree 1\n425                        Liv Up 1\n426                    LivePerson 1\n427                      Livspace 1\n428                       Loftium 1\n429                     Loftsmart 1\n430                         Loggi 1\n431                      Lokalise 1\n432                          Loop 1\n433                        Loopio 1\n434                        LoopMe 1\n435                      Lunchbox 1\n436                         Lusha 1\n437                          Lyst 1\n438                MadeiraMadeira 1\n439                    Magic Leap 1\n440                    Mainstreet 1\n441                  Malwarebytes 1\n442                   Marketforce 1\n443                   MasterClass 1\n444                    Matterport 1\n445                         Maven 1\n446                     MediaMath 1\n447                        Meesho 1\n448                        Mercos 1\n449                          Meta 1\n450                        Metigy 1\n451                         MFine 1\n452                        Minted 1\n453                      Mixpanel 1\n454         Mobile Premier League 1\n455                          Mogo 1\n456                     Momentive 1\n457                         Monzo 1\n458                          Moss 1\n459                      Movidesk 1\n460                       Mozilla 1\n461                         Mural 1\n462                           Mux 1\n463                           N26 1\n464                        Namely 1\n465                          Nate 1\n466                          Navi 1\n467                          Neon 1\n468                Next Insurance 1\n469                       Niantic 1\n470                          Noom 1\n471                      Notarize 1\n472                           NS8 1\n473                           NSO 1\n474                        Numbrs 1\n475                         NuoDB 1\n476                          Nuri 1\n477                          Nuro 1\n478                       Nutanix 1\n479                         NYDIG 1\n480                         Nylas 1\n481                         Ocavu 1\n482                           Ola 1\n483                         Olive 1\n484                          Omie 1\n485                        OneWeb 1\n486                      Opencare 1\n487                       OpenWeb 1\n488                         OpenX 1\n489                    Optimizely 1\n490                         OrCam 1\n491                       Oriente 1\n492                  Oscar Health 1\n493                OutboundEngine 1\n494                      Outbrain 1\n495                      Outreach 1\n496                     Outschool 1\n497                       Outside 1\n498                      Overtime 1\n499                     OwnBackup 1\n500                  Oye Rickshaw 1\n501                        Pacaso 1\n502                      Packable 1\n503                   PaisaBazaar 1\n504                   Panda Squad 1\n505                      Parsable 1\n506                    PatientPop 1\n507                 Pavilion Data 1\n508                        PayJoy 1\n509             Pear Therapeutics 1\n510            Pear Therapeutics  1\n511                     Peerspace 1\n512                    PeerStreet 1\n513                         Pendo 1\n514                     People.ai 1\n515                  Perimeter 81 1\n516                        Perion 1\n517                      PerkSpot 1\n518                     Permutive 1\n519                 PickYourTrail 1\n520                       Picsart 1\n521                     Pipedrive 1\n522                          Pipl 1\n523                         Pitch 1\n524                         Plaid 1\n525                      Planetly 1\n526                         Plato 1\n527                      Playdots 1\n528                          Pleo 1\n529                        Pliops 1\n530                          Plum 1\n531                    Pocketmath 1\n532                  PolicyGenius 1\n533                         Polly 1\n534                Pomelo Fashion 1\n535                     Postmates 1\n536                        Preply 1\n537                       Procore 1\n538                     Project44 1\n539                        Puppet 1\n540                            Q4 1\n541                       Quandoo 1\n542                     Quanterix 1\n543                        Quanto 1\n544                        Quidax 1\n545                        Qumulo 1\n546                          Rasa 1\n547                         Reali 1\n548                      Recharge 1\n549               Redesign Health 1\n550                         Redox 1\n551                          Reef 1\n552                        Remote 1\n553                   Remote Year 1\n554   Repertoire Immune Medicines 1\n555                  ResearchGate 1\n556                       Revolut 1\n557                         Rhino 1\n558                       Rhumbix 1\n559                        Ribbon 1\n560                      Ridecell 1\n561                         RigUp 1\n562                  Rock Content 1\n563                          Roku 1\n564                Root Insurance 1\n565                         Rover 1\n566                          Rows 1\n567               Rubicon Project 1\n568                        Rubius 1\n569                        Rupeek 1\n570                     SafeGraph 1\n571             Sage Therapeutics 1\n572                          Sami 1\n573                       Samsara 1\n574                         Sanar 1\n575                    Sandbox VR 1\n576                    Sauce Labs 1\n577                   ScaleFactor 1\n578                         Scoop 1\n579                       Segment 1\n580                         Sema4 1\n581                          SEND 1\n582                        Sendle 1\n583                     Sensibill 1\n584                     ShareChat 1\n585                        Shippo 1\n586                        Shipsi 1\n587                        Shogun 1\n588                       Shop101 1\n589                       Shopify 1\n590                       Showpad 1\n591                Sidecar Health 1\n592                    SimilarWeb 1\n593                  Simple Feast 1\n594                        SIRCLO 1\n595                       Sisense 1\n596                          Skai 1\n597                       Skedulo 1\n598                        Skillz 1\n599                    Skyscanner 1\n600                          Snap 1\n601                          Snyk 1\n602                        Socure 1\n603                          SoFi 1\n604                        Sojern 1\n605                        Soluto 1\n606                         Sonos 1\n607                    SoundHound 1\n608                   Sourcegraph 1\n609                          Spin 1\n610                        Splunk 1\n611                      SpotHero 1\n612                         Spyce 1\n613                        SQream 1\n614                Stack Overflow 1\n615                         Stash 1\n616               Stash Financial 1\n617                     Stashaway 1\n618                   Stay Alfred 1\n619                         Stedi 1\n620                         Stord 1\n621                      Storytel 1\n622                        Stream 1\n623                        Stripe 1\n624                        Studio 1\n625                   Submittable 1\n626                      Substack 1\n627                     SummerBio 1\n628                         SumUp 1\n629                        Sunday 1\n630                     SundaySky 1\n631                    Superhuman 1\n632               Superpedestrian 1\n633                       Swappie 1\n634                    Sweetgreen 1\n635                          SWVL 1\n636                         Swyft 1\n637                     SynapseFI 1\n638                     Synapsica 1\n639                  Synergysuite 1\n640                      Synthego 1\n641                          Syte 1\n642                       Taboola 1\n643                         Tally 1\n644                 Tamara Mellon 1\n645                        TaskUs 1\n646                     Teachmint 1\n647                      TealBook 1\n648                      Teleport 1\n649                        Textio 1\n650                  The Athletic 1\n651                   The Grommet 1\n652                     The Guild 1\n653               The Mom Project 1\n654          The Predictive Index 1\n655                  The RealReal 1\n656                      The Sill 1\n657                      TheSkimm 1\n658                       Thimble 1\n659                     Thinkific 1\n660                     ThirdLove 1\n661                       Thriver 1\n662                     Thumbtack 1\n663                 Tier Mobility 1\n664                         TIFIN 1\n665                         Toast 1\n666                          Tomo 1\n667                         Tonal 1\n668                       Tonkean 1\n669                       Top Hat 1\n670                           Tor 1\n671                   TouchBistro 1\n672                      Transfix 1\n673             Transmit Security 1\n674                TravelTriangle 1\n675                     Treehouse 1\n676                         Trell 1\n677                   TripActions 1\n678                   TripAdvisor 1\n679                    Triplebyte 1\n680                    TripleLift 1\n681                       TrueCar 1\n682                     TrueLayer 1\n683                       Truiloo 1\n684                         Trybe 1\n685                         Tufin 1\n686                          Turo 1\n687                    TutorMundi 1\n688                         Twiga 1\n689                        Twilio 1\n690                       Twitter 1\n691                          Ualá 1\n692                      Uberflip 1\n693                         Udaan 1\n694                       Udacity 1\n695                         Udayy 1\n696                           Ula 1\n697                       Unbabel 1\n698                      Unbounce 1\n699                      Uncapped 1\n700                        Unison 1\n701                         Unity 1\n702           Unstoppable Domains 1\n703                        UPshow 1\n704                       Upstart 1\n705             Urban Sports Club 1\n706                      Usermind 1\n707                         uShip 1\n708                       Varonis 1\n709                          Veev 1\n710                      Vendease 1\n711                        Verbit 1\n712                        Veriff 1\n713              VerSe Innovation 1\n714         Vesalius Therapeutics 1\n715                       Vezeeta 1\n716                      VideoAmp 1\n717              Virgin Hyperloop 1\n718                           Voi 1\n719                         Vouch 1\n720                     Vox Media 1\n721                    Voyage SMS 1\n722                         Vroom 1\n723                          VSCO 1\n724                          VTEX 1\n725                          Wave 1\n726 Wave Sports and Entertainment 1\n727                       Wayfair 1\n728                      Wayflyer 1\n729                          Waze 1\n730                  Wealthsimple 1\n731                         Weee! 1\n732                 Welkin Health 1\n733                        WeWork 1\n734                         Wheel 1\n735                   When I Work 1\n736                         WHOOP 1\n737              Wildlife Studios 1\n738                          Wish 1\n739                        Wonder 1\n740                  Wonderschool 1\n741                        Wonolo 1\n742                    Wordstream 1\n743                      Workable 1\n744                      WorkRamp 1\n745                     Worksmith 1\n746                         Woven 1\n747                   Xiaohongshu 1\n748                          Yelp 1\n749                         Yojak 1\n750                         Yotpo 1\n751                           Zak 1\n752                          Zego 1\n753                      Zeitgold 1\n754                       Zencity 1\n755                       Zendesk 1\n756                      Zenefits 1\n757                        Zenoti 1\n758                        Zipcar 1\n759                  ZipRecruiter 1\n760                          Zoox 1\n761                          Zume 1\n762                        Zumper 1\n\n\n\n\nCode\ncor(df$funds_raised, df$total_laid_off)\n\n\n[1] 0.1677711\n\n\n\n\nCode\n# Using the total_laid_off column as the x-axis and the funds_raised column as the y-axis\n# Group the data by the company column and color the lines red\nggplot(df, aes(x = total_laid_off, y = funds_raised, group = company, color = \"red\")) +\n  geom_line() +\n  labs(x = \"Total Laid Off\", y = \"Funds Raised\", title = \"Funds Raised vs. Total Laid Off\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nCode\n# Use the total_laid_off column as the x-axis and the funds_raised column as the y-axis\n# Group the data by the company column and color the lines red\nggplot(df, aes(x = funds_raised, y = total_laid_off, group = company, color = \"red\")) +\n  geom_line() +\n  labs(x = \"Funds Raised\", y = \"Total Laid Off\", title = \"Total Laid Off vs. Funds Raised\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\n# Creating a scatterplot\nplot_ly(data = df, x = ~funds_raised, y = ~total_laid_off, type = \"scatter\", mode = \"markers\", text = ~company)\n\n\n\n\n\n\n\nCode\n# x-axis represents the amount of funds raised by each company\n# y-axis represents the total number of employees laid off by each company\n\n\n\n\nCode\nlibrary(plotly)\n\n# Creating a scatterplot\nplot_ly(data = df, x = ~total_laid_off, y = ~funds_raised, type = \"scatter\", mode = \"markers\", text = ~company)\n\n\n\n\n\n\n\nCode\n# x-axis represents the total number of employees laid off by each company\n# y-axis represents the amount of funds raised by each company\n\n\nThe above plot shows that there is no relationship or dependence between the funds raised and the number of layoffs.\n\n\nCode\nunique(df$stage)\n\n\n [1] Series D       Unknown        IPO            Series C       Series E      \n [6] Series B       Private Equity Series A       Series J       Series F      \n[11] Acquired       Series H       Series G       Seed           Series I      \n15 Levels: Acquired IPO Private Equity Seed Series A Series B ... Unknown\n\n\n\n\nCode\ndistinct(df, stage)\n\n\n            stage\n1        Series D\n2         Unknown\n3             IPO\n4        Series C\n5        Series E\n6        Series B\n7  Private Equity\n8        Series A\n9        Series J\n10       Series F\n11       Acquired\n12       Series H\n13       Series G\n14           Seed\n15       Series I\n\n\nThis is a list of all the stages that a company goes through, which in total consists of 15 stages.\n\n\nCode\nlibrary(plotly)\n# Create a new dataframe called df_stage that groups the df by the stage column and summarizes the total_laid_off column by summing the values\ndf_stage <- df %>%\n  group_by(stage) %>%\n  summarize(total_laid_off = sum(total_laid_off)) %>%\n# Sorting the resulting dataframe in descending order\n  arrange(desc(total_laid_off))\n# create a horizontal bar chart using the df_stage. x-axis- total_laid_off & y-axis- stage\nplot_ly(data = df_stage, x = ~total_laid_off, y = ~stage, type = \"bar\", orientation = \"h\", text = ~total_laid_off, color = c('orange')) %>%\n  layout(title = \"Layoffs & company stage\", plot_bgcolor = 'lightblue')\n\n\n\n\n\n\nThe companies that were in the initial public offering (IPO) stage had the highest number of layoffs, followed by start-up companies whose stage was unknown. On the other hand, the least amount of layoffs occurred in seed stage companies. This pattern suggests that the companies that were further along in their development and had already gone public were more likely to experience layoffs, while the newer, smaller companies that were still in the early stages of development were less likely to do so."
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html",
    "href": "posts/MariiaDubyk_FinalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(rmarkdown)\nlibrary(viridis)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#introduction",
    "href": "posts/MariiaDubyk_FinalProject.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nPolitical violence and its causes is a broad topic in sociology, political science, social psychology, etc. A lot of research papers on protests, social movements and radicalization are based on qualitative data analysis (storytelling, qualitative interviews, content analysis) with a focus on organizations, ideology, group interaction, etc. Armed Conflict Location & Event Data Project (ACLED) gathers political violence events all over the world into a database. This approach focuses not on specific ideology or organization but on changes of a number of different types of events (protests, riots, battles, etc.) It gives the opportunity to monitor radicalization dynamics in different regions and specific countries.\nIt is important to note that there is a discussion on which factors play role in protests and radicalization. Is that economic reasons, ideology, group dynamics, or specifics of the regime type (democracy vs authoritarian)? I chose data related to 7 European Union countries with similar populations for 2020-2021. These countries are all liberal democracies, members of the EU and have christian religious tradition. But they have economic differences. Some countries like Sweden, Belgium, and Austria have higher GDP per capita than Greece, Hungary, Portugal.\nMy research questions are\n\nif there are some differences in protest types and numbers between more and less rich countries\ndid the economic crisis related to covid19 change the level of protest and radicalization\n\nI understand that economic differences are relatively small because all countries are progressive high-income countries. However, we can look if there may be some trend in protest dynamics related specifically to more and less rich European countries."
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#data-reading-and-description",
    "href": "posts/MariiaDubyk_FinalProject.html#data-reading-and-description",
    "title": "Final Project",
    "section": "Data reading and description",
    "text": "Data reading and description\nThe dataset was downloaded from Armed Conflict Location & Event Data Project (ACLED), https://acleddata.com/. On the website I chose 7 countries:\n\nSweden\nBelgium\nAustria\nGreece\nHungary\nPortugal\nCzech Republic\n\nData includes years from 2020-2022. I could not include 2019 because data have been gathered from 2020 only.\n\n\nCode\nprotest_original <- read_csv(\"_data/protests eu.csv\")\n\n\nRows: 11601 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): event_id_cnty, event_date, event_type, sub_event_type, actor1, ass...\ndbl (13): data_id, iso, event_id_no_cnty, year, time_precision, inter1, inte...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\npaged_table(protest_original)\n\n\n\n\n  \n\n\n\nThe dataset includes 11601 cases. Each case is an event (protest, riot, or other action related to political violence). Variables show when and where it happened and who participated (participant group is called actor). For the analysis, I will remove variables that include descriptive qualitative information. Also, I will delete variables with some numerical and categorical identifiers which I do not need for exploration.\n\n\nCode\nprotest_selected <- select(protest_original, \"data_id\",\n                           \"event_date\", \"year\", \"event_type\",\n                           \"sub_event_type\", \"inter1\", \"inter2\",\n                           \"interaction\",\n                           \"country\",\n                           \"admin1\",\n                           \"location\",\n                           \"latitude\",\n                           \"longitude\",\n                           \"source_scale\",\n                           \"fatalities\")\nprint(\n  dfSummary(protest_selected, \n            varnumbers   = FALSE,\n            na.col       = FALSE,\n            style        = \"multiline\",\n            plain.ascii  = FALSE,\n            headings     = FALSE,\n            graph.magnif = .8),\n  method = \"render\"\n)\n\n\n\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Valid\n    \n  \n  \n    \n      data_id\n[numeric]\n      Mean (sd) : 8469566 (784758.3)min ≤ med ≤ max:6862819 ≤ 8609394 ≤ 9709125IQR (CV) : 1416682 (0.1)\n      11601 distinct values\n      \n      11601\n(100.0%)\n    \n    \n      event_date\n[character]\n      1. 18 June 20202. 25 September 20203. 24 September 20214. 22 October 20215. 09 June 20206. 21 February 20217. 01 May 20218. 14 October 20229. 18 November 202210. 06 September 2020[ 1042 others ]\n      164(1.4%)71(0.6%)67(0.6%)64(0.6%)57(0.5%)54(0.5%)48(0.4%)47(0.4%)46(0.4%)44(0.4%)10939(94.3%)\n      \n      11601\n(100.0%)\n    \n    \n      year\n[numeric]\n      Mean (sd) : 2021 (0.8)min ≤ med ≤ max:2020 ≤ 2021 ≤ 2022IQR (CV) : 2 (0)\n      2020:3779(32.6%)2021:4333(37.4%)2022:3489(30.1%)\n      \n      11601\n(100.0%)\n    \n    \n      event_type\n[character]\n      1. Battles2. Explosions/Remote violenc3. Protests4. Riots5. Violence against civilian\n      12(0.1%)55(0.5%)10851(93.5%)596(5.1%)87(0.7%)\n      \n      11601\n(100.0%)\n    \n    \n      sub_event_type\n[character]\n      1. Abduction/forced disappea2. Armed clash3. Attack4. Excessive force against p5. Grenade6. Mob violence7. Peaceful protest8. Protest with intervention9. Remote explosive/landmine10. Sexual violence11. Violent demonstration\n      3(0.0%)12(0.1%)80(0.7%)5(0.0%)2(0.0%)275(2.4%)10489(90.4%)357(3.1%)53(0.5%)4(0.0%)321(2.8%)\n      \n      11601\n(100.0%)\n    \n    \n      inter1\n[numeric]\n      Mean (sd) : 5.9 (0.4)min ≤ med ≤ max:1 ≤ 6 ≤ 8IQR (CV) : 0 (0.1)\n      1:28(0.2%)2:1(0.0%)3:118(1.0%)4:2(0.0%)5:596(5.1%)6:10851(93.5%)8:5(0.0%)\n      \n      11601\n(100.0%)\n    \n    \n      inter2\n[numeric]\n      Mean (sd) : 0.3 (1.2)min ≤ med ≤ max:0 ≤ 0 ≤ 8IQR (CV) : 0 (4.2)\n      0:10477(90.3%)1:723(6.2%)3:7(0.1%)5:63(0.5%)6:149(1.3%)7:174(1.5%)8:8(0.1%)\n      \n      11601\n(100.0%)\n    \n    \n      interaction\n[numeric]\n      Mean (sd) : 56.8 (11.3)min ≤ med ≤ max:13 ≤ 60 ≤ 88IQR (CV) : 0 (0.2)\n      21 distinct values\n      \n      11601\n(100.0%)\n    \n    \n      country\n[character]\n      1. Austria2. Belgium3. Czech Republic4. Greece5. Hungary6. Portugal7. Sweden\n      1247(10.7%)2154(18.6%)751(6.5%)2165(18.7%)728(6.3%)1305(11.2%)3251(28.0%)\n      \n      11601\n(100.0%)\n    \n    \n      admin1\n[character]\n      1. Attica2. Vlaanderen3. Brussels4. Macedonia-Thrace5. Stockholms6. Lisboa7. Wallonie8. Wien9. Skane10. Vastra Gotalands[ 85 others ]\n      880(7.6%)816(7.0%)799(6.9%)681(5.9%)667(5.7%)550(4.7%)539(4.6%)537(4.6%)511(4.4%)442(3.8%)5179(44.6%)\n      \n      11601\n(100.0%)\n    \n    \n      location\n[character]\n      1. Brussels2. Athens - Central Athens3. Stockholm4. Lisbon5. Thessaloniki6. Vienna7. Prague8. Goteborg9. Malmo10. Umea[ 1493 others ]\n      645(5.6%)564(4.9%)515(4.4%)497(4.3%)490(4.2%)345(3.0%)323(2.8%)264(2.3%)203(1.7%)190(1.6%)7565(65.2%)\n      \n      11601\n(100.0%)\n    \n    \n      latitude\n[numeric]\n      Mean (sd) : 49 (7.8)min ≤ med ≤ max:32.6 ≤ 50.1 ≤ 67.9IQR (CV) : 15.2 (0.2)\n      1491 distinct values\n      \n      11601\n(100.0%)\n    \n    \n      longitude\n[numeric]\n      Mean (sd) : 12.4 (9.9)min ≤ med ≤ max:-31.2 ≤ 15.2 ≤ 28.2IQR (CV) : 14.5 (0.8)\n      1486 distinct values\n      \n      11601\n(100.0%)\n    \n    \n      source_scale\n[character]\n      1. National2. Other3. Subnational4. New media5. Subnational-National6. National-International7. Other-National8. International9. Regional10. Other-New media[ 9 others ]\n      6796(58.6%)2302(19.8%)1032(8.9%)467(4.0%)303(2.6%)145(1.2%)145(1.2%)136(1.2%)88(0.8%)51(0.4%)136(1.2%)\n      \n      11601\n(100.0%)\n    \n    \n      fatalities\n[numeric]\n      Mean (sd) : 0 (0.1)min ≤ med ≤ max:0 ≤ 0 ≤ 5IQR (CV) : 0 (29.8)\n      0:11581(99.8%)1:19(0.2%)5:1(0.0%)\n      \n      11601\n(100.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\nNow dataset includes 15 variables and 11601 cases. Variables are categorical.\n\nFirst column is data id.\nColumn 2-3 show event date and year.\nVariables “country”, “admin1”, “location”, “longitude” and “latitude” contain information about which country, its division and its city or town the event took place (column 9-13).\n“Inter1” and “inter2” contain information about actor who participated. Inter1 is the main group, the one that initiated an event. Inter2 is another group which participated, usually opposing like police or opposing organization. Variable “interaction” shows combination of two groups (column 6-8).\n“Event_type” gives general information about what happened. Was is protest or riot, or battle? It has relatively small number of categories (5 categories). Each category has subcategories which are presented in “sub_event_type” (11 categories) (column 4-5).\nColumn 14 shows scale of source where news about event were published.\nData in column 15 is numerical and shows number of fatalities during an event."
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#preparing-data",
    "href": "posts/MariiaDubyk_FinalProject.html#preparing-data",
    "title": "Final Project",
    "section": "Preparing data",
    "text": "Preparing data\nFirst of all I am going to mutate data in inter1, inter2 and interaction due to the codebook. I will also rename two into actor1 and actor2. We will see categorical data which shows two parts participating in event and their interaction. I will also change dates so that I have full date in one column and month+year in another to group data by month if needed.\nAlso I will reorder variables so that bar charts or other visualization look better. Last but not least I will include some categories in variable “source scale” into one category. I will leave main categories that are mentioned in the codebook.\nIf any other mutation or grouping will be needed they will be included i\n\n\nCode\n# Change variables due to the codebook\nprotest <- protest_selected %>%\n  mutate(inter1 = case_when(\n         inter1 == 1 ~ \"State Forces\",\n         inter1 == 2 ~ \"Rebel Groups\",\n         inter1 == 3 ~ \"Political Militas\",\n         inter1 == 4 ~ \"Identity Militas\",\n         inter1 == 5 ~ \"Rioters\",\n         inter1 == 6 ~ \"Protesters\",\n         inter1 == 8 ~ \"External/Other Forces\"))\nprotest <- protest %>%\n  mutate(inter2 = case_when(\n         inter2 == 1 ~ \"State Forces\",\n         inter2 == 3 ~ \"Political Militas\",\n         inter2 == 5 ~ \"Rioters\",\n         inter2 == 6 ~ \"Protesters\",\n         inter2 == 7 ~ \"Civilians\",\n         inter2 == 8 ~ \"External/Other Forces\",\n         inter2 == 0 ~ \"No actor\"))\nprotest <- protest %>%\n  mutate(interaction = case_when(interaction == 10 ~ \"sole military action\",\n         interaction == 13 ~ \"military vs political militia\",\n         interaction == 15 ~ \"military vs rioters\",\n         interaction == 16 ~ \"military vs protesters\",\n         interaction == 17 ~ \"military vs civilians\",\n         interaction == 18 ~ \"military vs other\",\n         interaction == 27 ~ \"rebels vs civilians\",\n         interaction == 30 ~ \"sole political militia action\",\n         interaction == 33 ~ \"political militia vs political militia\",\n         interaction == 36 ~ \"political militia vs protesters\",\n         interaction == 37 ~ \"political militia vs civilians\",\n         interaction == 47 ~ \"communal militia vs civilians\",\n         interaction == 50 ~ \"sole rioter action\",\n         interaction == 55 ~ \"rioters vs rioters\",\n         interaction == 56 ~ \"rioters vs protesters\",\n         interaction == 57 ~ \"rioters vs civilians\",\n         interaction == 58 ~ \"rioters vs others\",\n         interaction == 60 ~ \"sole protester action\",\n         interaction == 66 ~ \"protesters vs protesters\",\n         interaction == 68 ~ \"protesters vs other\",\n         interaction == 78 ~ \"other actor vs civilians\",\n         interaction == 88 ~ \"sole other action\"))\n\n# Rename\nprotest <- protest %>%\n  rename(\"actor1\" = \"inter1\", \"actor2\" = \"inter2\", \"admin\" = \"admin1\")\n\n# Change dates \nprotest <- protest %>%\n  separate(col=event_date, into=c(\"day\", \"month\", \"y\"), sep=\" \", remove = FALSE)\nprotest <- select(protest, !contains(\"day\"))\nprotest <- protest %>%\n  unite(\"m_y\", month, y, remove = TRUE)\nprotest$m_y <- str_replace(protest$m_y, \"_\", \" \")\nprotest <- protest %>%\n  mutate(event_date=str_replace_all(event_date, c(\" December \" = \"-12-\",\n                                                  \" November \" = \"-11-\")))\nprotest <- protest %>%\n  mutate(event_date=str_replace_all(event_date, c(\" October \" = \"-10-\",\n                                                  \" September \" = \"-09-\",\n                                                  \" August \" = \"-08-\",\n                                                  \" July \" = \"-07-\",\n                                                  \" June \" = \"-06-\",\n                                                  \" May \" = \"-05-\",\n                                                  \" April \" = \"-04-\",\n                                                  \" March \" = \"-03-\",\n                                                  \" February \" = \"-02-\",\n                                                  \" January \" = \"-01-\")))\n\n# Reorder variables\nprotest <- protest %>%\n  mutate(event_type = fct_relevel(event_type, \"Protests\",\n                                  \"Riots\",\n                                  \"Violence against civilians\",\n                                  \"Explosions/Remote violence\",\n                                  \"Battles\"))\n\nprotest <- protest %>%\n  mutate(sub_event_type = fct_relevel(sub_event_type, \"Peaceful protest\",\n                                      \"Protest with intervention\",\n                                      \"Violent demonstration\",\n                                      \"Mob violence\",\n                                      \"Attack\",\n                                      \"Remote explosive/landmine/IED\",\n                                      \"Armed clash\",\n                                      \"Excessive force against protesters\",\n                                      \"Sexual violence\",\n                                      \"Abduction/forced disappearance\",\n                                      \"Grenade\"))\nprotest <- protest %>%\n  mutate(actor1 = fct_relevel(actor1, \"Protesters\",\n                              \"Rioters\",\n                              \"Political Militas\",\n                              \"State Forces\",\n                              \"External/Other Forces\",\n                              \"Identity Militas\"))\nprotest <- protest %>%\n   mutate(actor2_type = fct_relevel(actor2, \"No actor\",\n                                    \"State Forces\",\n                                    \"Civilians\",\n                                    \"Protesters\",\n                                    \"Rioters\",\n                                    \"External/Other Forces\",\n                                    \"Political Militas\"))\nprotest <- protest %>%\n  mutate(country = fct_relevel(country, \"Sweden\",\n                               \"Greece\",\n                               \"Belgium\",\n                               \"Portugal\",\n                               \"Austria\",\n                               \"Czech Republic\",\n                               \"Hungary\"))\n\n# Recode source scale and relevel\nprotest <- protest %>%\n  mutate(source_scale = case_when(source_scale == \"National-Regional\" ~ \"Mixed other\",\n         source_scale == \"New media-National\" ~ \"Mixed other\",\n         source_scale == \"Other-New media\" ~ \"Mixed other\",\n         source_scale == \"Regional-International\" ~ \"Mixed other\",\n         source_scale == \"New media-Regional\" ~ \"Mixed other\",\n         source_scale == \"Other-International\" ~ \"Mixed other\",\n         source_scale == \"Other-Subnational\" ~ \"Mixed other\",\n         source_scale == \"New media-International\" ~ \"Mixed other\",\n         source_scale == \"New media-Subnational\" ~ \"Mixed other\",\n         source_scale == \"Subnational-International\" ~ \"Mixed other\",\n         source_scale == \"National\" ~ \"National\",\n         source_scale == \"Other\" ~ \"Other\",\n         source_scale == \"Subnational\" ~ \"Subnational\",\n         source_scale == \"New media\" ~ \"New media\",\n         source_scale == \"Regional\" ~ \"Regional\",\n         source_scale == \"Other-National\" ~ \"Other-National\",\n         source_scale == \"International\" ~ \"International\",\n         source_scale == \"National-International\" ~ \"National-International\",\n         source_scale == \"Subnational-National\" ~ \"Subnational-National\"))\n\nprotest <- protest %>%\n  mutate(source_scale = fct_relevel(source_scale, \"National\",\n                                    \"Other\", \"Subnational\",\n                                    \"New media\",\n                                    \"Subnational-National\",\n                                    \"Mixed other\",\n                                    \"Other-National\",\n                                    \"National-International\",\n                                    \"International\"))"
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#overview-of-event-types-in-all-countries-together",
    "href": "posts/MariiaDubyk_FinalProject.html#overview-of-event-types-in-all-countries-together",
    "title": "Final Project",
    "section": "Overview of event types in all countries together",
    "text": "Overview of event types in all countries together\nBefore we start comparing countries, we should look at general trend for all countries together to know what we speak about when we say protest and radicalization in EU countries. The vast majority of events are protests (93.5% of all events). Riots which include some level of violence like vandalism or making barricades constitute for 5.1% of events. The other three types of more violent events are 1,3% together. It means that the region is politically stable as expected.\n\n\nCode\nggplot(protest, aes(event_type)) + \n  geom_bar(fill=\"#440154ff\") + \n  xlab(\"Event type\") + \n  ylab (\"Number of events\") + \n  theme_bw() + \n  ggtitle (\"Event type in 7 EU countries 2020-2022\") + \n  labs (fill = \"Event type\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  theme(axis.text = element_text(size = 7))\n\n\n\n\n\nNext graph shows that most protest are peaceful. Rots are both “mob violence” (group against group) and “violent demonstration” (group against property).\n\n\nCode\nggplot(protest, aes(y=\"\", x=event_type, fill = sub_event_type)) +\n  geom_bar(position=\"fill\", stat=\"identity\") + \n  coord_flip() + \n  xlab(\"Event type\") + \n  ylab (\"\") + theme_bw() + \n  scale_fill_viridis(discrete = T) + \n  ggtitle (\"Proportion of sub event type in five event types\") + \n  labs (fill = \"Sub event type\") + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nNext chart shows that peaceful protest are most frequent (90,4%). Next goes protest with intervention, when protesters where stopped by police or other group (3,1%). On third and fourth places are violent demonstration (2,8%) and mob violence (2,4%). In general protests are not violent, police usually do not intervene political activists meetings and the region shows political stability.\n\n\nCode\nggplot(protest, aes(sub_event_type)) + \n  geom_bar(fill=\"#440154ff\") + \n  coord_flip() + \n  xlab(\"Sub event type\") + \n  ylab (\"Number of events\") + \n  theme_bw() + \n  ggtitle (\"Sub event type in 7 EU countries 2020-2022\") + \n  labs (fill = \"Sub event type\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  theme(axis.text = element_text(size = 7))"
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#comparing-more-and-less-rich-countries",
    "href": "posts/MariiaDubyk_FinalProject.html#comparing-more-and-less-rich-countries",
    "title": "Final Project",
    "section": "Comparing more and less rich countries",
    "text": "Comparing more and less rich countries\nThe graph shows that Sweden has the largest number of protests. However, it belongs to countries with higher GDP per capita. Czech Republic and Hungary are less rich countries but have the lowest number of protests. It basically answers the research question. Economic situation in EU countries today does not play a visible role in protest and radicalization. This conclusion refers only to European countries.\n\n\nCode\nggplot(protest, aes(country, fill = event_type)) + \n  geom_bar() + \n  xlab(\"Country\") + \n  ylab (\"Number of events\") + \n  theme_bw() + \n  ggtitle (\"Event type by country 2020-2022\") + \n  labs (fill = \"Event type\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  theme(axis.text = element_text(size = 7)) + scale_fill_viridis(discrete = T)\n\n\n\n\n\nFor the next graph I selected only 4 most frequent sub event types. The idea is to look if some specific sub event type is more common in certain countries. Again we see that in Sweden, Belgium, and Austria there are more events with radicalization like violent demonstration or mob violence than in Portugal, Czech Rebublic and Hungary that have lower GDP per capita.\nGreece has the biggest number of violent events. GDP per capita of Greece is close to GDP per capita of Hungary. It is worth noting that protest with intervention in Greece is relatively the same as in other countries. So we may conclude that peaceful protesters can engage in political activism. But the number of violent events should probably be explored closer.\n\n\nCode\nmyplot<-ggplot(protest, aes(country, fill = sub_event_type)) + \n  geom_bar() + \n  scale_fill_viridis(discrete = T) + \n  xlab(\"Country\") + \n  ylab (\"Number of events\") + \n  theme_bw() + \n  ggtitle (\"Four frequent sub event types by country\") + labs (fill = \"Sub event type\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  theme(axis.text = element_text(size = 7))\nmyplot %+% subset(protest, sub_event_type %in% c(\"Peaceful protest\",\n                                                 \"Protest with intervention\",\n                                                 \"Violent demonstration\",\n                                                 \"Mob violence\"))\n\n\n\n\n\nThe next charts are aimed to show if there is a difference between protest locations among countries. It might be that in some countries events are more concentrated in the capital city while other divisions have low number of protests. In our case, all countries have protests all over the country, with a concentration in the capital city. We can mention that Belgium events are more equally distributed than Hungary events. It may be interesting to look closer at the distribution of events in the future and check if it is related to the urbanization level or economy.\n\n\nCode\nlibrary(magrittr)\n\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nCode\nlibrary(maps)\n\n\nWarning: package 'maps' was built under R version 4.2.2\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:viridis':\n\n    unemp\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nCode\nlibrary(mapproj)\n\n\nWarning: package 'mapproj' was built under R version 4.2.2\n\n\nCode\nSweden_protest <- subset(protest, country %in% c(\"Sweden\"))\nWorld <- map_data(\"world\")\nSweden <- map_data(\"world\") %>% filter(region==\"Sweden\")\n \nggplot() + \n  geom_polygon(data = Sweden, aes(x=long, y = lat, group = group),\n               fill=\"#B0E2FF\", alpha=0.7) +\n  geom_point( data=Sweden_protest, aes(x=longitude, y=latitude),\n              color=\"#404788FF\",  alpha=1) +\n  theme_minimal() +\n  ggtitle(\"Sweden events 2020-2021\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = 'none',\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(), \n        axis.text = element_blank(), \n        axis.title = element_blank())\n\n\n\n\n\n\n\nCode\nBelgium_protest <- subset(protest, country %in% c(\"Belgium\"))\nBelgium <- map_data(\"world\") %>% filter(region==\"Belgium\")\nggplot() + \n  geom_polygon(data = Belgium, aes(x=long, y = lat, group = group),\n               fill=\"#B0E2FF\", alpha=0.7) +\n  geom_point( data=Belgium_protest, aes(x=longitude, y=latitude),\n              color=\"#404788FF\",  alpha=1) +\n  ggtitle(\"Belgium events 2020-2021\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none',\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(), \n        axis.text = element_blank(), \n        axis.title = element_blank())\n\n\n\n\n\n\n\nCode\nAustria_protest <- subset(protest, country %in% c(\"Austria\"))\nAustria <- map_data(\"world\") %>% filter(region==\"Austria\")\nggplot() + \n  geom_polygon(data = Austria, aes(x=long, y = lat, group = group),\n               fill=\"#B0E2FF\", alpha=0.7) +\n  geom_point( data=Austria_protest, aes(x=longitude, y=latitude),\n              color=\"#404788FF\",  alpha=1) +\n  ggtitle(\"Austria events 2020-2021\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none',\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(), \n        axis.text = element_blank(), \n        axis.title = element_blank())\n\n\n\n\n\n\n\nCode\nPortugal_protest <- subset(protest, country %in% c(\"Portugal\"))\nPortugal <- map_data(\"world\") %>% filter(region==\"Portugal\")\nggplot() + \n  geom_polygon(data = Portugal, aes(x=long, y = lat, group = group),\n               fill=\"#FFB6C1\", alpha=0.7) +\n  geom_point( data=Portugal_protest, aes(x=longitude, y=latitude),\n              color=\"#404788FF\",  alpha=1) +\n  ggtitle(\"Portugal events 2020-2021\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none',\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\nCode\nHungary_protest <- subset(protest, country %in% c(\"Hungary\"))\nHungary <- map_data(\"world\") %>% filter(region==\"Hungary\")\nggplot() + \n  geom_polygon(data = Hungary, aes(x=long, y = lat, group = group),\n               fill=\"#FFB6C1\", alpha=0.7) +\n  geom_point( data=Hungary_protest, aes(x=longitude, y=latitude),\n              color=\"#404788FF\",  alpha=1) +\n  ggtitle(\"Hungary events 2020-2021\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none',\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\nCode\nGreece_protest <- subset(protest, country %in% c(\"Greece\"))\nGreece <- map_data(\"world\") %>% filter(region==\"Greece\")\n\nggplot() + \n  geom_polygon(data = Greece, aes(x=long, y = lat, group = group),\n               fill=\"#FFB6C1\", alpha=0.7) +\n  geom_point( data=Greece_protest,\n              aes(x=longitude, y=latitude),\n              color=\"#404788FF\",  alpha=1) + \n  ggtitle(\"Greece events 2020-2022\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none',\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#protest-and-covid19-crisis",
    "href": "posts/MariiaDubyk_FinalProject.html#protest-and-covid19-crisis",
    "title": "Final Project",
    "section": "Protest and COVID19 crisis",
    "text": "Protest and COVID19 crisis\nThe next chart compares number of protest during three years. We do not see the difference between years an protest types.\n\n\nCode\nggplot(protest, aes(year, fill=event_type)) + geom_bar() + \n  xlab(\"Year\") + \n  ylab (\"Number of events\") + \n  theme_bw() + \n  ggtitle (\"Changes in number of event types 2020-2022\") + \n  labs (fill = \"Event type\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  theme(axis.text = element_text(size = 10)) + \n  scale_fill_viridis(discrete = T)\n\n\n\n\n\nUnfortunately, there is no data for 2019 year so I decided to compare number of protests in 2 months before the pandemic (Jan, Feb 2020) and the same month in 2021 and 2022 to see if number of events changed as a response to the crisis. No difference was found, as we can see in the next graph.\n\n\nCode\nprotest_grouped <- protest %>%\n  add_column(value = 1)\nprotest_grouped$value <- as.numeric(protest_grouped$value) \nprotest_grouped <- protest_grouped %>%\n  group_by(m_y) %>%\n  select(value) %>%\n  summarise_all(sum, na.rm = TRUE)\n\n\nAdding missing grouping variables: `m_y`\n\n\nCode\nprotest_grouped <- protest_grouped %>%\n  separate(col=m_y, into=c(\"Month\", \"Year\"), sep=\" \", remove = FALSE)\n\n\nmyplot<-ggplot(protest_grouped, aes(x=Month, y=value, fill=Year)) + geom_bar(position=\"dodge\", stat=\"identity\") + \n  xlab(\"Month\") + \n  ylab (\"Number of events\") + \n  theme_bw() + \n  ggtitle (\"Events in January, February 2020-2022\") + \n  labs (fill = \"Year\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  theme(axis.text = element_text(size = 7)) + \n  scale_fill_viridis(discrete = T)\nmyplot %+% subset(protest_grouped, Month %in% c(\"January\", \"February\"))"
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#reflection",
    "href": "posts/MariiaDubyk_FinalProject.html#reflection",
    "title": "Final Project",
    "section": "Reflection",
    "text": "Reflection\nI am glad that I had the opportunity to work with protest data. It gave me some ideas of how to think about political violence research another way. Using these large datasets, I can quickly see trends and percentages of different events among countries. I can look not at violent ideology in some countries but at the number of violent events and understand the level of radicalization.\nThe most challenging for me was having doubts about completely changing data to have numerical data. In this dataset, we have categorical data so it is nice to look at percentages and create bar charts, etc. I was thinking if I should change the data so that month, event type and country would be cases. This way I would have values for each case. I still decided to leave the data as it is gathered and change piece of it for last visualization. In the future, I should practice more with ways to quickly organize datasets for my purpose.\nSpeaking about the research idea I was also concerned about sample size and simplification of the exploration I did. My research is just a first glance and playing with data to find some assumptions about the economy and the number of protests."
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#conclusion",
    "href": "posts/MariiaDubyk_FinalProject.html#conclusion",
    "title": "Final Project",
    "section": "Conclusion",
    "text": "Conclusion\n\nFirst of all I did not find any patterns for event number, event type and sub event type related to economic differences between countries. Rich EU countries may have the much higher number of protests than countries with lower GDP.\nEU countries show political stability and low level of radicalization. Greece has higher level of political violence than other analyzed EU countries. It is related not to behavior of state forces but to repertories that protesters use. It would be reasonable to look closer at Greece case in the future.\nProtests usually are distributed along the countries (not including geographical specifics). Events are concentrated in capital cities. In some countries (like Belgium) protests are more scattered along the country than in others (Hungary).\nThe number of events in January and February 2022 did not increase compared to the same months before COVID19 pandemic. It also implies the idea that other factors play a more visible role in political protest than the economy."
  },
  {
    "objectID": "posts/MariiaDubyk_FinalProject.html#bibliography",
    "href": "posts/MariiaDubyk_FinalProject.html#bibliography",
    "title": "Final Project",
    "section": "Bibliography",
    "text": "Bibliography\n\nACLED. (2019). “Armed Conflict Location & Event Data Project (ACLED) Codebook, 2019.”\nArmed Conflict Location & Event Data Project (ACLED); www.acleddata.com.\nR Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html",
    "href": "posts/MatthewONeill_FinalPaper.html",
    "title": "Final Paper",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#introduction",
    "href": "posts/MatthewONeill_FinalPaper.html#introduction",
    "title": "Final Paper",
    "section": "Introduction",
    "text": "Introduction\nI will be analyzing a dataset for Massachusetts public schools and districts. The dataset was collected from the 2017 academic year and is a combination of several smaller datasets reported by all 288 public school districts in Massachusetts to the Massachusetts Department of Education. Each of the 376 rows of the dataset represents a high school and each school contains socioeconomic data(e.g. % economically disadvantaged, % African American, % White, etc) along with outcomes of students from those schools(% graduated, % pursuing college/university). Test scores(e.g. SAT, AP, MCAS) are also collected and can be used as a secondary metric of student success. Finally, the data includes school finances(e.g. teacher salary, district expenditures) which will be the primary focus of our analysis as predictor variables for student success. Schools in the dataset range from elementary to high school, but I will be working with just the subset of high schools.\nI will be making the scope of my analysis more narrow and focus solely on high schools, and as such, many of the MCAS related columns can be omitted to make the dataset easier to work with. The focus of my analysis will be how spending can affect test scores and graduation rates."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#central-question",
    "href": "posts/MatthewONeill_FinalPaper.html#central-question",
    "title": "Final Paper",
    "section": "Central Question",
    "text": "Central Question\nDifferent school districts vary widely when it comes to how their schools are funded and how their teachers are compensated. Much of the data on how school districts spend their money is public, we can be useful for an analysis of the factors which can lead to better outcomes among students in public schools.\nThe central question investigated in this paper is: does increasing teacher salary in public schools impact the success of students? Student success will be defined primarily by higher graduation rates but we will also look into higher test scores as a metric of success. During the process of exploring this question, we will also be exploring how the economic background of students and class size impacts school success, especially in conjunction with average teacher salary."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#data-cleaning",
    "href": "posts/MatthewONeill_FinalPaper.html#data-cleaning",
    "title": "Final Paper",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe dataset is in fairly nice shape to begin with, but we are only interested in a small portion of the original 300 columns of data and since we are only interested in high schools, we can safely omit Elementary and Middle schools which do not report a Graduation rate.\n\n\nCode\ndata<- read_csv(\"_data/MA_Public_Schools_2017.csv\",show_col_types = FALSE)\n\nkeeps <- c(1,2,10,13,14,15,26,27,28,29,30,31,32,34,36,38,40,41,42,43,44,45,46,47,48,49,50,51,52,53,55,62,64,68,70,71,72,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97)\n\nhs <- data[keeps]%>% \n      subset(! is.na(data$`% Graduated`))\n\ndim(hs)\n\n\n[1] 376  55\n\n\nCode\nlength(unique(hs$`District Code`))\n\n\n[1] 288\n\n\nAfter slimming down the dataset to only contain necessary data there are 376 rows, representing 376 Massachusetts high schools, and 53 columns of useful data for each. There are also 288 unique district codes, as larger districts have more than one high school. Districts are important to our central question because most districts have standardized pay scales and multiple schools in the same district will be reported to have the same average salary.\nThe variables that I am interested in specifically from this dataset are Average Teacher Salary, Graduation Rates, SAT Scores, Average AP Test Scores, Average Class Sizes, and percentage of students who are economically disadvantaged. The last of these may be the most important to our analysis, as economically disadvantaged students will have a harder time in school regardless of teacher salary, so we need to compare across schools with a similar proportion of economically disadvantaged students.\nAverage teacher salary is reported by district, which is unfortunate of our analysis as some districts have many schools which can vary in performance. However, the only real outlier in regards to this issue is Boston Public Schools. For this reason, our analysis will be done on Massachusetts as a whole, and Massachusetts excluding Boston Public schools. Teacher salary will be the focus of the analysis, as if a correlation with salary can be shown to affect student success then real steps could be taken by policy makers to improve student success by increasing teacher salary. Similarly, if a correlation with class size and student success can be shown, steps could be taken to hire more teachers, thus reducing the average class size.\nThe percent of economically disadvantaged students in each school will not be explored much explicitly when it comes to predicting student success, but it is very important to the analysis of teacher salary, as it plays a role in explaining some of the variance in the most trivial model of student success with teacher salary.\nSince our focus is primarily high school students, the metrics for success will be primarily Graduation rates, AP Scores, and SAT Scores. Graduation rates are recorded across the board, but not every student takes the SATs or AP tests, which can skew the average scores. More specifically, economically disadvantaged students and students who are already performing poorly in school are less likely to pay for these courses, so while we will explore how teacher salary affects these metrics, the focus will be on graduation rates.\n\n\nCode\nprint(\"Average Salary Breakdown\")\n\n\n[1] \"Average Salary Breakdown\"\n\n\nCode\nsummary(hs$`Average Salary`,na.rm=TRUE)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  53763   67556   73404   74267   78802  100731      36 \n\n\nCode\nprint(\"Graduation Rates Breakdown\")\n\n\n[1] \"Graduation Rates Breakdown\"\n\n\nCode\nsummary(hs$`% Graduated`,na.rm=TRUE)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   82.15   92.25   83.33   96.20  100.00 \n\n\nCode\nprint(\"Percentage Economically Disadvantaged Breakdown\")\n\n\n[1] \"Percentage Economically Disadvantaged Breakdown\"\n\n\nCode\nsummary(hs$`% Economically Disadvantaged`,na.rm=TRUE)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.10   13.18   25.55   30.82   44.33   93.90 \n\n\nAfter taking a very short look into some summary statistics for some of the variables we will be diving into, it appears that the average teacher salary across the state is $74000, without too much variance. Graduation rates average 83% statewide but this number appears to be skewed as the median is a fair bit higher at 92%. The percentage of economically disadvantages students in a given school also appears to be skewed a bit by outliers, as the average statewide is 30% but the median is 25.5%, indicating that there are some schools with a very high percentage of economically disadvantaged students."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#mutation",
    "href": "posts/MatthewONeill_FinalPaper.html#mutation",
    "title": "Final Paper",
    "section": "Mutation",
    "text": "Mutation\nWorking with the data can be made a lot easier by partitioning some of the variables we’re interested in into sections. For teacher salary, average expenditure, class size, and percentage of economically disadvantaged students, I will be creating a categorical variable placing schools into various brackets of each variable. This will make correlations a bit more clear as there is not necessarily a clear trend across all schools when it comes to student performance when plotted against any of these variables individually.\n\n\nCode\nhs <- mutate(hs,`Salary Bracket` = case_when(\n            `Average Salary` < 55000 ~ \"$55000 or Less\",\n            `Average Salary` < 60000 ~ \"$55000 to $60000\",\n            `Average Salary` < 65000 ~ \"$60000 to $65000\",\n            `Average Salary` < 70000 ~ \"$65000 to $70000\",\n            `Average Salary` < 75000 ~ \"$70000 to $75000\",\n            `Average Salary` < 80000 ~ \"$75000 to $80000\",\n            `Average Salary` < 85000 ~ \"$80000 to $85000\",\n            `Average Salary` <= 90000 ~ \"$85000 to $90000\",\n            `Average Salary` > 90000 ~ \"$90000 or more\"\n              ))\n\nhs <- mutate(hs,`Per Pupil Expenditure Bracket` = case_when(\n            `Average Expenditures per Pupil` < 12500 ~ \"$12500 or Less\",\n            `Average Expenditures per Pupil` < 15000 ~ \"$12500 to $15000\",\n            `Average Expenditures per Pupil` < 17500 ~ \"$15000 to $17500\",\n            `Average Expenditures per Pupil` < 20000 ~ \"$17500 to $20000\",\n            `Average Expenditures per Pupil` < 22500 ~ \"$20000 to $22500\",\n            `Average Expenditures per Pupil` <= 25000 ~ \"$22500 to $25000\",\n            `Average Expenditures per Pupil` > 25000 ~ \"$25000 or more\"\n              ))\n\nhs <- mutate(hs,`Economically Disadvantaged Bracket` = case_when(\n            `% Economically Disadvantaged` < 25.0 ~ \"25% or Less\",\n            `% Economically Disadvantaged` < 50.0 ~ \"25% to 50%\",\n            `% Economically Disadvantaged` < 75.0 ~ \"50% to 75%\",\n            `% Economically Disadvantaged` < 100.0 ~ \"75% or more\"\n              ))\n\nhs <- mutate(hs, `Class Size Bracket` = case_when(\n        `Average Class Size` < 5 ~ \"5 or fewer Students\",\n        `Average Class Size` < 10 ~ \"5 to 10 Students\",\n        `Average Class Size` < 15 ~ \"10 to 15 Students\",\n        `Average Class Size` < 20 ~ \"15 to 20 Students\",\n        `Average Class Size` < 25 ~ \"20 to 25 Students\",\n        `Average Class Size` < 30 ~ \"25 to 30 Students\",\n        `Average Class Size` <= 35 ~ \"30 to 35 Students\",\n        `Average Class Size` > 35 ~ \"35 or more Students\"\n))\n\nhs <- mutate(hs, `Average AP Score` = (`AP_Score=1` + 2*`AP_Score=2` + 3*`AP_Score=3` + 4*`AP_Score=4` + 5*`AP_Score=5`)/`AP_Tests Taken`)\nhs <- mutate(hs, `Average Cumulative SAT Score` = `Average SAT_Math` + `Average SAT_Reading`+`Average SAT_Writing`)"
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#teacher-salary-category-first-glance",
    "href": "posts/MatthewONeill_FinalPaper.html#teacher-salary-category-first-glance",
    "title": "Final Paper",
    "section": "Teacher Salary Category First Glance",
    "text": "Teacher Salary Category First Glance\nThese categories will be used to look at correlations of teacher salary and student success, while keeping some other key variables constant such as the percentage of economically disadvantaged or average class size. It’s also useful to break salary into brackets so see how student success is grouped across districts with different salaries.\nBelow are histograms visualizing graduation rates and AP test scores across different salary brackets:\n\n\nCode\nggplot(data=subset(hs,!is.na(`Salary Bracket`)), aes(x=`Salary Bracket`, y=`% Graduated`, fill=`Salary Bracket`)) +\n    geom_boxplot()  +\n    theme(\n      legend.position=\"right\",\n      plot.title = element_text(size=11),\n      axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()\n    ) +\n    ggtitle(\"Teacher Salary vs Graduation\")\n\n\n\n\n\nCode\nggplot(data=subset(hs,!is.na(`Salary Bracket`)), aes(x=`Salary Bracket`, y=`Average AP Score`, fill=`Salary Bracket`)) +\n    geom_boxplot()  +\n    theme(\n      legend.position=\"right\",\n      plot.title = element_text(size=11),\n      axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()\n    ) +\n    ggtitle(\"Teacher Salary vs AP Scores\")\n\n\n\n\n\nAt first glance, there does appear to be a slight trend upward in graduation rate as the average salary increases, but there is a much clearer trend in AP scores improving as average salary increases.\nBoth graphics have an outlier however in the 85000 to 90000 dollar range. This is because the Boston Public School District falls in this range and it includes many different schools which have various levels of socioeconomic diversity. We will be attempting to account for the variance across schools in Boston Public Schools by including more variables in our analysis and model, but it will also be useful to create a copy of our high school dataset which does not include Boston Public Schools.\n\n\nCode\nhs_WO_boston <- subset(hs, !(`District Name` == \"Boston\"))\n\n\nBelow are the same histograms visualizing graduation rates and AP test scores across different salary brackets, but excluding Boston Public Schools:\n\n\nCode\nggplot(data=subset(hs_WO_boston,!is.na(`Salary Bracket`)), aes(x=`Salary Bracket`, y=`% Graduated`, fill=`Salary Bracket`)) +\n    geom_boxplot()  +\n    theme(\n      legend.position=\"right\",\n      plot.title = element_text(size=11),\n      axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()\n    ) +\n    ggtitle(\"Teacher Salary vs Graduation\")\n\n\n\n\n\nCode\nggplot(data=subset(hs_WO_boston,!is.na(`Salary Bracket`)), aes(x=`Salary Bracket`, y=`Average AP Score`, fill=`Salary Bracket`)) +\n    geom_boxplot()  +\n    theme(\n      legend.position=\"right\",\n      plot.title = element_text(size=11),\n      axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()\n    ) +\n    ggtitle(\"Teacher Salary vs AP Scores\")\n\n\n\n\n\nThe exclusion of Boston Public Schools doesn’t yet appear to change the trend seen for graduation rate, but the trend for AP test scores become much clearer without Boston Public Schools, which is already suggestive of teacher salary being a predictor variable for student success.\nFinally, it would be good to look at how economic disadvantage impact graduation rate, regardless of teacher salary:\n\n\nCode\nggplot(data=subset(hs,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Economically Disadvantaged Bracket`, y=`% Graduated`, fill=`Economically Disadvantaged Bracket`)) +\n    geom_boxplot()  +\n    theme(\n      legend.position=\"right\",\n      plot.title = element_text(size=11),\n      axis.text.x=element_blank(),\n        axis.ticks.x=element_blank()\n    ) +\n    ggtitle(\"Economically Disadvantaged vs Graduation\")\n\n\n\n\n\nThe percentage of economically disadvantaged students in a school appears in the graphic above to be very strongly correlated to graduation rates, which is something we will keep an eye on in our analysis."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#multivariate-analysis",
    "href": "posts/MatthewONeill_FinalPaper.html#multivariate-analysis",
    "title": "Final Paper",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\nThe box plots from the previous section are useful for demonstrating that we are moving in the right direction with our research question, and that a correlation might be uncovered with a deeper dive into our dataset. The box plots showed some evidence of a trend as salary increases, but there are other factors to consider, and we will create some multivariate visualizations to try and uncover them.\nTo begin, we’ll look into how salary is correlated with graduation rates while taking into account economic disadvantage and average class size:\n\n\nCode\nggplot(data=subset(hs,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Average Salary`, y=`% Graduated`,color=`Economically Disadvantaged Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs Graduation Rate\", \n       subtitle = \"Partitioned by Economic Disadvantage\",\n        y = \"Graduation Rate\", x = \"Average Teacher Salary\")\n\n\n\n\n\nCode\nggplot(data=subset(hs,!is.na(`Class Size Bracket`)), aes(x=`Average Salary`, y=`% Graduated`,color=`Class Size Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs Graduation Rate\", \n       subtitle = \"Partitioned by Class Size\",\n       y = \"Graduation Rate\", x = \"Average Teacher Salary\")\n\n\n\n\n\nWhen partitioning based on the percentage of economically disadvantaged students, a very clear trend begins to come to light. Across all four categories of economically disadvantaged students, there is a positively sloped line of best fit for salary being plotted against graduation rate. The same is not exactly true for average class size, which suggests that if class size does have an impact on student success, it does not make up as much of the variance as economic disadvantage does.\nThe subset of data which omits Boston Public Schools produces similar trends:\n\n\nCode\nggplot(data=subset(hs_WO_boston,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Average Salary`, y=`% Graduated`,color=`Economically Disadvantaged Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs Graduation Rate(without Boston)\", \n       subtitle = \"Partitioned by Economic Disadvantage\",\n        y = \"Graduation Rate\", x = \"Average Teacher Salary\")\n\n\n\n\n\nCode\nggplot(data=subset(hs_WO_boston,!is.na(`Class Size Bracket`)), aes(x=`Average Salary`, y=`% Graduated`,color=`Class Size Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs Graduation Rate(without Boston)\", \n       subtitle = \"Partitioned by Class Size\",\n       y = \"Graduation Rate\", x = \"Average Teacher Salary\")\n\n\n\n\n\nThe same positive trends appear across all four categories of economic disadvantage when Boston is omitted, but interestingly a much clearer trend appears for class size after omitting Boston Public Schools, with all seven categories producing positively sloped lines of best fit. This makes intuitive sense, as Boston Public Schools generally have more students and so we would need to take into account both average class size and economic disadvantages.\nAnother metric of student success which may be correlated with Teacher Salary are test scores. There will be less of an emphasis on class size for these graphics, as not all students take AP courses and not all schools will have classes dedicated to preparing for the SATs.\n\n\nCode\nggplot(data=subset(hs,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Average Salary`, y=`Average AP Score`,color=`Economically Disadvantaged Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs Average AP Scores\", y = \"Average AP Scores\", x = \"Average Teacher Salary\")\n\n\n\n\n\nCode\nggplot(data=subset(hs,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Average Salary`, y=`Average Cumulative SAT Score`,color=`Economically Disadvantaged Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs SAT Scores\", y = \"Average Cumulative SAT Score\", x = \"Average Teacher Salary\")\n\n\n\n\n\nThe trends for test scores aren’t as clear as the trends for graduation rate. However, there does seem to be a collection of low test scores for Boston Public Schools, so it might be worth examining the same trends for the subset without Boston Public Schools. It’s also important to note that there is a positive relationship in the group of schools with fewer than 25% of their students being economically disadvantaged. This could be explained by having a higher proportion of students who can afford tutors, test preparation courses and textbooks, and have more time to study for specific standardized tests outside of their normal coursework.\n\n\nCode\nggplot(data=subset(hs_WO_boston,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Average Salary`, y=`Average AP Score`,color=`Economically Disadvantaged Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs AP Scores\", y = \"Average AP Scores\", x = \"Average Teacher Salary\")\n\n\n\n\n\nCode\nggplot(data=subset(hs_WO_boston,!is.na(`Economically Disadvantaged Bracket`)), aes(x=`Average Salary`, y=`Average Cumulative SAT Score`,color=`Economically Disadvantaged Bracket`)) + geom_point()+\n  theme_bw() +\n  geom_smooth(method=lm, se=FALSE) +\n  labs(title =\"Average Teacher Salary vs SAT Scores\", y = \"Average Cumulative SAT Score\", x = \"Average Teacher Salary\")\n\n\n\n\n\nThe trend for test scores improving as teacher salary improves among schools with a low proportion of economically disadvantaged students appears to become strong when omitting Boston Public schools. There still is not too much of a correlation one way or the other in schools with more students who are lower-income, which might suggest that regardless of teacher salary, students who are economically disadvantaged struggle more with standardized tests. This is a correlation that might be worth investigating deeper during research of the usefulness of standardized tests, but that is out of the scope of this paper."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#conclusions-from-multivariate-visuals",
    "href": "posts/MatthewONeill_FinalPaper.html#conclusions-from-multivariate-visuals",
    "title": "Final Paper",
    "section": "Conclusions From Multivariate Visuals",
    "text": "Conclusions From Multivariate Visuals\nIt appears that there is a correlation between teacher salary and graduation rates of students, especially after holding socioeconomic factors constant. The percentage of economically disadvantaged students in a school produces very clear positive trend lines for the graduation rate improving as salary increases across all categories of economic disadvantage. This appears to be true with and without the Boston Public Schools outliers being included. The trends for the same correlation with class size held constant are smaller and less conclusive, but when omitting Boston Public Schools these trends again begin to show a correlation between average teacher salary and graduation rate.\nThere doesn’t appear to be as clear of a correlation between average teacher salary and test scores, particularly in our multivariate graphs which consider economic factors and class size. Some potential explanations for this include that not all students take AP or SAT tests, and that performing well on these tests often requires a fair bit of independent studying, often with a tutor or some sort of paid study guides."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#linear-regression-model",
    "href": "posts/MatthewONeill_FinalPaper.html#linear-regression-model",
    "title": "Final Paper",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nOur first level of analysis was very high level, just looking at a box plot of graduation rates and test scores for different categories of teacher salary. There seemed to be some trend, so we investigated further with a multivariate analysis of how teacher salary across schools with similar percentages of economically disadvantaged students affects graduation rates and test scores. There was a very clear trend for graduation rates and the trends became even more positively sloped when omitting the outlier of Boston Public Schools.\nTo finish the analysis, I will be creating Linear Models for graduation rate using the three main variables that were explored so far, teacher salary, percentage of economically disadvantaged students, and class size) as predictor variables.\n\n\nCode\nlinmod <- lm(hs$`% Graduated` ~ hs$`Average Salary`+hs$`% Economically Disadvantaged`+hs$`Average Class Size`)\n\n\nsummary(linmod)\n\n\n\nCall:\nlm(formula = hs$`% Graduated` ~ hs$`Average Salary` + hs$`% Economically Disadvantaged` + \n    hs$`Average Class Size`)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.330  -4.758  -0.264   5.833  41.791 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        9.041e+01  6.297e+00  14.357  < 2e-16 ***\nhs$`Average Salary`               -2.368e-05  8.117e-05  -0.292    0.771    \nhs$`% Economically Disadvantaged` -6.800e-01  3.278e-02 -20.741  < 2e-16 ***\nhs$`Average Class Size`            1.069e+00  1.964e-01   5.443 1.03e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.12 on 330 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6573,    Adjusted R-squared:  0.6542 \nF-statistic:   211 on 3 and 330 DF,  p-value: < 2.2e-16\n\n\nThe two big takeaways from this model is it has an R-square value of .6542, indicating that 65% of the variance in graduation rate can be explained by our model. The model also shows that the percentage of economically disadvantaged students and average class size have statistically significant impacts on graduation rate. Average salary does not appear to have as big of an impact in this model as we might’ve originally thought, but this is due to the other predictor variables making up a higher percentage of the variance in graduation rate.\nTo stay consistent, we will create the same model on the subset of data without Boston Public Schools:\n\n\nCode\nlinmod_no_boston <- lm(hs_WO_boston$`% Graduated` ~ hs_WO_boston$`Average Salary`+hs_WO_boston$`% Economically Disadvantaged`+hs_WO_boston$`Average Class Size`)\n\nsummary(linmod_no_boston)\n\n\n\nCall:\nlm(formula = hs_WO_boston$`% Graduated` ~ hs_WO_boston$`Average Salary` + \n    hs_WO_boston$`% Economically Disadvantaged` + hs_WO_boston$`Average Class Size`)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.985  -4.491  -0.261   5.088  41.307 \n\nCoefficients:\n                                              Estimate Std. Error t value\n(Intercept)                                  9.215e+01  7.660e+00  12.029\nhs_WO_boston$`Average Salary`               -3.260e-05  8.958e-05  -0.364\nhs_WO_boston$`% Economically Disadvantaged` -6.808e-01  3.618e-02 -18.815\nhs_WO_boston$`Average Class Size`            9.940e-01  2.081e-01   4.777\n                                            Pr(>|t|)    \n(Intercept)                                  < 2e-16 ***\nhs_WO_boston$`Average Salary`                  0.716    \nhs_WO_boston$`% Economically Disadvantaged`  < 2e-16 ***\nhs_WO_boston$`Average Class Size`           2.79e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.12 on 299 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6775,    Adjusted R-squared:  0.6743 \nF-statistic: 209.4 on 3 and 299 DF,  p-value: < 2.2e-16\n\n\nWhen excluding Boston Public Schools, there is actually a slightly higher R-squared value of .6743, which means this model accounts for a slightly higher proportion of variance for graduation rate excluding Boston, but the p-value of each of the predictor variables is very similar."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#conclusions",
    "href": "posts/MatthewONeill_FinalPaper.html#conclusions",
    "title": "Final Paper",
    "section": "Conclusions",
    "text": "Conclusions\nOur multivariate visualizations suggest that increasing teacher salary is correlated with higher rates of graduation across schools with similar numbers of economically disadvantaged students, and similar average class sizes. Increasing teacher salaries does not currently appear to impact SAT or AP test scores, but this might be due to SAT and AP tests requiring a fair bit of studying and tutoring on top of normal classes.\nOur linear regression models indicate however, that the percentage of economically disadvantaged is a much more statistically significant predictor of graduation rate than average teacher salary. Which does make sense sociologically, as regardless of the quality or number of teachers you have as a student, making ends meet is going to always take top priority over school work. Many economically disadvantaged students will need to work jobs after school, have less access to quality internet or technology, and be unable to afford extra study materials that more fortunate students may have access to.\nHowever, teacher salary still does appear to play a significant role in student success when economic factors are held constant and it is much easier to improve teacher salaries than it is to reduce the number of economically disadvantaged students. Thus, it would be beneficial to students to increase the average salary of teachers in public schools."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#reflection",
    "href": "posts/MatthewONeill_FinalPaper.html#reflection",
    "title": "Final Paper",
    "section": "Reflection",
    "text": "Reflection\nThere are some limitations in my analysis, namely related to the broad scope of the dataset. Salaries are reported on a district by district basis, test scores are reported school wide, as is the percentage of economically disadvantaged students. In some ways, it can be beneficial to have a broad dataset, as it makes our analysis more widely applicable, but there could be a lot of variables which explain variance in student success that are hidden within these broad statistics.\nThis dataset is also limited in scope when it comes to location. It explores public school data in just one state in the United States. Philosophy on how to run public education varies widely across the country and each state is going to have different levels of economic disadvantage and tax budgets. The question we attempted to answer was on the broad scale of trying to provide statistical evidence to the claim that teachers should be paid more, but due to the scope of the dataset, this claim can only be statistically supported in the context of Massachusetts Public Schools.\nSome questions I still have in regards to this topic after doing an analysis is if teacher salary must be increased across the United States, and what predictor variables play a significant role in the variance of individual student success."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#what-i-learned",
    "href": "posts/MatthewONeill_FinalPaper.html#what-i-learned",
    "title": "Final Paper",
    "section": "What I Learned",
    "text": "What I Learned\nI learned a lot over the course of the past semester working on this project. At a low level, I learned to use R in a much more in depth way. Previously, my experience in R was limited to creating linear models and doing statistical analysis by hand using variables from the model. I had very limited data visualization experience before now and I had only ever worked with datasets that did not require any cleaning or mutating. I now feel like I have a much stronger grasp on data visualization and cleaning in R and I also know how to learn new things in R very quickly, which will make it easier to learn R in a deeper way in the future.\nAt a high level, I learned to transform a dataset in a way that is useful for solving a research question. Sometimes your data needs to be cleaned to make it useful, such as removing unneeded columns or rows which won’t contribute effectively to analysis. Once data is in a clean state, it will often require some mutation to get data in the form that would be most useful to your analysis. In my example, it made sense to categorize some of my variables to make comparison easier to understand visually."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#what-i-would-improve-in-the-future",
    "href": "posts/MatthewONeill_FinalPaper.html#what-i-would-improve-in-the-future",
    "title": "Final Paper",
    "section": "What I Would Improve In the Future",
    "text": "What I Would Improve In the Future\nMy biggest regret for this project was not taking more time to vet my dataset at the beginning. I dove into analysis quickly after cleaning it, but I ended up being left without too many avenues of analysis to go down. In the future I would pose a research question as my first step after finding a dataset and do some simple visualizations to see if I’m on the right track. I also would like to work with a more challenging dataset in the future and integrate more types of visualizations in R."
  },
  {
    "objectID": "posts/MatthewONeill_FinalPaper.html#bibliography",
    "href": "posts/MatthewONeill_FinalPaper.html#bibliography",
    "title": "Final Paper",
    "section": "Bibliography",
    "text": "Bibliography\n[1] R Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\n[2] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\n[3] Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686.\n[4] Massachusetts. Department of Education: Statewide Public School Reports 2017, https://profiles.doe.mass.edu/state_report/\n[5] Wickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media"
  },
  {
    "objectID": "posts/MichaelaBowen_Final.html",
    "href": "posts/MichaelaBowen_Final.html",
    "title": "Final Project: Flower Sales over the Fall Semester",
    "section": "",
    "text": "IntroductionRead InFlower Transactions over Time by Strain\n\n\n\nData Summary\nThroughout the fall of 2022, the cannabis industry has been going through drastic market changes. With the rise of inflation, and an over saturated market, the cost of a pound of flower(bud) has decreased drastically. This data comes from a mid level adult recreational dispensary that is in a community of nearly 12 other dispensaries with similar product and clientele. There are three months worth of transaction data from the months of September, October, November and the beginning of December. Promotional items, sales, and deals have been a theme of many dispensaries in the Pioneer Valley and this dispensary has not been excluded from that.\nThe transaction data contains cumulatively about 36,000 transactions over the course of 3 months. Each transaction contains information for analyzing sales, and for the purposes of this document, the flower promotional sales. These transaction are inventory based, meaning that each product sold is one observation.\n\n\nResearch Questions\n\nWere fall flower promotions effective?:\n\nWas the most drastic promotion effective?\nAre the strains still popular outside of the promotional window?\n\n\n\n\n\n\nData Wrangling and Manipulation of Main Data\nEach of the 3 months of transactions were read in similarly with unused columns deleted, and the rest renamed with more functional naming conventions. This data was of separate inventory transactions, there needed to be a full join to include all observations (transactions). There are several muated columns that were necessary in the overall analysis of the data.\n\n\nCode\n#September transactions read in\ntransactions_sept_orig <- read_excel(\"_data/Inventory Transactions Sept_2022.xlsx\",\n    skip = 5,\n    col_names = c(\"pos_id\",\"product\",\"delete\",\"patient_name\",\"transaction_date\",\"qty_sold\",\"daily_allottment_oz\",\"weight_grams\",\"cost\",\"price\",\"owner_name\",\"owner_location\",\"vendor\",\"sold_by\",\"receipt_no\",\"delete\",\"delete\",\"delete\",\"delete\",\"delete\"))%>%\n  filter(sold_by != \"Michaela Bowen\")%>%\n  select(!contains(\"delete\"))\n\n\n\n#October transactions read in\ntransactions_oct_orig <- read_excel(\"_data/Inventory Transactions Oct_2022.xlsx\",\n    skip = 5,\n    col_names = c(\"pos_id\",\"product\",\"delete\",\"patient_name\",\"transaction_date\",\"qty_sold\",\"daily_allottment_oz\",\"weight_grams\",\"cost\",\"price\",\"owner_name\",\"owner_location\",\"vendor\",\"sold_by\",\"receipt_no\",\"delete\",\"delete\",\"delete\",\"delete\",\"delete\"))%>%\n  filter(sold_by != \"Michaela Bowen\")%>%\n  select(!contains(\"delete\"))\n\n\n#November transactions read in\ntransactions_nov_orig <- read_excel(\"_data/Inventory Transactions Nov_Current_2022.xlsx\",\n    skip = 5,\n    col_names = c(\"pos_id\",\"product\",\"delete\",\"patient_name\",\"transaction_date\",\"qty_sold\",\"daily_allottment_oz\",\"weight_grams\",\"cost\",\"price\",\"owner_name\",\"owner_location\",\"vendor\",\"sold_by\",\"receipt_no\",\"delete\",\"delete\",\"delete\",\"delete\",\"delete\"))%>%\n  filter(sold_by != \"Michaela Bowen\")%>%\n  select(!contains(\"delete\"))\n\n\n\n#joint all three dataframes of transactions, also uniting common columns\ntransactions_fall <- full_join(transactions_sept_orig,transactions_oct_orig, by = 'transaction_date')%>%\n  full_join(.,transactions_nov_orig)%>%\n  unite(\"product\", c(product, product.x, product.y), na.rm = TRUE)%>%\n  unite(\"pos_id\", c(pos_id, pos_id.x, pos_id.y), na.rm = TRUE)%>%\n  unite(\"patient_name\", c(patient_name, patient_name.x, patient_name.y), na.rm = TRUE)%>%\n  unite(\"qty_sold\", c(qty_sold, qty_sold.x, qty_sold.y), na.rm = TRUE)%>%\n  unite(\"daily_allottment_oz\", c(daily_allottment_oz, daily_allottment_oz.x, daily_allottment_oz.y), na.rm = TRUE)%>%\n  unite(\"weight_grams\", c(weight_grams, weight_grams.x, weight_grams.y), na.rm = TRUE)%>%\n  unite(\"cost\", c(cost, cost.x, cost.y), na.rm = TRUE)%>%\n  unite(\"price\", c(price, price.x, price.y), na.rm = TRUE)%>%\n  unite(\"owner_name\", c(owner_name, owner_name.x, owner_name.y), na.rm = TRUE)%>%\n  unite(\"owner_location\", c(owner_location, owner_location.x, owner_location.y), na.rm = TRUE)%>%\n  unite(\"vendor\", c(vendor, vendor.x, vendor.y), na.rm = TRUE)%>%\n  unite(\"sold_by\", c(sold_by, sold_by.x, sold_by.y), na.rm = TRUE)%>%\n  unite(\"receipt_no\", c(receipt_no, receipt_no.x, receipt_no.y), na.rm = TRUE)\n  \n\n  \n#mutating necessary columns\ntransactions_fall <- transactions_fall%>%\n#creating date, day, hour, minute, second, columns \n  mutate(\n    date = as.Date(transaction_date),\n    day = day(transaction_date),\n    hour = hour(transaction_date),\n    minute = minute(transaction_date),\n    second = second(transaction_date))%>%\n  mutate(\n    format_date = format(date, \"%m/%d/%Y\"),\n    format_hour = paste(hour, minute, second, sep = \":\")\n  )%>%\n#pulling the category abbreviation to determine category and create a category column \n  mutate(\n    category = substr(product,1,3),\n  )%>%\n#changing the abbreviations into full category names\n  mutate(\n    category_names = case_when(\n      category == \"FLO\" | category == \"Flo\" ~ \"Flower\",\n      category == \"PRJ\" & weight_grams == 0.5 ~ \"Joint 0.5g\",\n      category == \"PRJ\" & weight_grams == 1 ~ \"Joint 1g\",\n      category == \"PRJ\" & weight_grams == 2.5 ~ \"Joint 0.5g 5pk\",\n      category == \"PRJ\" ~ \"Other Joint\",\n      category == \"EDI\" ~ \"Edible\",\n      category == \"CON\" | category == \"Con\" | category == \"MIP\" ~ \"Concentrate\",\n      category == \"VAP\" | category == \"Vap\" ~ \"Vaporizer\",\n      category == \"ACC\" | category == \"Pax\" | category == \"PAX\" | category == \"Hig\" | category == \"Bov\" ~ \"Accessories\",\n      category == \"CLO\" | category == \"Res\" ~ \"Clothing\",\n      category == \"HTC\" ~ \"ignore\",\n      category == \"SAM\" ~ \"ignore\",\n      category == \"TOP\" ~ \"Topical\",\n      category == \"REW\" ~ \"ignore\")\n  )%>%\n#created a logical variable to determine if flower was in house, or 3rd party\n  mutate(\n    house_product = case_when(\n      vendor == \"Resinate, Inc.\" ~ TRUE,\n      vendor != \"Resinate, Inc.\" ~ FALSE\n    )\n)\n\n\n\n\nData Wrangling and Manipulation of Flower Data\nGiven that the price of the pound of flower is down, lowering the price of the 8th, as well as promoting different strains is necessary to compete within the market. In order to do that, Flower transactions need to be isolated. There are several mutated variables that are necessary in the flower analysis included, total product sold by date, products sold by date and strain, and percent of products sold by date and strain.\n\n\nCode\n#create flower transactions only dataframe \n\ntransactions_flower <- transactions_fall%>%\n  filter(category == \"FLO\")%>%\n\n  #create strain variable  \n  \n  mutate(\n    weight_grams = as.double(weight_grams),\n    strain = case_when(\n      grepl(\"Flower\", product) & (weight_grams == 1 | weight_grams == 14) ~ (str_extract(product,\"(?<=Flower ).+(?= 1)\" )),\n      grepl(\"Flower\", product) & (weight_grams == 3.5)~ (str_extract(product,\"(?<=Flower ).+(?= 3.)\" )),\n      grepl(\"Material\", product) & (weight_grams == 7)~ (str_extract(product,\"(?<=Material ).+(?= 7)\" )),\n      grepl(\"Shake\", product) & (weight_grams == 7) ~ (str_extract(product,\"(?<=Shake ).+(?= 7)\" )),\n      grepl(\"Shake\", product) & (weight_grams == 1 | weight_grams == 14) ~ (str_extract(product,\"(?<=Shake ).+(?= 1)\" )),\n      grepl(\"Popcorn\", product) ~ (str_extract(product,\"(?<=Popcorn ).+(?= 3.)\" )),\n      grepl(\"Collective\", product) ~ (str_extract(product,\"(?<=Collective ).+(?= 3.)\" ))\n      )\n    )%>%\n  \n  #create total  product sold by strain and date\n  \n    group_by(strain,date)%>%\n    mutate(sold_by_strain = sum(as.double(qty_sold)),\n  )%>%\n  \n  #create total product sold by date\n\n  group_by(date)%>%\n    mutate(sold_by_date = sum(as.double(qty_sold)),\n  )%>%\n  \n  mutate(percent_sold = 100*(sold_by_strain/sold_by_date),\n        )\n\n\n\n\n\n\nAll Resinate Strains\n\n\nCode\npercent_strain_day <- transactions_flower%>%\n  filter(house_product == TRUE)%>%\n  arrange(date, -percent_sold)%>%\n  distinct(strain, .keep_all = TRUE)%>%\n  ggplot(aes(x = date, y = percent_sold, group = strain, fill = strain)) +\n  geom_area() +\n  scale_fill_viridis(discrete = TRUE) +\n  labs(\n    title =  \"% Sold by Strain\",\n    caption = \"Data provided by Resinate, Inc.\"\n    )+\n  xlab(\" \") +\n  ylab(\"% of flower sales\") +\n  theme_bw() +\n  theme(\n      legend.position=\"none\",\n      panel.spacing = unit(0.2, \"lines\"),\n      panel.spacing.x = unit(.5, \"lines\"),\n      strip.text.x = element_text(size = 8),\n      plot.title = element_text(size=14)\n    ) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~strain)\n\npercent_strain_day\n\n\n\n\n\n\n\nBest Promoted Strains\nWhen referring to the Biweekly promotions it is clear that the promotions were effective as each strain has peaks related to the weeks of their promotions. The steepest promotions of the fall were for the month of November from 11/2 - 11/17, where four strains (Truthband, Lemon 18, Cappachino and Jigglers), were promoted for the price of $18.\n\n\nCode\nbest_promo <- transactions_flower%>%\n  filter(strain == c(\"Lemon 18\", \"Truthband\", \"Cappachino\", \"Jigglers\"))%>%\n  ggplot(aes(x = date, y = percent_sold, group = strain, color = strain)) +\n  geom_point(size = 1.5) +\n  geom_line() +\n  scale_x_date(\n    date_labels  = \"%d-%b\",\n    date_breaks = \"2 weeks\",\n    date_minor_breaks = \"days\"\n  ) +\n  scale_color_manual(values = c(\"#440154FF\", \"#31688EFF\", \"#35B779FF\", \"#FDE725FF\")) +\n  labs(\n        title =  \"Percentage of Strain Sales by Date\",\n        subtitle = \"$18 Eighths\",\n       caption = \"Data provided by Resinate, Inc.\")+\n  xlab(\" \") +\n  ylab(\"% of flower sales\") +\n  ylim(0,100) +\n  theme_bw() +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 60, hjust = 1)\n    ) +\n  facet_wrap(~strain)\n\nbest_promo\n\n\n\n\n\n\nWe can see that on 11/3 there is a sharp increase in Truthband sales, when the weeks before sales had been relatively low. We can see for the two week period in which Truthband was $18, that strain accounted for 40%+ of total flower sales for that day, and when the price increased to $25, sales dropped off.\n\nLemon 18 was also price dropped to $18 on the same date as the Truthband, it is interesting to note that there was not a drastic increase in sales for this strain\n\nDuring the second wave of $18 eighths, Cappachino and Jigglers were featured, and it is clear that each of those strains accounted for a larger percentage of the total flower sales for the day.\n\n\n\nCommon Strain\n\n\nCode\nsundae_sunset_transactions <- transactions_flower%>%\n  filter(strain == \"Sundae Sunset\")%>%\n  ggplot(aes(x = date, y = percent_sold, group = strain, color = strain)) +\n  geom_point(size = 1.5) +\n  geom_line() +\n  scale_x_date(\n    date_labels  = \"%d-%b\",\n    date_breaks = \"2 weeks\",\n    date_minor_breaks = \"days\"\n  ) +\n  scale_color_manual(values = \"#440154FF\") +\n  labs(\n        title =  \"Percentage of Sundae Sunset Sales by Date\",\n       caption = \"Data provided by Resinate, Inc.\")+\n  xlab(\" \") +\n  ylab(\"% of flower sales\") +\n  ylim(0,100) +\n  theme_bw() +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 60, hjust = 1)\n    ) +\n  facet_wrap(~strain)\n\nsundae_sunset_transactions\n\n\n\n\n\n\nSundae Sunset is a strain that is almost always in stock, it is a staple well liked strain. During the first month of fall from 9/8 - 10/5, it was on sale for $25 and then $20. During that timeframe the overall percentage of Sundae Sunset sales was significantly higher, and drops off drastically on 10/5, when the item fell off promotion for the rest of the season.\n\n\n\nPromotions\nWithin a two week promotional period, each category has one or two deals or promotional items. Typically, the flower category will always contain one strain priced between $18-$35, which tend to be the most popular for the time they are on sale.\n\n9/8 - 9/21\n\nLemon 18 & Sundae Sunset $25\n\n9/22 - 10/5\n\nPhone Home, Jigglers & Sundae Sunset $20\nCappachino $35\n\n10/6 - 10/19\n\nPhone Home, Lava Cake $20\nCappachino $35\n\n10/20 - 11/02\n\nPhone Home, Lava Cake $20\nCappachino $30\n\n11/03 - 11/17\n\nLemon 18, Truthband $18\nCappachino $25\n\n11/18 - 11/30\n\nJigglers, Cappachino $18\nTruthband $25\n\n12/1 - 12/14\n\nEl Chapo $20\nCappachino $25"
  },
  {
    "objectID": "posts/NaughtonFinal.html",
    "href": "posts/NaughtonFinal.html",
    "title": "Naughton Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(plotly)\nlibrary(hrbrthemes)\n\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/NaughtonFinal.html#final-project-turnstyle-midwest",
    "href": "posts/NaughtonFinal.html#final-project-turnstyle-midwest",
    "title": "Naughton Final Project",
    "section": "Final Project: Turnstyle Midwest",
    "text": "Final Project: Turnstyle Midwest\n\nIntroduction\nBackground and Context for this Dataset:\nI am a technology high school teacher in Boston, and I chose to enroll in the DACSS certificate program because I wanted to learn new skills in a growing field. This is my first time working with R, and although there were times I was very frustrated, I learned so much in this semester through problem solving, trying things out, and working through the frustration! In addition to teaching, I also teach spin in Boston, MA. The studio that I teach for, Turnstyle, also has a sister studio in the Midwest. I asked the owners if I could analyze their data for this final because I am familiar with the company, but I am not directly reflected in the data! So this work is for fun but is real data from the company. I’m am also interested in learning about trends within the fitness industry post 2020. This project has been exciting for me to try new things and practice the new skills I have learned this semester.\n\n\nDescription of the Data\nThis dataset is from Turnstyle Cycle Midwest, a spin studio with locations in Chicago, Illinois, and Madison, Wisconsin. The dataset is every reservation at Turnstyle Midwest from November 2020 to October 2022. Each case is an individual sign up for a single class, and the columns are Reservation ID, Status (if the client is checked-in, standard cancelled, penalty cancelled, class cancelled, or no show), Class ID, Class Date, Class Time, Class Day, Class Name, Class Public, Class Tags, Capacity, Location (Madison or Chicago), Instructor ID, Substitute (true or false), and Customer ID. I then chose to filter only the Madison location and if the class was open to the public and therefore was not cancelled. This gave me 57,204 reservations.\nMy research questions:\n\nWhich class time and day are most popular? Any trends with day of the week/time?\nDoes a substitute instructor affect reservation counts?\nThemes: which are more popular? Is hip hop more popular than EDM or pop or are general themes better?\n\n\n\nCode\n#read in the data, filtering out unnecessary columns\nTS <- read_csv(\"_data/reservations_Naughton.csv\",\n               skip=2,\n col_names = c(\"ReservationID\", \"del\", \"Status\", rep(\"del\", 12),\"ClassID\", \"ClassDate\", \"ClassTime\", \"ClassDay\", \"ClassName\", \"ClassPublic\", \"ClassTags\", rep(\"del\", 6), \"Capacity\", \"del\", \"Location\", \"del\", \"InstructorID\", \"del\", \"Substitute\", \"CustomerID\", rep(\"del\", 5)))%>%\n  \n    select(!starts_with(\"del\"))%>%\n  filter(Location == \"Madison\") %>%\n  filter(ClassPublic == TRUE) %>%\n  filter(Status != \"class cancelled\")\n\n\n\nWeekday ClassesSubstitute InstructorsThemes\n\n\n\nWeekdays from Novemebr 2020 to October 2022\nThe first part of the data I wanted to look at was the utilization of the space. I took the count of each checked-in client for a unique class and calculated the percent usage of the space given the capacity of the room. The capacity changes throughout the 2 years because of different Covid restrictions. The red line in the graphs below denotes when capacity was less than 42 bikes - when the studio opened, Covid restrictions limited capacity to 14 bikes. Throughout November and the winter, capacity was 14 and increased to 23. On July 19, 2021, capacity increased and remained at 42 bikes. The first graph shows the daily average utilization, so for every class on that day, how many bikes were booked out of the possible bikes, and the second graph shows the total number of checked-in reservations each day.\nOn both graphs, it is evident summer is especially low. The Madison location is located near University of Wisconsin, so there are a lot of student riders. Therefore, when students are home, like during summer and holiday breaks, sign-ups are less, which is also evident in both graphs. These graphs use plotly so you can highlight each individual data point or zoom in to a particular range of dates. **Note, these graphs no longer have plotly for the final project to appear on the blog.\nI was surprised to see that May 4, 2021, which is a date with capacity restrictions, had similar total bike counts to the maximum bike count for after restrictions were lifted. On May 4, there were 7 classes offered, which was the most classes Midwest has had in a day. Their next highest day with most reservations was December 14, 2021. I can assume that both of these days are at the end of the semester. Thus, students want to workout before they leave for the semester. Offering more classes at the end of the semester and before breaks would be a smart business move.\n\n\nCode\n# Weekday reservations\nWeekday <- filter(TS, ClassDay != \"Saturday\", ClassDay !=\"Sunday\") %>%\n  mutate(across(ends_with('ID'), as.character)) %>%\n  mutate(across(ends_with('Time'), as.character)) %>%\n  filter(Status== \"check in\") %>%\n  filter(ClassDate>= '2020-11-01')\n\n\n## each class's average capacity\nReservationsbyClassID<- Weekday %>% \n  group_by(ClassID, ClassDate, Capacity)  %>% \n  summarize(reservations = n_distinct(ReservationID)) %>%  \n  mutate(percent_Capacity = (reservations/Capacity)*100) %>%\n  arrange(ClassDate)\n\n## each day's average capacity\nWeekdayDailyCapacityAvg <- ReservationsbyClassID %>% \n  group_by(ClassDate)  %>% \n  summarize(average = mean(percent_Capacity))\n\n#Graph of weekday daily capacity average with plotly\np <- WeekdayDailyCapacityAvg %>%\n  ggplot( aes(x=ClassDate, y = average ), group(ClassDate)) +\n  geom_line(color= \"grey\") +\n     geom_point(shape=21, color=\"black\", fill=\"#69b3a2\", size=1) +\n   labs(x = \"Class Date\", \n       y = \"Average Utilization\", \n       title = \"Daily Average Utilization\"\n       ) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2021-07-19\")), linetype=4, colour=\"red\")\n\n#p <- ggplotly(p)\np\n\n\n\n\n\nCode\n## each day's total bike reservation\nWeekdayDailyReservationCount <- ReservationsbyClassID %>% \n  group_by(ClassDate)  %>% \n  summarize(sum = sum(reservations))\n\n#Graph of weekday total bike count with plotly\nb <- WeekdayDailyReservationCount %>%\n  ggplot( aes(x=ClassDate, y = sum ), group(ClassDate)) +\n  geom_line(color= \"grey\") +\n     geom_point(shape=21, color=\"black\", fill=\"#69b3a2\", size=1) +\n   labs(x = \"Class Date\", \n       y = \"Bike Count\", \n       title = \"Daily Reservation Count\"\n       ) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2021-07-19\")), linetype=4, colour=\"red\")\n\n#b <- ggplotly(b)\nb\n\n\n\n\n\n\n\nAverage Weekday Booking by Month\nNext, I wanted to zoom in on each month and see if there are any trends that emerge for weekdays. Using lubridate, I converted the date into days and month so that I could average the days. Within each month, I averaged the capacity of each day with the average utilization and found the total number of bikes booked each day.\nWith the graph of average utilization, everything is pretty even, with Friday having lowest utilization. By looking at number of bikes, we can parse out more information. Tuesdays and Wednesdays have the most bookings. Again, months where students are likely gone from campus (December, July and August) are especially low in both sign-ups and utilization.\n\n\nCode\n#graph of average utilization broken up by month and grouped by day\nWeekdayDailyCapacityAvg %>% \n  mutate(day = wday(ClassDate, label = TRUE, abbr = TRUE), month = month(ClassDate, label = TRUE, abbr = FALSE))%>%\n  group_by(day, month)  %>% \n  summarise(DailyAvgCap = mean(average)) %>%\n  arrange(month) %>% \nggplot(aes(fill=day, y=DailyAvgCap, x=day)) + \n    geom_bar( stat=\"identity\") +\n    facet_wrap(~month) +\n    theme_ipsum() +\n    labs(x = \"Class Day\", \n       y = \"Average Utilization\", \n       title = \"Average Utilization per Month by Day\"\n       ) +\n  theme(legend.position=\"none\") +\n  theme(panel.spacing = unit(.5, \"mm\")) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) \n\n\n\n\n\nCode\n#graph of total bikes broken up by month and grouped by day\nReservationsbyClassID %>% \n  mutate(day = wday(ClassDate, label = TRUE, abbr = TRUE), month = month(ClassDate, label = TRUE, abbr = FALSE))%>%\n  group_by(day, month)  %>% \n  summarise(DailyReservations = sum(reservations)) %>%\n  arrange(month) %>% \nggplot(aes(fill=day, y=DailyReservations, x=day)) + \n    geom_col() +\n    facet_wrap(~month) +\n    theme_ipsum() +\n    labs(x = \"Class Day\", \n       y = \"Total Reservations\", \n       title = \"Total Reservations per Month by Day\"\n       ) +\n  theme(legend.position=\"none\") +\n  theme(panel.spacing = unit(.5, \"mm\")) +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nWeekdays in October 2022\nNext, I wanted to look at a specific month and see if there are any trends for class times. I chose to look at October 2022 because it is the most recent month of data that I have, and there are limited holidays. Also the studio will be celebrating their 2 year anniversary at the end of the month so they have had almost two years to grow and expand their business and footprint in the Madison area.\n\n\nCode\n## weekday in Oct 2022, checked in reservations\nWeekdayOct <- filter(TS, ClassDay != \"Saturday\", ClassDay !=\"Sunday\") %>%\n  mutate(across(ends_with('ID'), as.character)) %>%\n  mutate(across(ends_with('Time'), as.character)) %>%\n  filter(Status== \"check in\") %>%\n  filter(str_starts(ClassDate, \"2022-10\")) \n\n## total number of reservations by ClassID for weekday classes in Oct 2022, checked in reservations\nWeekdayOctTotalReservation<- WeekdayOct %>% \n  group_by(ClassID, ClassName, ClassDate, ClassTime) %>% \n  summarize(reservations = n_distinct(ReservationID),\n            avgCap = (reservations/42)*100) %>% \n  arrange(desc(reservations))\n\n\nThese graphs are messy - each weekday has different time slots so the days are hard to compare signups. For example, Monday offers a 4:30pn, but Friday does not. Each day can also vary - on three out of the four Thursdays in October, there is a 3:30pm class. There is not a clear winner of which day has the most reservations, but Friday consistently has the least amount of sign-ups. In general, Tuesdays and Wednesdays seem to usually have the most reservations, but these are also the days when they typically offer 5 classes (6:30pm). Proportionally, Monday 4:30pm does the best with highest utilization.\n\n\nCode\n# bar graph for number of reservations in October separated by day and time\n#c <- \n  ggplot(WeekdayOct) +\n  geom_bar(mapping = aes(x = ClassDate, fill=ClassTime)) + \n  labs(x = \"Class Date\", \n       y = \"Number of reservations\", \n       title = \"Class Day vs Number of Reservations\",\n       fill = \"Class Time\",\n       )+\n  scale_fill_discrete(labels=c('7AM', '8:30AM', '9:30AM', '12:30PM','3:30PM', '4:00PM', '4:30PM', '5:30PM', '6:30PM')) \n\n\n\n\n\nCode\n#plotly::ggplotly(c)\n\n##proportion of class times offered on each day\n ggplot(data=WeekdayOctTotalReservation, aes(x=ClassDate, y = reservations, fill=ClassTime)) +\n    geom_bar(position=\"fill\", stat=\"identity\") +\n  labs(x = \"Class Date\", \n       y = \"Proportion of Total Classes\", \n       title = \"Proportion of Class Times\",\n       fill = \"Class Time\",\n       )+\n  scale_fill_discrete(labels=c('7AM', '8:30AM', '9:30AM', '12:30PM','3:30PM', '4:00PM', '4:30PM', '5:30PM', '6:30PM')) \n\n\n\n\n\n\n\nConclusion for Weekday Offerings\nMy take-aways for this section is that Fridays are the least attended day of the week, so Turnstyle should offer fewer classes on these days. Tuesday and Wednesday consistently do well so keeping the 6:30pm would be smart. Turnstyle might want to consider adding this for Mondays as well to increase reservations. Another take-away is to consult with the University of Wisconsin’s academic schedule. It is evident that right before break, reservations increase. At those times, Turnstyle should offer an abundance of classes to meet the demand. Over the summer and breaks, Turnstyle can lessen the amount of classes. They might also want to offer incentives or sales to ride during this time.\n\n\n\n\nSubstitute Instructors and the Effect on Sign-ups\nThe next question I have from this dataset is does a substitute teacher affect class sign-ups? Instructors will have a set schedule, and they might need to request a class off. I am curious if having a substitute impacts the overall signup. Sometimes the client knows well in advance if there will be a substitute. Other times, the instructor changes can happen last minute. It is likely, however, that the client will be notified of the switch prior to taking class.\n\n\nSubstitute Instructors from November 2021 to Current\nThis first graph is looking at checked-in reservations for classes between November 1, 2021, and November 1, 2022. This is a full year of data. I chose to not include 2020 because the studio had just opened in 2020, and there were still Covid restrictions. Because it just opened, the schedule was still being finalized and every instructor was seen as “new.” The longer you teach, the more of a following you can develop.\nOn the graphs below, the false column is classes that are taught by the original instructor, the true column is if the class was taught by a substitute. Each point on the graph is percent utilization of each class. As you can see, the utilization of classes taught by the original instructor is slightly higher than if it was taught by a substitute. This was really surprising to me - I assumed that the difference would be much larger. I assumed that there would be fewer sign-ups for a substitute.\n\n\nCode\n# reservations between Nov 2021 and Nov 2022, utilization of each class, graph of it taught by subsitute (TRUE) or by scheduled instructor (FALSE)\nTS %>% filter(Status== \"check in\") %>% \n  filter(ClassDate>= '2021-11-01')%>%\n  filter(ClassDate<= '2022-11-01') %>%\n  group_by(ClassID, ClassDate, ClassName, Capacity, Substitute, InstructorID) %>% \n  summarize(reservations = n_distinct(ReservationID))  %>% \n  arrange(desc(reservations)) %>% \n  mutate(percent_Utilization = (reservations/Capacity)*100)  %>% \nggplot( aes(x=Substitute, y=percent_Utilization, fill=Substitute)) +\n    geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Class Taught by Scheduled Instructor vs Substitute\") +\n    xlab(\"Substitute?\") +\n    ylab(\"Percent Untilization\")\n\n\n\n\n\n\n\nStandard Cancellations with Substitutes from November 2021 to Current\nI then wanted to look at reservations that were standard cancelled in the last year. This means that the rider cancelled a class greater than 12 hours before the class started. If there is a last-minute change, the client is emailed and notified about the new instructor. The amount of cancellations per class is pretty similar, which was surprising. I thought it would be more for classes taught by a substitute.\nI also wanted to compare number of late cancellations. The second graph below shows number of reservations per class that were not checked in and not standard cancelled. According to the graph, the number of late cancellations is fewer for classes taught by a substitute. I wonder if this is because a client will rearrange their schedule earlier if they know it is a substitute, therefore they will not need to make changes within the 12 hour penalty period. Also, I know the company often does not penalize clients for cancelling within the penalty window if the instructor is changed close to the start of class. Therefore, this data might not be entirely accurate.\n\n\nCode\n## number of standard cancellations grouped by Class ID\nTSnotInClass <- TS %>% \n  filter(Status== \"standard cancel\") %>%\n  filter(ClassDate>= '2021-11-01')%>%\n  select(ClassID, ClassDate, ReservationID, ClassName, Capacity, Substitute, InstructorID) %>%\n  group_by(ClassID, Substitute)  %>%\n  summarise(count = n_distinct(ReservationID)) %>% \n  ggplot( aes(x=Substitute, y=count, fill=Substitute)) +\n    geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Number of Early Cancellations in a Class\") +\n    xlab(\"Substitute?\")\nTSnotInClass\n\n\n\n\n\nCode\n## number of late cancellations grouped by Class ID\nTSnotInClassLateCancel <- TS %>% \n  filter(Status!= \"standard cancel\") %>%\n  filter(ClassDate>= '2021-11-01')%>%\n  filter(Status!= \"check in\") %>%\n  select(ClassID, ClassDate, ReservationID, ClassName, Capacity, Substitute, InstructorID) %>%\n  group_by(ClassID, Substitute)  %>%\n  summarise(count = n_distinct(ReservationID)) %>% \n  ggplot( aes(x=Substitute, y=count, fill=Substitute)) +\n    geom_boxplot() +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Number of Late Cancellations in a Class\") +\n    xlab(\"Substitute?\")\nTSnotInClassLateCancel\n\n\n\n\n\n\n\nPrimetime Weekday & Weekend Classes with Substitutes\nFinally, I picked specific class times to examine substitutes - 4:30pm, 5:30pm, and 6:30pm from January 1, 2022 to present and 8:00am, 9:00am, and 10:00am from January 21, 2022. I chose these times because they are the most popular class times and they are all consistently offered. I chose after January 21, 2022, because it was when students would be back on campus and when Turnstyle’s numbers started to rebound from break. As you can see from the weekday graphs, for the 4:30 class, the scheduled instructor has a much higher utilization than a substitute. The 5:30 also has a higher utilization for the scheduled instructor, but the difference is smaller. The upper quartile has more dispersion as well. For the 6:30, the opposite is true - the substitute has a higher utilization. The average for the 6:30pm class is the lowest out of the 3 - in order to increase sign-ups, Turnstyle should consider having a different regular instructor teach this class. They could also run a special for 6:30pm classes.\nFor the weekend morning classes, the 8am doesn’t have much data to analyze. However, the 8am does well with the scheduled instructor and has the highest utilization! Turnstyle might want to consider adding this class more consistently. The 9am class does best with the regularly scheduled instructor, however there is a large dispersion for the upper quartile for the substitute. Although the mean is less, there are quite a few data points that have a high utilization with the substitute. Finally, the 10am, which has been the most consistently offered weekend class, has a higher utilization with the substitute. This shows that weekends at 10am will be popular no matter who is teaching. They also offer limited classes on weekends so having higher numbers on these days compared to weekdays makes sense.\n\n\nCode\n# Weeknights reservations for 4:30pm, 5:30pm, and 6:30pm, graphs the average utilization for a substitute vs scheduled instructor\nTS %>% filter(Status== \"check in\") %>% \n  filter(ClassDate>= '2022-01-21')%>%\n  filter(ClassDay != \"Saturday\" | ClassDay != \"Sunday\") %>% \n  filter(ClassTime == hms(\"17:30:00\") | ClassTime == hms(\"16:30:00\") | ClassTime == hms(\"18:30:00\")) %>%\n  group_by(ClassID, ClassDate, ClassTime, ClassName, Capacity, Substitute, InstructorID) %>% \n  summarize(reservations = n_distinct(ReservationID))  %>% \n  arrange(ClassDate) %>% \n  mutate(percent_Utilization = (reservations/Capacity)*100) %>% \nggplot( aes(x=Substitute, y=percent_Utilization, fill=Substitute)) +\n    geom_boxplot() +\n  facet_wrap(~ClassTime, scale=\"free\") +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Class Taught by Scheduled Instructor vs Substitute for Weeknight Classes\") +\n    xlab(\"Substitute?\") +\n    ylab(\"Percent Untilization\")\n\n\n\n\n\nCode\n# Weekend reservations from 10:00am and earlier, graphs the average utilization for a substitute vs scheduled instructor\nWeekendReservations <- TS %>% filter(Status== \"check in\") %>% \n  filter(ClassDate>= '2022-01-22')%>%\n  filter(ClassDay== \"Saturday\" | ClassDay==\"Sunday\") %>%\n  filter(ClassTime<= hms(\"10:00:00\")) %>%\n  group_by(ClassID, ClassDate, ClassTime, ClassName, Capacity, Substitute, InstructorID) %>% \n  summarize(reservations = n_distinct(ReservationID))  %>% \n  arrange(ClassDate) %>% \n  mutate(percent_Utilization = (reservations/Capacity)*100) %>% \nggplot( aes(x=Substitute, y=percent_Utilization, fill=Substitute)) +\n    geom_boxplot() +\n  facet_wrap(~ClassTime, scale=\"free\") +\n    geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"Class Taught by Scheduled Instructor vs Substitute for Weekend Morning Classes\") +\n    xlab(\"Substitute?\") +\n    ylab(\"Percent Untilization\")\nWeekendReservations\n\n\n\n\n\n\n\nConclusions for Substitutes:\nThere is not a distinct trend for if a class is taught by an originally planned instructor vs a substitute. There are a couple of reasons why this might be. For one, the instructor might be changed last minute, and a rider didn’t have enough time to cancel. The data doesn’t tell me when a class instructor was changed. This also could be affected by who is the substitute - if it is an instructor with a large following or who is more tenured, they are likely to sway the data.\nWhen looking at the time of classes with substitutes, utilization of weekend morning classes are less dependent on if there is a substitute compared to weekday evening classes. This shows me that the business should try to limit substitutes during the week and be strategic about who is covering the classes. They also can be more relaxed on having instructors here on the weekend. They also might want to consider having a popular instructor teach during the 6:30 time slot to increase reservations during this time.\n\n\n\n\nThemes and the Effect on Signups\nFinally, I want to look at which themes are most popular. This studio chooses themes as their class name. Each instructor curates their own theme. There are 767 unique themes that have been taught over the last 2 years. Some themes are verses (VS) so there are two artists, others have 3 artists, and others are more generic.\nI separated the Class Name into 2 new columns - one called Primary Artist and second called other. I separated by , or VS. We are told to name themes with the most popular and most used artists first so I knew I could use that primary artist as my baseline. I then categorized the themes into POP, HIP HOP, PUNK ROCK, EDM, GENERAL, and OTHER. For pop, hip hop, punk rock, and EDM, I took the primary artist and used case_when looking for specific artists and organizing them appropriately. Then, GENERAL were themes that did not have an artist and were generic, such as BEST OF 2000s. OTHER was reserved for event type rides such as birthdays, charities, or anniversaries. I also took out 60 minute classes and classes taught by New Instructors.\nMy next step is to find the average capacity for the themed classes and then find the which ones are most popular. I added two more columns: 1 that categorized the season and the other that categorized the utilization range (0 - 20%, 21 - 40%, 41-60%, 61 - 80%, >=81%).\n\n\nCode\n##Theme names, filtered by Checked in, not 60 min OR taught by a new instuctor, created 2 new columns for if it is a VS or multiple artists\nTSThemesNames <- TS %>%  filter(Status== \"check in\") %>% \n  filter(!(str_starts(ClassName, \"60 MIN\"))) %>%\n  filter(!(str_starts(ClassName, \"NEW INSTRUCTOR\"))) %>%\n  group_by(ClassID, ClassName, ClassDate, ClassTime) %>% \n  summarize(reservations = n_distinct(ReservationID))  %>% \n  arrange(desc(reservations))  %>%\n  mutate(themeVS = case_when(str_detect(ClassName,\"VS\") ~ \"TRUE\",\n         TRUE ~ \"FALSE\"),\n         themeMultipleArtists = case_when(str_detect(ClassName,\", | . \") ~ \"TRUE\",\n         TRUE ~ \"FALSE\"))\n\n##separating Classname by VS or , into primary artist and other\nTSThemesNameFinal <- TSThemesNames %>% \n  separate(ClassName, into = c(\"PrimaryArtist\", \"other\"), sep = \" VS|,\",\n           remove = FALSE,\n           convert = TRUE,\n           extra = \"merge\",\n           fill = \"warn\") \nTSThemesNameFinal\n\n\n\n\n  \n\n\n\n\n\nCode\n## categorizing by primary artist into pop, hip hop, general, other, and EDM. Also adding another column to note time of year (summer 2021, fall 2021, winter 2022, spring 2022, summer 2022, fall 2022)\nTSThemesFinal <- TSThemesNameFinal %>% \n  mutate(percent_Capacity = (reservations/42)*100,\n    themeCategory = case_when(str_detect(PrimaryArtist, \"BEST OF MASHUPS\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"GIRL GROUPS\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"BOY BANDS\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"THROWBACK\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"KINGS OF POP\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"QUEENS OF POP\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"TAYLOR\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"BEYONC(É|E)\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"RIHANNA\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"JAY\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"TRAVIS SCOTT\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"CARDI\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"KANYE\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"MISSY\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"NICKI MINAJ\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"DRAKE\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"DUA LIPA\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"JUSTIN\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"BIEBER\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"BRITNEY\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"DISNEY\") ~ \"other\",\n                              str_detect(PrimaryArtist, \"BROADWAY\") ~ \"other\",\n                              str_starts(PrimaryArtist, \"LIL\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"TIKTOK\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"THE WEEKND\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"POST MALONE\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"ILLENIUM\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"ED SHEERAN\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"SIA\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"ONE DIRECTION\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"PRIDE\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"THE CHAINSMOKERS\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"HIP HOP MUSIC VS EDM\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"EDM\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"FIERCE FEMALE\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"HIP HOP REMIXED\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"ODESZA\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"AVA MAX\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"ARIANA\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"JUICE\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"KENDRICK\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"50\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"LIZZO\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"LOUIS THE CHILD\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"BADGER\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"BIG SEAN\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"BILLIE\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"DOJA\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"CHANCE\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"ELLIE\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"HALSEY\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"HARRY STYLES\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"JACK HARLOW\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"KHALID\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"LADY\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"MAC MILLER\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"MAINSTAGE\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"MEGAN THEE STALLION\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"MIGOS\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"MILEY\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"QUINN XCII\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"STEVE A\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"USHER\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"BEST OF POP\") ~ \"general\",\n                              str_starts(PrimaryArtist, \"BACKSTREET\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"AWARENESS\") ~ \"other\",\n                              str_starts(PrimaryArtist, \"CALVIN\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"DESTINY'S\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"DEMI\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"DJ\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"EMINEM\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"FERGIE\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"GALANTIS\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"GRYFFIN\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"GREEN DAY\") ~ \"punk rock\",\n                              str_starts(PrimaryArtist, \"KYGO\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"KESHA\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"KE[$]HA\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"OLIVIA R\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"PANIC!\") ~ \"punk rock\",\n                              str_starts(PrimaryArtist, \"SAM SMITH\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"SHAWN M\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"TIËSTO\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"WHITNEY\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"WIZ\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"G-EAZY\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"AVICII\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"BEST OF SUPER\") ~ \"other\",\n                              str_starts(PrimaryArtist, \"CHARLI XCX\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"DISCLOSURE\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"IMAGINE DRAGONS\") ~ \"punk rock\",\n                              str_detect(PrimaryArtist, \"P!NK\") ~ \"punk rock\",\n                              str_detect(PrimaryArtist, \"RAP\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"NELLY\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"HIP HOP\") ~ \"hip hop\",\n                              str_starts(PrimaryArtist, \"RITA ORA\") ~ \"EDM\",\n                              str_starts(PrimaryArtist, \"SELENA\") ~ \"pop\",\n                              str_detect(PrimaryArtist, \"ILLENIUM\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"FISHER\") ~ \"EDM\",\n                              str_detect(PrimaryArtist, \"HIP HOP REMIXED\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"BEST OF TURNSTYLE\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"THE WEEKND\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"HIP HOP MUSIC VS\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"HOLIDAY HITS\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"KINGS OF HIP HOP MUSIC\") ~ \"hip hop\",\n                              str_detect(PrimaryArtist, \"NEW YEARS PREGAME\") ~ \"general\",\n                              str_detect(PrimaryArtist, \"PRIVATE\") ~ \"other\",\n                              str_starts(PrimaryArtist, \"TROYE\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"KATY\") ~ \"pop\",\n                              str_starts(PrimaryArtist, \"TURNSTYLE 101\") ~ \"other\",\n                              str_detect(PrimaryArtist, \"BEST OF 2021\") ~ \"general\",\n                              TRUE ~ \"other\"\n                              \n      ),\n    season = case_when(\n           (ClassDate>=ymd('2021-06-01') & ClassDate<ymd('2021-09-01')) ~ \"Summer 2021\",\n           (ClassDate>=ymd('2021-09-01') & ClassDate<ymd('2021-12-01')) ~ \"Fall 2021\",\n           (ClassDate>=ymd('2021-12-01') & ClassDate<ymd('2022-03-01')) ~ \"Winter 2021\",\n           (ClassDate>=ymd('2022-03-01') & ClassDate<ymd('2022-06-01')) ~ \"Spring 2022\",\n           (ClassDate>=ymd('2022-06-01') & ClassDate<ymd('2022-09-01')) ~ \"Summer 2022\",\n           (ClassDate>=ymd('2022-09-01') & ClassDate<ymd('2022-12-01')) ~ \"Fall 2022\",\n         TRUE ~ \"other\"))  %>%\n  mutate(percentageGrouping =  case_when(\n      (percent_Capacity<21) ~ \"0 - 20%\",\n      (percent_Capacity<41) ~ \"21 - 40%\",\n      (percent_Capacity<61) ~ \"41 - 60%\",\n      (percent_Capacity<71) ~ \"61 - 80%\",\n      (percent_Capacity<101) ~ \"81 - 100%\"\n    ))%>%\n  select(ClassID, ClassName, PrimaryArtist, other, reservations, percent_Capacity, themeCategory, ClassDate, ClassTime, season, percentageGrouping) %>%\n  arrange(themeCategory)\n\n## frequency of the theme by category and the average capacity\nThemesFrequency <- TSThemesFinal %>% \n  group_by(ClassName, themeCategory) %>%\n  summarize(\n    frequencyTheme = n_distinct(ClassID),\n    averageCapacity = mean(percent_Capacity)) %>%\n  arrange(desc(averageCapacity),desc(frequencyTheme))\nThemesFrequency\n\n\n\n\n  \n\n\n\nCode\n## total number of classes for every theme category and average capacity of every theme \nThemesTotalCategory<- TSThemesFinal %>% \n    group_by(themeCategory) %>%\n    summarize(\n        avgCapPercentage = mean(percent_Capacity),\n        sum = n_distinct(ClassID)\n    ) \n  \n\n\n#unique(TS$ClassName)\n\n\n\n\nClasses Taught by Theme, November 2020 to Current\nI started by graphing how many classes were taught for each theme category and what the average utilization was for each category. I found that pop is the most often used theme that instructors choose for their classes, then hip hop and next general. For utilization, general and other have the highest average with about 43% utilization. Next is hip hop with 41%. Punk Rock and EDM have the lowest with about 25% and 30% respectively.\n\n\nCode\n##total number of classes taught under each theme from Nov 2020 to Current\nThemesTotalCategory %>% \nggplot(aes(x=themeCategory, y = sum)) + \n  geom_bar(stat=\"identity\") +\n    theme_bw() +\n    labs(title = \"Frequency of Themes\", \n         y = \"Total Classes\", \n         x = \"Theme Category\") +\n    geom_text(aes(label =  sum), vjust=-.5)\n\n\n\n\n\nCode\n# Average Utilization for Theme Category from Nov 2020 to current \nThemesTotalCategory %>% \nggplot(aes(x=themeCategory, y = avgCapPercentage)) + \n  geom_bar(stat=\"identity\") +\n    theme_bw() +\n    labs(title = \"Average Utilization for Theme Category\", \n         y = \"Average Utilization\", \n         x = \"Theme Category\") +\n    geom_text(aes(label = paste(format(round(avgCapPercentage)), \" %\")), vjust=-.5)\n\n\n\n\n\n\n\nUtilization of Theme Categories, November 2020 to Current\nNext, I wanted to break down themes even more. The first graph is a violin plot. This shows that every theme category, except for punk rock, was able to have a class with close to 100% utilization. However, EDM’s distribution is more evenly distributed between 20 and 50%. “Other themes have the most evenly distributed across all utilization.\nThe second graph looks at the total number of themes offered broken down by utilization percentage grouping by theme category. You can see that most classes are in the 21-40% category. Punk rock and EDM have the lowest utilization.\n\n\nCode\n##this graph shows the average capacity for each theme category from Nov 2020 to Oct 2022\nTSThemesFinal %>% \n    ggplot(aes(x=themeCategory, y=percent_Capacity, fill=themeCategory)) + \n    geom_violin() +\n    labs(title = \"Percent Utilization of Themes, Nov 2020 - Oct 2022\", \n         y = \"Percent Capacity\", \n         x = \"Theme Category\")\n\n\n\n\n\nCode\n## Themes the total number of themes offered broken down by utilization percentage groupings\n#e <- \nTSThemesFinal %>% \n    ggplot(aes(x=percentageGrouping, fill=themeCategory)) + \n    geom_histogram(stat=\"count\") +\n    labs(title = \"Number of Classes By Utilization, Nov 2020 - Oct 2022\", \n         y = \"Count of Classes\", \n         x = \"Average Utilization Group\") \n\n\n\n\n\nCode\n#plotly::ggplotly(e)\n\n\n\n\nUtilization for Theme Categories, After Capacity Restrictions are Lifted (July 19, 2021)\nFinally, I wanted to look at theme categories after July 19, 2021, which is when Covid restrictions are lifted and there are now 42 bikes to fill.\nI was then curious about if the seasons and themes affect reservations. The first graph shows what theme categories are offered from Summer 2021 to Fall 2022. As you can see, instructors proportionally offer the same variety across all seasons.\nThe second graph below shows the total number of classes by theme in each utilization group. As a business, you want utilization to be in the 81-100% group. This graph shows that EDM and punk rock are not very popular and are unlikely to sell the most classes. Pop and hip hop are relatively similar. However a lot of pop classes are in the 21-40% utilization group\nMy final graph is from 10am, 4:30pm, and 5:30pm classes after July 17, 2021. I chose these 3 class times because they are the most popular and I was curious to see if there are any different trends that emerge. Even at the most popular time slot, EDM and punk rock do not do well. Another takeaway is that general themes also well at this time slot. Other themes offered at 4:30 or 5:30 are also always about 21% utilization.\n\n\nCode\n## Themes from Summer 2021 to current, the total number of themes offered during the seasons\n#a<- \nTSThemesFinal %>% filter(ClassDate>= '2021-07-19') %>% \n  mutate(season = factor(season, levels=c(\"Summer 2021\", \"Fall 2021\", \"Winter 2021\", \"Spring 2022\", \"Summer 2022\", \"Fall 2022\"))) %>%\n    ggplot(aes(x=season, fill=themeCategory)) + \n    geom_histogram(stat=\"count\") +\n    labs(title = \"Number of Classes By Theme from July 17, 2021 to Current, Broken Down by Season\", \n         y = \"Count of Classes\", \n         x = \"Theme Category\") \n\n\n\n\n\nCode\n#plotly::ggplotly(a)\n\n## Theme categories from Summer 2021 to current, the total classes in each utilization group\n#x<- \nTSThemesFinal %>% filter(ClassDate>= '2021-07-19') %>% \n   ggplot(aes(x=percentageGrouping, fill=themeCategory)) + \n    geom_histogram(stat=\"count\") +\n    labs(title = \"Number of Classes in Utilization Group, July 19, 2021 to Current\", \n         y = \"Count of Classes\", \n         x = \"Average Utilization Group\") \n\n\n\n\n\nCode\n#plotly::ggplotly(x)\n\n\n#y<- \nTSThemesFinal %>% filter(ClassDate>= '2021-07-19') %>% \n  filter(ClassTime== hms(\"17:30:00\") | ClassTime == hms(\"16:30:00\")| ClassTime == hms(\"10:00:00\")) %>%\n   ggplot(aes(x=percentageGrouping, fill=themeCategory)) + \n    geom_histogram(stat=\"count\") +\n    labs(title = \"Number of 10am, 4:30pm & 5:30pm Classes By Utilization, July 19, 2021 to Current\", \n         y = \"Count of Classes\", \n         x = \"Average Utilization by Theme\") \n\n\n\n\n\nCode\n#plotly::ggplotly(y)\n\n\n\n\nConclustions for Themes:\nMy overall takeaway for themes is that punk rock and EDM are not the best themes to offer. Including a pop or a hip hop artists with EDM artists could be a better solution. Pop and hip hop are equally popular so it will be important to continue to offer music in both genres. General themes are also popular which tells me that clients might not just be signing up for the music."
  },
  {
    "objectID": "posts/NaughtonFinal.html#final-conclusion-and-takeaways",
    "href": "posts/NaughtonFinal.html#final-conclusion-and-takeaways",
    "title": "Naughton Final Project",
    "section": "Final Conclusion and Takeaways",
    "text": "Final Conclusion and Takeaways\nMy final takeaways from the three areas I looked at is that Turnstyle should consult with University of Wisconsin’s academic schedule and map their classes to fit their breaks. Turnstyle should emphasize instructors keeping a consistent schedule during the week but allowing instructors on the weekend to take off classes as needed. Additionally, Turnstyle should continue offering a wide offering of themes, focusing on pop and hip hop. This also showed me that there are many reasons why a client signs up and shows up for a class - this includes class time, price point, instructor, and theme. Isolating each part took away the whole story.\nI was very surprised with my findings. I thought themes would show me more information. I was surprised that EDM and punk rock were so low comparatively. I also thought “other” would have been higher for utilization because these are unique, celebration rides. I was also surprised by substitutes. I definitely thought having a sub would majorly affect sign-ups. I know I am not seeing the whole story, but this was still surprising. It was also really interesting to see how the calendar affects sign-ups. With this studio being in a college town, which is different from Boston, it is majorly affected when students are off campus. Although summer is slow for most fitness studios, this was especially evident in this dataset.\n\nNext steps\nWith more time and more skills developed using R, there is a lot more I would like to be able to do with this dataset. Some next steps include:\n\nThemes: categorize using a function instead of my way of hard-coding. I would love to look at 2 artist themes vs multiple themes and really compare that with the general ones. I’m also interested in seeing trends of which artists are most popular.\nSpecific Instructors: Why are some instructors more popular than others? How can the business support instructors? Analyze what goes well for the instructors (is it theme, class time, class day, number of classes taught in total, number of class offerings in one week) I also wanted to compare this to the substitute data.\nTime of sign-up: look at how early a client signs up - does the theme affect that? Class time? Instructor?\nSales: there is so much I can look at with sales! I didn’t even try looking at this yet, but I could always look at what packages people get, when do they purchase, do discounts affect number of sales, etc.\n\n\n\nReflection\nThis class was so challenging yet very rewarding. Any time I would work through something, experience a challenge, and then figure it out, it felt satisfying because I knew I spent so much time tinkering and exploring. I really enjoyed learning this content, and I feel like I have just scratched the surface. I am excited to learn more and practice my skills on new datasets and see where these skills take me.\nThank you for this opportunity to learn and experience with trying something new - it was humbling yet soul satisfying being a student again! I also want to give a shout out to Maddi Hertz for her patience and willingness to support me throughout this term.\n\n\nReferences\nGrolemund, G., & Wickham, H. (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.\nR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html",
    "href": "posts/NeeharikaKaranam_FinalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(summarytools)\n\nWarning: package 'summarytools' was built under R version 4.2.2\n\n\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\nlibrary(leaflet)\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.2.2\n\nlibrary(gapminder)\n\nWarning: package 'gapminder' was built under R version 4.2.2\n\nlibrary(ggplotify)\n\nError in library(ggplotify): there is no package called 'ggplotify'\n\nlibrary(ggridges)\nlibrary(hrbrthemes)\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#introduction",
    "href": "posts/NeeharikaKaranam_FinalProject.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nLiving and studying in Amherst, Massachusetts and the Boston Metro being the closest I always wondered how safe it was to explore the place. Boston Metro is in the 69th percentile in terms of safety which means that 31% of the metro areas are safer and the 69% of the metro areas are very dangerous. The crime rate in Boston is about 19.92 per every 1000 residents during a typical year and majority of the residents and locals believe that Southwest part of the Boston metro to be very safe. Therefore, the chance of you being the victim in the central neighborhoods is as high as 1 in 32 and in the southwest part of the Boston Metro it is as low as 1 in 92. As per the researchers the abandoned buildings, areas filled with graffiti, panhandling and all of the various signs which make it look suspicious in the neighborhoods generally tend to create an environment leading to more crimes. Therefore, I wanted to perform an in-depth analysis on the crime data of Boston Metro and visualize my observations.. All of the data used for this analysis applies to the actual Boston Metro boundaries only and the data ranges from 2017 to 2022(Till March).\nI want to understand and analyze on the following and more:\n\nIf there is any kind of a relationship between the crimes and the region that they actually take place\nWhat has changed in the criminal activities in the past few years?\nWhat is the crime rate on a daily basis in each of the districts in Boston?"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#dataset-description",
    "href": "posts/NeeharikaKaranam_FinalProject.html#dataset-description",
    "title": "Final Project",
    "section": "Dataset Description",
    "text": "Dataset Description\nI have collected the Boston Crime dataset from Kaggle from the year 2017 to 2022(till March). The dataset consists of 4,46,093 rows/records and 18 different columns/categories to help analyze the data. Let us now understand what each category of the dataset tells us."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#column-names-along-with-their-descriptions",
    "href": "posts/NeeharikaKaranam_FinalProject.html#column-names-along-with-their-descriptions",
    "title": "Final Project",
    "section": "Column Names along with their descriptions",
    "text": "Column Names along with their descriptions\n\nS.No - Gives the serial number of the crime record.\nIncident Number - Gives the internal BPD report number for each of the incidents and it cannot be NULL.\nOffense Code - Gives the numerical code value of the offense description.\nOffense Code Group - Gives the high level offense code group name.\nOffense Description - Gives the detailed description of the offense and the internal categorization of the offense.\nDistrict - Gives the district where the crime has taken place.\nReporting Area - Gives the number of the reporting area where the crime has taken place.\nShooting - Gives the numerical value of any kinds of shootings that have taken place.\nOccurred on Date - Gives the date and time of when the crime has taken place.\nYear - Gives the year when the crime has taken place.\nMonth - Gives the month when the crime has taken place.\nDay of Week - Gives the day of the week when the crime has taken place.\nHour - Gives the hour when the crime has taken place.\nUCR part - Gives the Universal Crime Reporting Part Number.\nStreet - Gives the Street name of where the crime has taken place.\nLat - Gives the latitude of where the crime has taken place.\nLong - Gives the longitude of where the crime has taken place.\nLocation - Gives the location of where the crime has taken place."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#read-data",
    "href": "posts/NeeharikaKaranam_FinalProject.html#read-data",
    "title": "Final Project",
    "section": "Read Data",
    "text": "Read Data\nNow, let us read our dataset into our dataframe.\n\n#Read the data\nboston_crime <- read.csv(\"_data/Boston_crime_2017_2022.csv\")\n\nhead(boston_crime)\n\n  X INCIDENT_NUMBER OFFENSE_CODE OFFENSE_CODE_GROUP\n1 0       225520077         3126                   \n2 1       222648862         3831                   \n3 2       222201764          724                   \n4 3       222201559          301                   \n5 4       222111641          619                   \n6 5       222107076         3126                   \n                         OFFENSE_DESCRIPTION DISTRICT REPORTING_AREA SHOOTING\n1 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT      D14            786        0\n2      M/V - LEAVING SCENE - PROPERTY DAMAGE       B2            288        0\n3                                 AUTO THEFT       C6            200        0\n4                                    ROBBERY       D4             NA        0\n5                         LARCENY ALL OTHERS      D14            778        0\n6 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT       D4             NA        0\n     OCCURRED_ON_DATE YEAR MONTH DAY_OF_WEEK HOUR UCR_PART\n1 2022-02-02 00:00:00 2022     2   Wednesday    0         \n2 2022-02-05 18:25:00 2022     2    Saturday   18         \n3 2022-01-09 00:00:00 2022     1      Sunday    0         \n4 2022-03-05 13:00:00 2022     3    Saturday   13         \n5 2022-02-14 12:30:00 2022     2      Monday   12         \n6 2022-03-11 10:45:00 2022     3      Friday   10         \n                                                STREET      Lat      Long\n1                                        WASHINGTON ST 42.34308 -71.14172\n2                                        WASHINGTON ST 42.32975 -71.08454\n3                                           W BROADWAY 42.34129 -71.05468\n4                                            ALBANY ST 42.33318 -71.07394\n5                                        WASHINGTON ST 42.34906 -71.15050\n6 MASSACHUSETTS AVE & ALBANY ST\\nBOSTON  MA 02118\\nUNI 42.33350 -71.07351\n                                  Location\n1  (42.34308127134165, -71.14172267328729)\n2 (42.329748204791635, -71.08454011649543)\n3 (42.341287504390436, -71.05467932649397)\n4 (42.333184490911954, -71.07393881002383)\n5  (42.34905600030506, -71.15049849975023)\n6  (42.33349998017161, -71.07350999617319)\n\n\nThe dimensions of the dataset.\n\ndim(boston_crime)\n\n[1] 446093     18\n\n\nThe various column names of the dataset.\n\ncolnames(boston_crime)\n\n [1] \"X\"                   \"INCIDENT_NUMBER\"     \"OFFENSE_CODE\"       \n [4] \"OFFENSE_CODE_GROUP\"  \"OFFENSE_DESCRIPTION\" \"DISTRICT\"           \n [7] \"REPORTING_AREA\"      \"SHOOTING\"            \"OCCURRED_ON_DATE\"   \n[10] \"YEAR\"                \"MONTH\"               \"DAY_OF_WEEK\"        \n[13] \"HOUR\"                \"UCR_PART\"            \"STREET\"             \n[16] \"Lat\"                 \"Long\"                \"Location\"           \n\n\nThe summary of the dataset.\n\nsummary(boston_crime)\n\n       X          INCIDENT_NUMBER     OFFENSE_CODE   OFFENSE_CODE_GROUP\n Min.   :     0   Length:446093      Min.   :  100   Length:446093     \n 1st Qu.: 19091   Class :character   1st Qu.: 1102   Class :character  \n Median : 41395   Mode  :character   Median : 3006   Mode  :character  \n Mean   : 42678                      Mean   : 2358                     \n 3rd Qu.: 63700                      3rd Qu.: 3201                     \n Max.   :101337                      Max.   :99999                     \n                                                                       \n OFFENSE_DESCRIPTION   DISTRICT         REPORTING_AREA    SHOOTING        \n Length:446093       Length:446093      Min.   :  0.0   Length:446093     \n Class :character    Class :character   1st Qu.:177.0   Class :character  \n Mode  :character    Mode  :character   Median :348.0   Mode  :character  \n                                        Mean   :382.8                     \n                                        3rd Qu.:540.0                     \n                                        Max.   :962.0                     \n                                        NA's   :58372                     \n OCCURRED_ON_DATE        YEAR          MONTH        DAY_OF_WEEK       \n Length:446093      Min.   :2017   Min.   : 1.000   Length:446093     \n Class :character   1st Qu.:2018   1st Qu.: 3.000   Class :character  \n Mode  :character   Median :2019   Median : 6.000   Mode  :character  \n                    Mean   :2019   Mean   : 6.413                     \n                    3rd Qu.:2020   3rd Qu.: 9.000                     \n                    Max.   :2022   Max.   :12.000                     \n                                                                      \n      HOUR         UCR_PART            STREET               Lat       \n Min.   : 0.00   Length:446093      Length:446093      Min.   :-1.00  \n 1st Qu.: 9.00   Class :character   Class :character   1st Qu.:42.28  \n Median :14.00   Mode  :character   Mode  :character   Median :42.31  \n Mean   :12.98                                         Mean   :35.27  \n 3rd Qu.:18.00                                         3rd Qu.:42.34  \n Max.   :23.00                                         Max.   :42.40  \n                                                       NA's   :13458  \n      Long          Location        \n Min.   :-71.18   Length:446093     \n 1st Qu.:-71.09   Class :character  \n Median :-71.07   Mode  :character  \n Mean   :-59.24                     \n 3rd Qu.:-71.05                     \n Max.   :  0.00                     \n NA's   :13458"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#tidy-data",
    "href": "posts/NeeharikaKaranam_FinalProject.html#tidy-data",
    "title": "Final Project",
    "section": "Tidy Data",
    "text": "Tidy Data\nLet us now check if there are any NA values in the dataset.\n\nna_values <- colSums(is.na(boston_crime))\nna_values\n\n                  X     INCIDENT_NUMBER        OFFENSE_CODE  OFFENSE_CODE_GROUP \n                  0                   0                   0                   0 \nOFFENSE_DESCRIPTION            DISTRICT      REPORTING_AREA            SHOOTING \n                  0                   0               58372                   0 \n   OCCURRED_ON_DATE                YEAR               MONTH         DAY_OF_WEEK \n                  0                   0                   0                   0 \n               HOUR            UCR_PART              STREET                 Lat \n                  0                   0                   0               13458 \n               Long            Location \n              13458                   0 \n\n\nNow we use str() which is used for compactly displaying the internal structure of a R object.\n\nstr(boston_crime)\n\n'data.frame':   446093 obs. of  18 variables:\n $ X                  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ INCIDENT_NUMBER    : chr  \"225520077\" \"222648862\" \"222201764\" \"222201559\" ...\n $ OFFENSE_CODE       : int  3126 3831 724 301 619 3126 801 611 619 3201 ...\n $ OFFENSE_CODE_GROUP : chr  \"\" \"\" \"\" \"\" ...\n $ OFFENSE_DESCRIPTION: chr  \"WARRANT ARREST - OUTSIDE OF BOSTON WARRANT\" \"M/V - LEAVING SCENE - PROPERTY DAMAGE\" \"AUTO THEFT\" \"ROBBERY\" ...\n $ DISTRICT           : chr  \"D14\" \"B2\" \"C6\" \"D4\" ...\n $ REPORTING_AREA     : int  786 288 200 NA 778 NA 235 77 186 574 ...\n $ SHOOTING           : chr  \"0\" \"0\" \"0\" \"0\" ...\n $ OCCURRED_ON_DATE   : chr  \"2022-02-02 00:00:00\" \"2022-02-05 18:25:00\" \"2022-01-09 00:00:00\" \"2022-03-05 13:00:00\" ...\n $ YEAR               : int  2022 2022 2022 2022 2022 2022 2022 2022 2022 2022 ...\n $ MONTH              : int  2 2 1 3 2 3 2 2 1 1 ...\n $ DAY_OF_WEEK        : chr  \"Wednesday\" \"Saturday\" \"Sunday\" \"Saturday\" ...\n $ HOUR               : int  0 18 0 13 12 10 22 10 15 13 ...\n $ UCR_PART           : chr  \"\" \"\" \"\" \"\" ...\n $ STREET             : chr  \"WASHINGTON ST\" \"WASHINGTON ST\" \"W BROADWAY\" \"ALBANY ST\" ...\n $ Lat                : num  42.3 42.3 42.3 42.3 42.3 ...\n $ Long               : num  -71.1 -71.1 -71.1 -71.1 -71.2 ...\n $ Location           : chr  \"(42.34308127134165, -71.14172267328729)\" \"(42.329748204791635, -71.08454011649543)\" \"(42.341287504390436, -71.05467932649397)\" \"(42.333184490911954, -71.07393881002383)\" ...\n\n\nWe can now get all of the classes/data type of all of the columns in the dataset.\n\nsapply(boston_crime, class)\n\n                  X     INCIDENT_NUMBER        OFFENSE_CODE  OFFENSE_CODE_GROUP \n          \"integer\"         \"character\"           \"integer\"         \"character\" \nOFFENSE_DESCRIPTION            DISTRICT      REPORTING_AREA            SHOOTING \n        \"character\"         \"character\"           \"integer\"         \"character\" \n   OCCURRED_ON_DATE                YEAR               MONTH         DAY_OF_WEEK \n        \"character\"           \"integer\"           \"integer\"         \"character\" \n               HOUR            UCR_PART              STREET                 Lat \n          \"integer\"         \"character\"         \"character\"           \"numeric\" \n               Long            Location \n          \"numeric\"         \"character\" \n\n\nWe now observe that the MONTH column is an integer and the values are in integer values of the different months. Let us first convert the MONTH column from integer to character.\n\nboston_crime <- transform(boston_crime, MONTH = as.character(MONTH))\n\n\nsapply(boston_crime, class)\n\n                  X     INCIDENT_NUMBER        OFFENSE_CODE  OFFENSE_CODE_GROUP \n          \"integer\"         \"character\"           \"integer\"         \"character\" \nOFFENSE_DESCRIPTION            DISTRICT      REPORTING_AREA            SHOOTING \n        \"character\"         \"character\"           \"integer\"         \"character\" \n   OCCURRED_ON_DATE                YEAR               MONTH         DAY_OF_WEEK \n        \"character\"           \"integer\"         \"character\"         \"character\" \n               HOUR            UCR_PART              STREET                 Lat \n          \"integer\"         \"character\"         \"character\"           \"numeric\" \n               Long            Location \n          \"numeric\"         \"character\" \n\n\nWe can now observe that the class/data type of the column MONTH has been changed to character. Now let us replace all of the numeric values in the MONTHS to their corresponding character months.\n\nboston_crime <- boston_crime %>% \n  mutate(MONTH = str_replace(MONTH, \"3\", \"March\"), MONTH = str_replace(MONTH, \"4\", \"April\"), MONTH = str_replace(MONTH, \"5\", \"May\"), MONTH = str_replace(MONTH, \"6\", \"June\"), MONTH = str_replace(MONTH, \"7\", \"July\"), MONTH = str_replace(MONTH, \"8\", \"August\"), MONTH = str_replace(MONTH, \"9\", \"September\"), MONTH = str_replace(MONTH, \"10\", \"October\"), MONTH = str_replace(MONTH, \"11\", \"November\"), MONTH = str_replace(MONTH, \"12\", \"December\"), MONTH = str_replace(MONTH, \"1\", \"Janurary\"), MONTH = str_replace(MONTH, \"2\", \"February\"))\nhead(boston_crime)\n\n  X INCIDENT_NUMBER OFFENSE_CODE OFFENSE_CODE_GROUP\n1 0       225520077         3126                   \n2 1       222648862         3831                   \n3 2       222201764          724                   \n4 3       222201559          301                   \n5 4       222111641          619                   \n6 5       222107076         3126                   \n                         OFFENSE_DESCRIPTION DISTRICT REPORTING_AREA SHOOTING\n1 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT      D14            786        0\n2      M/V - LEAVING SCENE - PROPERTY DAMAGE       B2            288        0\n3                                 AUTO THEFT       C6            200        0\n4                                    ROBBERY       D4             NA        0\n5                         LARCENY ALL OTHERS      D14            778        0\n6 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT       D4             NA        0\n     OCCURRED_ON_DATE YEAR    MONTH DAY_OF_WEEK HOUR UCR_PART\n1 2022-02-02 00:00:00 2022 February   Wednesday    0         \n2 2022-02-05 18:25:00 2022 February    Saturday   18         \n3 2022-01-09 00:00:00 2022 Janurary      Sunday    0         \n4 2022-03-05 13:00:00 2022    March    Saturday   13         \n5 2022-02-14 12:30:00 2022 February      Monday   12         \n6 2022-03-11 10:45:00 2022    March      Friday   10         \n                                                STREET      Lat      Long\n1                                        WASHINGTON ST 42.34308 -71.14172\n2                                        WASHINGTON ST 42.32975 -71.08454\n3                                           W BROADWAY 42.34129 -71.05468\n4                                            ALBANY ST 42.33318 -71.07394\n5                                        WASHINGTON ST 42.34906 -71.15050\n6 MASSACHUSETTS AVE & ALBANY ST\\nBOSTON  MA 02118\\nUNI 42.33350 -71.07351\n                                  Location\n1  (42.34308127134165, -71.14172267328729)\n2 (42.329748204791635, -71.08454011649543)\n3 (42.341287504390436, -71.05467932649397)\n4 (42.333184490911954, -71.07393881002383)\n5  (42.34905600030506, -71.15049849975023)\n6  (42.33349998017161, -71.07350999617319)\n\n\n\nhead(boston_crime)\n\n  X INCIDENT_NUMBER OFFENSE_CODE OFFENSE_CODE_GROUP\n1 0       225520077         3126                   \n2 1       222648862         3831                   \n3 2       222201764          724                   \n4 3       222201559          301                   \n5 4       222111641          619                   \n6 5       222107076         3126                   \n                         OFFENSE_DESCRIPTION DISTRICT REPORTING_AREA SHOOTING\n1 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT      D14            786        0\n2      M/V - LEAVING SCENE - PROPERTY DAMAGE       B2            288        0\n3                                 AUTO THEFT       C6            200        0\n4                                    ROBBERY       D4             NA        0\n5                         LARCENY ALL OTHERS      D14            778        0\n6 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT       D4             NA        0\n     OCCURRED_ON_DATE YEAR    MONTH DAY_OF_WEEK HOUR UCR_PART\n1 2022-02-02 00:00:00 2022 February   Wednesday    0         \n2 2022-02-05 18:25:00 2022 February    Saturday   18         \n3 2022-01-09 00:00:00 2022 Janurary      Sunday    0         \n4 2022-03-05 13:00:00 2022    March    Saturday   13         \n5 2022-02-14 12:30:00 2022 February      Monday   12         \n6 2022-03-11 10:45:00 2022    March      Friday   10         \n                                                STREET      Lat      Long\n1                                        WASHINGTON ST 42.34308 -71.14172\n2                                        WASHINGTON ST 42.32975 -71.08454\n3                                           W BROADWAY 42.34129 -71.05468\n4                                            ALBANY ST 42.33318 -71.07394\n5                                        WASHINGTON ST 42.34906 -71.15050\n6 MASSACHUSETTS AVE & ALBANY ST\\nBOSTON  MA 02118\\nUNI 42.33350 -71.07351\n                                  Location\n1  (42.34308127134165, -71.14172267328729)\n2 (42.329748204791635, -71.08454011649543)\n3 (42.341287504390436, -71.05467932649397)\n4 (42.333184490911954, -71.07393881002383)\n5  (42.34905600030506, -71.15049849975023)\n6  (42.33349998017161, -71.07350999617319)\n\n\nWe can observe that the values of the column MONTH have been changed successfuly. Let us now perform our analysis."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#research-questions",
    "href": "posts/NeeharikaKaranam_FinalProject.html#research-questions",
    "title": "Final Project",
    "section": "Research Questions",
    "text": "Research Questions\nOnce I am done with cleaning and observing the data, now I want to perform my analysis/visualization to answer my research questions. My major focus is to understand the relation between the crime and the region, time of the day and the crime and various other interesting observations.\nLet us now look at each one of them in detail along with my observation."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#what-are-the-different-crime-categories-in-boston-and-what-are-the-most-common-crimes-among-them-from-2017-2022",
    "href": "posts/NeeharikaKaranam_FinalProject.html#what-are-the-different-crime-categories-in-boston-and-what-are-the-most-common-crimes-among-them-from-2017-2022",
    "title": "Final Project",
    "section": "1. What are the different crime categories in Boston and what are the most common crimes among them from 2017-2022?",
    "text": "1. What are the different crime categories in Boston and what are the most common crimes among them from 2017-2022?\nFirst, let us find out what are all the various crime categories and how many records we have for each one of these crime categories in their decreasing order..\n\ncommon_crimes <- as.data.frame(table(boston_crime$OFFENSE_DESCRIPTION))\ncolnames(common_crimes) <- c(\"Offense_code_group\", \"Total\")\ncommon_crimes <- common_crimes[order(common_crimes$Total, decreasing = T),]\ncommon_crimes\n\n                                                                  Offense_code_group\n106                                                               INVESTIGATE PERSON\n131                                            M/V - LEAVING SCENE - PROPERTY DAMAGE\n223                                                    SICK/INJURED/MEDICAL - PERSON\n239                                                                        VANDALISM\n107                                                             INVESTIGATE PROPERTY\n230                                                              TOWED MOTOR VEHICLE\n240                                                                   VERBAL DISPUTE\n16                                                          ASSAULT SIMPLE - BATTERY\n123                                            LARCENY THEFT FROM MV - NON-ACCESSORY\n120                                                              LARCENY SHOPLIFTING\n229                                                        THREATS TO DO BODILY HARM\n221                                                                      SICK ASSIST\n121                                                      LARCENY THEFT FROM BUILDING\n115                                                               LARCENY ALL OTHERS\n194                                                                  PROPERTY - LOST\n13                                                                  ASSAULT - SIMPLE\n90                                                   FRAUD - FALSE PRETENSE / SCHEME\n11                                                              ASSAULT - AGGRAVATED\n139                                                   M/V ACCIDENT - PERSONAL INJURY\n174                                                         MISSING PERSON - LOCATED\n249                                                                   WARRANT ARREST\n137                                                             M/V ACCIDENT - OTHER\n141                                                  M/V ACCIDENT - PROPERTY  DAMAGE\n193                                                                 PROPERTY - FOUND\n196                                                         PROPERTY - LOST/ MISSING\n17                                                                        AUTO THEFT\n142                                                   M/V ACCIDENT - PROPERTY DAMAGE\n12                                                    ASSAULT - AGGRAVATED - BATTERY\n98                                                                        HARASSMENT\n69                                      DRUGS - POSSESSION/ SALE/ MANUFACTURING/ USE\n124                                                         LARCENY THEFT OF BICYCLE\n89                                                   FRAUD - CREDIT CARD / ATM FRAUD\n231                                                                      TRESPASSING\n173                                                                   MISSING PERSON\n233                                                  VAL - OPERATING AFTER REV/SUSP.\n99                                                   HARASSMENT/ CRIMINAL HARASSMENT\n91                                                             FRAUD - IMPERSONATION\n237                                                      VAL - VIOLATION OF AUTO LAW\n125                                          LARCENY THEFT OF MV PARTS & ACCESSORIES\n224                                                    SICK/INJURED/MEDICAL - POLICE\n227                                                                     SUDDEN DEATH\n126                                                        LICENSE PREMISE VIOLATION\n219                                                SERVICE TO OTHER PD INSIDE OF MA.\n32                                                            BURGLARY - RESIDENTIAL\n214                                                                 ROBBERY - STREET\n58                                              DRUGS - POSS CLASS B - COCAINE, ETC.\n46                                                               DEATH INVESTIGATION\n59                                    DRUGS - POSS CLASS B - INTENT TO MFR DIST DISP\n208                                                                          ROBBERY\n222                                               SICK ASSIST - DRUG RELATED ILLNESS\n238                                              VAL - VIOLATION OF AUTO LAW - OTHER\n130                                            M/V - LEAVING SCENE - PERSONAL INJURY\n34                                                    BURGLARY - RESIDENTIAL - FORCE\n88                                                          FORGERY / COUNTERFEITING\n35                                                 BURGLARY - RESIDENTIAL - NO FORCE\n20                                                         BALLISTICS EVIDENCE/FOUND\n205                       RECOVERED - MV RECOVERED IN BOSTON (STOLEN OUTSIDE BOSTON)\n191                                                     PROPERTY - ACCIDENTAL DAMAGE\n236                                                  VAL - OPERATING WITHOUT LICENSE\n70                                                      DRUGS - SALE / MANUFACTURING\n143                                       M/V ACCIDENT INVOLVING PEDESTRIAN - INJURY\n19                                                 AUTO THEFT - MOTORCYCLE / SCOOTER\n140                                                    M/V ACCIDENT - POLICE VEHICLE\n56                                              DRUGS - POSS CLASS A - HEROIN, ETC. \n138                                                M/V ACCIDENT - OTHER CITY VEHICLE\n92                                                                   FRAUD - WELFARE\n217                                                                   SEARCH WARRANT\n81                                                                       FIRE REPORT\n57                                    DRUGS - POSS CLASS A - INTENT TO MFR DIST DISP\n135                                     M/V ACCIDENT - INVOLVING PEDESTRIAN - INJURY\n226                                STOLEN PROPERTY - BUYING / RECEIVING / POSSESSING\n25                                                             BURGLARY - COMMERICAL\n83                                               FIRE REPORT - HOUSE, BUILDING, ETC.\n48                                                                DISORDERLY CONDUCT\n175                                          MISSING PERSON - NOT REPORTED - LOCATED\n127                                                      LIQUOR - DRINKING IN PUBLIC\n71                                                      DRUGS - SICK ASSIST - HEROIN\n86                                             FIREARM/WEAPON - FOUND OR CONFISCATED\n251                                       WARRANT ARREST - OUTSIDE OF BOSTON WARRANT\n197                                                               PROPERTY - MISSING\n114                                                        LANDLORD - TENANT SERVICE\n144                                                                M/V PLATES - LOST\n242                                           VIOL. OF RESTRAINING ORDER W NO ARREST\n113                                                                LANDLORD - TENANT\n234                                                 VAL - OPERATING UNREG/UNINS  CAR\n252                                    WEAPON - FIREARM - CARRYING / POSSESSING, ETC\n257                   WEAPON VIOLATION - CARRY/ POSSESSING/ SALE/ TRAFFICKING/ OTHER\n64                                    DRUGS - POSS CLASS D - INTENT TO MFR DIST DISP\n93                                                                      FRAUD - WIRE\n55                                                                     DRUGS - OTHER\n132                                       M/V ACCIDENT - INVOLVING  BICYCLE - INJURY\n218                                                          SERVICE TO OTHER AGENCY\n27                                                     BURGLARY - COMMERICAL - FORCE\n136                                  M/V ACCIDENT - INVOLVING PEDESTRIAN - NO INJURY\n18                                                AUTO THEFT - LEASED/RENTED VEHICLE\n179                                                      NOISY PARTY/RADIO-NO ARREST\n235                                         VAL - OPERATING W/O AUTHORIZATION LAWFUL\n49                                                              DISTURBING THE PEACE\n186                                                                    OTHER OFFENSE\n134                                     M/V ACCIDENT - INVOLVING BICYCLE - NO INJURY\n33                                                  BURGLARY - RESIDENTIAL - ATTEMPT\n97                                                                          GRAFFITI\n211                                                             ROBBERY - COMMERCIAL\n255                                      WEAPON - OTHER - CARRYING / POSSESSING, ETC\n213                                                                  ROBBERY - OTHER\n243                                                       VIOLATION - CITY ORDINANCE\n117                                                              LARCENY PICK-POCKET\n76                                                                      EVADING FARE\n50  DISTURBING THE PEACE/ DISORDERLY CONDUCT/ GATHERING CAUSING ANNOYANCE/ NOISY PAR\n198                                                 PROPERTY - STOLEN THEN RECOVERED\n133                                        M/V ACCIDENT - INVOLVING BICYCLE - INJURY\n82                                                    FIRE REPORT - CAR, BRUSH, ETC.\n45                                                  DANGEROUS OR HAZARDOUS CONDITION\n80                                                            EXTORTION OR BLACKMAIL\n228                                                        SUICIDE / SUICIDE ATTEMPT\n75                                                                      EMBEZZLEMENT\n100                                                      HARBOR INCIDENT / VIOLATION\n184                                            OPERATING UNDER THE INFLUENCE ALCOHOL\n8                                        ANIMAL INCIDENTS (DOG BITES, LOST DOG, ETC)\n60                                                              DRUGS - POSS CLASS C\n65                                                              DRUGS - POSS CLASS E\n195                                                     PROPERTY - LOST THEN LOCATED\n62                                                              DRUGS - POSS CLASS D\n6                                                  ANIMAL CONTROL - DOG BITES - ETC.\n129                                              LIQUOR/ALCOHOL - DRINKING IN PUBLIC\n94                                                             FUGITIVE FROM JUSTICE\n105                                                             INTIMIDATING WITNESS\n250                           WARRANT ARREST - BOSTON WARRANT (MUST BE SUPPLEMENTAL)\n177                                              MURDER, NON-NEGLIGIENT MANSLAUGHTER\n7                                                                   ANIMAL INCIDENTS\n182                                      OPERATING UNDER THE INFLUENCE (OUI) ALCOHOL\n3                                                                             AFFRAY\n28                                                  BURGLARY - COMMERICAL - NO FORCE\n84                                                         FIRE REPORT/ALARM - FALSE\n52                                         DRUGS - CLASS B TRAFFICKING OVER 18 GRAMS\n248                                        VIOLATION - RESTRAINING ORDER (NO ARREST)\n61                                    DRUGS - POSS CLASS C - INTENT TO MFR DIST DISP\n73                                              DRUGS - SICK ASSIST - OTHER NARCOTIC\n241                                              VIOL. OF RESTRAINING ORDER W ARREST\n47                                                               DEMONSTRATIONS/RIOT\n108                                                 INVESTIGATION FOR ANOTHER AGENCY\n220                                               SERVICE TO OTHER PD OUTSIDE OF MA.\n30                                                          BURGLARY - OTHER - FORCE\n128                                                             LIQUOR LAW VIOLATION\n51                                         DRUGS - CLASS A TRAFFICKING OVER 18 GRAMS\n187                                                  POSSESSION OF BURGLARIOUS TOOLS\n202                                                        PROSTITUTION - SOLICITING\n37                                                                CHILD ENDANGERMENT\n72                                          DRUGS - SICK ASSIST - OTHER HARMFUL DRUG\n253                                               WEAPON - FIREARM - OTHER VIOLATION\n207                                                    REPORT AFFECTING OTHER DEPTS.\n23                                         BREAKING AND ENTERING (B&E) MOTOR VEHICLE\n10                                                                             ARSON\n119                                                 LARCENY PURSE SNATCH - NO FORCE \n31                                                       BURGLARY - OTHER - NO FORCE\n66                                    DRUGS - POSS CLASS E - INTENT TO MFR DIST DISP\n209                                                                   ROBBERY - BANK\n4                                                                 AIRCRAFT INCIDENTS\n68                                           DRUGS - POSSESSION OF DRUG PARAPHANALIA\n38                                                   CHILD ENDANGERMENT (NO ASSAULT)\n22                                                                       BOMB THREAT\n256                                                 WEAPON - OTHER - OTHER VIOLATION\n5                                                                       ANIMAL ABUSE\n245                                          VIOLATION - HARASSMENT PREVENTION ORDER\n204       RECOVERED - MV RECOVERED IN BOSTON (STOLEN IN BOSTON) MUST BE SUPPLEMENTAL\n43                                                               CRIMINAL HARASSMENT\n180                                                  OBSCENE MATERIALS - PORNOGRAPHY\n26                                                   BURGLARY - COMMERICAL - ATTEMPT\n24                    BREAKING AND ENTERING (B&E) MOTOR VEHICLE (NO PROPERTY STOLEN)\n104                                                   INJURY BICYCLE NO M/V INVOLVED\n210                                                            ROBBERY - CAR JACKING\n166                                                  Migrated Report - Other Larceny\n212                                                          ROBBERY - HOME INVASION\n168                                                 Migrated Report - Other Part III\n54                                        DRUGS - CONSP TO VIOL CONTROLLED SUBSTANCE\n67                                                                DRUGS - POSSESSION\n206                                                           RECOVERED STOLEN PLATE\n9                                                             ANNOYING AND ACCOSTING\n185                                              OPERATING UNDER THE INFLUENCE DRUGS\n101                                                                    HOME INVASION\n149                Migrated Report - Aggravated Assault/Aggravated Assault & Battery\n157                    Migrated Report - Drugs - Possession/Manufacturing/Distribute\n118                                                  LARCENY PURSE SNATCH - NO FORCE\n110                                               KIDNAPPING - ENTICING OR ATTEMPTED\n111                                                  KIDNAPPING/CUSTODIAL KIDNAPPING\n150                                      Migrated Report - Assault/Assault & Battery\n192                                                     PROPERTY - CONCEALING LEASED\n181                                                              OBSCENE PHONE CALLS\n225                                                                         STALKING\n78                                                    EXPLOSIVES - POSSESSION OR USE\n188                                             PRISONER - SUICIDE / SUICIDE ATTEMPT\n79                                                   EXPLOSIVES - TURNED IN OR FOUND\n167                                                  Migrated Report - Other Part II\n203                                                 PROTECTIVE CUSTODY / SAFEKEEPING\n199                                                                     PROSTITUTION\n29                                                        BURGLARY - OTHER - ATTEMPT\n183                                        OPERATING UNDER THE INFLUENCE (OUI) DRUGS\n153                                 Migrated Report - Burglary/Breaking and Entering\n36                                                    CHILD ABANDONMENT (NO ASSAULT)\n232                                                                TRUANCY / RUNAWAY\n112                                       KIDNAPPING/CUSTODIAL KIDNAPPING/ ABDUCTION\n40                                                                             CHINS\n39                                        CHILD REQUIRING ASSISTANCE (FOMERLY CHINS)\n165                                            Migrated Report - Motor Vehicle Crash\n246                                                   VIOLATION - HAWKER AND PEDDLER\n41                                                        CONSPIRACY EXCEPT DRUG LAW\n156                                            Migrated Report - Death Investigation\n171                              Migrated Report - Vandalism/Destruction of Property\n87                                                             FIREARM/WEAPON - LOST\n122                                               LARCENY THEFT FROM COIN-OP MACHINE\n178                                                         NOISY PARTY/RADIO-ARREST\n244                                   VIOLATION - CITY ORDINANCE CONSTRUCTION PERMIT\n74                                                                       DRUNKENNESS\n53                                         DRUGS - CLASS D TRAFFICKING OVER 50 GRAMS\n159                                                          Migrated Report - Fraud\n162                                           Migrated Report - Investigate Property\n169                                                        Migrated Report - Robbery\n161                                             Migrated Report - Investigate Person\n172                                              Migrated Report - Weapons Violation\n152                                                     Migrated Report - Auto Theft\n254                                            WEAPON - FIREARM - SALE / TRAFFICKING\n96                                                       GATHERING CAUSING ANNOYANCE\n151                                             Migrated Report - Auto Law Violation\n164                                                Migrated Report - Larceny From MV\n201                                                PROSTITUTION - COMMON NIGHTWALKER\n42                                              CONTRIBUTING TO DELINQUENCY OF MINOR\n95                                                     GAMBLING - BETTING / WAGERING\n102                                          HUMAN TRAFFICKING - COMMERCIAL SEX ACTS\n147                                              MANSLAUGHTER - VEHICLE - NEGLIGENCE\n85                                        FIREARM/WEAPON - ACCIDENTAL INJURY / DEATH\n155                                              Migrated Report - Criminal Homicide\n215                                                  ROBBERY - UNARMED - CHAIN STORE\n2                                                               ABDUCTION - INTICING\n44                                                              CUSTODIAL KIDNAPPING\n154                                         Migrated Report - Counterfeiting/Forgery\n190                                             PRISONER ESCAPE / ESCAPE & RECAPTURE\n200                                            PROSTITUTION - ASSISTING OR PROMOTING\n21                                                                BIOLOGICAL THREATS\n116                                                  LARCENY IN A BUILDING UNDER $50\n170                                                Migrated Report - Stolen Property\n176                                               MURDER, NON-NEGLIGENT MANSLAUGHTER\n1                                                              A&B ON POLICE OFFICER\n14                                                                 ASSAULT & BATTERY\n63                                       DRUGS - POSS CLASS D - INTENT MFR DIST DISP\n109                                                             Justifiable Homicide\n148                 Migrated Report - Affray/Disturbing the Peace/Disorderly Conduct\n216                                                       ROBBERY - UNARMED - STREET\n15                                   ASSAULT & BATTERY D/W - OTHER ON POLICE OFFICER\n77                                                        Evidence Tracker Incidents\n103                                        HUMAN TRAFFICKING - INVOLUNTARY SERVITUDE\n145                                                        MANSLAUGHTER - NEGLIGENCE\n146                                          MANSLAUGHTER - NON-VEHICLE - NEGLIGENCE\n158                                                   Migrated Report - Embezzlement\n160                                    Migrated Report - Injured/Medical/Sick Assist\n163                                                     Migrated Report - Kidnapping\n189                                                       PRISONER ATTEMPT TO RESCUE\n247                                                    VIOLATION - RESTRAINING ORDER\n    Total\n106 31616\n131 24504\n223 23739\n239 19790\n107 18890\n230 17325\n240 16023\n16  13382\n123 12522\n120 12460\n229 11854\n221 11497\n121 11482\n115  8855\n194  8099\n13   7853\n90   7147\n11   6648\n139  6506\n174  6410\n249  6089\n137  5942\n141  5697\n193  5429\n196  5063\n17   5044\n142  4824\n12   4197\n98   3977\n69   3845\n124  3773\n89   3441\n231  3383\n173  3200\n233  2959\n99   2811\n91   2748\n237  2706\n125  2627\n224  2626\n227  2508\n126  2242\n219  2165\n32   2155\n214  2141\n58   2047\n46   1973\n59   1968\n208  1889\n222  1834\n238  1831\n130  1802\n34   1684\n88   1631\n35   1569\n20   1445\n205  1435\n191  1378\n236  1377\n70   1366\n143  1268\n19   1252\n140  1216\n56   1193\n138  1137\n92   1128\n217  1103\n81   1070\n57   1051\n135  1041\n226  1034\n25   1033\n83   1030\n48    982\n175   920\n127   906\n71    886\n86    876\n251   863\n197   859\n114   837\n144   834\n242   825\n113   821\n234   768\n252   759\n257   723\n64    718\n93    707\n55    698\n132   694\n218   690\n27    662\n136   604\n18    591\n179   555\n235   547\n49    496\n186   485\n134   467\n33    466\n97    463\n211   454\n255   448\n213   438\n243   437\n117   433\n76    416\n50    415\n198   413\n133   409\n82    408\n45    405\n80    391\n228   387\n75    360\n100   354\n184   353\n8     347\n60    339\n65    336\n195   334\n62    311\n6     306\n129   274\n94    270\n105   264\n250   251\n177   247\n7     241\n182   237\n3     235\n28    230\n84    211\n52    207\n248   205\n61    198\n73    196\n241   196\n47    186\n108   181\n220   180\n30    172\n128   157\n51    152\n187   149\n202   148\n37    146\n72    145\n253   138\n207   135\n23    133\n10    127\n119   124\n31    122\n66    114\n209   105\n4     102\n68    101\n38     99\n22     97\n256    92\n5      90\n245    82\n204    81\n43     80\n180    78\n26     76\n24     71\n104    70\n210    69\n166    63\n212    61\n168    60\n54     59\n67     59\n206    57\n9      56\n185    56\n101    53\n149    52\n157    52\n118    48\n110    39\n111    39\n150    39\n192    38\n181    37\n225    37\n78     34\n188    34\n79     31\n167    31\n203    31\n199    30\n29     29\n183    29\n153    26\n36     24\n232    24\n112    23\n40     21\n39     19\n165    19\n246    18\n41     17\n156    17\n171    16\n87     15\n122    15\n178    15\n244    15\n74     12\n53     11\n159    11\n162    11\n169    11\n161    10\n172    10\n152     9\n254     9\n96      8\n151     7\n164     7\n201     7\n42      6\n95      6\n102     6\n147     6\n85      5\n155     5\n215     5\n2       4\n44      4\n154     4\n190     4\n200     4\n21      3\n116     3\n170     3\n176     3\n1       2\n14      2\n63      2\n109     2\n148     2\n216     2\n15      1\n77      1\n103     1\n145     1\n146     1\n158     1\n160     1\n163     1\n189     1\n247     1\n\n\nWe can see that there are 257 different crime categories in the Boston metro region. I have also observed that there are few categories of crime which are extremely minimal and which are quite rare in the few years which are not our main focus. We need to mainly focus on the crime categories which are very high in number and are contributing to the crime rate in the Boston metro. To identify these crime categories we now select the top 10 crime categories from the decreasing order.\n\ntop_crime_data <- common_crimes[1:10,]\ntop_crime_data\n\n                       Offense_code_group Total\n106                    INVESTIGATE PERSON 31616\n131 M/V - LEAVING SCENE - PROPERTY DAMAGE 24504\n223         SICK/INJURED/MEDICAL - PERSON 23739\n239                             VANDALISM 19790\n107                  INVESTIGATE PROPERTY 18890\n230                   TOWED MOTOR VEHICLE 17325\n240                        VERBAL DISPUTE 16023\n16               ASSAULT SIMPLE - BATTERY 13382\n123 LARCENY THEFT FROM MV - NON-ACCESSORY 12522\n120                   LARCENY SHOPLIFTING 12460\n\n\nWe now plot a bar graph to represent the crime categories and the number of the crimes from 2017- 2022.\n\nggplot(data = top_crime_data, mapping = aes(x= Total, y= reorder(Offense_code_group, Total)))+\n  geom_col(aes(fill = Offense_code_group))+\n  geom_text(data = top_crime_data[c(1,39),],mapping = aes(label = Total))+\n   theme_minimal()+\n  labs(title = \"Common Crime Category in Boston Metro\",\n       y = \"Crime categories\",\n       x = \"Total number of crimes\") +\n theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen a bar graph as it conveys the relational information more easily and quickly. Each of the bars display the value of the particular crime category. I have used geom_col() instead of geom_bar() because I want the height of the bars to represent/show the values. From the graph it is very clear that “INVESTIGATE PERSON” is the most common crime category which is then followed by the “M/V - LEAVING SCENE - PROPERTY DAMAGE” then “SICK/INJURED/MEDICAL -PERSON” and so on."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#which-year-has-contributed-for-the-maximum-number-of-crimes-in-the-boston-metro-and-did-the-crimes-increase-of-decrease-from-2017-2022",
    "href": "posts/NeeharikaKaranam_FinalProject.html#which-year-has-contributed-for-the-maximum-number-of-crimes-in-the-boston-metro-and-did-the-crimes-increase-of-decrease-from-2017-2022",
    "title": "Final Project",
    "section": "2. Which year has contributed for the maximum number of crimes in the Boston Metro and did the crimes increase of decrease from 2017-2022",
    "text": "2. Which year has contributed for the maximum number of crimes in the Boston Metro and did the crimes increase of decrease from 2017-2022\nLet us check if the class/data_type of the column YEAR to check if it is a numeric value or not if it isn’t then let’s transform the YEAR to a numeric value.\n\nsapply(boston_crime, class)\n\n                  X     INCIDENT_NUMBER        OFFENSE_CODE  OFFENSE_CODE_GROUP \n          \"integer\"         \"character\"           \"integer\"         \"character\" \nOFFENSE_DESCRIPTION            DISTRICT      REPORTING_AREA            SHOOTING \n        \"character\"         \"character\"           \"integer\"         \"character\" \n   OCCURRED_ON_DATE                YEAR               MONTH         DAY_OF_WEEK \n        \"character\"           \"integer\"         \"character\"         \"character\" \n               HOUR            UCR_PART              STREET                 Lat \n          \"integer\"         \"character\"         \"character\"           \"numeric\" \n               Long            Location \n          \"numeric\"         \"character\" \n\n\nWe can see that the column YEAR is a numeric value and there is no need for us to transform.\nNow, let us get the count of the crime records for each year from 2017 to 2022.\n\ncrimes_per_year <- boston_crime %>% \n  group_by(YEAR) %>% \n  summarise(Total = n())\n\ncrimes_per_year\n\n# A tibble: 6 × 2\n   YEAR  Total\n  <int>  <int>\n1  2017 101338\n2  2018  98888\n3  2019  87184\n4  2020  70894\n5  2021  71721\n6  2022  16068\n\n\nWe now plot a line graph to represent the different years and the number of the crimes in each of the years from 2017- 2022(Till March).\n\nggplot(crimes_per_year, aes(x = YEAR, y = Total))+\n  geom_line(color = \"grey\")+\n  geom_point(size = 3, color = \"red\")+\n  theme_minimal()+\n  labs(title = \"Crimes per Year in Boston Metro\",\n       x = \"Years\",\n       y = \"Total number of crimes\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-1",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-1",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen a line graph as it helps in tracking the changes that have taken place over a short of a long period of time. It also helps us in making observations if they are consistently increasing or decreasing. From the graph we can observe that it is a downward slope which is an extremely positive sign as it indicates that the crimes per year have decreased over time. From 2017-2018 there is a slight decrease in the crimes where there is a significant decrease in the number of crimes from 2018-2020 . We can also observe that from 2020-2021 the number of crimes have very slightly increased. We can ignore the downward slope to 2022 because our dataset consists of the crime records for only the first 3 months of 2022 thereby, not providing the accurate analysis for 2022. We can conclude that 2017 has the maximum number of crimes."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#which-months-of-the-year-have-seen-the-highest-number-of-crimes-is-it-during-the-holiday-months-or-not",
    "href": "posts/NeeharikaKaranam_FinalProject.html#which-months-of-the-year-have-seen-the-highest-number-of-crimes-is-it-during-the-holiday-months-or-not",
    "title": "Final Project",
    "section": "3. Which months of the year have seen the highest number of crimes? Is it during the holiday months or not?",
    "text": "3. Which months of the year have seen the highest number of crimes? Is it during the holiday months or not?\nLet us now check if there is any difference in the number of crimes that happen based on the months.\n\nmonthly_crimes <- boston_crime %>% \n  group_by(MONTH) %>% \n  summarise(Total = n())\nmonthly_crimes\n\n# A tibble: 12 × 2\n   MONTH     Total\n   <chr>     <int>\n 1 April     33086\n 2 August    39815\n 3 December  32800\n 4 February  36662\n 5 Janurary  39755\n 6 July      38604\n 7 June      38052\n 8 March     39738\n 9 May       37126\n10 November  33794\n11 October   37980\n12 September 38681\n\n\nWe now plot a bar graph to represent the monthly crimes and the number of the crimes for the 12 months.\n\nggplot(monthly_crimes, aes(x = reorder(MONTH, -Total), y = Total))+\n  geom_col(fill = \"salmon\")+\n  geom_text(aes(label = Total), col = \"black\")+\n  theme_minimal()+\n  labs(title = \"Monthly Crime in Boston Metro\",\n       y = \"Total Number of Crimes\",\n       x = \"Months\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-2",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-2",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen a bar graph as it conveys the relational information more easily and quickly. Each of the bars display the value of the particular crime category. I have used geom_col() instead of geom_bar() because I want the height of the bars to represent/show the values. From this graph we can observe that there is not a very huge difference between the crime rate in different months but we can observe that the crime during the holiday season like December and November have a low crime rate than the other months. We can also observe that the month right after the holiday month January is almost the highest month with the crime rate.\nWe know that the year with the highest crimes is 2017. Now, let us know the month of 2017 with the highest crime rate.\n\nmonthly_crimes_2017 <- boston_crime %>% \n  filter(YEAR == 2017) %>%\n  group_by(MONTH) %>% \n  summarise(Total = n())\nmonthly_crimes_2017\n\n# A tibble: 12 × 2\n   MONTH     Total\n   <chr>     <int>\n 1 April      8101\n 2 August     9251\n 3 December   7603\n 4 February   7429\n 5 Janurary   8024\n 6 July       9109\n 7 June       9016\n 8 March      8194\n 9 May        8745\n10 November   7983\n11 October    8899\n12 September  8984\n\n\nWe now plot a bar graph to represent the monthly crimes for the year 2017 and the number of the crimes for the 12 months.\n\nggplot(monthly_crimes_2017, aes(x = reorder(MONTH, -Total), y = Total))+\n  geom_col(fill = \"pink\")+\n  geom_text(aes(label = Total), col = \"black\")+\n  theme_minimal()+\n  labs(title = \"Monthly Crime in Boston Metro\",\n       y = \"Total Number of Crimes\",\n       x = \"Months\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-3",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-3",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nFrom this graph we can observe the same interpretation as that of the monthly crimes graph for the past 6 years. Now we also understand that the highest crime month of the year 2017 is August which is very closely followed by July, June and so on. Again we can observe that the holiday months December and November are among the bottom 3 months."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#what-is-the-daily-crime-rate-in-the-different-streets-of-the-boston-metro",
    "href": "posts/NeeharikaKaranam_FinalProject.html#what-is-the-daily-crime-rate-in-the-different-streets-of-the-boston-metro",
    "title": "Final Project",
    "section": "4. What is the daily crime rate in the different streets of the Boston Metro?",
    "text": "4. What is the daily crime rate in the different streets of the Boston Metro?\nLet us first filter, group_by and summarize based on the year with the highest number of crimes and then with the month with the highest number of crimes.\n\ndaily_crime <- boston_crime %>% \n  filter(YEAR == 2017, MONTH == \"August\") %>% \n  group_by(DAY_OF_WEEK, DISTRICT) %>% \n  summarise(Total = n())\nhead(daily_crime)\n\n# A tibble: 6 × 3\n# Groups:   DAY_OF_WEEK [1]\n  DAY_OF_WEEK DISTRICT Total\n  <chr>       <chr>    <int>\n1 Friday      \"\"          10\n2 Friday      \"A1\"       172\n3 Friday      \"A15\"       21\n4 Friday      \"A7\"        51\n5 Friday      \"B2\"       180\n6 Friday      \"B3\"       150\n\n\nWe now have the count for the total number of crimes based on the day of the week and the street in which they have taken place.\nWe now plot a 2-dimensional frequency graph using the geom_count() to represent the crime rate in the different districts of the Boston Metro.\n\nggplot(daily_crime, aes(x = DAY_OF_WEEK, y = DISTRICT))+\n  geom_count(aes(size = Total), col = \"turquoise3\")+\n  theme_minimal()+\n  labs(\n    title = \"Daily Crime in Boston Metro - 2017\",\n    subtitle = \"Crimes in August\",\n    x= NULL,\n    y = \"Districts\"\n  )"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-4",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-4",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen a 2-dimensional frequency graph using the geom_count as it helps in counting a different number of observations at each of the locations and then maps the count in order to point to the area. From the above graph we can understand that maximum intensity of the dots is maximum on the districts of B2, B3, C11 and D4. However, the crime rate is relatively less in the A and E.This graph also represents which day of the week has the highest number of crimes.\nNow, let us visualize the data for the second month with the high crime rate.\n\ndaily_crime <- boston_crime %>% \n  filter(YEAR == 2017, MONTH == \"July\") %>% \n  group_by(DAY_OF_WEEK, DISTRICT) %>% \n  summarise(Total = n())\nhead(daily_crime)\n\n# A tibble: 6 × 3\n# Groups:   DAY_OF_WEEK [1]\n  DAY_OF_WEEK DISTRICT Total\n  <chr>       <chr>    <int>\n1 Friday      \"\"          12\n2 Friday      \"A1\"       151\n3 Friday      \"A15\"       33\n4 Friday      \"A7\"        48\n5 Friday      \"B2\"       172\n6 Friday      \"B3\"       125\n\n\nWe now have the count for the total number of crimes based on the day of the week and the street in which they have taken place.\nWe now plot a 2-dimensional frequency graph using the geom_count() to represent the crime rate in the different districts of the Boston Metro.\n\nggplot(daily_crime, aes(x = DAY_OF_WEEK, y = DISTRICT))+\n  geom_count(aes(size = Total), col = \"turquoise4\")+\n  theme_minimal()+\n  labs(\n    title = \"Daily Crime in Boston Metro - 2017\",\n    subtitle = \"Crimes in July\",\n    x= NULL,\n    y = \"Districts\"\n  )"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-5",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-5",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nFrom the above graph we can understand that maximum intensity of the dots is maximum on the districts of B2, C11. However, the crime rate is relatively less in the A and E. When compared with the month of August, we can observe that the districts of B3 and D4 have slightly less crimes in July. However, B2 and C11 districts are ranked as the top crime districts in both the months. We can now conclude that these two districts are the districts with the highest crime rate."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#which-day-of-the-week-the-top-crime-category-have-taken-place-the-most-is-it-most-during-the-weekends-or-during-the-weekdays",
    "href": "posts/NeeharikaKaranam_FinalProject.html#which-day-of-the-week-the-top-crime-category-have-taken-place-the-most-is-it-most-during-the-weekends-or-during-the-weekdays",
    "title": "Final Project",
    "section": "5. Which day of the week, the top crime category have taken place the most? Is it most during the weekends or during the weekdays?",
    "text": "5. Which day of the week, the top crime category have taken place the most? Is it most during the weekends or during the weekdays?\nWe know that the top crime category of the Boston Metro is the INVESTIGATE PERSON so, let us now check on which days of the week it is the highest.\n\ntop_crime <- boston_crime %>% \n  filter(OFFENSE_DESCRIPTION == \"INVESTIGATE PERSON\") %>% \n  group_by(DAY_OF_WEEK) %>% \n  summarise(Total = n())\ntop_crime\n\n# A tibble: 7 × 2\n  DAY_OF_WEEK Total\n  <chr>       <int>\n1 Friday       4786\n2 Monday       4553\n3 Saturday     4221\n4 Sunday       3935\n5 Thursday     4714\n6 Tuesday      4645\n7 Wednesday    4762\n\n\nWe have the total number of crimes taken place based on our top category of crime.\nWe now plot a bar graph to represent the day of the week the of when the crime has happened and the number of the crimes.\n\nggplot(top_crime, aes(x = Total, y = reorder(DAY_OF_WEEK, Total)))+\n  geom_col(fill = \"aquamarine2\")+\n  geom_text(aes(label = Total), col= \"azure4\")+\n  geom_vline(xintercept = mean(top_crime$Total))+\n  geom_label(label = paste(\"Mean \", round(mean(top_crime$Total))),\n             x = mean(top_crime$Total),\n             y = 9)+\n  labs(\n    title = \"INVESTIGATE PERSON - Crime by day of the week\",\n    subtitle = \"From 2017 - 2022\",\n    x = \"Total Crime \",\n    y = NULL\n  )+\n  theme_minimal()"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-6",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-6",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen a bar graph as it conveys the relational information more easily and quickly. Each of the bars display the value of the particular crime category. I have used geom_col() instead of geom_bar() because I want the height of the bars to represent/show the values. From this graph we can observe that the crime for this category has majorly happened during the weekdays than on the weekends. There is a significant decrease in the count during the weekends. During the weekends people like to spend time with their families and enjoy the weekend. Whereas, on the weekdays/business hours it is much probable to investigate a person.\nNow, let us check for the top second crime category and present our analysis. Our second top crime category is “M/V - LEAVING SCENE - PROPERTY DAMAGE”.\n\ntop_crime <- boston_crime %>% \n  filter(OFFENSE_DESCRIPTION == \"M/V - LEAVING SCENE - PROPERTY DAMAGE\") %>% \n  group_by(DAY_OF_WEEK) %>% \n  summarise(Total = n())\ntop_crime\n\n# A tibble: 7 × 2\n  DAY_OF_WEEK Total\n  <chr>       <int>\n1 Friday       4018\n2 Monday       3359\n3 Saturday     3716\n4 Sunday       3412\n5 Thursday     3361\n6 Tuesday      3289\n7 Wednesday    3349\n\n\nWe have the total number of crimes taken place based on our top second category of crime.\nWe now plot a bar graph to represent the day of the week the of when the crime has happened and the number of the crimes.\n\nggplot(top_crime, aes(x = Total, y = reorder(DAY_OF_WEEK, Total)))+\n  geom_col(fill = \"aquamarine4\")+\n  geom_text(aes(label = Total), col= \"black\")+\n  geom_vline(xintercept = mean(top_crime$Total))+\n  geom_label(label = paste(\"Mean \", round(mean(top_crime$Total))),\n             x = mean(top_crime$Total),\n             y = 9)+\n  labs(\n    title = \"IM/V - LEAVING SCENE - PROPERTY DAMAGE - Crime by day of the week\",\n    subtitle = \"From 2017 - 2022\",\n    x = \"Total Crime \",\n    y = NULL\n  )+\n  theme_minimal()"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-7",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-7",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nOn the contrary from the graph on the top crime we can observe that the crime has taken place the most during the weekends and has significantly decreased during the weekdays. Friday has the maximum number of property damage crime reports logged followed by Saturday and Sunday. It is also very clear that the business working days have seen comparatively less crimes."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#when-did-the-top-10-crime-categories-actually-take-place-is-it-during-the-morning-evening-or-the-night",
    "href": "posts/NeeharikaKaranam_FinalProject.html#when-did-the-top-10-crime-categories-actually-take-place-is-it-during-the-morning-evening-or-the-night",
    "title": "Final Project",
    "section": "6. When did the top 10 crime categories actually take place? Is it during the morning, evening or the night?",
    "text": "6. When did the top 10 crime categories actually take place? Is it during the morning, evening or the night?\nLet us first list out our top 10 crime categories.\n\ntop10_crimes <- unique(common_crimes$Offense_code_group)[1:10]\ntop10_crimes <- droplevels(top10_crimes)\ntop10_crimes\n\n [1] INVESTIGATE PERSON                    M/V - LEAVING SCENE - PROPERTY DAMAGE\n [3] SICK/INJURED/MEDICAL - PERSON         VANDALISM                            \n [5] INVESTIGATE PROPERTY                  TOWED MOTOR VEHICLE                  \n [7] VERBAL DISPUTE                        ASSAULT SIMPLE - BATTERY             \n [9] LARCENY THEFT FROM MV - NON-ACCESSORY LARCENY SHOPLIFTING                  \n10 Levels: ASSAULT SIMPLE - BATTERY INVESTIGATE PERSON ... VERBAL DISPUTE\n\n\nNow, we have our top 10 crime categories listed. Let us write a fucntion in order to segregate our column hour into the different time zones say “12am to 8am” , “8am to 4pm” and “4pm to 12am”.\n\npw <- function(x){ \n    if(x < 8){\n      x <- \"12am to 8am\"\n    }else if(x >= 8 & x < 16){\n      x <- \"8am to 4pm\"\n    }else{\n      x <- \"4pm to 12am\"\n    }\n}\n\nLet us create a new column called the “Hour_category” which reflects the time zone the crime has actually taken place.\n\nboston_crime$Hour_category <- sapply(boston_crime$HOUR, pw)\nboston_crime$Hour_category <- as.factor(boston_crime$Hour_category)\nhead(boston_crime)\n\n  X INCIDENT_NUMBER OFFENSE_CODE OFFENSE_CODE_GROUP\n1 0       225520077         3126                   \n2 1       222648862         3831                   \n3 2       222201764          724                   \n4 3       222201559          301                   \n5 4       222111641          619                   \n6 5       222107076         3126                   \n                         OFFENSE_DESCRIPTION DISTRICT REPORTING_AREA SHOOTING\n1 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT      D14            786        0\n2      M/V - LEAVING SCENE - PROPERTY DAMAGE       B2            288        0\n3                                 AUTO THEFT       C6            200        0\n4                                    ROBBERY       D4             NA        0\n5                         LARCENY ALL OTHERS      D14            778        0\n6 WARRANT ARREST - OUTSIDE OF BOSTON WARRANT       D4             NA        0\n     OCCURRED_ON_DATE YEAR    MONTH DAY_OF_WEEK HOUR UCR_PART\n1 2022-02-02 00:00:00 2022 February   Wednesday    0         \n2 2022-02-05 18:25:00 2022 February    Saturday   18         \n3 2022-01-09 00:00:00 2022 Janurary      Sunday    0         \n4 2022-03-05 13:00:00 2022    March    Saturday   13         \n5 2022-02-14 12:30:00 2022 February      Monday   12         \n6 2022-03-11 10:45:00 2022    March      Friday   10         \n                                                STREET      Lat      Long\n1                                        WASHINGTON ST 42.34308 -71.14172\n2                                        WASHINGTON ST 42.32975 -71.08454\n3                                           W BROADWAY 42.34129 -71.05468\n4                                            ALBANY ST 42.33318 -71.07394\n5                                        WASHINGTON ST 42.34906 -71.15050\n6 MASSACHUSETTS AVE & ALBANY ST\\nBOSTON  MA 02118\\nUNI 42.33350 -71.07351\n                                  Location Hour_category\n1  (42.34308127134165, -71.14172267328729)   12am to 8am\n2 (42.329748204791635, -71.08454011649543)   4pm to 12am\n3 (42.341287504390436, -71.05467932649397)   12am to 8am\n4 (42.333184490911954, -71.07393881002383)    8am to 4pm\n5  (42.34905600030506, -71.15049849975023)    8am to 4pm\n6  (42.33349998017161, -71.07350999617319)    8am to 4pm\n\n\nWe know that maximum number of crimes have taken place in the year 2017 so let us check at what time these top 10 crime categories have taken place in 2017.\n\ncrime_when <- boston_crime %>% \n  filter(OFFENSE_DESCRIPTION %in% top10_crimes, YEAR == \"2017\") %>% \n  group_by(OFFENSE_DESCRIPTION, Hour_category) %>% \n  summarise(Total = n())\n\ncrime_when\n\n# A tibble: 30 × 3\n# Groups:   OFFENSE_DESCRIPTION [10]\n   OFFENSE_DESCRIPTION      Hour_category Total\n   <chr>                    <fct>         <int>\n 1 ASSAULT SIMPLE - BATTERY 12am to 8am     994\n 2 ASSAULT SIMPLE - BATTERY 4pm to 12am    2079\n 3 ASSAULT SIMPLE - BATTERY 8am to 4pm     1566\n 4 INVESTIGATE PERSON       12am to 8am     926\n 5 INVESTIGATE PERSON       4pm to 12am    2946\n 6 INVESTIGATE PERSON       8am to 4pm     2796\n 7 INVESTIGATE PROPERTY     12am to 8am     869\n 8 INVESTIGATE PROPERTY     4pm to 12am    1702\n 9 INVESTIGATE PROPERTY     8am to 4pm     1413\n10 LARCENY SHOPLIFTING      12am to 8am     137\n# … with 20 more rows\n\n\nWe have the data now based on the crime category, hour category and the total number of crimes that have taken place.\nWe now plot a bar graph to represent the the time category of when the crime has taken placed for the top 10 crimes.\n\nggplot(data = crime_when, mapping = aes(x = Total, y = reorder(OFFENSE_DESCRIPTION, Total))) +\n  geom_col(mapping = aes(fill = Hour_category), position = \"dodge\") + \n  labs(x = \"Total Count\", y = NULL,\n       fill = NULL,\n       title = \"Crime categories with the time frame of occurrence.\",\n       subtitle = \"Year 2017\") +\n  scale_fill_brewer(palette = 4) +\n  theme_minimal() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-8",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-8",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen a bar graph as it conveys the relational information more easily and quickly. Each of the bars display the value of the particular crime category. I have used geom_col() instead of geom_bar() because I want the height of the bars to represent/show the values. From the above graph we can understand very clearly of which time period each of the crime has taken place. Like, INVESTIGATE PERSON crime category has taken place mostly during the evenings or during the business working hours than compared to the late night. In the similar way, we can observe that the LARENCY SHOPLIFTING has taken place mostly during the business working hours of 8am to 4pm than late in the night. This may be because the shops/malls are generally closed during the night. In the similar fashion we can draw conclusions for all of the crime categories and this graph gives us an in-depth analysis of the time frame of the crime.\nLet us now check if we will observe similar observations for the year 2021 which has has the least numer of crimes.\n\ncrime_when <- boston_crime %>% \n  filter(OFFENSE_DESCRIPTION %in% top10_crimes, YEAR == \"2021\") %>% \n  group_by(OFFENSE_DESCRIPTION, Hour_category) %>% \n  summarise(Total = n())\n\ncrime_when\n\n# A tibble: 27 × 3\n# Groups:   OFFENSE_DESCRIPTION [9]\n   OFFENSE_DESCRIPTION                   Hour_category Total\n   <chr>                                 <fct>         <int>\n 1 INVESTIGATE PERSON                    12am to 8am    1290\n 2 INVESTIGATE PERSON                    4pm to 12am    2836\n 3 INVESTIGATE PERSON                    8am to 4pm     2715\n 4 INVESTIGATE PROPERTY                  12am to 8am     918\n 5 INVESTIGATE PROPERTY                  4pm to 12am    1416\n 6 INVESTIGATE PROPERTY                  8am to 4pm     1197\n 7 LARCENY SHOPLIFTING                   12am to 8am     111\n 8 LARCENY SHOPLIFTING                   4pm to 12am     961\n 9 LARCENY SHOPLIFTING                   8am to 4pm     1244\n10 LARCENY THEFT FROM MV - NON-ACCESSORY 12am to 8am     452\n# … with 17 more rows\n\n\nWe have the data now based on the crime category, hour category and the total number of crimes that have taken place.\nWe now plot a bar graph to represent the the time category of when the crime has taken placed for the top 10 crimes.\n\nggplot(data = crime_when, mapping = aes(x = Total, y = reorder(OFFENSE_DESCRIPTION, Total))) +\n  geom_col(mapping = aes(fill = Hour_category), position = \"dodge\") + \n  labs(x = \"Total Count\", y = NULL,\n       fill = NULL,\n       title = \"Crime categories with the time frame of occurrence.\",\n       subtitle = \"Year 2021\") +\n  scale_fill_brewer(palette = 5) +\n  theme_minimal() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-9",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-9",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nFrom the graph from 2017 and 2021 we can still draw the same conclusions on the time frame that the crimes have taken place. It is very evident that the crimes are still taking place in the same time frames. For LARENCY SHOPLIFTING the crime is still taking place during the business working hours than in the night and even INVESTIGATE PERSON is happening more during the evenings and the mornings than late in the night. This shows that the time frame of occurrence has not changed as the time passed."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#how-does-the-density-of-the-top-3-crime-categories-change-each-year",
    "href": "posts/NeeharikaKaranam_FinalProject.html#how-does-the-density-of-the-top-3-crime-categories-change-each-year",
    "title": "Final Project",
    "section": "7. How does the density of the top 3 crime categories change each year?",
    "text": "7. How does the density of the top 3 crime categories change each year?\nLet us make a list of the top 3 crime categories and then find the total crimes based on the year.\n\nlist <- c(\"INVESTIGATE PERSON\", \"M/V - LEAVING SCENE - PROPERTY DAMAGE\", \"SICK/INJURED/MEDICAL - PERSON\")\ncrime_density <- boston_crime %>% \n  filter(OFFENSE_DESCRIPTION %in% list) %>% \n  group_by(OFFENSE_DESCRIPTION, YEAR) %>% \n  summarise(Total = n())\ncrime_density\n\n# A tibble: 18 × 3\n# Groups:   OFFENSE_DESCRIPTION [3]\n   OFFENSE_DESCRIPTION                    YEAR Total\n   <chr>                                 <int> <int>\n 1 INVESTIGATE PERSON                     2017  6668\n 2 INVESTIGATE PERSON                     2018  5467\n 3 INVESTIGATE PERSON                     2019  5733\n 4 INVESTIGATE PERSON                     2020  5122\n 5 INVESTIGATE PERSON                     2021  6841\n 6 INVESTIGATE PERSON                     2022  1785\n 7 M/V - LEAVING SCENE - PROPERTY DAMAGE  2017  5221\n 8 M/V - LEAVING SCENE - PROPERTY DAMAGE  2018  5019\n 9 M/V - LEAVING SCENE - PROPERTY DAMAGE  2019  4910\n10 M/V - LEAVING SCENE - PROPERTY DAMAGE  2020  3603\n11 M/V - LEAVING SCENE - PROPERTY DAMAGE  2021  4678\n12 M/V - LEAVING SCENE - PROPERTY DAMAGE  2022  1073\n13 SICK/INJURED/MEDICAL - PERSON          2017  6279\n14 SICK/INJURED/MEDICAL - PERSON          2018  6812\n15 SICK/INJURED/MEDICAL - PERSON          2019  5895\n16 SICK/INJURED/MEDICAL - PERSON          2020  2442\n17 SICK/INJURED/MEDICAL - PERSON          2021  2010\n18 SICK/INJURED/MEDICAL - PERSON          2022   301\n\n\nWe now have all of the data ready for us to make a density plot.\nLet us know plot a density graph to help us represent the top 3 crime categories based on the total crime count and how it changes for different years.\n\nggplot(crime_density, aes(x =Total, y= OFFENSE_DESCRIPTION, fill = OFFENSE_DESCRIPTION))+\n  geom_density_ridges2()+\n  labs(x = \"Total Count\", y = NULL,\n       title = \"Density of the top 3 crime categories.\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-10",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-10",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen density plot as it shows how the data is distributed over a period of time and the value peaks in the region where there is a maximum concentration. It is also used to smooth out the distribution of the values and thereby reduce the noise of the data. From the above graph we can observe that the values are in a high low format and it clearly indicates how the values are distributed for the entire interval."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#in-which-streets-did-the-maximum-crime-take-place-for-a-crime-category.-can-we-predict-which-parts-of-the-boston-metro-are-safer-than-the-others",
    "href": "posts/NeeharikaKaranam_FinalProject.html#in-which-streets-did-the-maximum-crime-take-place-for-a-crime-category.-can-we-predict-which-parts-of-the-boston-metro-are-safer-than-the-others",
    "title": "Final Project",
    "section": "8. In which streets did the maximum crime take place for a crime category. Can we predict which parts of the Boston Metro are safer than the others?",
    "text": "8. In which streets did the maximum crime take place for a crime category. Can we predict which parts of the Boston Metro are safer than the others?\nLet us get all the latitudes and the longitudes of the crime category.\n\nmap_drug <- boston_crime %>% \n  filter(OFFENSE_DESCRIPTION == \"ROBBERY\",\n         YEAR == \"2019\",\n         STREET != \"BROOKSIDE AVE\") %>% \n  select(STREET, Long, Lat)\nmap_drug\n\n                                                  STREET      Long      Lat\n1                                                 GILMER -71.09722 42.28281\n2                                              CENTRE ST -71.10033 42.32280\n3                                          BENNINGTON ST -71.03474 42.37644\n4                                           WORCESTER SQ -71.07407 42.33615\n5   422 COLUMBIA RD\\nDORCHESTER  MA 02125\\nUNITED STATES -71.06890 42.31236\n6                                           WALK HILL ST -71.09585 42.27906\n7                                             W EAGLE ST -71.03929 42.37082\n8                                             WINDSOR ST -71.08357 42.33474\n9                                               RIVER ST -71.12402 42.25622\n10                                              BEACH ST -71.06168 42.35146\n11                                          READVILLE ST -71.13232 42.23772\n12                                           HARVARD AVE -71.13181 42.35205\n13  2400 WASHINGTON ST\\nROXBURY  MA 02119\\nUNITED STATES -71.08563 42.32866\n14                                         CITY HALL PLZ -71.05852 42.35972\n15                                            ORLANDO ST -71.09817 42.27544\n16                                  PUBLIC ALLEY NO. 714 -71.07602 42.33650\n17  HAROLD ST & ABBOTSFORD ST\\nROXBURY  MA 02121\\nUNITED -71.09154 42.31427\n18  NEW SUDBURY ST & HAYMARKET SQ\\nBOSTON  MA 02109\\nUNI -71.05748 42.36276\n19                                            TALBOT AVE -71.07270 42.29042\n20                                             STUART ST -71.06400 42.35094\n21                                          HARRISON AVE -71.07561 42.33455\n22                                          HUMBOLDT AVE -71.08784 42.31521\n23                                            CHELSEA ST -71.03666 42.37172\n24                                              HEATH ST -71.09866 42.32497\n25                                               LAWN ST -71.10492 42.32609\n26                                         BLUE HILL AVE -71.09286 42.27968\n27  BEACON ST & CHARLES ST\\nBOSTON  MA 02108\\nUNITED STA -71.06944 42.35618\n28                                             VALLAR RD -71.03929 42.37082\n29                                         WASHINGTON ST -71.08285 42.33095\n30                                   AMERICAN LEGION HWY -71.11517 42.28224\n31                                          HARRISON AVE -71.06941 42.33954\n32                                               PARK DR -71.10359 42.34417\n33                                         GALLIVAN BLVD -71.04531 42.28454\n34                                         WASHINGTON ST -71.06921 42.34149\n35  WASHINGTON ST & WEST ST\\nBOSTON  MA 02111\\nUNITED ST -71.06171 42.35434\n36          24 THANE ST\\nBOSTON  MA 02124\\nUNITED STATES -71.07619 42.29673\n37                                             SACHEM ST -71.10790 42.33087\n38                                            TREMONT ST -71.10378 42.33381\n39                                             MORTON ST -71.12152 42.29370\n40                                            HARWOOD ST -71.08763 42.28577\n41  HUMBOLDT AVE & CRAWFORD ST\\nROXBURY  MA 02121\\nUNITE -71.08940 42.31327\n42                                          KITTREDGE ST -71.12953 42.28474\n43  BEACH ST & HARRISON AVE\\nBOSTON  MA 02111\\nUNITED ST -71.06117 42.35150\n44                                          RIDGEMONT ST -71.14146 42.35129\n45  EDGEWATER DR & TESLA ST\\nBOSTON  MA 02126\\nUNITED ST -71.09601 42.26565\n46                                         WASHINGTON ST -71.07918 42.30422\n47                                           CUMMINS HWY -71.10101 42.27057\n48    160 HOMESTEAD ST\\nROXBURY  MA 02121\\nUNITED STATES -71.08759 42.31014\n49  MASSACHUSETTS AVE & HARRISON AVE\\nBOSTON  MA 02118\\n -71.07517 42.33491\n50                                         BLUE HILL AVE -71.09335 42.27777\n51                                             DITSON ST -71.06391 42.30108\n52                                           S MARKET ST -71.05234 42.35605\n53     85 DRAPER ST\\nDORCHESTER  MA 02122\\nUNITED STATES -71.06547 42.30541\n54                                         GALLIVAN BLVD -71.04831 42.28349\n55                                            TALBOT AVE -71.05971 42.29756\n56                                             SCHOOL ST -71.07577 42.29703\n57                                      COMMONWEALTH AVE -71.16642 42.34006\n58                                             WALDEN ST -71.10450 42.32561\n59                                              ADAMS ST -71.05991 42.30172\n60  WESTVILLE ST & CORWIN ST\\nDORCHESTER  MA 02122\\nUNIT -71.06229 42.30227\n61                                          WADSWORTH ST -71.12674 42.35515\n62     40 GIBSON ST\\nDORCHESTER  MA 02122\\nUNITED STATES -71.05971 42.29756\n63                                           COLUMBIA RD -71.06261 42.31959\n64                                      BUSINESS TERRACE -71.12741 42.25289\n65  FREEMAN ST & CHARLES ST\\nDORCHESTER  MA 02122\\nUNITE -71.06284 42.30029\n66     19 JUSTINIAN WAY\\nBOSTON  MA 02134\\nUNITED STATES -71.05517 42.28503\n67                                            TROTTER CT -71.08009 42.33594\n68  HARRISON AVE & E SPRINGFIELD ST\\nBOSTON  MA 02118\\nU -71.07447 42.33545\n69                                             SUMMER ST -71.06013 42.35522\n70                                               HIGH ST -71.05976 42.36184\n71                                         WASHINGTON ST -71.08029 42.33384\n72  2400 WASHINGTON ST\\nROXBURY  MA 02119\\nUNITED STATES -71.08563 42.32866\n73                                          BROMFIELD ST -71.06325 42.35771\n74                                      S HUNTINGTON AVE -71.11112 42.32955\n75  MASSACHUSETTS AVE & HARRISON AVE\\nBOSTON  MA 02118\\n -71.07517 42.33491\n76                                            CONCORD SQ -71.07899 42.34138\n77  PARK ST & TREMONT ST\\nBOSTON  MA 02108\\nUNITED STATE -71.06200 42.35650\n78  CENTRAL ST & MCKINLEY SQ\\nBOSTON  MA 02109\\nUNITED S -71.05321 42.35884\n79  DITSON ST & WESTVILLE ST\\nBOSTON  MA 02122\\nUNITED S -71.06431 42.30176\n80                                         W TREMLETT ST -71.07307 42.29435\n81                                           ALLSTATE RD -71.06322 42.32810\n82                                         BENNINGTON ST -71.01730 42.38298\n83                                     MASSACHUSETTS AVE -71.07755 42.33689\n84     69 PARIS ST\\nEAST BOSTON  MA 02128\\nUNITED STATES -71.03929 42.37082\n85                                             BEACON ST -71.07168 42.35564\n86                                            STANTON ST -71.09137 42.28483\n87                                          HARRISON AVE -71.06941 42.33954\n88                                            TREMONT ST -71.06312 42.35541\n89  252 S HUNTINGTON AVE\\nJAMAICA PLAIN  MA 02130\\nUNITE -71.11231 42.32425\n90                                             CORONA ST -71.06896 42.30146\n91                                              BEACH ST -71.06248 42.35153\n92                                        DORCHESTER AVE -71.05669 42.31661\n93                                           MCLELLAN ST -71.08360 42.29946\n94                                          GREENWOOD ST -71.07975 42.30476\n95                                          MELBOURNE ST -71.06468 42.28440\n96                                               ERIE ST -71.07976 42.30272\n97                                           NORFOLK AVE -71.06894 42.32482\n98                                         WASHINGTON ST -71.07170 42.29132\n99                                             GORDON ST -71.13996 42.34967\n100    1 FOREST PL\\nCHARLESTOWN  MA 02129\\nUNITED STATES -71.06770 42.38000\n101                                             RIVER ST -71.09461 42.26726\n102   168 N BEACON ST\\nBRIGHTON  MA 02135\\nUNITED STATES -71.14692 42.35560\n103                                          N BEACON ST -71.15104 42.35666\n104                                            CENTRE ST -71.10328 42.32291\n105                                          LAGRANGE ST -71.06290 42.35123\n106                                            SUMNER ST -71.03925 42.36866\n107                                         HARRISHOF ST -71.08874 42.31688\n108 GORDON ST & RIDGEMONT ST\\nBOSTON  MA 02134\\nUNITED S -71.14009 42.35149\n109                                            CASTLE CT -71.06761 42.34519\n110      441 W BROADWAY\\nBOSTON  MA 02127\\nUNITED STATES -71.04665 42.33611\n111                                          STANWOOD ST -71.07215 42.32104\n112                                          CUMMINS HWY -71.11571 42.27833\n113                                           TREMONT ST -71.06214 42.35638\n114                                            WINTER ST -71.06178 42.35602\n115    40 GIBSON ST\\nDORCHESTER  MA 02122\\nUNITED STATES -71.05971 42.29756\n116                                        WASHINGTON ST -71.07959 42.33439\n117                                     S HUNTINGTON AVE -71.08563 42.32866\n118 GENEVA AVE & TOPLIFF ST\\nDORCHESTER  MA 02124\\nUNITE -71.06753 42.30109\n119 2400 WASHINGTON ST\\nROXBURY  MA 02119\\nUNITED STATES -71.08563 42.32866\n120 2400 WASHINGTON ST\\nROXBURY  MA 02119\\nUNITED STATES -71.08563 42.32866\n121 101 W BROADWAY\\nSOUTH BOSTON  MA 02127\\nUNITED STATE -71.05468 42.34129\n122                                         GLENBURNE ST -71.10795 42.30600\n123                                    MASSACHUSETTS AVE -71.08696 42.34570\n124                                            KEMBLE ST -71.07467 42.32983\n125 301 WASHINGTON ST\\nBRIGHTON  MA 02135\\nUNITED STATES -71.15050 42.34906\n126                                          SHAWMUT AVE -71.08145 42.33493\n127                                         WESTVILLE ST -71.06584 42.30139\n128                                            DITSON ST -71.06391 42.30108\n129 ALBANY ST & MASSACHUSETTS AVE\\nBOSTON  MA 02118\\nUNI -71.07351 42.33350\n130                                            CENTRE ST -71.10088 42.32469\n131                                           CHELSEA ST -71.03666 42.37172\n132                                            LONDON ST -71.03946 42.37288\n133    840 HARRISON AVE\\nBOSTON  MA 02118\\nUNITED STATES -71.07436 42.33556\n134                                        BLUE HILL AVE -71.08262 42.30938\n135                                              MILK ST -71.05289 42.35943\n136 HOLYOKE ST & CARLETON ST\\nBOSTON  MA 02116\\nUNITED S -71.07850 42.34518\n137                                     SAINT BOTOLPH ST -71.08058 42.34507\n138 MAVERICK SQ & MERIDIAN ST\\nEAST BOSTON  MA 02128\\nUN -71.03891 42.37014\n139                                         GREENWOOD ST -71.07955 42.30176\n140                                       HUNTINGTON AVE -71.09527 42.33792\n141         100 ARCH ST\\nBOSTON  MA 02110\\nUNITED STATES -71.05861 42.35487\n142                                         LEXINGTON ST -71.03744 42.37774\n143 MONTCALM AVE & MURDOCK ST\\nBRIGHTON  MA 02135\\nUNITE -71.14651 42.35273\n144                                           DEERING RD -71.09137 42.28483\n145                                        WASHINGTON ST -71.06256 42.35273\n146                                          STANHOPE ST -71.06941 42.33954\n147                                               FENWAY -71.09688 42.33728\n148 GREENBRIER ST & TONAWANDA ST\\nDORCHESTER  MA 02124\\n -71.07075 42.29766\n149 E SIXTH ST & M ST\\nSOUTH BOSTON  MA 02127\\nUNITED ST -71.03328 42.33316\n150 BRIGHTON AVE & CHESTER ST\\nBRIGHTON  MA 02134\\nUNITE -71.12836 42.35261\n151                                          LAGRANGE ST -71.06354 42.35157\n152                                         LYNDHURST ST -71.05925 42.31354\n153                                            SUMNER ST -71.03929 42.37082\n154                                          STANHOPE ST -71.09137 42.28483\n155                                           WALNUT AVE -71.09573 42.31286\n156                                           LEYLAND ST -71.07068 42.32066\n157                                    MASSACHUSETTS AVE -71.06461 42.32412\n158                                           DEERING RD -71.09305 42.28414\n159                                       SAVIN HILL AVE -71.05850 42.31281\n160                                        WASHINGTON ST -71.05976 42.36184\n161                                          ESTRELLA ST -71.10285 42.32243\n162                                        WASHINGTON ST -71.06276 42.35174\n163                                         N HARVARD ST -71.13020 42.36151\n164                                           TREMONT ST -71.06941 42.33954\n165                                        WASHINGTON ST -71.06643 42.34335\n166                                         HARRISHOF ST -71.09640 42.33167\n167                                           TALBOT AVE -71.07487 42.29106\n168                                            STUART ST -71.06400 42.35094\n169 HARVARD ST & CHAMBERLAIN ST\\nBOSTON  MA 02121\\nUNITE -71.07522 42.29890\n170                                           ASHMONT ST -71.06489 42.28556\n171                                           RITCHIE ST -71.09684 42.32200\n172                                        WASHINGTON ST -71.11985 42.29422\n173                                            ALBANY ST -71.05976 42.36184\n174                                        GILMER STREET -71.09722 42.28281\n175                                        WASHINGTON ST -71.08373 42.33044\n176                                            STUART ST -71.05976 42.36184\n177                                             RIVER ST -71.12317 42.25593\n178  ADAMS ST & LYON ST\\nBOSTON  MA 02122\\nUNITED STATES -71.06178 42.30601\n179                                          FRANKLIN ST -71.05928 42.35649\n180                                       SOUTHAMPTON ST -71.05696 42.33014\n181                                       HUNTINGTON AVE -71.10660 42.33333\n182                                           NEWBURY ST -71.07452 42.35176\n183                                         BRIGHTON AVE -71.12974 42.35267\n184                                           NEWBURY ST -71.08103 42.35000\n185 HEATH ST & BICKFORD ST\\nJAMAICA PLAIN  MA 02130\\nUNI -71.10134 42.32641\n186                                          BOYLSTON ST -71.06941 42.33954\n187                                          FREEPORT ST -71.05971 42.29756\n188                                            SCHOOL ST -71.06010 42.35789\n189                                        BROOKLINE AVE -71.10180 42.34411\n190                                     INTERNATIONAL PL -71.05234 42.35605\n191                                            SUMMER ST -71.05830 42.35388\n192                                       HUNTINGTON AVE -71.09925 42.33636\n193                             WILLIAM T MORRISSEY BLVD -71.04854 42.29682\n194                                         W CONCORD ST -71.07769 42.34072\n195                                          BOYLSTON ST -71.11822 42.35295\n196                                           TREMONT ST -71.05976 42.36184\n197                                       CLAREMONT PARK -71.08118 42.34210\n198                                     SAINT BOTOLPH ST -71.08495 42.34148\n199                                         HOLWORTHY ST -71.08923 42.31632\n200     711 BOYLSTON ST\\nBOSTON  MA 02116\\nUNITED STATES -71.08015 42.34932\n201                                            NORTON ST -71.06749 42.30522\n202                                       DORCHESTER AVE -71.06315 42.28990\n203                                               BOW ST -71.11786 42.24411\n204                                         E COTTAGE ST -71.05779 42.31853\n205                                            WARREN ST -71.08254 42.31697\n206                                              HIGH ST -71.05194 42.35662\n207                             WILLIAM T MORRISSEY BLVD -71.04674 42.29080\n208                                        WASHINGTON ST -71.08373 42.33044\n209 2400 WASHINGTON ST\\nROXBURY  MA 02119\\nUNITED STATES -71.08563 42.32866\n210                                        MERCHANTS ROW -71.05557 42.35930\n211                                         ATHELWOLD ST -71.07480 42.29647\n212                                       SOUTHAMPTON ST -71.07014 42.33211\n213                                                    B -71.05533 42.33962\n214                                         GREENWOOD ST -71.07176 42.29213\n215                                            BORDER ST -71.04052 42.37383\n216                                          FREEPORT ST -71.04902 42.29326\n217                                          E EIGHTH ST -71.04865 42.33130\n218 HOMES AVE & GENEVA AVE\\nBOSTON  MA 02122\\nUNITED STA -71.07020 42.30267\n219 WASHINGTON ST & WESTMINSTER AVE\\nROXBURY  MA 02119\\n -71.09706 42.31705\n220 BOYLSTON ST & DARTMOUTH ST\\nBOSTON  MA 02116\\nUNITED -71.07732 42.35010\n221                                          TRUMAN PKWY -71.12630 42.24125\n222                                         HARRISON AVE -71.07561 42.33455\n223                                        WASHINGTON ST -71.07179 42.29258\n224 ASHMONT ST & WASHINGTON ST\\nDORCHESTER  MA 02124\\nUN -71.07119 42.28534\n225                             FANEUIL HALL MARKETPLACE -71.05296 42.36160\n226                                          W NEWTON ST -71.07954 42.34386\n227 QUINCY ST & BLUE HILL AVENUE\\nBOSTON  MA 02125\\nUNIT -71.07882 42.31461\n228                                        WASHINGTON ST -71.05928 42.35649\n229  151 GENEVA AVE\\nDORCHESTER  MA 02121\\nUNITED STATES -71.07780 42.30575\n230                                      ANNUNCIATION RD -71.10300 42.33356\n231                                        WILLOWWOOD ST -71.09351 42.27219\n232                                        WASHINGTON ST -71.05850 42.35724\n233                                           TREMONT ST -71.06312 42.35541\n234                                        BLUE HILL AVE -71.09286 42.27968\n235                                    HENRY STERLING SQ -71.05488 42.32649\n236 MAVERICK ST & CHELSEA ST\\nEAST BOSTON  MA 02128\\nUNI -71.03872 42.37004\n237                                         WALK HILL ST -71.09755 42.28083\n238                                     BELLEVUE HILL RD -71.14929 42.27583\n239                                           BALLOU AVE -71.08563 42.32866\n240 CONGRESS ST & HANOVER ST\\nBOSTON  MA 02109\\nUNITED S -71.05758 42.36156\n241                                           FANEUIL ST -71.15050 42.34906\n242                                        WASHINGTON ST -71.12929 42.28537\n243                                        WASHINGTON ST -71.14822 42.28709\n244                                          PERCIVAL ST -71.06615 42.31558\n245                                           CHELSEA ST -71.03382 42.37416\n246                                     MELNEA CASS BLVD -71.08182 42.33330\n247                                       SAVIN HILL AVE -71.05850 42.31281\n248     1 DEACONESS RD\\nROXBURY  MA 02215\\nUNITED STATES -71.10880 42.33826\n249                                         ATLANTIC AVE -71.05642 42.35083\n250 WASHINGTON ST & WEST ST\\nBOSTON  MA 02111\\nUNITED ST -71.06237 42.35358\n251                                       HUNTINGTON AVE -71.08577 42.34222\n252                                            BEACON ST -71.05976 42.36184\n\n\nNow, we have all the data required to plot.\nI have chosen a icon which will act as a marker to help in locating the street where the crime has taken place on the map. Also, the map can we zoom IN and zoom OUT and when we click on the pointer we can see the name of the street.\n\nico <- makeIcon(iconUrl = \"https://cdn.iconscout.com/icon/free/png-256/drugs-26-129384.png\",iconWidth=47/2, iconHeight=41/2)\nmap2 <- leaflet()\nmap2 <- addTiles(map2)\nmap2 <- addMarkers(map2, data = map_drug, icon = ico, popup = map_drug[,\"STREET\"])\nmap2"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#interpretation-11",
    "href": "posts/NeeharikaKaranam_FinalProject.html#interpretation-11",
    "title": "Final Project",
    "section": "Interpretation",
    "text": "Interpretation\nI have chosen this map view in order to help understand where exactly the crime is concentrated i.e, which parts of the Boston Metro region so as to understand the safer streets in comparison. This map also helps us understand which streets are densely populated with the crimes. From the above map we can observe that most of the crime is densely present in the northern part of the Boston Metro and Southern part has much less crimes. As per our dataset description and prediction we can visually see it on the map the South-western parts of the Boston Metro have much less crime and is safer than the streets on the North or the North-eastern parts of the Boston metro."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#reflection",
    "href": "posts/NeeharikaKaranam_FinalProject.html#reflection",
    "title": "Final Project",
    "section": "Reflection",
    "text": "Reflection\nThis is my first time working with R and I am truly impressed with the various possibilities in terms of the visualizations and analysis. Being a Computer Science student R plays an extremely important part in Data Science and we can run the code in R without the help of any compiler because R is an interpreted language. Data cleaning is an essential task when it comes to analyze a dataset as the dataset may contain dirty data or NULL values or there might be some columns that are completely irrelevant for the analysis. Therefore, cleaning the dataset is very important. As they help in understanding the various relationships between the variables and how one variable affects the value of the other variable. We can find the dependencies of the values in order to help understand the dataset and the possibilities of the visualizations.\nWhen I initially chose the Boston Metro crime data I expected it to very straight forward and informative as is but when I kept diving into the dataset I have encountered the different kinds of crimes and the area where they have taken place. Thereby, making me interested in trying to find out which parts of the Boston Metro were safe and which ones were not. I would like to say that the Boston Metro dataset has left me with some interesting findings from the visualizations.\nI have started out by initially trying to understand the different columns in the dataset and understand what each one of them are reflecting. Then I started checking if there are any NULL values in the dataset and changed the class of the columns. I have also replaced the values of the columns in the dataset in order to increase the understandability of the dataset and thereby help us in a better visualization and analysis. I have then made my visualizations on the dataset by starting out with most common crimes categories of the Boston Metro and then analyse in-dept based on the year, month, time and day of the week and make my own analysis from the dataset.\nThis project has been very interesting and challenging at the same time as I wanted to understand the various types of visualizations and how they help us in our analysis. I have done my research in trying to find out some interesting visualizations like the density graph and on how to plot the crimes that have taken place in a particular area onto the map. It was quite challenging for me to understand and interpret the same in my dataset but I had lots of fun doing it. This class has been extremely helpful to me and helped me learn in perform different kinds of analysis."
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#conclusion",
    "href": "posts/NeeharikaKaranam_FinalProject.html#conclusion",
    "title": "Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nLet us start with the different number of crime categories that are present in the Boston Metro region and I have observed that there are 257 different crime categories. I have also observed that there are some of the categories with very less reported records in the past 6 years which do not provide much information. I have then found the top 10 crime categories of the Boston metro and found that the INVESTIGATE PERSON crime category tops the list of crimes.\nI have found that the crimes per year have been decreasing over the past 6 years and there is a significant decrease from 2017-2020 whereas there is an extremely slight increase in the number of cases for 2020-2021. I have not considered the year 2022 because there is only data for the first 3 months and it will not be helpful in analyzing the data year-wise.\nInterestingly, I have also observed that there is a low crime rate in the holiday months of the year namely December and November when compared to the other months. Also, there is a very high crime rate in the month after the holiday season. The crime rate is relatively higher during the weekdays than the weekends for the majority of the crime categories which was surprising.\nWhen do most of the crimes take place? They are very specific to each of the crime categories as the INVESTIGATE PERSON takes place majorly in the evening whereas the LARCENY SHOPLIFTING takes place during the business working hours where most of the shops/malls are open. This is very specific to each of the crime categories as they all are from different genres and they take place during different timings.\nI was very interested in trying to understand in which parts of the Boston Metro most of the crime takes place. As this will help us understand which streets are safer when compared to the others. I have plotted the crimes using the markers on the graph to help us understand the streets with the higher crime in comparison to the other streets. I have observed that the Northern part or the North-eastern part of the city is densely populated with the various crimes whereas the Southern part or the South-western part of the Boston Metro are much safer. Even after all of the analysis there are still a few questions that are not answered. How does the region and a specific district related to each other? Which crime categories have reduced over time?"
  },
  {
    "objectID": "posts/NeeharikaKaranam_FinalProject.html#bibliography",
    "href": "posts/NeeharikaKaranam_FinalProject.html#bibliography",
    "title": "Final Project",
    "section": "Bibliography",
    "text": "Bibliography\n\nhttp://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Density%20Plot - for the various kinds of graphs.\nhttps://www.kaggle.com/datasets/shivamnegi1993/boston-crime-dataset-2022 - Boston crime dataset.\nhttps://plotly.com/r/ - Plotly R Open Sourcing Graphing Library\nWickham, H., & Grolemund, G. (2016). R for data science: Visualize, model, transform, tidy, and import data. OReilly Media. - Textbook\nWickham, H. (2019). Advanced R. Chapman and Hall/CRC. - Textbook\nWickham, H. (2010). A layered grammar of graphics. Journal of Computational I and Graphical Statistics, 19(1), 3-28. - Textbook"
  },
  {
    "objectID": "posts/NikitaMasanagi_Final_Project.html",
    "href": "posts/NikitaMasanagi_Final_Project.html",
    "title": "San Francisco Crime Data Exploration",
    "section": "",
    "text": "Dataset Description\n\nThe dataset is San Francisco dataset, from 2018 to present. The description of each column is -\nIncident Datetime - The date and time when the incident occurred\nIncident Date - The date the incident occurred\nIncident Time- The time the incident occurred\nIncident Year - The year the incident occurred, provided as a convenience for filtering\nIncident Day of Week - The day of week the incident occurred\nReport Datetime - Distinct from Incident Datetime, Report Datetime is when the report was filed.\nRow ID - A unique identifier for each row of data in the dataset\nIncident ID - This is the system generated identifier for incident reports. Incident IDs and Incident Numbers both uniquely identify reports, but Incident Numbers are used when referencing cases and report documents.\nIncident Number - The number issued on the report, sometimes interchangeably referred to as the Case Number. This number is used to reference cases and report documents.\nCAD Number - The Computer Aided Dispatch (CAD) is the system used by the Department of Emergency Management (DEM) to dispatch officers and other public safety personnel. CAD Numbers are assigned by the DEM system and linked to relevant incident reports (Incident Number).\nReport Type Code - A system code for report types, these have corresponding descriptions within the dataset.\nReport Type Description - The description of the report type, can be one of: Initial; Initial Supplement; Vehicle Initial; Vehicle Supplement; Coplogic Initial; Coplogic Supplement\nFiled Online - Non- emergency police reports can be filed online by members of the public using SFPD’s self-service reporting system called Coplogic Values in this field will be “TRUE” if Coplogic was used to file the report.\nIncident Code - Incident Codes are the system codes to describe a type of incident. A single incident report can have one or more incident types associated. In those cases you will see multiple rows representing a unique combination of the Incident ID and Incident Code.\nIncident Category - A category mapped on to the Incident Code used in statistics and reporting. Mappings provided by the Crime Analysis Unit of the Police Department.\nIncident Subcategory - A subcategory mapped to the Incident Code that is used for statistics and reporting. Mappings are provided by the Crime Analysis Unit of the Police Department.\nIncident Description - The description of the incident that corresponds with the Incident Code. These are generally self-explanatory.\nResolution - The resolution of the incident at the time of the report. Can be one of: • Cite or Arrest Adult • Cite or Arrest Juvenile* • Exceptional Adult • Exceptional Juvenile* • Open or Active • Unfounded Juvenile information not maintained in the dataset.\nIntersection - The 2 or more street names that intersect closest to the original incident separated by a backward slash.\nCNN - The unique identifier of the intersection for reference back to other related basemap datasets.\nPolice District - The Police District where the incident occurred. District boundaries can be reviewed in the link below.\nAnalysis Neighborhood - This field is used to identify the neighborhood where each incident occurs. Neighborhoods and boundaries are defined by the Department of Public Health and the Mayor’s Office of Housing and Community Development.\nSupervisor District - There are 11 members elected to the Board of Supervisors in San Francisco, each representing a geographic district. The Board of Supervisors is the legislative body for San Francisco. The districts are numbered 1 through 11.\nLatitude - The latitude coordinate in WGS84, spatial reference is EPSG:4326\nLongitude - The longitude coordinate in WGS84, spatial reference is EPSG:4326\nPoint - Geolocation in OGC WKT format (e.g, POINT(37.4,-122.3)\nAs we can see that this dataset is large, with many fields.To perform some infer analysis and infer from the data, we will need to perform some cleaning, tidying and identify which of these fields can be beneficial to us.\nAs we can see that this dataset is large, with many fields.To perform some infer analysis and infer from the data, we will need to perform some cleaning, tidying and identify which of these fields can be beneficial to us.\n###Dataset\nReading in the data\n\n# Read data from the csv file\ndata <- read.csv(\"_data/SF_Incident_Reports__2018_to_Present.csv\")\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nThe dimensions of the data\n\ndim(data)\n\nNULL\n\n\nThe datatype of each column\n\nstr(data)\n\nfunction (..., list = character(), package = NULL, lib.loc = NULL, verbose = getOption(\"verbose\"), \n    envir = .GlobalEnv, overwrite = TRUE)  \n\n\nNow let us check how many null values are present in the dataset.\n\nsum(is.na(data))\n\n[1] 0\n\n\nLet us check which columns have how many missing values.\n\nmissing_vals <- colSums(is.na(data))\n\nError in colSums(is.na(data)): 'x' must be an array of at least two dimensions\n\nmissing_vals[sapply(missing_vals, function(x) any(x > 0))]\n\nError in eval(expr, envir, enclos): object 'missing_vals' not found\n\n\nAs we can see, there are some columns with more than 5 lakh nulll values. We can handle that when we are cleaning the data\nDataframe summary\n\nprint(summarytools::dfSummary(data,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\nError in seq_len(ncol(x)): argument must be coercible to non-negative integer\n\n\n\nhead(data)\n\n                                                                            \n1 function (..., list = character(), package = NULL, lib.loc = NULL,        \n2     verbose = getOption(\"verbose\"), envir = .GlobalEnv, overwrite = TRUE) \n3 {                                                                         \n4     fileExt <- function(x) {                                              \n5         db <- grepl(\"\\\\\\\\.[^.]+\\\\\\\\.(gz|bz2|xz)$\", x)                     \n6         ans <- sub(\".*\\\\\\\\.\", \"\", x)                                      \n\n\n\nData Cleaning\nThe columns below have maximum of null values and wont be much benefical to us for analysis. Hence we can drop them. Civic.Center.Harm.Reduction.Project.Boundary HSOC.Zones.as.of.2018.06.05 Invest.In.Neighborhoods..IIN..Areas ESNCAG…Boundary.File Central.Market.Tenderloin.Boundary.Polygon…Updated Civic.Center.Harm.Reduction.Project.Boundary\n\ncleaned_data <- select(data,-28,-29,-30,-31,-32,-33,-34)\n\nError in UseMethod(\"select\"): no applicable method for 'select' applied to an object of class \"function\"\n\nhead(cleaned_data)\n\nError in head(cleaned_data): object 'cleaned_data' not found\n\n\n###Mutate data\nWe can split the column Incident Time column into Minute and hour, so we can manipulate the hour column in further analysis.\n\nmutated_data <- cleaned_data %>%\n  separate('Incident.Time',c('Incident.Hour','Incident.Minute'),sep = \":\")\n\nError in separate(., \"Incident.Time\", c(\"Incident.Hour\", \"Incident.Minute\"), : object 'cleaned_data' not found\n\nmutated_data <- mutated_data %>% \n  mutate(Incident.Month = month(Incident.Datetime, label = T))\n\nError in mutate(., Incident.Month = month(Incident.Datetime, label = T)): object 'mutated_data' not found\n\n\n\ncrime <- mutated_data\n\nError in eval(expr, envir, enclos): object 'mutated_data' not found\n\n\n###QUESTIONS:\nAfter observing the data, certain questions pop up in my mind.We can make observations about crime has increased over the years, how COVID has affected the crime rate, the district wise distribution of crime, what is the most frequent resolution in different districts,what are the top crimes and what time does it occur,what are the crime hotspots.\nI will now go over these questions and try to plot interesting graphs so we can get some inference from it.\n##Crime over the years:\n#a)Distribution of crime over the years from 2018 - Present.\nWe can group by the Incident Year and plot the graph.\n\ncrime_per_year <- crime %>% \n  group_by(Incident.Year) %>% \n  summarise(total = n())\n\nError in group_by(., Incident.Year): object 'crime' not found\n\ncolnames(crime_per_year) <- c(\"Year\",\"Total\") \n\nError in colnames(crime_per_year) <- c(\"Year\", \"Total\"): object 'crime_per_year' not found\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\n# Plot\ng <- ggplot(crime_per_year, aes(Year, Total))\n\nError in ggplot(crime_per_year, aes(Year, Total)): object 'crime_per_year' not found\n\ng + geom_bar(stat=\"identity\", width = 0.5, fill=\"blue\") + \n      labs(title=\"Crime in San Francisco\", \n           subtitle=\"2018-2022\", \n           caption=\" Total crime over the years\") +\n      theme(axis.text.x = element_text(angle=65, vjust=0.6))\n\nError in eval(expr, envir, enclos): object 'g' not found\n\n\nFrom this graph we can observe that the crime has reduced during COVID years of 2020-2021, which can be an indicator that either the crime had reduced due to lockdown conditions or the reported crime had reduced during those years. We can also observe the crime decreasing slightly from 2018 to Present day.\n#b) Distribution of crime per category\nWe can create a dataframe, by grouping by on the basis of Incident Category and also calculate the percentage.\n\ncrime_category <- sort(table(crime$Incident.Category),decreasing = TRUE)\n\nError in table(crime$Incident.Category): object 'crime' not found\n\ncrime_category <- data.frame(crime_category[crime_category> 10000])\n\nError in data.frame(crime_category[crime_category > 10000]): object 'crime_category' not found\n\ncolnames(crime_category) <- c(\"Category\", \"Frequency\")\n\nError in colnames(crime_category) <- c(\"Category\", \"Frequency\"): object 'crime_category' not found\n\ncrime_category$Percentage <- crime_category$Frequency / sum(crime_category$Frequency)\n\nError in eval(expr, envir, enclos): object 'crime_category' not found\n\n\n\ncrime_category\n\nError in eval(expr, envir, enclos): object 'crime_category' not found\n\n\n\nlibrary(ggplot2)\nlibrary(ggrepel)\nbp<-ggplot(crime_category, aes(x=Category, y=Frequency, fill=Category)) + geom_bar(stat=\"identity\") + \n  theme(axis.text.x=element_blank()) + geom_text_repel(data=crime_category, aes(label=Category))\n\nError in ggplot(crime_category, aes(x = Category, y = Frequency, fill = Category)): object 'crime_category' not found\n\nbp\n\nError in eval(expr, envir, enclos): object 'bp' not found\n\n\nFrom the plot we can observe that count of Larceny Theft, is much much greater than the other categories.Disorderly conduct is the least frequent category.\n#c) Time Series graph for Daily Crimes from 2018-2022\nWe can mutate the date and group by date, to get the number of crimes each day.\n\nlibrary(dplyr)\n\ndf_crime_daily <- crime %>%\n  mutate(Date = as.Date(Incident.Date, \"%Y/%m/%d\")) %>%\n  group_by(Date) %>%\n  summarize(count = n()) %>%\n  arrange(Date)\n\nError in mutate(., Date = as.Date(Incident.Date, \"%Y/%m/%d\")): object 'crime' not found\n\nhead(df_crime_daily)\n\nError in head(df_crime_daily): object 'df_crime_daily' not found\n\n\n\nlibrary(ggplot2)\nlibrary(scales)\nplot <- ggplot(df_crime_daily, aes(x = Date, y = count)) +\n  geom_line(color = \"#F2CA27\", size = 0.1) +\n  geom_smooth(color = \"#1A1A1A\") +\n  # fte_theme() +\n  scale_x_date(breaks = date_breaks(\"1 year\"), labels = date_format(\"%Y\")) +\n  labs(x = \"Date of Crime\", y = \"Number of Crimes\", title = \"Daily Crimes in San Francisco from 2018 – 2022\")\n\nError in ggplot(df_crime_daily, aes(x = Date, y = count)): object 'df_crime_daily' not found\n\nplot\n\nfunction (x, y, ...) \nUseMethod(\"plot\")\n<bytecode: 0x000002760bdd8520>\n<environment: namespace:base>\n\n\nWe can again oberve the dip during the COVID lockdown period, and in general observe around 300-500 crimes per day. With unual low as well as high spikes.\n##Crime in District -wise\n#a) District wise Crime Distribution\nWe can create a dataframe by grouping by the Police district.\n\ncrime_per_district <- crime %>% \n  group_by(Police.District) %>% \n  summarise(n = n())\n\nError in group_by(., Police.District): object 'crime' not found\n\ncolnames(crime_per_district) <- c(\"Police.District\", \"Total\")\n\nError in colnames(crime_per_district) <- c(\"Police.District\", \"Total\"): object 'crime_per_district' not found\n\nhead(crime_per_district )\n\nError in head(crime_per_district): object 'crime_per_district' not found\n\n\n\ng <- ggplot(crime_per_district, aes(Police.District, Total))\n\nError in ggplot(crime_per_district, aes(Police.District, Total)): object 'crime_per_district' not found\n\ng + geom_bar(stat=\"identity\", width = 0.5, fill=\"pink\") + \n      labs(title=\"District-wise crime in San Francisco\", \n           subtitle=\"2018-2022\", \n           caption=\" Total crime in each district\") +\n      theme(axis.text.x = element_text(angle=65, vjust=0.6))\n\nError in eval(expr, envir, enclos): object 'g' not found\n\n\nFrom the graph, we can see Central having maximum number of crime and Out of SF is the least.Park can be said to be the safest district as it has the least amount of crime.\n#b) Resolution rates in each district\n\ng <- ggplot(crime, aes(Police.District))\n\nError in ggplot(crime, aes(Police.District)): object 'crime' not found\n\ng + geom_bar(aes(fill=Resolution), width = 0.5) + \n  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +\n  labs(title=\"Resolution in each District\", \n     )\n\nError in eval(expr, envir, enclos): object 'g' not found\n\n\nFrom this graph, we can observe that maximum number of cases in all the districts are open or active.Tenderloin has the most arrests and Park has the least. There are some unfounded resolutions in each District.\n##Temporal Trends\n#a) Hourly Crime Distribution\nWe can create a dataframe by grouping by Incident Hour.\n\ncrime_perhours <- crime %>% \n  group_by(Incident.Hour) %>% \n  summarise(TotalCrime = n())\n\nError in group_by(., Incident.Hour): object 'crime' not found\n\nhead(crime_perhours)\n\nError in head(crime_perhours): object 'crime_perhours' not found\n\n\n\nggplot(crime_perhours, aes(x = Incident.Hour, y = TotalCrime))+\n  geom_col(fill = \"orange\")+\n  theme_minimal()+\n  labs(\n    title = \"Crime per hour, San Francisco 2018 - 2022\",\n    x = \"Hours\",\n    y = \"Total Crime\"\n  )\n\nError in ggplot(crime_perhours, aes(x = Incident.Hour, y = TotalCrime)): object 'crime_perhours' not found\n\n\nWe can observe that maximum crimes occur around 12 in the afternoon, then another spike at midnight. It then gradually decreases from 1 am - 7 am in the morning.\n#b) Theft time Heatmap\nOf the above hourly districution, let us focus on Larcent Theft and plot the heatmap.\n\ndf_theft_time <- crime %>%\n  filter(Incident.Category==\"Larceny Theft\")%>%\n  group_by(Incident.Day.of.Week, Incident.Hour) %>%\n  summarize(count = n())\n\nError in filter(., Incident.Category == \"Larceny Theft\"): object 'crime' not found\n\nhead(df_theft_time)\n\nError in head(df_theft_time): object 'df_theft_time' not found\n\n\n\nplot <- ggplot(df_theft_time, aes(x = Incident.Hour, y = Incident.Day.of.Week, fill = count)) +\n  geom_tile() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.6), legend.title = element_blank(), legend.position=\"top\", legend.direction=\"horizontal\", legend.key.width=unit(2, \"cm\"), legend.key.height=unit(0.25, \"cm\"), legend.margin=unit(-0.5,\"cm\"), panel.margin=element_blank()) +\n  labs(x = \"Hour of Theft (Local Time)\", y = \"Day of Week of Theft\", title = \"Number of Thefts in San Francisco from 2018 – 2022, by Time of Theft\") +\n  scale_fill_gradient(low = \"white\", high = \"orange\")\n\nError in ggplot(df_theft_time, aes(x = Incident.Hour, y = Incident.Day.of.Week, : object 'df_theft_time' not found\n\nplot\n\nfunction (x, y, ...) \nUseMethod(\"plot\")\n<bytecode: 0x000002760bdd8520>\n<environment: namespace:base>\n\n\nFrom this heatmap, we see a spike in crimes over the weekends in the midnight. We can also seespikes during weekdays around the evenig time from 5pm-8-pm.\n##Crime hotspots\n#a) Map of San Francisco\n\nlibrary(ggmap)\n\nsf = get_stamenmap(bbox = c(left = -122.5164, bottom = 37.7066, right = -122.3554, top = 37.8103), \nmaptype = c(\"toner-lite\"), zoom = 13)\n\nmap = ggmap(sf)\nmap\n\n\n\n\n#b) Plotting the first 500 random points on the map\n\nmap + geom_point(data = sample_n(crime, 500), aes(x = Longitude, y = Latitude))\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 4th layer.\nCaused by error in `FUN()`:\n! object 'Longitude' not found\n\n\nWe can already observe a clustering towards the city center.\n#c) Density Plot\n\nmap + \nstat_density2d( data = sample_frac(crime, 0.2), aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..), size = 1, bins = 50, geom = 'polygon') +\nscale_fill_gradient('Crime\\nDensity', low = 'blue', high = 'orange') +\nscale_alpha(range = c(.2, .3), guide = FALSE) +\nguides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))\n\nError in `stat_density2d()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 4th layer.\nCaused by error in `FUN()`:\n! object 'Longitude' not found\n\n\nWe can observe certain intersections and having higher density and the maximum density being at the city center area.The parks and grassland regions have no reported crimes.\n###Conclusion\nThus with the help of the SAN FRANCISCO crime dataset, we could plot thee graphs and gain some insight."
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html",
    "href": "posts/OwenTibbyFinalProject601_submission.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(distill)\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(summarytools)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(vindecodr)\n\n\nError in library(vindecodr): there is no package called 'vindecodr'\n\n\nCode\nlibrary(plotrix)\nlibrary(sparklyr)\n\n\nError in library(sparklyr): there is no package called 'sparklyr'\n\n\nCode\nlibrary(purrr)\nlibrary(rmarkdown)\nlibrary(knitr)\nlibrary(psych) \n\n\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, paged.print= TRUE)"
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#bonus-section",
    "href": "posts/OwenTibbyFinalProject601_submission.html#bonus-section",
    "title": "Final Project",
    "section": "Bonus Section",
    "text": "Bonus Section\n\nHow does RV% compare among volume models?\n\n\nCode\n#Volume models\n\nvol.models<- carprices %>% \n  filter( Age_years >0) %>% \n  group_by(model) %>% \n  tally() %>% \n  arrange(desc(n)) %>% \n  ungroup() %>% \n  slice(1:10)\n\nvol.models.passenger <- carprices %>% \n  filter(Age_years >0, Body_Type== passenger) %>% \n  group_by(model) %>% \n  tally() %>% \n  arrange(desc(n)) %>% \n  ungroup() %>% \n  slice(1:10)\n\npopular.models <-  vol.models.passenger$model\n\n\n\n\nCode\nvol.models.passenger %>% \n  ggplot()+ aes(x=reorder(model, -n), y= n) +\n  geom_bar(stat = \"identity\", fill= \"blue\") +\n  labs(x= \"Model\", y=\"Number of Transactions\")+\n  ggtitle(\"Graph 5.1: Most Auctioned Models\")\n\n\n\n\n\nFrom Graph 5.1, it is easy to assume that the Nissan Altima is a highly sought-after vehicle and may therefore hold its value well. However, that may not be entirely true. Graph 5.2 will explore this further.\nDo the most auctioned vehicles retain the best value ?\n\n\nCode\n#Finding the models with the highest RV\n\nBVM <- carprices %>% filter(model== vol.models.passenger$model) %>% \n  filter(Age_years> 1, Age_years <6, Body_Type== passenger) %>% \n  group_by(make, model,Age_years,  RV_percent, `Error %`, odometer, condition, Segment) %>% tally() %>% \n  summarise(RV_percentage = mean(RV_percent), `Error %`=mean(`Error %`, na.rm=TRUE),  odometer=mean(odometer), condition=mean(condition), .groups = \"keep\") %>% tally() %>% \n  summarise(make=make, model=model, Age_years= Age_years, RV_percent= mean(RV_percent), `Error %`=mean(`Error %`, na.rm=TRUE), odometer=mean(odometer),condition=mean(condition),  n = sum(n), .groups = \"keep\") %>% ungroup() %>% \n   group_by(make, model, Age_years) %>% \n  summarise(make=make, model=model, Age_years= Age_years, RV_percent= mean(RV_percent),`Error %`=mean(`Error %`, na.rm=TRUE), mileage=mean(odometer), condition=mean(condition),  n = sum(n), .groups = \"keep\") %>%\n  slice(1:1) %>% arrange(desc(RV_percent)) %>%\n   ungroup() %>% \n  #Filtering a sample size 20 or more\n  filter(n>19) %>%\narrange((Age_years))\n(kable(BVM))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmake\nmodel\nAge_years\nRV_percent\nError %\nmileage\ncondition\nn\n\n\n\n\nFord\nEscape\n2\n59.80893\n5.371429\n32660.32\n4.105357\n56\n\n\nFord\nFocus\n2\n54.93243\n5.670270\n30007.46\n4.068919\n74\n\n\nFord\nFusion\n2\n53.94091\n5.018182\n26895.30\n4.066667\n66\n\n\nDodge\nCharger\n2\n52.56486\n4.002703\n27400.24\n3.859459\n37\n\n\nToyota\nCamry\n2\n51.63448\n5.879310\n32260.52\n3.751724\n29\n\n\nNissan\nAltima\n2\n50.76418\n4.607463\n32063.15\n3.797015\n67\n\n\nDodge\nGrand Caravan\n2\n43.03421\n6.200000\n43253.37\n3.742105\n38\n\n\nINFINITI\nG37\n2\n40.27436\n3.200000\n21123.33\n4.069231\n39\n\n\nChevrolet\nImpala\n2\n37.74390\n3.300000\n45675.93\n3.465854\n41\n\n\nFord\nEscape\n3\n51.96250\n5.818750\n39425.44\n3.896875\n32\n\n\nFord\nFocus\n3\n45.75909\n4.818182\n41420.00\n3.945454\n22\n\n\nToyota\nCamry\n3\n45.49048\n5.928571\n41978.05\n3.519048\n21\n\n\nFord\nFusion\n3\n44.69697\n5.978788\n45612.97\n3.896970\n33\n\n\nNissan\nAltima\n3\n41.21406\n4.793750\n38131.33\n3.482813\n64\n\n\nINFINITI\nG37\n3\n36.94400\n3.144000\n23883.64\n3.896000\n25\n\n\nChevrolet\nImpala\n3\n30.49630\n2.703704\n56683.44\n3.311111\n27\n\n\nFord\nFocus\n4\n44.11364\n3.872727\n51600.68\n3.709091\n22\n\n\nToyota\nCamry\n4\n43.64286\n5.735714\n39723.24\n3.280952\n42\n\n\nHonda\nAccord\n4\n40.65000\n3.453125\n40321.59\n3.281250\n32\n\n\nFord\nEscape\n4\n40.50000\n6.360606\n48104.48\n3.906061\n33\n\n\nNissan\nAltima\n4\n39.35612\n5.263265\n40394.43\n3.583674\n98\n\n\nFord\nFusion\n4\n38.64286\n6.714286\n54026.71\n3.592857\n56\n\n\nINFINITI\nG37\n4\n38.01042\n3.000000\n35770.04\n3.814583\n48\n\n\nChevrolet\nImpala\n4\n28.63704\n2.137037\n62979.44\n3.307407\n27\n\n\n\n\n\nGraph 5.2 shows that the most auctioned vehicles do not necessarily retain the best value. This may be due to an oversupply of these vehicles at auction, and with a higher supply, a lower price often follows.\nAnother reason could be low consumer sentiment toward these vehicles. After looking at the nature of the sellers, one can conclude that most of these vehicles are lease returns. Vehicles with high lease returns can imply a low consumer sentiment, therefore decreasing the long-term resale value.\n\n\nCode\nvol.p.models <- vol.models.passenger$model %>% as.factor()\nvol.vs.best.passenger <- BVM %>% filter(model== vol.p.models)\n\nBVM %>%\n  ggplot()+ aes(x=reorder(model, -RV_percent), y= RV_percent) +\n  geom_bar(stat = \"identity\",position = \"dodge\", fill= \"dark orange\" ) +\n  labs(x= \"Model\", y=\"Mean Retained Value %\")+\n  ggtitle(\"Graph 5.2: Retained values of the Most Auctioned Models\" , subtitle = \"Mean Retained Value Percentage from Year 2 to 5\")\n\n\n\n\n\n\n\nBest mainstream cars to buy post-2015\n\n\nCode\n#Finding the models with the highest RV\nbest_value_models <- carprices %>%\n  filter(Age_years> 1, Age_years <6, Body_Type== passenger, `Error %` < 5) %>% \n  group_by(make, model,Age_years,  RV_percent, `Error %`, odometer, condition, Segment) %>% tally() %>% \n  summarise(RV_percentage = mean(RV_percent), `Error %`=mean(`Error %`, na.rm=TRUE),  odometer=mean(odometer), condition=mean(condition), .groups = \"keep\") %>% tally() %>% \n  summarise(make=make, model=model, Age_years= Age_years, RV_percent= mean(RV_percent), `Error %`=mean(`Error %`, na.rm=TRUE), odometer=mean(odometer),condition=mean(condition),  n = sum(n), .groups = \"keep\") %>% ungroup() %>% \n   group_by(make, model, Age_years) %>% \n  summarise(make=make, model=model, Age_years= Age_years, RV_percent= mean(RV_percent),`Error %`=mean(`Error %`, na.rm=TRUE), mileage=mean(odometer), condition=mean(condition),  n = sum(n), .groups = \"keep\") %>%\n  slice(1:1) %>% arrange(desc(RV_percent)) %>%\n   ungroup() %>% \n  #Filtering a sample size 20 or more\n  filter(n>19) %>%\narrange((Age_years))\nprint(kable(best_value_models, caption= \"Passengers Cars with Best RV\"))\n\n\n\n\nTable: Passengers Cars with Best RV\n\n|make          |model             | Age_years| RV_percent|   Error %|  mileage| condition|   n|\n|:-------------|:-----------------|---------:|----------:|---------:|--------:|---------:|---:|\n|Jeep          |Wrangler          |         2|   86.01758| 4.3000000| 20580.66|  4.551648|  91|\n|BMW           |4 Series          |         2|   77.63913| 3.4000000| 10463.70|  4.413043|  23|\n|Jeep          |Cherokee          |         2|   75.13750| 2.5000000| 19183.85|  4.295833|  48|\n|Toyota        |Corolla           |         2|   73.25143| 3.7000000| 22960.35|  3.816190| 105|\n|GMC           |Acadia            |         2|   70.34545| 4.5000000| 24056.27|  4.281818|  22|\n|Nissan        |Versa Note        |         2|   70.17021| 4.7000000| 17958.79|  4.031915|  47|\n|Subaru        |Impreza           |         2|   70.00741| 2.9000000| 19390.85|  4.137037|  27|\n|Subaru        |Forester          |         2|   69.92857| 3.7178571| 29487.36|  4.185714|  28|\n|Subaru        |Outback           |         2|   69.26308| 3.8615385| 27320.11|  4.324615|  65|\n|Mercedes-Benz |GLK-Class         |         2|   68.75676| 1.7945946| 24537.11|  4.237838|  37|\n|Subaru        |Legacy            |         2|   68.14815| 4.5000000| 18589.44|  4.437037|  27|\n|Dodge         |Durango           |         2|   67.41754| 3.8701754| 28284.00|  4.330702| 114|\n|Honda         |CR-V              |         2|   66.99825| 4.6421053| 23874.30|  4.161403|  57|\n|Ford          |Escape            |         2|   66.94753| 4.8000000| 28125.73|  4.154321| 162|\n|Nissan        |Sentra            |         2|   66.43500| 4.9000000| 27945.42|  3.745000| 120|\n|INFINITI      |Q50               |         2|   66.01951| 4.0000000| 16612.83|  4.075610|  41|\n|Ford          |Expedition        |         2|   66.01094| 2.7125000| 30222.42|  4.381250|  64|\n|FORD          |FUSION            |         2|   65.81795| 4.7000000| 32849.51|  4.120513|  39|\n|INFINITI      |JX35              |         2|   65.78182| 0.0000000| 37006.95|  3.827273|  22|\n|Honda         |Civic             |         2|   65.74870| 4.4000000| 20880.17|  3.703478| 115|\n|Chevrolet     |Tahoe             |         2|   65.53418| 3.9341772| 43381.96|  3.941772|  79|\n|Honda         |Pilot             |         2|   65.27895| 4.8000000| 26278.13|  4.405263|  38|\n|Ford          |Explorer          |         2|   65.15935| 4.9000000| 37372.12|  4.013415| 248|\n|Toyota        |RAV4              |         2|   64.64468| 4.9000000| 43028.23|  3.844681|  47|\n|Jeep          |Compass           |         2|   63.02941| 3.1705882| 25850.37|  3.990196|  51|\n|Toyota        |Venza             |         2|   62.92727| 2.3181818| 37372.82|  3.795454|  22|\n|Lincoln       |MKX               |         2|   62.45714| 0.0000000| 24495.98|  4.382143|  56|\n|Jeep          |Patriot           |         2|   61.94444| 3.2875000| 27918.83|  3.979167|  72|\n|Honda         |Accord            |         2|   61.91981| 3.3000000| 21560.34|  3.904717| 106|\n|HYUNDAI       |SANTA FE          |         2|   61.59630| 4.3000000| 22150.04|  4.066667|  27|\n|Hyundai       |Accent            |         2|   60.31239| 2.7982301| 32131.84|  3.645133| 113|\n|Hyundai       |Santa Fe          |         2|   60.24667| 2.9500000| 34383.43|  4.243333|  30|\n|Ford          |Edge              |         2|   60.19865| 4.3407407| 29632.14|  4.168350| 297|\n|BMW           |3 Series          |         2|   59.97794| 3.5441176| 21352.10|  4.235294|  68|\n|Hyundai       |Veloster          |         2|   59.82917| 3.4250000| 19199.08|  3.962500|  24|\n|Ford          |Fiesta            |         2|   59.62087| 4.0404348| 28080.17|  3.996957| 230|\n|Chevrolet     |Cruze             |         2|   59.39677| 4.3705069| 32445.41|  3.726728| 434|\n|Lincoln       |MKZ               |         2|   59.17288| 3.6135593| 27934.53|  3.993220|  59|\n|Audi          |A4                |         2|   59.09655| 3.1000000| 23538.52|  4.113793|  29|\n|Chevrolet     |Suburban          |         2|   58.27467| 3.4400000| 45680.87|  3.742667|  75|\n|GMC           |Terrain           |         2|   58.20926| 2.9629630| 29662.24|  4.175926|  54|\n|Toyota        |Camry Hybrid      |         2|   58.01724| 2.4551724| 52920.24|  3.827586|  29|\n|Toyota        |Prius             |         2|   57.60000| 0.0000000| 39000.53|  3.828889|  45|\n|Kia           |Rio               |         2|   57.48706| 3.3482353| 24315.29|  3.796471|  85|\n|Ford          |Fusion            |         2|   56.95585| 4.7000000| 29392.94|  4.018729| 299|\n|Kia           |Forte             |         2|   56.76382| 4.5197368| 25516.53|  3.965790| 152|\n|Kia           |Soul              |         2|   56.64366| 4.4415493| 26490.04|  4.057042| 142|\n|Nissan        |Xterra            |         2|   56.34545| 2.6060606| 29998.09|  4.275758|  33|\n|Hyundai       |Elantra           |         2|   56.19311| 3.2101639| 32625.45|  3.751148| 305|\n|Jeep          |Grand Cherokee    |         2|   56.14301| 4.6709677| 28248.74|  4.338710|  93|\n|FIAT          |500L              |         2|   55.99667| 3.7000000| 15683.33|  3.946667|  30|\n|Nissan        |Pathfinder        |         2|   55.98182| 4.9000000| 38281.15|  3.878788|  33|\n|GMC           |SAVANA            |         2|   55.83457| 2.8000000|  9982.42|  4.019753|  81|\n|Dodge         |Dart              |         2|   55.76623| 3.0870130| 23011.52|  3.833766|  77|\n|Kia           |Sorento           |         2|   54.32987| 4.2000000| 37149.03|  4.176623|  77|\n|Toyota        |Avalon            |         2|   53.99524| 2.7714286| 28262.24|  3.676191|  21|\n|Chrysler      |300               |         2|   53.95328| 2.2934426| 20879.19|  4.071312| 122|\n|Toyota        |Sienna            |         2|   52.97969| 4.9000000| 50011.92|  3.328125|  64|\n|Dodge         |Charger           |         2|   52.67958| 3.9783784| 25396.25|  3.830931| 333|\n|Mazda         |Mazda5            |         2|   52.32812| 3.7000000| 42385.16|  3.881250|  32|\n|Lincoln       |Navigator         |         2|   52.25294| 0.0000000| 32941.73|  4.147059|  51|\n|FIAT          |500               |         2|   51.46765| 4.7735294| 22999.85|  4.108823|  34|\n|Chevrolet     |Sonic             |         2|   51.28548| 4.5612903| 30371.71|  3.675000| 124|\n|Volkswagen    |Jetta             |         2|   51.22667| 4.8391667| 32209.28|  3.727500| 120|\n|Kia           |Optima            |         2|   50.68600| 4.1000000| 36728.58|  3.728000| 100|\n|Chevrolet     |Traverse          |         2|   49.00820| 4.8000000| 41554.28|  3.914754|  61|\n|Chevrolet     |Equinox           |         2|   48.76236| 2.9528090| 47249.62|  3.961236| 178|\n|GMC           |Yukon XL          |         2|   48.65833| 3.1750000| 51680.88|  3.704167|  24|\n|Buick         |LaCrosse          |         2|   48.16296| 2.7444444| 37040.11|  3.759259|  27|\n|Nissan        |Altima            |         2|   47.72500| 4.1000000| 35259.59|  3.787393| 468|\n|Volkswagen    |Passat            |         2|   47.06605| 3.0715596| 39397.33|  3.747706| 109|\n|FORD          |E-350             |         2|   47.00460| 1.5275862| 28985.15|  4.006897|  87|\n|Volkswagen    |Beetle            |         2|   46.64167| 3.1666667| 25872.67|  3.904167|  24|\n|CHEVROLET     |IMPALA            |         2|   45.22981| 4.1000000| 20998.53|  3.725961| 104|\n|Chevrolet     |Camaro            |         2|   44.90741| 4.6000000| 34273.46|  3.477778|  54|\n|Lincoln       |MKS               |         2|   44.73000| 4.5000000| 28769.10|  4.015000|  20|\n|Nissan        |Maxima            |         2|   43.70693| 0.0000000| 40495.78|  3.629703| 101|\n|Buick         |Regal             |         2|   43.63333| 2.2428571| 30861.29|  3.838095|  21|\n|Chevrolet     |Malibu            |         2|   43.43846| 3.0000000| 41455.15|  3.652308| 130|\n|Hyundai       |Sonata            |         2|   43.27382| 3.2000000| 36382.62|  3.816754| 191|\n|Chevrolet     |Impala            |         2|   41.39091| 1.2263158| 41388.05|  3.441627| 211|\n|INFINITI      |G37               |         2|   40.44035| 3.2000000| 21670.57|  4.022807| 342|\n|Ford          |C-Max Energi      |         2|   33.81351| 0.0000000| 54144.97|  3.905405|  37|\n|Jeep          |Wrangler          |         3|   71.61875| 3.7750000| 35712.94|  4.085417|  48|\n|INFINITI      |JX35              |         3|   65.75714| 0.0000000| 38125.10|  3.980952|  21|\n|Nissan        |Sentra            |         3|   64.52836| 4.9000000| 25291.78|  3.770149|  67|\n|Toyota        |RAV4              |         3|   63.29111| 4.9000000| 43456.78|  3.857778|  45|\n|Honda         |CR-V              |         3|   60.71282| 4.5358974| 36615.13|  4.100000|  39|\n|Ford          |Explorer          |         3|   60.43046| 4.8072848| 43170.23|  3.885431| 151|\n|Mercedes-Benz |GLK-Class         |         3|   58.55172| 1.8758621| 35145.90|  3.989655|  29|\n|Toyota        |Sienna            |         3|   56.21071| 4.8571429| 47409.21|  3.385714|  28|\n|Hyundai       |Accent            |         3|   55.46000| 2.6250000| 36618.82|  3.385000|  40|\n|Hyundai       |Santa Fe          |         3|   55.39130| 3.4521739| 37180.09|  4.026087|  23|\n|Chevrolet     |Tahoe             |         3|   55.35128| 4.1153846| 48663.49|  4.015385|  39|\n|Lincoln       |MKX               |         3|   54.81290| 0.0000000| 29539.19|  3.925807|  31|\n|Ford          |Edge              |         3|   54.52848| 4.6000000| 36691.99|  4.040000| 165|\n|Honda         |Pilot             |         3|   54.51569| 4.8000000| 37825.63|  3.729412|  51|\n|Honda         |Civic             |         3|   54.50256| 4.4807692| 32930.03|  3.392949| 156|\n|Hyundai       |Elantra           |         3|   54.36608| 3.1699647| 30497.44|  3.709541| 283|\n|Nissan        |Pathfinder        |         3|   52.74167| 4.9000000| 43492.21|  3.433333|  24|\n|Dodge         |Dart              |         3|   52.73830| 2.8000000| 34579.32|  3.455319|  47|\n|LEXUS         |ES                |         3|   51.63871| 0.0000000| 27737.00|  3.664516|  31|\n|GMC           |Acadia            |         3|   50.96364| 4.5000000| 47060.68|  3.940909|  22|\n|Dodge         |Durango           |         3|   50.93488| 3.8930233| 42862.72|  3.904651|  43|\n|Mazda         |Mazda3            |         3|   49.81905| 4.3000000| 34426.67|  3.547619|  21|\n|Kia           |Optima            |         3|   49.03590| 4.1000000| 37632.88|  3.642308|  78|\n|Nissan        |Versa             |         3|   48.90952| 3.8000000| 47725.57|  3.247619|  21|\n|Chevrolet     |Cruze             |         3|   48.42838| 4.4891892| 49285.97|  3.312162| 148|\n|Ford          |Fiesta            |         3|   48.33125| 4.1500000| 43942.94|  3.900000|  48|\n|Audi          |A4                |         3|   48.30625| 2.8500000| 31768.03|  3.518750|  32|\n|Jeep          |Grand Cherokee    |         3|   48.27143| 4.0714286| 38348.69|  4.261905|  42|\n|BMW           |3 Series          |         3|   47.76333| 3.9633333| 31383.45|  4.036667|  60|\n|Acura         |TSX               |         3|   47.29722| 3.1972222| 33591.17|  3.686111|  36|\n|Toyota        |Prius             |         3|   46.66500| 0.0000000| 48570.85|  3.675000|  20|\n|Kia           |Soul              |         3|   46.65000| 4.3640000| 39954.70|  3.710000|  50|\n|Chrysler      |300               |         3|   46.54146| 2.4268293| 40909.68|  3.963415|  41|\n|Kia           |Sorento           |         3|   46.07667| 4.4166667| 42597.77|  4.010000|  30|\n|Chevrolet     |Camaro            |         3|   45.67059| 4.4823529| 36956.50|  3.438235|  34|\n|Volkswagen    |Jetta             |         3|   45.37532| 4.4883117| 38001.36|  3.358442|  77|\n|Lincoln       |MKZ               |         3|   45.22394| 2.4661972| 28236.54|  3.898592|  71|\n|Honda         |Accord            |         3|   44.64216| 3.4686486| 33459.25|  3.463243| 185|\n|Dodge         |Charger           |         3|   44.60811| 3.7945946| 44637.21|  3.473874| 111|\n|Nissan        |Altima            |         3|   44.50635| 4.1000000| 39511.74|  3.561905| 252|\n|Cadillac      |SRX               |         3|   44.38500| 3.3000000| 43314.55|  4.205000|  20|\n|Kia           |Forte             |         3|   43.51000| 4.6000000| 48393.67|  3.600000|  30|\n|Hyundai       |Sonata            |         3|   43.47830| 3.2000000| 32270.52|  3.860377| 106|\n|Acura         |TL                |         3|   43.32791| 3.1976744| 35469.05|  3.548837|  43|\n|Chevrolet     |Equinox           |         3|   43.06573| 2.7825175| 55362.20|  3.783916| 143|\n|Ford          |Focus             |         3|   42.85327| 3.4000000| 46549.72|  3.690654| 107|\n|Mazda         |Mazda5            |         3|   42.20000| 3.6500000| 50315.96|  3.291667|  24|\n|Chevrolet     |Malibu            |         3|   41.72375| 3.0000000| 46994.26|  3.345000|  80|\n|Chevrolet     |Sonic             |         3|   41.24321| 4.7962963| 46250.40|  3.301235|  81|\n|Nissan        |Maxima            |         3|   40.47798| 0.0000000| 44073.43|  3.494495| 109|\n|Volkswagen    |Passat            |         3|   39.28133| 3.2573333| 44069.43|  3.406667|  75|\n|INFINITI      |G37               |         3|   39.24766| 3.1437500| 24226.81|  3.978906| 256|\n|Jeep          |Liberty           |         3|   39.12692| 3.6000000| 52681.38|  3.861538|  26|\n|Chevrolet     |Traverse          |         3|   36.26923| 4.3615385| 55017.27|  3.776923|  26|\n|Chevrolet     |Impala            |         3|   30.75497| 0.2251462| 55432.18|  3.361988| 171|\n|Chevrolet     |Volt              |         3|   30.53810| 0.0000000| 54729.71|  3.857143|  21|\n|Jeep          |Wrangler          |         4|   75.20741| 2.9888889| 42801.36|  4.300000|  81|\n|Subaru        |Impreza           |         4|   67.69259| 3.6962963| 39555.85|  3.507407|  27|\n|Toyota        |Venza             |         4|   61.15909| 2.0000000| 39317.82|  4.054545|  22|\n|Ford          |Explorer          |         4|   58.60196| 4.5000000| 50376.91|  3.871569| 102|\n|Lexus         |CT 200h           |         4|   57.79583| 0.0000000| 35932.88|  3.712500|  24|\n|Audi          |Q5                |         4|   57.32059| 3.9000000| 51596.26|  4.023529|  34|\n|Honda         |CR-V              |         4|   56.23788| 4.5431818| 40221.67|  3.818182| 132|\n|Mercedes-Benz |GLK-Class         |         4|   55.13077| 2.2000000| 37288.62|  3.896154|  26|\n|Subaru        |Forester          |         4|   54.71481| 3.8444444| 48844.67|  3.725926|  27|\n|MINI          |Cooper Countryman |         4|   53.85217| 3.8347826| 44235.48|  4.069565|  23|\n|Toyota        |RAV4              |         4|   53.72235| 3.3000000| 36436.60|  3.804706|  85|\n|Ford          |Expedition        |         4|   53.00937| 2.8000000| 60793.34|  3.925000|  32|\n|Toyota        |Sienna            |         4|   52.66667| 4.8000000| 53710.91|  3.372727|  33|\n|Honda         |Civic             |         4|   52.57509| 4.4762264| 37476.50|  3.323774| 265|\n|Scion         |tC                |         4|   52.50000| 2.5100000| 46759.65|  3.345000|  20|\n|Hyundai       |Elantra           |         4|   52.22784| 3.4350515| 40491.87|  3.551546|  97|\n|Toyota        |Prius             |         4|   51.12903| 0.0000000| 45233.87|  3.717742|  62|\n|Ford          |Edge              |         4|   50.97045| 3.4761364| 43969.42|  3.969886| 176|\n|INFINITI      |QX56              |         4|   50.61034| 0.0000000| 58183.31|  3.817241|  29|\n|Jeep          |Compass           |         4|   50.58182| 2.7181818| 52089.86|  3.672727|  22|\n|Toyota        |Yaris             |         4|   50.53226| 4.8000000| 50187.90|  3.235484|  31|\n|Dodge         |Durango           |         4|   50.33409| 3.7113636| 52097.09|  3.777273|  44|\n|Honda         |Pilot             |         4|   50.17882| 4.8000000| 47525.02|  3.737647|  85|\n|Chevrolet     |Camaro            |         4|   49.75714| 3.9500000| 41388.69|  3.828571|  70|\n|Chevrolet     |Tahoe             |         4|   49.31053| 4.0657895| 68148.87|  3.665789|  38|\n|Jeep          |Grand Cherokee    |         4|   49.24959| 4.0024390| 47589.85|  4.069919| 123|\n|LEXUS         |ES                |         4|   48.72388| 0.0000000| 33727.04|  3.425373|  67|\n|Lincoln       |MKX               |         4|   48.10857| 0.0000000| 44200.06|  3.877143|  35|\n|Subaru        |Outback           |         4|   48.09375| 3.7468750| 63853.06|  3.696875|  32|\n|Ford          |Fiesta            |         4|   48.03623| 4.4405797| 48355.91|  3.775362|  69|\n|Mazda         |Mazda3            |         4|   47.47419| 4.3000000| 44949.55|  3.351613|  62|\n|GMC           |Acadia            |         4|   47.40779| 4.5441558| 48458.53|  3.864935|  77|\n|Kia           |Optima            |         4|   47.14811| 4.1000000| 38191.49|  3.704717| 106|\n|Hyundai       |Santa Fe          |         4|   46.72692| 4.3000000| 54209.92|  3.776923|  26|\n|Acura         |TSX               |         4|   46.55806| 3.2338710| 35527.16|  3.601613|  62|\n|Kia           |Soul              |         4|   46.52222| 3.7698413| 47183.29|  3.549206|  63|\n|Chevrolet     |Cruze             |         4|   46.50952| 4.2000000| 50090.81|  3.493651| 126|\n|GMC           |Terrain           |         4|   46.19615| 2.4673077| 59465.44|  4.036538|  52|\n|Dodge         |Charger           |         4|   45.67246| 3.5434783| 57199.70|  3.581159|  69|\n|Cadillac      |SRX               |         4|   45.38431| 3.6764706| 39831.75|  3.803922|  51|\n|Hyundai       |Accent            |         4|   45.34324| 3.0621622| 52956.81|  3.121622|  37|\n|Audi          |A4                |         4|   44.86735| 2.5204082| 38563.92|  3.542857|  49|\n|BMW           |3 Series          |         4|   44.27928| 3.4432432| 36599.33|  3.978679| 333|\n|Jeep          |Patriot           |         4|   44.02500| 3.0250000| 58410.16|  3.381250|  32|\n|Nissan        |Versa             |         4|   43.88049| 3.8878049| 55532.85|  3.086585|  82|\n|Nissan        |Maxima            |         4|   43.59048| 0.0000000| 38026.68|  3.641604| 399|\n|Buick         |Enclave           |         4|   43.54242| 3.6969697| 56386.21|  3.875758|  33|\n|Ford          |Focus             |         4|   43.50830| 3.4000000| 45173.65|  3.722707| 229|\n|Acura         |TL                |         4|   43.22727| 3.2136364| 36481.30|  3.693939|  66|\n|BMW           |5 Series          |         4|   42.88000| 4.2000000| 48081.27|  3.886667|  30|\n|Chrysler      |300               |         4|   42.42273| 3.1000000| 49107.95|  3.527273|  22|\n|Honda         |Accord            |         4|   42.41246| 3.4483283| 38273.13|  3.450456| 329|\n|Volkswagen    |Jetta             |         4|   42.33014| 4.2506849| 51149.84|  3.064384| 146|\n|Kia           |Rio               |         4|   42.14500| 3.1300000| 52014.50|  3.360000|  20|\n|Chevrolet     |Equinox           |         4|   40.78000| 2.6700000| 63188.88|  3.845625| 160|\n|Cadillac      |CTS               |         4|   40.33673| 2.5346939| 38945.00|  3.934694|  49|\n|Buick         |LaCrosse          |         4|   40.19730| 3.1729730| 52565.89|  3.881081|  37|\n|BMW           |6 Series          |         4|   40.18837| 1.6000000| 32316.05|  4.048837|  43|\n|Jeep          |Liberty           |         4|   39.92308| 3.4153846| 57545.36|  3.898718|  78|\n|Kia           |Forte             |         4|   39.80517| 4.5120690| 53322.69|  3.232759|  58|\n|Kia           |Sorento           |         4|   39.73827| 3.6876543| 63941.74|  3.790123|  81|\n|INFINITI      |G37               |         4|   37.71002| 3.0000000| 35357.35|  3.795918| 539|\n|Volkswagen    |Passat            |         4|   37.59808| 3.3000000| 39239.24|  3.173077| 104|\n|Chevrolet     |Sonic             |         4|   36.81333| 4.2000000| 52144.40|  3.526667|  30|\n|Lincoln       |MKZ               |         4|   36.78070| 1.7210526| 30884.04|  3.722807|  57|\n|Chevrolet     |Traverse          |         4|   36.68434| 4.2000000| 56128.76|  3.781928|  83|\n|Lincoln       |MKS               |         4|   35.97826| 3.3652174| 48471.30|  3.721739|  23|\n|Hyundai       |Sonata            |         4|   35.90896| 4.7000000| 67052.13|  3.322388|  67|\n|Nissan        |Altima            |         4|   35.44706| 4.9000000| 57169.67|  3.390588|  85|\n|Buick         |Regal             |         4|   35.15000| 2.2681818| 40792.77|  3.490909|  22|\n|Chevrolet     |Aveo              |         4|   29.87000| 3.5000000| 71829.75|  2.920000|  20|\n|Chevrolet     |Volt              |         4|   29.78148| 0.0000000| 52625.59|  3.785185|  27|\n|Chevrolet     |Impala            |         4|   27.49082| 1.6990338| 71288.84|  3.237198| 207|\n|Chevrolet     |HHR               |         4|   26.44483| 3.3000000| 83317.52|  3.172414|  29|\n|NISSAN        |LEAF              |         4|   26.13492| 2.0000000| 20135.89|  4.234921|  63|\n|Jeep          |Wrangler          |         5|   78.32105| 2.9947368| 55195.53|  4.057895|  38|\n|Chevrolet     |Camaro            |         5|   52.46944| 4.0666667| 51603.28|  3.494444|  36|\n|Chevrolet     |Tahoe             |         5|   51.20909| 3.9318182| 81026.32|  3.740909|  22|\n|Honda         |CR-V              |         5|   50.10714| 4.6000000| 62472.29|  3.685714|  28|\n|Toyota        |RAV4              |         5|   50.08667| 3.2333333| 49581.80|  3.553333|  30|\n|Honda         |Civic             |         5|   45.74762| 3.6761905| 67440.90|  2.952381|  21|\n|Jeep          |Grand Cherokee    |         5|   43.58000| 3.9100000| 71344.35|  3.775000|  20|\n|Nissan        |Maxima            |         5|   41.72857| 0.3857143| 45851.00|  3.307936|  63|\n|BMW           |3 Series          |         5|   39.89468| 2.8148936| 46153.87|  3.747872|  94|\n|Chevrolet     |Equinox           |         5|   39.25789| 2.8657895| 80041.71|  3.647368|  38|\n|INFINITI      |G37               |         5|   39.16471| 3.0191176| 37705.79|  3.848529|  68|\n|Kia           |Soul              |         5|   39.04412| 3.3000000| 80797.09|  3.264706|  34|\n|Honda         |Accord            |         5|   37.88077| 2.8615385| 70439.73|  3.084615|  26|\n|Volkswagen    |Jetta             |         5|   37.68667| 3.5800000| 68689.33|  2.860000|  30|\n|Ford          |Edge              |         5|   36.96667| 0.9800000| 71057.47|  3.653333|  30|\n|Kia           |Sorento           |         5|   36.25862| 2.7000000| 88477.97|  3.703448|  29|\n|BMW           |5 Series          |         5|   35.42000| 4.1800000| 58319.00|  3.495000|  20|\n|Chevrolet     |Traverse          |         5|   34.70000| 4.2000000| 70663.95|  3.463636|  22|\n|Hyundai       |Sonata            |         5|   34.47755| 4.6510204| 76378.59|  3.234694|  49|\n|Dodge         |Charger           |         5|   33.86207| 2.7344828| 78959.48|  2.924138|  29|\n|Nissan        |Altima            |         5|   33.25417| 4.8500000| 86557.52|  3.175000|  48|\n|Chevrolet     |HHR               |         5|   29.40500| 3.3000000| 77872.90|  3.275000|  20|\n|Chevrolet     |Impala            |         5|   25.73364| 2.0439252| 85397.18|  3.039252| 107|\n\n\nWhile I won’t be making any formal recommendations in this paper, the above table provides a guide on which vehicles can be expected to retain good value in the future. Please pay close attention to the error rating and the sample size."
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#appendix-a",
    "href": "posts/OwenTibbyFinalProject601_submission.html#appendix-a",
    "title": "Final Project",
    "section": "Appendix A:",
    "text": "Appendix A:\nOriginal data set: car_prices.csv\n\n\nCode\ndfSummary(df)\n\n\nData Frame Summary  \ndf  \nDimensions: 514613 x 16  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values                  Freqs (% of Valid)       Graph                 Valid      Missing  \n---- -------------- ------------------------------- ------------------------ --------------------- ---------- ---------\n1    year           1. 2012                         102315 (19.9%)           III                   514613     0        \n     [character]    2. 2013                          98168 (19.1%)           III                   (100.0%)   (0.0%)   \n                    3. 2014                          81070 (15.8%)           III                                       \n                    4. 2011                          48548 ( 9.4%)           I                                         \n                    5. 2008                          31502 ( 6.1%)           I                                         \n                    6. 2007                          30845 ( 6.0%)           I                                         \n                    7. 2006                          26913 ( 5.2%)           I                                         \n                    8. 2010                          26485 ( 5.1%)           I                                         \n                    9. 2005                          21394 ( 4.2%)                                                     \n                    10. 2009                         20594 ( 4.0%)                                                     \n                    [ 2 others ]                     26779 ( 5.2%)           I                                         \n\n2    make           1. Ford                          87687 (17.3%)           III                   506671     7942     \n     [character]    2. Chevrolet                     56022 (11.1%)           II                    (98.5%)    (1.5%)   \n                    3. Nissan                        51594 (10.2%)           II                                        \n                    4. Toyota                        36029 ( 7.1%)           I                                         \n                    5. Dodge                         29544 ( 5.8%)           I                                         \n                    6. Honda                         23220 ( 4.6%)                                                     \n                    7. Hyundai                       21155 ( 4.2%)                                                     \n                    8. BMW                           18983 ( 3.7%)                                                     \n                    9. Kia                           17786 ( 3.5%)                                                     \n                    10. Chrysler                     16613 ( 3.3%)                                                     \n                    [ 76 others ]                   148038 (29.2%)           IIIII                                     \n\n3    model          1. Altima                        18638 ( 3.7%)                                 506659     7954     \n     [character]    2. F-150                         13584 ( 2.7%)                                 (98.5%)    (1.5%)   \n                    3. Fusion                        12946 ( 2.6%)                                                     \n                    4. Escape                        11578 ( 2.3%)                                                     \n                    5. Camry                         11226 ( 2.2%)                                                     \n                    6. Focus                          9999 ( 2.0%)                                                     \n                    7. Grand Caravan                  7769 ( 1.5%)                                                     \n                    8. Impala                         7505 ( 1.5%)                                                     \n                    9. G Sedan                        7417 ( 1.5%)                                                     \n                    10. 3 Series                      7394 ( 1.5%)                                                     \n                    [ 782 others ]                  398603 (78.7%)           IIIIIIIIIIIIIII                           \n\n4    trim           1. Base                          48584 ( 9.6%)           I                     506394     8219     \n     [character]    2. SE                            41841 ( 8.3%)           I                     (98.4%)    (1.6%)   \n                    3. LX                            18863 ( 3.7%)                                                     \n                    4. Limited                       17396 ( 3.4%)                                                     \n                    5. LT                            16636 ( 3.3%)                                                     \n                    6. XLT                           15169 ( 3.0%)                                                     \n                    7. LE                            11748 ( 2.3%)                                                     \n                    8. S                             11647 ( 2.3%)                                                     \n                    9. GLS                           11587 ( 2.3%)                                                     \n                    10. LS                           11360 ( 2.2%)                                                     \n                    [ 1524 others ]                 301563 (59.6%)           IIIIIIIIIII                               \n\n5    body           1. Sedan                        183305 (36.3%)           IIIIIII               504678     9935     \n     [character]    2. SUV                          110102 (21.8%)           IIII                  (98.1%)    (1.9%)   \n                    3. sedan                         39476 ( 7.8%)           I                                         \n                    4. suv                           23120 ( 4.6%)                                                     \n                    5. Hatchback                     20382 ( 4.0%)                                                     \n                    6. Minivan                       20051 ( 4.0%)                                                     \n                    7. Crew Cab                      12699 ( 2.5%)                                                     \n                    8. Wagon                         12646 ( 2.5%)                                                     \n                    9. Coupe                         12572 ( 2.5%)                                                     \n                    10. Convertible                   7372 ( 1.5%)                                                     \n                    [ 74 others ]                    62953 (12.5%)           II                                        \n\n6    transmission   1. automatic                    440307 (96.8%)           IIIIIIIIIIIIIIIIIII   454932     59681    \n     [character]    2. manual                        14599 ( 3.2%)                                 (88.4%)    (11.6%)  \n                    3. sedan                            15 ( 0.0%)                                                     \n                    4. Sedan                            11 ( 0.0%)                                                     \n\n7    vin            1. automatic                        22 (  0.0%)                                514609     4        \n     [character]    2. wbanv13588cz57827                 5 (  0.0%)                                (100.0%)   (0.0%)   \n                    3. 1ftfw1cv5afb30053                 4 (  0.0%)                                                    \n                    4. 5n1ar1nn2bc632869                 4 (  0.0%)                                                    \n                    5. 5uxfe43579l274932                 4 (  0.0%)                                                    \n                    6. trusc28n241022003                 4 (  0.0%)                                                    \n                    7. wddgf56x78f009940                 4 (  0.0%)                                                    \n                    8. 1c3bcbeb9dn526808                 3 (  0.0%)                                                    \n                    9. 1c4pjlak8cw134088                 3 (  0.0%)                                                    \n                    10. 1c4pjlak9cw151871                3 (  0.0%)                                                    \n                    [ 506479 others ]               514553 (100.0%)          IIIIIIIIIIIIIIIIIII                       \n\n8    state          1. fl                            76001 (14.8%)           II                    514613     0        \n     [character]    2. ca                            66121 (12.8%)           II                    (100.0%)   (0.0%)   \n                    3. pa                            51776 (10.1%)           II                                        \n                    4. tx                            43185 ( 8.4%)           I                                         \n                    5. ga                            31731 ( 6.2%)           I                                         \n                    6. nj                            25933 ( 5.0%)           I                                         \n                    7. il                            22648 ( 4.4%)                                                     \n                    8. tn                            20312 ( 3.9%)                                                     \n                    9. oh                            20028 ( 3.9%)                                                     \n                    10. nc                           18478 ( 3.6%)                                                     \n                    [ 54 others ]                   138400 (26.9%)           IIIII                                     \n\n9    condition      Mean (sd) : 3.5 (0.9)           41 distinct values                   : .       506371     8242     \n     [numeric]      min < med < max:                                                     : : :     (98.4%)    (1.6%)   \n                    1 < 3.7 < 5                                                  :   : . : : : .                       \n                    IQR (CV) : 1.4 (0.3)                                         : : : : : : : :                       \n                                                                             .   : : : : : : : :                       \n\n10   odometer       Mean (sd) : 61323 (46816.3)     154189 distinct values   :                     514573     40       \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    1 < 47172 < 999999                                       :                                         \n                    IQR (CV) : 61684 (0.8)                                   :                                         \n                                                                             : :                                       \n\n11   color          1. black                        104598 (20.4%)           IIII                  513921     692      \n     [character]    2. white                         99846 (19.4%)           III                   (99.9%)    (0.1%)   \n                    3. gray                          78945 (15.4%)           III                                       \n                    4. silver                        75627 (14.7%)           II                                        \n                    5. blue                          46293 ( 9.0%)           I                                         \n                    6. red                           39932 ( 7.8%)           I                                         \n                    7. â€”                           24323 ( 4.7%)                                                     \n                    8. gold                           8547 ( 1.7%)                                                     \n                    9. burgundy                       7947 ( 1.5%)                                                     \n                    10. green                         7723 ( 1.5%)                                                     \n                    [ 36 others ]                    20140 ( 3.9%)                                                     \n\n12   interior       1. black                        237563 (46.2%)           IIIIIIIII             513921     692      \n     [character]    2. gray                         159295 (31.0%)           IIIIII                (99.9%)    (0.1%)   \n                    3. beige                         53104 (10.3%)           II                                        \n                    4. tan                           37782 ( 7.4%)           I                                         \n                    5. â€”                           13575 ( 2.6%)                                                     \n                    6. brown                          7925 ( 1.5%)                                                     \n                    7. red                            1256 ( 0.2%)                                                     \n                    8. silver                          972 ( 0.2%)                                                     \n                    9. blue                            648 ( 0.1%)                                                     \n                    10. off-white                      478 ( 0.1%)                                                     \n                    [ 7 others ]                      1323 ( 0.3%)                                                     \n\n13   seller         1. nissan-infiniti lt            19693 ( 3.8%)                                 514613     0        \n     [character]    2. ford motor credit company     19160 ( 3.7%)                                 (100.0%)   (0.0%)   \n                    3. the hertz corporation         18236 ( 3.5%)                                                     \n                    4. santander consumer            15078 ( 2.9%)                                                     \n                    5. avis corporation              12540 ( 2.4%)                                                     \n                    6. nissan infiniti lt             9962 ( 1.9%)                                                     \n                    7. wells fargo dealer servic      8457 ( 1.6%)                                                     \n                    8. tdaf remarketing               6897 ( 1.3%)                                                     \n                    9. enterprise veh exchange/r      6810 ( 1.3%)                                                     \n                    10. hyundai motor finance         6660 ( 1.3%)                                                     \n                    [ 13125 others ]                391120 (76.0%)           IIIIIIIIIIIIIII                           \n\n14   mmr            Mean (sd) : 14740.9 (9452.6)    1101 distinct values     :                     514587     26       \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    25 < 12900 < 182000                                      :                                         \n                    IQR (CV) : 10375 (0.6)                                   : .                                       \n                                                                             : :                                       \n\n15   sellingprice   Mean (sd) : 14573.8 (9537.2)    1869 distinct values     :                     514613     0        \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    1 < 12900 < 230000                                       :                                         \n                    IQR (CV) : 10500 (0.7)                                   :                                         \n                                                                             : :                                       \n\n16   saledate       1. Tue Feb 10 2015 01:30:00       4984 ( 1.0%)                                 514613     0        \n     [character]    2. Tue Feb 17 2015 01:30:00       4760 ( 0.9%)                                 (100.0%)   (0.0%)   \n                    3. Tue Jan 27 2015 01:30:00       4729 ( 0.9%)                                                     \n                    4. Tue Jan 20 2015 01:30:00       4464 ( 0.9%)                                                     \n                    5. Tue Mar 03 2015 01:30:00       4409 ( 0.9%)                                                     \n                    6. Tue Feb 03 2015 01:30:00       4301 ( 0.8%)                                                     \n                    7. Tue Jun 02 2015 02:30:00       4114 ( 0.8%)                                                     \n                    8. Tue Jun 16 2015 02:30:00       3983 ( 0.8%)                                                     \n                    9. Tue Mar 10 2015 02:30:00       3806 ( 0.7%)                                                     \n                    10. Tue Feb 24 2015 01:30:00      3448 ( 0.7%)                                                     \n                    [ 3619 others ]                 471615 (91.6%)           IIIIIIIIIIIIIIIIII                        \n-----------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#appendix-b",
    "href": "posts/OwenTibbyFinalProject601_submission.html#appendix-b",
    "title": "Final Project",
    "section": "Appendix B:",
    "text": "Appendix B:\nCanadian vehicle catalog\n\n\nCode\ndfSummary(ca_catalog)\n\n\nData Frame Summary  \nca_catalog  \nDimensions: 5268 x 10  \nDuplicates: 0  \n\n------------------------------------------------------------------------------------------------------------------------\nNo   Variable          Stats / Values                  Freqs (% of Valid)     Graph                 Valid      Missing  \n---- ----------------- ------------------------------- ---------------------- --------------------- ---------- ---------\n1    Combined          1. 2002 Cadillac DeVille           2 ( 0.0%)                                 5268       0        \n     [character]       2. 2002 Chevrolet Impala           2 ( 0.0%)                                 (100.0%)   (0.0%)   \n                       3. 2002 Chevrolet Silverado        2 ( 0.0%)                                                     \n                       4. 2002 Chevrolet Silverado        2 ( 0.0%)                                                     \n                       5. 2002 Chrysler Intrepid          2 ( 0.0%)                                                     \n                       6. 2002 Ford Excursion             2 ( 0.0%)                                                     \n                       7. 2002 Ford Super Duty F-35       2 ( 0.0%)                                                     \n                       8. 2002 Ford Super Duty F-35       2 ( 0.0%)                                                     \n                       9. 2002 GMC Savana Passenger       2 ( 0.0%)                                                     \n                       10. 2002 GMC Sierra 2500           2 ( 0.0%)                                                     \n                       [ 5037 others ]                 5248 (99.6%)           IIIIIIIIIIIIIIIIIII                       \n\n2    YearOfLaunch      Mean (sd) : 2007.6 (4.8)        17 distinct values           . .   .   . :   5268       0        \n     [numeric]         min < med < max:                                         :   : :   :   : :   (100.0%)   (0.0%)   \n                       1999 < 2008 < 2015                                     : :   : : . : . : :                       \n                       IQR (CV) : 8 (0)                                       : : : : : : : : : :                       \n                                                                              : : : : : : : : : :                       \n\n3    Segment           1. Compact Car                  721 (16.8%)            III                   4295       973      \n     [character]       2. Mid-Size Car                 638 (14.9%)            II                    (81.5%)    (18.5%)  \n                       3. Mid-Size SUV/Crossover       501 (11.7%)            II                                        \n                       4. Compact SUV/Crossover        372 ( 8.7%)            I                                         \n                       5. Minivan                      354 ( 8.2%)            I                                         \n                       6. Full-Size Car                338 ( 7.9%)            I                                         \n                       7. Subcompact Car               324 ( 7.5%)            I                                         \n                       8. Sports Car                   266 ( 6.2%)            I                                         \n                       9. Full-Size SUV/Crossover      260 ( 6.1%)            I                                         \n                       10. HD Full-Size Pickup         155 ( 3.6%)                                                      \n                       [ 3 others ]                    366 ( 8.5%)            I                                         \n\n4    Trim_Count        Mean (sd) : 16.2 (58.3)         172 distinct values    :                     5268       0        \n     [numeric]         min < med < max:                                       :                     (100.0%)   (0.0%)   \n                       1 < 5 < 2524                                           :                                         \n                       IQR (CV) : 8 (3.6)                                     :                                         \n                                                                              :                                         \n\n5    AVG_MSRP_CAD      Mean (sd) : 46772.7 (27047)     3695 distinct values     :                   5268       0        \n     [numeric]         min < med < max:                                         :                   (100.0%)   (0.0%)   \n                       15 < 39895 < 251165                                      :                                       \n                       IQR (CV) : 25285.8 (0.6)                                 : .                                     \n                                                                              : : : .                                   \n\n6    MSRP_Range_CAD    Mean (sd) : 13307.5 (16252.5)   554 distinct values    :                     5268       0        \n     [numeric]         min < med < max:                                       :                     (100.0%)   (0.0%)   \n                       0 < 9600 < 211900                                      :                                         \n                       IQR (CV) : 12200 (1.2)                                 :                                         \n                                                                              : :                                       \n\n7    StdDev_MSRP_CAD   Mean (sd) : 5559.8 (6429.4)     3764 distinct values   :                     4674       594      \n     [numeric]         min < med < max:                                       :                     (88.7%)    (11.3%)  \n                       0 < 3988.3 < 63828                                     :                                         \n                       IQR (CV) : 3664.6 (1.2)                                :                                         \n                                                                              : :                                       \n\n8    Error.MSRP_CAD    Mean (sd) : 2290.2 (3396.2)     3695 distinct values   :                     4674       594      \n     [numeric]         min < med < max:                                       :                     (88.7%)    (11.3%)  \n                       0 < 1396.9 < 36851.1                                   :                                         \n                       IQR (CV) : 1443.9 (1.5)                                :                                         \n                                                                              : .                                       \n\n9    Date              1. 08/29/08                     345 (10.2%)            II                    3370       1898     \n     [character]       2. 08/29/14                     354 (10.5%)            II                    (64.0%)    (36.0%)  \n                       3. 08/30/13                     342 (10.1%)            II                                        \n                       4. 08/31/06                     340 (10.1%)            II                                        \n                       5. 08/31/07                     335 ( 9.9%)            I                                         \n                       6. 08/31/09                     319 ( 9.5%)            I                                         \n                       7. 08/31/10                     318 ( 9.4%)            I                                         \n                       8. 08/31/11                     327 ( 9.7%)            I                                         \n                       9. 08/31/12                     332 ( 9.9%)            I                                         \n                       10. 08/31/15                    358 (10.6%)            II                                        \n\n10   Close             Mean (sd) : 1.1 (0.1)           10 distinct values     I                     3370       1898     \n     [numeric]         min < med < max:                                       I                     (64.0%)    (36.0%)  \n                       1 < 1.1 < 1.3                                          II                                        \n                       IQR (CV) : 0 (0.1)                                     I                                         \n                                                                              II                                        \n                                                                              I                                         \n                                                                              II                                        \n                                                                              I                                         \n                                                                              II                                        \n                                                                              II                                        \n------------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#appendix-c",
    "href": "posts/OwenTibbyFinalProject601_submission.html#appendix-c",
    "title": "Final Project",
    "section": "Appendix C:",
    "text": "Appendix C:\nFirst iteration of unmatched records labelled as false_keys\n\n\nCode\ndfSummary(false_keys)\n\n\nData Frame Summary  \nfalse_keys  \nDimensions: 57328 x 14  \nDuplicates: 0  \n\n----------------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values                  Freqs (% of Valid)      Graph                 Valid      Missing  \n---- -------------- ------------------------------- ----------------------- --------------------- ---------- ---------\n1    trim           1. Base                         13031 (24.7%)           IIII                  52679      4649     \n     [character]    2. Touring                       4393 ( 8.3%)           I                     (91.9%)    (8.1%)   \n                    3. G37x                          4172 ( 7.9%)           I                                         \n                    4. G37 Journey                   2902 ( 5.5%)           I                                         \n                    5. E-250                         1557 ( 3.0%)                                                     \n                    6. 2500                          1097 ( 2.1%)                                                     \n                    7. Lariat                        1064 ( 2.0%)                                                     \n                    8. S                             1003 ( 1.9%)                                                     \n                    9. SL                             993 ( 1.9%)                                                     \n                    10. LT Fleet                      963 ( 1.8%)                                                     \n                    [ 418 others ]                  21504 (40.8%)           IIIIIIII                                  \n\n2    body           1. SEDAN                        11677 (22.5%)           IIII                  51866      5462     \n     [character]    2. SUV                           8030 (15.5%)           III                   (90.5%)    (9.5%)   \n                    3. G SEDAN                       7417 (14.3%)           II                                        \n                    4. MINIVAN                       6020 (11.6%)           II                                        \n                    5. HATCHBACK                     4448 ( 8.6%)           I                                         \n                    6. VAN                           3291 ( 6.3%)           I                                         \n                    7. CREW CAB                      2614 ( 5.0%)           I                                         \n                    8. E-SERIES VAN                  1822 ( 3.5%)                                                     \n                    9. G COUPE                       1593 ( 3.1%)                                                     \n                    10. QUAD CAB                      974 ( 1.9%)                                                     \n                    [ 25 others ]                    3980 ( 7.7%)           I                                         \n\n3    Body_Type      1. Convertible                   1293 ( 2.5%)                                 51866      5462     \n     [character]    2. Coupe                         2744 ( 5.3%)           I                     (90.5%)    (9.5%)   \n                    3. Hatchback                     4448 ( 8.6%)           I                                         \n                    4. Minivan                      11211 (21.6%)           IIII                                      \n                    5. Pickup                        4294 ( 8.3%)           I                                         \n                    6. Sedan                        19094 (36.8%)           IIIIIII                                   \n                    7. SUV                           8030 (15.5%)           III                                       \n                    8. Wagon                          752 ( 1.4%)                                                     \n\n4    transmission   1. automatic                    48340 (96.6%)           IIIIIIIIIIIIIIIIIII   50066      7262     \n     [character]    2. manual                        1726 ( 3.4%)                                 (87.3%)    (12.7%)  \n\n5    vin            1. 1FBSS3BL7CDA01868                3 ( 0.0%)                                 57328      0        \n     [character]    2. 1FT7W2BTXDEA03416                3 ( 0.0%)                                 (100.0%)   (0.0%)   \n                    3. 1FT8W3DT1CEB32666                3 ( 0.0%)                                                     \n                    4. 1FTSW21568EA33810                3 ( 0.0%)                                                     \n                    5. 1FTSW21R48EB47680                3 ( 0.0%)                                                     \n                    6. 1FTXW43R28EA88031                3 ( 0.0%)                                                     \n                    7. 1GYEE637780184619                3 ( 0.0%)                                                     \n                    8. 1ZVBP8JZ2E5263940                3 ( 0.0%)                                                     \n                    9. 2C4RC1BG2DR821210                3 ( 0.0%)                                                     \n                    10. 2FABP7BV5AX113795               3 ( 0.0%)                                                     \n                    [ 56475 others ]                57298 (99.9%)           IIIIIIIIIIIIIIIIIII                       \n\n6    state          1. FL                           10682 (18.6%)           III                   57328      0        \n     [character]    2. CA                            8762 (15.3%)           III                   (100.0%)   (0.0%)   \n                    3. PA                            4972 ( 8.7%)           I                                         \n                    4. TX                            4545 ( 7.9%)           I                                         \n                    5. GA                            3274 ( 5.7%)           I                                         \n                    6. IL                            3051 ( 5.3%)           I                                         \n                    7. TN                            2925 ( 5.1%)           I                                         \n                    8. NJ                            2816 ( 4.9%)                                                     \n                    9. OH                            1667 ( 2.9%)                                                     \n                    10. MO                           1518 ( 2.6%)                                                     \n                    [ 28 others ]                   13116 (22.9%)           IIII                                      \n\n7    condition      Mean (sd) : 3.7 (0.9)           41 distinct values                  . : .     56763      565      \n     [numeric]      min < med < max:                                                    : : :     (99.0%)    (1.0%)   \n                    1 < 3.8 < 5                                                     .   : : : .                       \n                    IQR (CV) : 1.3 (0.2)                                        : . : : : : : :                       \n                                                                                : : : : : : : :                       \n\n8    odometer       Mean (sd) : 51493.7 (44181.5)   43772 distinct values   :                     57325      3        \n     [numeric]      min < med < max:                                        :                     (100.0%)   (0.0%)   \n                    1 < 35757 < 999999                                      :                                         \n                    IQR (CV) : 50951 (0.9)                                  :                                         \n                                                                            : .                                       \n\n9    color          1. white                        16292 (28.4%)           IIIII                 57267      61       \n     [character]    2. black                        11733 (20.5%)           IIII                  (99.9%)    (0.1%)   \n                    3. gray                          8879 (15.5%)           III                                       \n                    4. silver                        6166 (10.8%)           II                                        \n                    5. blue                          4576 ( 8.0%)           I                                         \n                    6. red                           3244 ( 5.7%)           I                                         \n                    7. â€”                           2798 ( 4.9%)                                                     \n                    8. gold                           817 ( 1.4%)                                                     \n                    9. burgundy                       656 ( 1.1%)                                                     \n                    10. beige                         569 ( 1.0%)                                                     \n                    [ 9 others ]                     1537 ( 2.7%)                                                     \n\n10   interior       1. black                        27397 (47.8%)           IIIIIIIII             57267      61       \n     [character]    2. gray                         17084 (29.8%)           IIIII                 (99.9%)    (0.1%)   \n                    3. beige                         6468 (11.3%)           II                                        \n                    4. tan                           3547 ( 6.2%)           I                                         \n                    5. brown                         1155 ( 2.0%)                                                     \n                    6. â€”                           1062 ( 1.9%)                                                     \n                    7. red                            191 ( 0.3%)                                                     \n                    8. silver                         116 ( 0.2%)                                                     \n                    9. blue                            68 ( 0.1%)                                                     \n                    10. white                          46 ( 0.1%)                                                     \n                    [ 7 others ]                      133 ( 0.2%)                                                     \n\n11   seller         1. nissan infiniti lt            9958 (17.4%)           III                   57328      0        \n     [character]    2. lexus financial services      3627 ( 6.3%)           I                     (100.0%)   (0.0%)   \n                    3. u-haul                        1439 ( 2.5%)                                                     \n                    4. nissan-infiniti lt            1420 ( 2.5%)                                                     \n                    5. gm remarketing                1241 ( 2.2%)                                                     \n                    6. the hertz corporation         1053 ( 1.8%)                                                     \n                    7. ge fleet services for its     1015 ( 1.8%)                                                     \n                    8. enterprise veh exchange/r     1009 ( 1.8%)                                                     \n                    9. santander consumer             874 ( 1.5%)                                                     \n                    10. avis corporation              830 ( 1.4%)                                                     \n                    [ 4601 others ]                 34862 (60.8%)           IIIIIIIIIIII                              \n\n12   mmr            Mean (sd) : 19003.7 (10836)     1062 distinct values    : :                   57328      0        \n     [numeric]      min < med < max:                                        : :                   (100.0%)   (0.0%)   \n                    175 < 18600 < 182000                                    : :                                       \n                    IQR (CV) : 10600 (0.6)                                  : :                                       \n                                                                            : : .                                     \n\n13   sellingprice   Mean (sd) : 18860 (10893.5)     1083 distinct values    : .                   57328      0        \n     [numeric]      min < med < max:                                        : :                   (100.0%)   (0.0%)   \n                    300 < 18400 < 183000                                    : :                                       \n                    IQR (CV) : 11100 (0.6)                                  : :                                       \n                                                                            : : .                                     \n\n14   saledate       1. Tue Jan 27 2015 01:30:00       633 ( 1.1%)                                 57328      0        \n     [character]    2. Tue Feb 17 2015 01:30:00       628 ( 1.1%)                                 (100.0%)   (0.0%)   \n                    3. Tue Feb 03 2015 01:30:00       604 ( 1.1%)                                                     \n                    4. Tue Mar 03 2015 01:30:00       577 ( 1.0%)                                                     \n                    5. Tue Jan 20 2015 01:30:00       570 ( 1.0%)                                                     \n                    6. Tue Jun 16 2015 02:30:00       562 ( 1.0%)                                                     \n                    7. Tue Feb 10 2015 01:30:00       550 ( 1.0%)                                                     \n                    8. Tue Jun 02 2015 02:30:00       476 ( 0.8%)                                                     \n                    9. Thu Jan 15 2015 04:30:00       452 ( 0.8%)                                                     \n                    10. Tue Jan 06 2015 01:30:00      436 ( 0.8%)                                                     \n                    [ 2131 others ]                 51840 (90.4%)           IIIIIIIIIIIIIIIIII                        \n----------------------------------------------------------------------------------------------------------------------\n\n\nThis dataset contains all the YMM keys from carprices that failed to map to the us_catalog."
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#appendix-d",
    "href": "posts/OwenTibbyFinalProject601_submission.html#appendix-d",
    "title": "Final Project",
    "section": "Appendix D:",
    "text": "Appendix D:\nAll records cleaned through NHTSA’s vindecodr package lablled as cleaned_keys\n\n\nCode\ndfSummary(cleaned_keys)\n\n\nData Frame Summary  \ncleaned_keys  \nDimensions: 38208 x 26  \nDuplicates: 1090  \n\n----------------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values                  Freqs (% of Valid)      Graph                 Valid      Missing  \n---- -------------- ------------------------------- ----------------------- --------------------- ---------- ---------\n1    year           Mean (sd) : 2011.5 (2.2)        2007 : 3684 ( 9.6%)     I                     38208      0        \n     [numeric]      min < med < max:                2008 : 2518 ( 6.6%)     I                     (100.0%)   (0.0%)   \n                    2007 < 2012 < 2015              2009 : 1194 ( 3.1%)                                               \n                    IQR (CV) : 2 (0)                2010 : 1856 ( 4.9%)                                               \n                                                    2011 : 4979 (13.0%)     II                                        \n                                                    2012 : 8787 (23.0%)     IIII                                      \n                                                    2013 : 8609 (22.5%)     IIII                                      \n                                                    2014 : 6340 (16.6%)     III                                       \n                                                    2015 :  241 ( 0.6%)                                               \n\n2    make           1. INFINITI                     13977 (36.6%)           IIIIIII               38208      0        \n     [character]    2. LEXUS                         9007 (23.6%)           IIII                  (100.0%)   (0.0%)   \n                    3. FORD                          4012 (10.5%)           II                                        \n                    4. CHEVROLET                     3208 ( 8.4%)           I                                         \n                    5. NISSAN                        2272 ( 5.9%)           I                                         \n                    6. GMC                            686 ( 1.8%)                                                     \n                    7. MAZDA                          661 ( 1.7%)                                                     \n                    8. HYUNDAI                        538 ( 1.4%)                                                     \n                    9. VOLKSWAGEN                     505 ( 1.3%)                                                     \n                    10. DODGE                         474 ( 1.2%)                                                     \n                    [ 25 others ]                    2868 ( 7.5%)           I                                         \n\n3    model          1. G37                           8779 (23.0%)           IIII                  38208      0        \n     [character]    2. RX                            2727 ( 7.1%)           I                     (100.0%)   (0.0%)   \n                    3. IS                            2213 ( 5.8%)           I                                         \n                    4. ES                            2055 ( 5.4%)           I                                         \n                    5. E-350                         1651 ( 4.3%)                                                     \n                    6. IMPALA                        1406 ( 3.7%)                                                     \n                    7. LEAF                          1359 ( 3.6%)                                                     \n                    8. G25                            995 ( 2.6%)                                                     \n                    9. M37                            964 ( 2.5%)                                                     \n                    10. EXPRESS                       885 ( 2.3%)                                                     \n                    [ 170 others ]                  15174 (39.7%)           IIIIIII                                   \n\n4    trim           1. Base                         11399 (32.9%)           IIIIII                34646      3562     \n     [character]    2. G37x                          4250 (12.3%)           II                    (90.7%)    (9.3%)   \n                    3. G37 Journey                   2956 ( 8.5%)           I                                         \n                    4. 2500                          1108 ( 3.2%)                                                     \n                    5. E-350 Super Duty XLT           956 ( 2.8%)                                                     \n                    6. G37                            906 ( 2.6%)                                                     \n                    7. SL                             843 ( 2.4%)                                                     \n                    8. SV                             607 ( 1.8%)                                                     \n                    9. LT Fleet                       583 ( 1.7%)                                                     \n                    10. SE                            579 ( 1.7%)                                                     \n                    [ 276 others ]                  10459 (30.2%)           IIIIII                                    \n\n5    body           1. SEDAN                        10160 (30.0%)           IIIII                 33919      4289     \n     [character]    2. G SEDAN                       7567 (22.3%)           IIII                  (88.8%)    (11.2%)  \n                    3. SUV                           6266 (18.5%)           III                                       \n                    4. VAN                           3128 ( 9.2%)           I                                         \n                    5. HATCHBACK                     2423 ( 7.1%)           I                                         \n                    6. G COUPE                       1617 ( 4.8%)                                                     \n                    7. CONVERTIBLE                    593 ( 1.7%)                                                     \n                    8. E-SERIES VAN                   454 ( 1.3%)                                                     \n                    9. COUPE                          403 ( 1.2%)                                                     \n                    10. G CONVERTIBLE                 329 ( 1.0%)                                                     \n                    [ 15 others ]                     979 ( 2.9%)                                                     \n\n6    transmission   1. automatic                    32662 (97.0%)           IIIIIIIIIIIIIIIIIII   33672      4536     \n     [character]    2. manual                        1010 ( 3.0%)                                 (88.1%)    (11.9%)  \n\n7    vin            1. 1FBSS3BL7CDA01868                9 ( 0.0%)                                 38208      0        \n     [character]    2. 1GYEE637780184619                9 ( 0.0%)                                 (100.0%)   (0.0%)   \n                    3. 1ZVBP8JZ2E5263940                9 ( 0.0%)                                                     \n                    4. 2FABP7BV5AX113795                9 ( 0.0%)                                                     \n                    5. 2T2HK31U67C016215                9 ( 0.0%)                                                     \n                    6. JN1CV6AP0DM715990                9 ( 0.0%)                                                     \n                    7. JN1CV6APXCM936687                9 ( 0.0%)                                                     \n                    8. JN1DV6AR6CM860083                9 ( 0.0%)                                                     \n                    9. JNKBV61F67M805602                9 ( 0.0%)                                                     \n                    10. JTHBK1EG6B2428322               9 ( 0.0%)                                                     \n                    [ 35968 others ]                38118 (99.8%)           IIIIIIIIIIIIIIIIIII                       \n\n8    state          1. FL                           8073 (21.1%)            IIII                  38208      0        \n     [character]    2. CA                           6531 (17.1%)            III                   (100.0%)   (0.0%)   \n                    3. PA                           2950 ( 7.7%)            I                                         \n                    4. TX                           2498 ( 6.5%)            I                                         \n                    5. TN                           2482 ( 6.5%)            I                                         \n                    6. GA                           2374 ( 6.2%)            I                                         \n                    7. NJ                           2087 ( 5.5%)            I                                         \n                    8. IL                           2039 ( 5.3%)            I                                         \n                    9. MO                           1119 ( 2.9%)                                                      \n                    10. OH                          1040 ( 2.7%)                                                      \n                    [ 28 others ]                   7015 (18.4%)            III                                       \n\n9    condition      Mean (sd) : 3.7 (0.8)           41 distinct values                  . : :     37891      317      \n     [numeric]      min < med < max:                                                    : : :     (99.2%)    (0.8%)   \n                    1 < 3.9 < 5                                                     .   : : : :                       \n                    IQR (CV) : 1.1 (0.2)                                        : . : : : : : :                       \n                                                                                : : : : : : : :                       \n\n10   odometer       Mean (sd) : 48049.3 (42571.8)   29571 distinct values   :                     38203      5        \n     [numeric]      min < med < max:                                        :                     (100.0%)   (0.0%)   \n                    1 < 32961 < 999999                                      :                                         \n                    IQR (CV) : 41625 (0.9)                                  :                                         \n                                                                            : .                                       \n\n11   color          1. white                        10119 (26.5%)           IIIII                 38176      32       \n     [character]    2. black                         8876 (23.3%)           IIII                  (99.9%)    (0.1%)   \n                    3. gray                          6824 (17.9%)           III                                       \n                    4. silver                        3821 (10.0%)           II                                        \n                    5. blue                          2699 ( 7.1%)           I                                         \n                    6. â€”                           2108 ( 5.5%)           I                                         \n                    7. red                           1671 ( 4.4%)                                                     \n                    8. gold                           390 ( 1.0%)                                                     \n                    9. burgundy                       351 ( 0.9%)                                                     \n                    10. beige                         294 ( 0.8%)                                                     \n                    [ 9 others ]                     1023 ( 2.7%)                                                     \n\n12   interior       1. black                        18588 (48.7%)           IIIIIIIII             38176      32       \n     [character]    2. gray                         10169 (26.6%)           IIIII                 (99.9%)    (0.1%)   \n                    3. beige                         5328 (14.0%)           II                                        \n                    4. tan                           2450 ( 6.4%)           I                                         \n                    5. brown                          647 ( 1.7%)                                                     \n                    6. â€”                            614 ( 1.6%)                                                     \n                    7. red                            136 ( 0.4%)                                                     \n                    8. silver                          80 ( 0.2%)                                                     \n                    9. blue                            37 ( 0.1%)                                                     \n                    10. white                          36 ( 0.1%)                                                     \n                    [ 7 others ]                       91 ( 0.2%)                                                     \n\n13   seller         1. nissan infiniti lt           10064 (26.3%)           IIIII                 38208      0        \n     [character]    2. lexus financial services      3674 ( 9.6%)           I                     (100.0%)   (0.0%)   \n                    3. nissan-infiniti lt            1268 ( 3.3%)                                                     \n                    4. gm remarketing                 885 ( 2.3%)                                                     \n                    5. avis budget group              688 ( 1.8%)                                                     \n                    6. infiniti financial servic      683 ( 1.8%)                                                     \n                    7. nissan north america inc.      630 ( 1.6%)                                                     \n                    8. ge fleet services for its      594 ( 1.6%)                                                     \n                    9. r hollenshead auto sales       518 ( 1.4%)                                                     \n                    10. u-haul                        486 ( 1.3%)                                                     \n                    [ 3213 others ]                 18718 (49.0%)           IIIIIIIII                                 \n\n14   mmr            Mean (sd) : 19735.9 (9262.3)    989 distinct values       :                   38208      0        \n     [numeric]      min < med < max:                                          : .                 (100.0%)   (0.0%)   \n                    250 < 19700 < 106000                                      : :                                     \n                    IQR (CV) : 10400 (0.5)                                  : : :                                     \n                                                                            : : : .                                   \n\n15   sellingprice   Mean (sd) : 19619.4 (9364.1)    878 distinct values       :                   38208      0        \n     [numeric]      min < med < max:                                          :                   (100.0%)   (0.0%)   \n                    325 < 19500 < 113000                                      : .                                     \n                    IQR (CV) : 10600 (0.5)                                  : : :                                     \n                                                                            : : : .                                   \n\n16   saledate       1. Tue Jan 27 2015 01:30:00       507 ( 1.3%)                                 38208      0        \n     [character]    2. Tue Feb 17 2015 01:30:00       492 ( 1.3%)                                 (100.0%)   (0.0%)   \n                    3. Tue Jan 20 2015 01:30:00       487 ( 1.3%)                                                     \n                    4. Tue Feb 03 2015 01:30:00       471 ( 1.2%)                                                     \n                    5. Tue Jun 16 2015 02:30:00       444 ( 1.2%)                                                     \n                    6. Tue Mar 03 2015 01:30:00       430 ( 1.1%)                                                     \n                    7. Thu Jan 15 2015 04:30:00       412 ( 1.1%)                                                     \n                    8. Tue Feb 10 2015 01:30:00       404 ( 1.1%)                                                     \n                    9. Tue Jan 06 2015 01:30:00       367 ( 1.0%)                                                     \n                    10. Tue Jun 02 2015 02:30:00      365 ( 1.0%)                                                     \n                    [ 1756 others ]                 33829 (88.5%)           IIIIIIIIIIIIIIIII                         \n\n17   Body_Type      1. Convertible                   1043 ( 3.1%)                                 33919      4289     \n     [character]    2. Coupe                         2374 ( 7.0%)           I                     (88.8%)    (11.2%)  \n                    3. Hatchback                     2423 ( 7.1%)           I                                         \n                    4. Minivan                       3614 (10.7%)           II                                        \n                    5. Pickup                          95 ( 0.3%)                                                     \n                    6. Sedan                        17727 (52.3%)           IIIIIIIIII                                \n                    7. SUV                           6266 (18.5%)           III                                       \n                    8. Wagon                          377 ( 1.1%)                                                     \n\n18   YMM            1. 2013 INFINITI G37             3669 ( 9.6%)           I                     38208      0        \n     [character]    2. 2012 INFINITI G37             2812 ( 7.4%)           I                     (100.0%)   (0.0%)   \n                    3. 2011 INFINITI G37             2037 ( 5.3%)           I                                         \n                    4. 2014 CHEVROLET IMPALA         1116 ( 2.9%)                                                     \n                    5. 2012 LEXUS RX                 1000 ( 2.6%)                                                     \n                    6. 2014 FORD E-350                859 ( 2.2%)                                                     \n                    7. 2012 LEXUS IS                  802 ( 2.1%)                                                     \n                    8. 2013 NISSAN LEAF               802 ( 2.1%)                                                     \n                    9. 2012 INFINITI M37              766 ( 2.0%)                                                     \n                    10. 2013 LEXUS RX                 705 ( 1.8%)                                                     \n                    [ 483 others ]                  23640 (61.9%)           IIIIIIIIIIII                              \n\n19   Segment        1. Mid-Size Car                 14266 (39.6%)           IIIIIII               36048      2160     \n     [character]    2. Compact Car                   6068 (16.8%)           III                   (94.3%)    (5.7%)   \n                    3. Compact SUV/Crossover         4214 (11.7%)           II                                        \n                    4. Minivan                       2715 ( 7.5%)           I                                         \n                    5. Full-Size Car                 2180 ( 6.0%)           I                                         \n                    6. Subcompact Car                1715 ( 4.8%)                                                     \n                    7. Mid-Size SUV/Crossover        1669 ( 4.6%)                                                     \n                    8. Subcompact SUV/Crossover      1200 ( 3.3%)                                                     \n                    9. Full-Size SUV/Crossover       1145 ( 3.2%)                                                     \n                    10. Sports Car                    404 ( 1.1%)                                                     \n                    [ 2 others ]                      472 ( 1.3%)                                                     \n\n20   AVG_MSRP       Mean (sd) : 45858.3 (14383)     511 distinct values         . :               38208      0        \n     [numeric]      min < med < max:                                            : :               (100.0%)   (0.0%)   \n                    14 < 45530 < 151762                                         : :                                   \n                    IQR (CV) : 18086 (0.3)                                    . : :                                   \n                                                                              : : : .                                 \n\n21   MSRP_Range     Mean (sd) : 12627 (8787.1)      415 distinct values     :                     38208      0        \n     [numeric]      min < med < max:                                        : .                   (100.0%)   (0.0%)   \n                    0 < 11672 < 116984                                      : :                                       \n                    IQR (CV) : 12425 (0.7)                                  : : .                                     \n                                                                            : : :                                     \n\n22   Trim_Count     Mean (sd) : 8.3 (8.4)           30 distinct values      :                     38208      0        \n     [numeric]      min < med < max:                                        :                     (100.0%)   (0.0%)   \n                    1 < 7 < 131                                             :                                         \n                    IQR (CV) : 11 (1)                                       :                                         \n                                                                            : :                                       \n\n23   StdDev_MSRP    Mean (sd) : 5388.7 (3148.9)     453 distinct values       :                   35115      3093     \n     [numeric]      min < med < max:                                        : :                   (91.9%)    (8.1%)   \n                    0 < 5693.7 < 47165.7                                    : :                                       \n                    IQR (CV) : 3900.9 (0.6)                                 : :                                       \n                                                                            : : .                                     \n\n24   Error.MSRP     Mean (sd) : 2234.8 (1857.4)     453 distinct values     :                     35115      3093     \n     [numeric]      min < med < max:                                        :                     (91.9%)    (8.1%)   \n                    0 < 1683.4 < 21093.1                                    :                                         \n                    IQR (CV) : 1199 (0.8)                                   : .                                       \n                                                                            : : . .                                   \n\n25   Error %        Mean (sd) : 4.7 (2.8)           103 distinct values       : :                 35115      3093     \n     [numeric]      min < med < max:                                          : : .               (91.9%)    (8.1%)   \n                    0 < 4.1 < 15.1                                            : : : .                                 \n                    IQR (CV) : 3.1 (0.6)                                      : : : :                                 \n                                                                            : : : : : : . .   .                       \n\n26   %MSRP_Range    Mean (sd) : 28.1 (17.1)         310 distinct values           :               38208      0        \n     [numeric]      min < med < max:                                              :               (100.0%)   (0.0%)   \n                    0 < 30.8 < 107.2                                        :   . :                                   \n                    IQR (CV) : 25.9 (0.6)                                   : : : :                                   \n                                                                            : : : : : .                               \n----------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#appendix-e",
    "href": "posts/OwenTibbyFinalProject601_submission.html#appendix-e",
    "title": "Final Project",
    "section": "Appendix E:",
    "text": "Appendix E:\nRemaining records from false_keys that failed to map after decoding VINs labelled as unmatched_keys.\n\n\nCode\ndfSummary(unmatched_keys)\n\n\nData Frame Summary  \nunmatched_keys  \nDimensions: 20826 x 26  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values                  Freqs (% of Valid)      Graph                 Valid      Missing   \n---- -------------- ------------------------------- ----------------------- --------------------- ---------- ----------\n1    year           1. 2007                         1612 ( 7.7%)            I                     20826      0         \n     [character]    2. 2008                         3207 (15.4%)            III                   (100.0%)   (0.0%)    \n                    3. 2009                          803 ( 3.9%)                                                       \n                    4. 2010                         1817 ( 8.7%)            I                                          \n                    5. 2011                         1847 ( 8.9%)            I                                          \n                    6. 2012                         2605 (12.5%)            II                                         \n                    7. 2013                         2676 (12.8%)            II                                         \n                    8. 2014                         5654 (27.1%)            IIIII                                      \n                    9. 2015                          605 ( 2.9%)                                                       \n\n2    make           1. CHRYSLER                     5334 (27.7%)            IIIII                 19287      1539      \n     [character]    2. FORD                         5026 (26.1%)            IIIII                 (92.6%)    (7.4%)    \n                    3. DODGE                        1343 ( 7.0%)            I                                          \n                    4. CHEVROLET                    1066 ( 5.5%)            I                                          \n                    5. MITSUBISHI                    705 ( 3.7%)                                                       \n                    6. MINI                          663 ( 3.4%)                                                       \n                    7. MERCURY                       646 ( 3.3%)                                                       \n                    8. VOLKSWAGEN                    593 ( 3.1%)                                                       \n                    9. NISSAN                        565 ( 2.9%)                                                       \n                    10. SCION                        540 ( 2.8%)                                                       \n                    [ 31 others ]                   2806 (14.5%)            II                                         \n\n3    model          1. TOWN AND COUNTRY             5319 (27.6%)            IIIII                 19287      1539      \n     [character]    2. F-250 SUPER DUTY             1946 (10.1%)            II                    (92.6%)    (7.4%)    \n                    3. E-SERIES VAN                 1372 ( 7.1%)            I                                          \n                    4. CAPTIVA SPORT                1016 ( 5.3%)            I                                          \n                    5. RAM PICKUP 1500               996 ( 5.2%)            I                                          \n                    6. F-350 SUPER DUTY              853 ( 4.4%)                                                       \n                    7. COOPER                        663 ( 3.4%)                                                       \n                    8. TRANSIT CONNECT               555 ( 2.9%)                                                       \n                    9. OUTLANDER SPORT               381 ( 2.0%)                                                       \n                    10. TC                           327 ( 1.7%)                                                       \n                    [ 125 others ]                  5859 (30.4%)            IIIIII                                     \n\n4    trim           1. Touring                      4333 (22.5%)            IIII                  19273      1553      \n     [character]    2. Base                         2131 (11.1%)            II                    (92.5%)    (7.5%)    \n                    3. E-250                        1550 ( 8.0%)            I                                          \n                    4. Lariat                       1064 ( 5.5%)            I                                          \n                    5. XLT                           815 ( 4.2%)                                                       \n                    6. SLT                           760 ( 3.9%)                                                       \n                    7. XL                            715 ( 3.7%)                                                       \n                    8. S                             584 ( 3.0%)                                                       \n                    9. Touring-L                     564 ( 2.9%)                                                       \n                    10. LX                           411 ( 2.1%)                                                       \n                    [ 171 others ]                  6346 (32.9%)            IIIIII                                     \n\n5    body           1. MINIVAN                      5988 (31.4%)            IIIIII                19085      1741      \n     [character]    2. CREW CAB                     2550 (13.4%)            II                    (91.6%)    (8.4%)    \n                    3. HATCHBACK                    2129 (11.2%)            II                                         \n                    4. SUV                          2052 (10.8%)            II                                         \n                    5. SEDAN                        1816 ( 9.5%)            I                                          \n                    6. E-SERIES VAN                 1372 ( 7.2%)            I                                          \n                    7. QUAD CAB                      974 ( 5.1%)            I                                          \n                    8. SUPERCAB                      438 ( 2.3%)                                                       \n                    9. WAGON                         383 ( 2.0%)                                                       \n                    10. VAN                          322 ( 1.7%)                                                       \n                    [ 10 others ]                   1061 ( 5.6%)            I                                          \n\n6    transmission   1. automatic                    17115 (95.6%)           IIIIIIIIIIIIIIIIIII   17912      2914      \n     [character]    2. manual                         797 ( 4.4%)                                 (86.0%)    (14.0%)   \n\n7    vin            1. 1FT7W2BTXDEA03416                3 ( 0.0%)                                 20826      0         \n     [character]    2. 1FT8W3DT1CEB32666                3 ( 0.0%)                                 (100.0%)   (0.0%)    \n                    3. 1FTSW21568EA33810                3 ( 0.0%)                                                      \n                    4. 1FTSW21R48EB47680                3 ( 0.0%)                                                      \n                    5. 1FTXW43R28EA88031                3 ( 0.0%)                                                      \n                    6. 2C4RC1BG2DR821210                3 ( 0.0%)                                                      \n                    7. 2G4WD582571126964                3 ( 0.0%)                                                      \n                    8. 1D3HU18268J106114                2 ( 0.0%)                                                      \n                    9. 1D7HA16N28J150504                2 ( 0.0%)                                                      \n                    10. 1D7HA18278J186420               2 ( 0.0%)                                                      \n                    [ 20497 others ]                20799 (99.9%)           IIIIIIIIIIIIIIIIIII                        \n\n8    state          1. FL                           2926 (14.0%)            II                    20826      0         \n     [character]    2. CA                           2470 (11.9%)            II                    (100.0%)   (0.0%)    \n                    3. TX                           2208 (10.6%)            II                                         \n                    4. PA                           2134 (10.2%)            II                                         \n                    5. IL                           1063 ( 5.1%)            I                                          \n                    6. GA                           1027 ( 4.9%)                                                       \n                    7. MI                            959 ( 4.6%)                                                       \n                    8. NJ                            800 ( 3.8%)                                                       \n                    9. OH                            670 ( 3.2%)                                                       \n                    10. MN                           621 ( 3.0%)                                                       \n                    [ 27 others ]                   5948 (28.6%)            IIIII                                      \n\n9    condition      Mean (sd) : 3.5 (0.9)           40 distinct values                    :       20543      283       \n     [numeric]      min < med < max:                                                    : :       (98.6%)    (1.4%)    \n                    1 < 3.7 < 5                                                 .   :   : : : .                        \n                    IQR (CV) : 1.4 (0.3)                                        : . : : : : : :                        \n                                                                                : : : : : : : :                        \n\n10   odometer       Mean (sd) : 59768.7 (47714.6)   19162 distinct values   :                     20825      1         \n     [numeric]      min < med < max:                                        :                     (100.0%)   (0.0%)    \n                    1 < 46884 < 397857                                      : :                                        \n                    IQR (CV) : 64874 (0.8)                                  : : :                                      \n                                                                            : : : : .                                  \n\n11   color          1. white                        6646 (32.0%)            IIIIII                20796      30        \n     [character]    2. black                        3286 (15.8%)            III                   (99.9%)    (0.1%)    \n                    3. silver                       2522 (12.1%)            II                                         \n                    4. gray                         2247 (10.8%)            II                                         \n                    5. blue                         1991 ( 9.6%)            I                                          \n                    6. red                          1653 ( 7.9%)            I                                          \n                    7. â€”                           772 ( 3.7%)                                                       \n                    8. gold                          454 ( 2.2%)                                                       \n                    9. burgundy                      328 ( 1.6%)                                                       \n                    10. beige                        301 ( 1.4%)                                                       \n                    [ 8 others ]                     596 ( 2.9%)                                                       \n\n12   interior       1. black                        9610 (46.2%)            IIIIIIIII             20796      30        \n     [character]    2. gray                         7364 (35.4%)            IIIIIII               (99.9%)    (0.1%)    \n                    3. beige                        1356 ( 6.5%)            I                                          \n                    4. tan                          1259 ( 6.1%)            I                                          \n                    5. brown                         527 ( 2.5%)                                                       \n                    6. â€”                           489 ( 2.4%)                                                       \n                    7. red                            57 ( 0.3%)                                                       \n                    8. silver                         44 ( 0.2%)                                                       \n                    9. blue                           34 ( 0.2%)                                                       \n                    10. purple                        15 ( 0.1%)                                                       \n                    [ 6 others ]                      41 ( 0.2%)                                                       \n\n13   seller         1. u-haul                         958 ( 4.6%)                                 20826      0         \n     [character]    2. enterprise veh exchange/r      679 ( 3.3%)                                 (100.0%)   (0.0%)    \n                    3. the hertz corporation          630 ( 3.0%)                                                      \n                    4. avis corporation               544 ( 2.6%)                                                      \n                    5. santander consumer             517 ( 2.5%)                                                      \n                    6. enterprise holdings/gdp        503 ( 2.4%)                                                      \n                    7. chrysler capital               497 ( 2.4%)                                                      \n                    8. hertz corporation/gdp          485 ( 2.3%)                                                      \n                    9. pv holding inc/gdp             454 ( 2.2%)                                                      \n                    10. ge fleet services for its     432 ( 2.1%)                                                      \n                    [ 2945 others ]                 15127 (72.6%)           IIIIIIIIIIIIII                             \n\n14   mmr            Mean (sd) : 17493.5 (13122.6)   1032 distinct values    :                     20826      0         \n     [numeric]      min < med < max:                                        :                     (100.0%)   (0.0%)    \n                    175 < 16050 < 182000                                    : .                                        \n                    IQR (CV) : 11100 (0.8)                                  : :                                        \n                                                                            : : .                                      \n\n15   sellingprice   Mean (sd) : 17296.7 (13121.9)   907 distinct values     :                     20826      0         \n     [numeric]      min < med < max:                                        :                     (100.0%)   (0.0%)    \n                    300 < 15900 < 183000                                    : .                                        \n                    IQR (CV) : 11200 (0.8)                                  : :                                        \n                                                                            : : .                                      \n\n16   saledate       1. Tue Feb 10 2015 01:30:00       165 ( 0.8%)                                 20826      0         \n     [character]    2. Tue Mar 03 2015 01:30:00       164 ( 0.8%)                                 (100.0%)   (0.0%)    \n                    3. Tue Feb 17 2015 01:30:00       151 ( 0.7%)                                                      \n                    4. Tue Feb 03 2015 01:30:00       149 ( 0.7%)                                                      \n                    5. Thu Feb 05 2015 01:30:00       148 ( 0.7%)                                                      \n                    6. Thu Feb 26 2015 03:00:00       142 ( 0.7%)                                                      \n                    7. Thu Jan 22 2015 01:30:00       141 ( 0.7%)                                                      \n                    8. Tue Jan 27 2015 01:30:00       139 ( 0.7%)                                                      \n                    9. Tue Jun 16 2015 02:30:00       127 ( 0.6%)                                                      \n                    10. Thu Mar 05 2015 01:30:00      125 ( 0.6%)                                                      \n                    [ 1675 others ]                 19375 (93.0%)           IIIIIIIIIIIIIIIIII                         \n\n17   Body_Type      1. Convertible                   282 ( 1.5%)                                  19085      1741      \n     [character]    2. Coupe                         438 ( 2.3%)                                  (91.6%)    (8.4%)    \n                    3. Hatchback                    2129 (11.2%)            II                                         \n                    4. Minivan                      7760 (40.7%)            IIIIIIII                                   \n                    5. Pickup                       4225 (22.1%)            IIII                                       \n                    6. Sedan                        1816 ( 9.5%)            I                                          \n                    7. SUV                          2052 (10.8%)            II                                         \n                    8. Wagon                         383 ( 2.0%)                                                       \n\n18   YMM            1. 2014 Chrysler Town and Co     2829 (13.6%)           II                    20826      0         \n     [character]    2. 2014 Ford E-Series Van        1114 ( 5.3%)           I                     (100.0%)   (0.0%)    \n                    3. 2013 Chrysler Town and Co      897 ( 4.3%)                                                      \n                    4. 2012 NA NA                     781 ( 3.8%)                                                      \n                    5. 2014 Chevrolet Captiva Sp      509 ( 2.4%)                                                      \n                    6. 2008 Ford F-250 Super Dut      474 ( 2.3%)                                                      \n                    7. 2008 Dodge Ram Pickup 150      473 ( 2.3%)                                                      \n                    8. 2013 Chevrolet Captiva Sp      406 ( 1.9%)                                                      \n                    9. 2008 Chrysler Town and Co      384 ( 1.8%)                                                      \n                    10. 2010 Dodge Ram Pickup 150     379 ( 1.8%)                                                      \n                    [ 339 others ]                  12580 (60.4%)           IIIIIIIIIIII                               \n\n19   Segment        All NA's                                                                      0          20826     \n     [character]                                                                                  (0.0%)     (100.0%)  \n\n20   AVG_MSRP       All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n\n21   MSRP_Range     All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n\n22   Trim_Count     All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n\n23   StdDev_MSRP    All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n\n24   Error.MSRP     All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n\n25   Error %        All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n\n26   %MSRP_Range    All NA's                                                                      0          20826     \n     [numeric]                                                                                    (0.0%)     (100.0%)  \n-----------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/OwenTibbyFinalProject601_submission.html#appendix-f",
    "href": "posts/OwenTibbyFinalProject601_submission.html#appendix-f",
    "title": "Final Project",
    "section": "Appendix F:",
    "text": "Appendix F:\nFinalized and cleaned auction data carprices.\n\n\nCode\ndfSummary(carprices)\n\n\nData Frame Summary  \ncarprices  \nDimensions: 440582 x 24  \nDuplicates: 1043  \n\n-----------------------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values                  Freqs (% of Valid)       Graph                 Valid      Missing  \n---- -------------- ------------------------------- ------------------------ --------------------- ---------- ---------\n1    year           Mean (sd) : 2011.7 (2.1)        2007 :  29503 ( 6.7%)    I                     440582     0        \n     [numeric]      min < med < max:                2008 :  28910 ( 6.6%)    I                     (100.0%)   (0.0%)   \n                    2007 < 2012 < 2015              2009 :  20172 ( 4.6%)                                              \n                    IQR (CV) : 2 (0)                2010 :  25510 ( 5.8%)    I                                         \n                                                    2011 :  48099 (10.9%)    II                                        \n                                                    2012 : 100873 (22.9%)    IIII                                      \n                                                    2013 :  97936 (22.2%)    IIII                                      \n                                                    2014 :  80986 (18.4%)    III                                       \n                                                    2015 :   8593 ( 2.0%)                                              \n\n2    make           1. Ford                          76396 (17.3%)           III                   440582     0        \n     [character]    2. Chevrolet                     51555 (11.7%)           II                    (100.0%)   (0.0%)   \n                    3. Nissan                        43409 ( 9.9%)           I                                         \n                    4. Toyota                        31100 ( 7.1%)           I                                         \n                    5. Dodge                         28416 ( 6.4%)           I                                         \n                    6. Honda                         19364 ( 4.4%)                                                     \n                    7. Hyundai                       18239 ( 4.1%)                                                     \n                    8. Kia                           16219 ( 3.7%)                                                     \n                    9. BMW                           15552 ( 3.5%)                                                     \n                    10. INFINITI                     13916 ( 3.2%)                                                     \n                    [ 59 others ]                   126416 (28.7%)           IIIII                                     \n\n3    model          1. Altima                        17283 ( 3.9%)                                 440582     0        \n     [character]    2. Fusion                        12607 ( 2.9%)                                 (100.0%)   (0.0%)   \n                    3. Impala                        12084 ( 2.7%)                                                     \n                    4. Focus                         11924 ( 2.7%)                                                     \n                    5. F-150                         11780 ( 2.7%)                                                     \n                    6. Escape                        10868 ( 2.5%)                                                     \n                    7. Camry                         10407 ( 2.4%)                                                     \n                    8. G37                            8765 ( 2.0%)                                                     \n                    9. Charger                        7902 ( 1.8%)                                                     \n                    10. Grand Caravan                 7134 ( 1.6%)                                                     \n                    [ 508 others ]                  329828 (74.9%)           IIIIIIIIIIIIII                            \n\n4    trim           1. SE                            43220 ( 9.9%)           I                     437241     3341     \n     [character]    2. Base                          38248 ( 8.7%)           I                     (99.2%)    (0.8%)   \n                    3. LX                            16546 ( 3.8%)                                                     \n                    4. LT                            16386 ( 3.7%)                                                     \n                    5. Limited                       15506 ( 3.5%)                                                     \n                    6. XLT                           12476 ( 2.9%)                                                     \n                    7. SXT                           10820 ( 2.5%)                                                     \n                    8. GLS                           10553 ( 2.4%)                                                     \n                    9. LE                            10456 ( 2.4%)                                                     \n                    10. 2.5 S                        10374 ( 2.4%)                                                     \n                    [ 1116 others ]                 252656 (57.8%)           IIIIIIIIIII                               \n\n5    transmission   1. automatic                    380465 (97.3%)           IIIIIIIIIIIIIIIIIII   391040     49542    \n     [character]    2. manual                        10575 ( 2.7%)                                 (88.8%)    (11.2%)  \n\n6    Body_Type      1. Convertible                    7080 ( 1.6%)                                 436601     3981     \n     [character]    2. Coupe                         14742 ( 3.4%)                                 (99.1%)    (0.9%)   \n                    3. Hatchback                     22578 ( 5.2%)           I                                         \n                    4. Minivan                       17672 ( 4.0%)                                                     \n                    5. Pickup                        32990 ( 7.6%)           I                                         \n                    6. Sedan                        213307 (48.9%)           IIIIIIIII                                 \n                    7. SUV                          115740 (26.5%)           IIIII                                     \n                    8. Wagon                         12492 ( 2.9%)                                                     \n\n7    state          1. FL                            66449 (15.1%)           III                   440582     0        \n     [character]    2. CA                            53754 (12.2%)           II                    (100.0%)   (0.0%)   \n                    3. PA                            46643 (10.6%)           II                                        \n                    4. TX                            37667 ( 8.5%)           I                                         \n                    5. GA                            27253 ( 6.2%)           I                                         \n                    6. NJ                            22365 ( 5.1%)           I                                         \n                    7. IL                            20557 ( 4.7%)                                                     \n                    8. TN                            19572 ( 4.4%)                                                     \n                    9. OH                            17293 ( 3.9%)                                                     \n                    10. MO                           14683 ( 3.3%)                                                     \n                    [ 28 others ]                   114346 (26.0%)           IIIII                                     \n\n8    condition      Mean (sd) : 3.6 (0.8)           36 distinct values                   . :       440582     0        \n     [numeric]      min < med < max:                                                     : :       (100.0%)   (0.0%)   \n                    1.5 < 3.8 < 5                                                  .   . : : : .                       \n                    IQR (CV) : 1.3 (0.2)                                       :   : . : : : : :                       \n                                                                               : : : : : : : : :                       \n\n9    odometer       Mean (sd) : 51566.9 (37924.8)   125674 distinct values   :                     440569     13       \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    1 < 40632 < 999999                                       :                                         \n                    IQR (CV) : 45156 (0.7)                                   :                                         \n                                                                             : .                                       \n\n10   color          1. black                        93179 (21.2%)            IIII                  439981     601      \n     [character]    2. white                        86178 (19.6%)            III                   (99.9%)    (0.1%)   \n                    3. gray                         69021 (15.7%)            III                                       \n                    4. silver                       62762 (14.3%)            II                                        \n                    5. blue                         36722 ( 8.3%)            I                                         \n                    6. red                          34815 ( 7.9%)            I                                         \n                    7. â€”                          24059 ( 5.5%)            I                                         \n                    8. burgundy                      6431 ( 1.5%)                                                      \n                    9. brown                         5583 ( 1.3%)                                                      \n                    10. gold                         5469 ( 1.2%)                                                      \n                    [ 10 others ]                   15762 ( 3.6%)                                                      \n\n11   mmr            Mean (sd) : 16064.2 (9019.3)    1070 distinct values     :                     440582     0        \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    75 < 13950 < 176000                                      :                                         \n                    IQR (CV) : 9900 (0.6)                                    : :                                       \n                                                                             : :                                       \n\n12   sellingprice   Mean (sd) : 15926 (9096.5)      1733 distinct values     :                     440582     0        \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    275 < 13900 < 173000                                     :                                         \n                    IQR (CV) : 9900 (0.6)                                    : :                                       \n                                                                             : :                                       \n\n13   sale_year      1. 2014                          32075 ( 7.3%)           I                     440582     0        \n     [character]    2. 2015                         408507 (92.7%)           IIIIIIIIIIIIIIIIII    (100.0%)   (0.0%)   \n\n14   launch_date    1. 2006-09-01                   29503 ( 6.7%)            I                     440582     0        \n     [Date]         2. 2007-09-01                   28910 ( 6.6%)            I                     (100.0%)   (0.0%)   \n                    3. 2008-09-01                   20172 ( 4.6%)                                                      \n                    4. 2009-09-01                   25510 ( 5.8%)            I                                         \n                    5. 2010-09-01                   48099 (10.9%)            II                                        \n                    6. 2011-09-01                   100873 (22.9%)           IIII                                      \n                    7. 2012-09-01                   97936 (22.2%)            IIII                                      \n                    8. 2013-09-01                   80986 (18.4%)            III                                       \n                    9. 2014-09-01                   8593 ( 2.0%)                                                       \n\n15   Date_Sold      min : 2014-01-01                168 distinct values                  . :       440582     0        \n     [Date]         med : 2015-02-17                                                     : :   .   (100.0%)   (0.0%)   \n                    max : 2015-07-21                                                     : :   :                       \n                    range : 1y 6m 20d                                                    : :   :                       \n                                                                                         : : . :                       \n\n16   AVG_MSRP       Mean (sd) : 35480.9 (16141.4)   2166 distinct values     :                     440582     0        \n     [numeric]      min < med < max:                                         : :                   (100.0%)   (0.0%)   \n                    10478 < 31287 < 215813                                   : :                                       \n                    IQR (CV) : 13329 (0.5)                                   : :                                       \n                                                                             : : :                                     \n\n17   Trim_Count     Mean (sd) : 14.1 (23.9)         81 distinct values       :                     440582     0        \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    1 < 8 < 224                                              :                                         \n                    IQR (CV) : 7 (1.7)                                       :                                         \n                                                                             :                                         \n\n18   Error %        Mean (sd) : 4.7 (2.9)           159 distinct values      :                     440582     0        \n     [numeric]      min < med < max:                                         :                     (100.0%)   (0.0%)   \n                    0 < 4.6 < 99.9                                           :                                         \n                    IQR (CV) : 2.7 (0.6)                                     :                                         \n                                                                             :                                         \n\n19   %MSRP_Range    Mean (sd) : 40.7 (22.7)         695 distinct values        : .                 440582     0        \n     [numeric]      min < med < max:                                           : :                 (100.0%)   (0.0%)   \n                    0 < 39.1 < 199.7                                           : :                                     \n                    IQR (CV) : 20.2 (0.6)                                    : : :                                     \n                                                                             : : : : . .                               \n\n20   Segment        1. Mid-Size Car                 110030 (26.3%)           IIIII                 418309     22273    \n     [character]    2. Compact Car                   80614 (19.3%)           III                   (94.9%)    (5.1%)   \n                    3. Compact SUV/Crossover         51377 (12.3%)           II                                        \n                    4. Mid-Size SUV/Crossover        45038 (10.8%)           II                                        \n                    5. Full-Size Car                 26881 ( 6.4%)           I                                         \n                    6. LD Full-Size Pickup           24419 ( 5.8%)           I                                         \n                    7. Subcompact Car                20785 ( 5.0%)                                                     \n                    8. Minivan                       16623 ( 4.0%)                                                     \n                    9. Full-Size SUV/Crossover       13921 ( 3.3%)                                                     \n                    10. Sports Car                   12985 ( 3.1%)                                                     \n                    [ 3 others ]                     15636 ( 3.7%)                                                     \n\n21   Age_months     Mean (sd) : 46.6 (25.8)         71 distinct values             :               440582     0        \n     [numeric]      min < med < max:                                           . . :               (100.0%)   (0.0%)   \n                    0 < 42 < 108                                               : : : :                                 \n                    IQR (CV) : 29 (0.6)                                        : : : :       . .                       \n                                                                             . : : : : : : : : :                       \n\n22   Age_years      Mean (sd) : 3.9 (2.2)           0 :   3385 ( 0.8%)                             440582     0        \n     [numeric]      min < med < max:                1 :  35432 ( 8.0%)       I                     (100.0%)   (0.0%)   \n                    0 < 4 < 9                       2 : 112822 (25.6%)       IIIII                                     \n                    IQR (CV) : 3 (0.6)              3 :  63738 (14.5%)       II                                        \n                                                    4 : 103786 (23.6%)       IIII                                      \n                                                    5 :  23061 ( 5.2%)       I                                         \n                                                    6 :  30003 ( 6.8%)       I                                         \n                                                    7 :  12962 ( 2.9%)                                                 \n                                                    8 :  36563 ( 8.3%)       I                                         \n                                                    9 :  18830 ( 4.3%)                                                 \n\n23   RV_percent     Mean (sd) : 45.6 (17.7)         1230 distinct values           :               440582     0        \n     [numeric]      min < med < max:                                             . : :             (100.0%)   (0.0%)   \n                    0.7 < 45.1 < 124.7                                           : : :                                 \n                    IQR (CV) : 23 (0.4)                                        : : : : :                               \n                                                                             . : : : : : .                             \n\n24   Annual_miles   Mean (sd) : Inf (NaN)           36291 distinct values                          440569     13       \n     [numeric]      min < med < max:                                                               (100.0%)   (0.0%)   \n                    0 < 12375 < Inf                                                                                    \n                    IQR (CV) : 8504 (NaN)                                                                              \n-----------------------------------------------------------------------------------------------------------------------\n\n\nMore about the NHTSA’s vindecodr package:\nBelow is an example of the API call from the NHTSA’s website: https://vpic.nhtsa.dot.gov/api/Home/Index/LanguageExamples\nrequire(RJSONIO)\n# User-defined functions ----------------------------------\nVehicleAPIrConnect <- function(VINin){\n    # Lookup VIN information from https://vpic.nhtsa.dot.gov/api\n    #\n    # Args:\n    #   VINin: VIN\n    #\n    # Returns:\n    #   Data frame with vehicle information.\n    #\n    # For Testing:\n    # VINin <- \"5UXWX7C5*BA\"\n    tempCall <- paste0(\"https://vpic.nhtsa.dot.gov/api/vehicles/DecodeVinValues/\", VINin, \"?format=json\")\n    tempExtract <- fromJSON(tempCall)\n    dfOut <- data.frame(t(unlist(tempExtract$Results)),stringsAsFactors=FALSE)\n    dfOut\n}\nVehicleAPIrConnect(\"5UXWX7C5*BA\")"
  },
  {
    "objectID": "posts/PrajaktiKapade_Final_Project.html",
    "href": "posts/PrajaktiKapade_Final_Project.html",
    "title": "SF Library Usage Analysis",
    "section": "",
    "text": "Prior to computerization, library tasks were performed manually and independently from one another. Selectors ordered materials with ordering slips, cataloguers manually catalogued sources and indexed them with the card catalog system (in which all bibliographic data was kept on a single index card), fines were collected by local bailiffs, and users signed books out manually, indicating their name on clue cards which were then kept at the circulation desk.\nEarly mechanization came in 1936, when the University of Texas began using a punch card system to manage library circulation. While the punch card system allowed for more efficient tracking of loans, library services were far from being integrated, and no other library task was affected by this change. This led to customer dissatisfaction and they wanted to move towards a very systematic way of keeping track, then came in Integrated Library systems, one example of which is my dataset for the final project.\nThis system is perfect for all educational institutions that wish to maintain minimum operational costs. The system helps with better management, lesser wastage of time and improves engagement and productivity. With this data, we can analyze the trends about different patrons based on their age, based on the registered year, their renewals and checkouts, and see the circulation trend.\nInstalling and importing necessary packages\n\n#install.packages(\"hrbrthemes\")\n#install.packages(\"viridis\")\n#install.packages('patchwork')\n#install.packages('zoo')\n#install.packages('directlabels')\n\n\nlibrary(tidyverse) \n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(hrbrthemes)\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.2.2\n\n\nLoading required package: viridisLite\n\n\nWarning: package 'viridisLite' was built under R version 4.2.2\n\nlibrary(RColorBrewer)\nlibrary(patchwork)\nlibrary(ggbeeswarm)\n\nWarning: package 'ggbeeswarm' was built under R version 4.2.2\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(directlabels)\n\nWarning: package 'directlabels' was built under R version 4.2.2\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/PrajaktiKapade_Final_Project.html#data",
    "href": "posts/PrajaktiKapade_Final_Project.html#data",
    "title": "SF Library Usage Analysis",
    "section": "DATA",
    "text": "DATA\n\nLoading the dataset\nThe dataset - SF Integrated Learning System is sourced from Kaggle.com and is composed of bibliographic records including inventoried items, patron records, and circulation data. The data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. This dataset represents the usage of inventoried items by patrons (~420K records). It contains data starting from 2003 uptill 2016.\n\nlibrary_dataset <- read_csv(\"_data/Library_Usage.csv\")\nlibrary_dataset\n\n# A tibble: 423,448 × 15\n   Patron Type…¹ Patro…² Total…³ Total…⁴ Age R…⁵ Home …⁶ Home …⁷ Circu…⁸ Circu…⁹\n           <dbl> <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1             3 SENIOR       28      13 65 to … X       Main L… Novemb… 2012   \n 2             0 ADULT        21      10 55 to … X       Main L… October 2015   \n 3             0 ADULT       275     559 60 to … X       Main L… January 2015   \n 4             0 ADULT        73      38 45 to … M8      Missio… Februa… 2016   \n 5             0 ADULT       182      90 45 to … X       Main L… July    2016   \n 6             3 SENIOR        1       0 65 to … X       Main L… July    2004   \n 7            16 DIGITA…      31       4 65 to … P7      Potrero January 2012   \n 8             0 ADULT       547     177 60 to … X       Main L… July    2016   \n 9             0 ADULT        28       1 60 to … X       Main L… Decemb… 2015   \n10             0 ADULT        77       3 60 to … X       Main L… July    2016   \n# … with 423,438 more rows, 6 more variables: `Notice Preference Code` <chr>,\n#   `Notice Preference Definition` <chr>, `Provided Email Address` <lgl>,\n#   `Year Patron Registered` <dbl>, `Outside of County` <lgl>,\n#   `Supervisor District` <dbl>, and abbreviated variable names\n#   ¹​`Patron Type Code`, ²​`Patron Type Definition`, ³​`Total Checkouts`,\n#   ⁴​`Total Renewals`, ⁵​`Age Range`, ⁶​`Home Library Code`,\n#   ⁷​`Home Library Definition`, ⁸​`Circulation Active Month`, …\n\n\n\n\nSummary of the dataset\n\ndim(library_dataset)\n\n[1] 423448     15\n\n\nThe dataset contains 423448 records and 15 columns.\n\nglimpse(library_dataset)\n\nRows: 423,448\nColumns: 15\n$ `Patron Type Code`             <dbl> 3, 0, 0, 0, 0, 3, 16, 0, 0, 0, 3, 3, 0,…\n$ `Patron Type Definition`       <chr> \"SENIOR\", \"ADULT\", \"ADULT\", \"ADULT\", \"A…\n$ `Total Checkouts`              <dbl> 28, 21, 275, 73, 182, 1, 31, 547, 28, 7…\n$ `Total Renewals`               <dbl> 13, 10, 559, 38, 90, 0, 4, 177, 1, 3, 9…\n$ `Age Range`                    <chr> \"65 to 74 years\", \"55 to 59 years\", \"60…\n$ `Home Library Code`            <chr> \"X\", \"X\", \"X\", \"M8\", \"X\", \"X\", \"P7\", \"X…\n$ `Home Library Definition`      <chr> \"Main Library\", \"Main Library\", \"Main L…\n$ `Circulation Active Month`     <chr> \"November\", \"October\", \"January\", \"Febr…\n$ `Circulation Active Year`      <chr> \"2012\", \"2015\", \"2015\", \"2016\", \"2016\",…\n$ `Notice Preference Code`       <chr> \"z\", \"z\", \"z\", \"z\", \"z\", \"z\", \"z\", \"p\",…\n$ `Notice Preference Definition` <chr> \"email\", \"email\", \"email\", \"email\", \"em…\n$ `Provided Email Address`       <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU…\n$ `Year Patron Registered`       <dbl> 2003, 2003, 2003, 2003, 2003, 2003, 200…\n$ `Outside of County`            <lgl> TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, …\n$ `Supervisor District`          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nThe above shows the columns of the dataset along with their datatype and the initial recordings of that particular columns. The distinct data types are : dbl - double/real number values, chr - characters and lgl - for boolean values.\nThe columns and their description are as follows:\n\nPatron Type Code- Type of patron record (adult, teen, child, senior, etc.)\nPatron Type Definition - Description of patron (adult, teen, child, senior, etc.).\nTotal Checkouts- Total number of items the patron has checked out from the library since the record was created.\nTotal Renewals - Total number of times the patron has renewed checked-out items.\nAge Range- Contains age ranges\nHome Library Code- Default value indicates the branch library where the patron was originally registered.\nHome Library Definition - Description of the branch library where the patron was originally registered.\nCirculation Active Month - Year the patron last checked out library materials, or last logged into the library’s subscription databases.\nCirculation Active Year - Month the patron last checked out library materials, or last logged into the library’s subscription databases.\nNotice Preference Code- This field is used to indicate the patron’s preferred method of receiving library notices.\nNotice Preference Definition- Description of the patron’s preferred method of receiving library notices.\nProvided Email Address - Indicates if the patron has provided email address or not.\nYear Patron Registered- Year patron registered with library system. No dates prior to 2003 due to system migration.\nOutside of County - If a patron’s home address is not in San Francisco, then flagged as true, otherwise false.\nSupervisor District - Based on patron address: San Francisco Supervisor District. Based on the website it suggested that “This is an automated field, please note that if”Outside of County” is true, then there will be no supervisor district. Also, if the input address was not well-formed, the supervisor district will be blank.”\n\n\n\nRenaming the columns\n\nlibrary_dataset <- library_dataset %>% \n  rename( 'out_of_county' = 'Outside of County', \"patron_type\" = 'Patron Type Definition', \"total_checkouts\" = 'Total Checkouts', \"total_renewals\"= 'Total Renewals', 'age' = 'Age Range', 'branch' = 'Home Library Definition','active_year' = 'Circulation Active Year', 'active_month' = 'Circulation Active Month', 'notice_preference' = 'Notice Preference Definition', 'provided_email' = 'Provided Email Address', 'year_registered' = 'Year Patron Registered', \"outside_of_county\" = 'Outside of County')\n\n\n\nCleaning the dataset\nSelected the columns with unique characteristic for our visualizations and analysis. Some columns contain the code for the definition example : Home Library Code is the subsequent code for the Home Library Definition.\n\nlibrary_data <- library_dataset%>%\n                select(patron_type,total_checkouts,total_renewals,age,branch,active_month,active_year,notice_preference,provided_email,year_registered,outside_of_county)\nlibrary_data\n\n# A tibble: 423,448 × 11\n   patron…¹ total…² total…³ age   branch activ…⁴ activ…⁵ notic…⁶ provi…⁷ year_…⁸\n   <chr>      <dbl>   <dbl> <chr> <chr>  <chr>   <chr>   <chr>   <lgl>     <dbl>\n 1 SENIOR        28      13 65 t… Main … Novemb… 2012    email   TRUE       2003\n 2 ADULT         21      10 55 t… Main … October 2015    email   TRUE       2003\n 3 ADULT        275     559 60 t… Main … January 2015    email   TRUE       2003\n 4 ADULT         73      38 45 t… Missi… Februa… 2016    email   TRUE       2003\n 5 ADULT        182      90 45 t… Main … July    2016    email   TRUE       2003\n 6 SENIOR         1       0 65 t… Main … July    2004    email   TRUE       2003\n 7 DIGITAL…      31       4 65 t… Potre… January 2012    email   TRUE       2003\n 8 ADULT        547     177 60 t… Main … July    2016    phone   FALSE      2003\n 9 ADULT         28       1 60 t… Main … Decemb… 2015    email   TRUE       2003\n10 ADULT         77       3 60 t… Main … July    2016    email   TRUE       2003\n# … with 423,438 more rows, 1 more variable: outside_of_county <lgl>, and\n#   abbreviated variable names ¹​patron_type, ²​total_checkouts, ³​total_renewals,\n#   ⁴​active_month, ⁵​active_year, ⁶​notice_preference, ⁷​provided_email,\n#   ⁸​year_registered\n\n\n\nlibrary_data$y_m <- paste(library_data$active_year,library_data$active_month)\n\n\n\nLoading the branch information\nThe dataset mentions the names of different branches of SF Public Library along with their code, but it does not give us any more information about these branches. I extracted some data about the branches from the public library website : https://sfpl.org/locations/#!/filters?sort_by=weight&sort_order=ASC&page=2 and created my own dataset “branch_info” (with 30 branches and their information). I have merged it with the original dataset for better information about each branch and their corresponding patrons.\n\nunique(library_data$branch)\n\n [1] \"Main Library\"                       \"Mission Bay\"                       \n [3] \"Potrero\"                            \"Sunset\"                            \n [5] \"Merced\"                             \"Noe Valley/Sally Brunn\"            \n [7] \"Excelsior\"                          \"Chinatown\"                         \n [9] \"Richmond\"                           \"North Beach\"                       \n[11] \"Presidio\"                           \"Mission\"                           \n[13] \"Park\"                               \"Marina\"                            \n[15] \"Parkside\"                           \"Eureka Valley/Harvey Milk Memorial\"\n[17] \"Anza\"                               \"West Portal\"                       \n[19] \"Ingleside\"                          \"Bernal Heights\"                    \n[21] \"Portola\"                            \"Ortega\"                            \n[23] \"Western Addition\"                   \"Unknown\"                           \n[25] \"Ocean View\"                         \"Glen Park\"                         \n[27] \"Visitacion Valley\"                  \"Bayview/Linda Brooks-Burton\"       \n[29] \"Golden Gate Valley\"                 \"Library on Wheels\"                 \n[31] \"Children's Bookmobile\"              \"Branch Bookmobile (Sunset)\"        \n[33] \"Branch Bookmobile (West Portal)\"    \"Branch Bookmobile (Excelsior)\"     \n[35] \"Branch Bookmobile (Marina)\"        \n\n\nThe above shows the different branches of San Franciso Public Library.\n\nbranch_data <- read_csv('_data/branch_info.csv')\nbranch_data\n\n# A tibble: 29 × 4\n   branch                             phone_number   address             pincode\n   <chr>                              <chr>          <chr>                 <dbl>\n 1 Anza                               (415) 355-5717 550 37th Ave. (nea…   94121\n 2 Bayview/Linda Brooks-Burton        (415) 355-5757 5075 Third St. (at…   94124\n 3 Bernal Heights                     (415) 355-2810 500 Cortland Ave. …   94110\n 4 Chinatown                          (415) 355-2888 1135 Powell St. (n…   94108\n 5 Eureka Valley/Harvey Milk Memorial (415) 355-5616 1 José Sarria Cour…   94114\n 6 Excelsior                          (415) 355-2868 4400 Mission St. (…   94112\n 7 Glen Park                          (415) 355-2858 2825 Diamond St. (…   94131\n 8 Golden Gate Valley                 (415) 355-5666 1801 Green St. (at…   94123\n 9 Ingleside                          (415) 355-2898 1298 Ocean Ave. (a…   94112\n10 Main Library                       (415) 557-4400 100 Larkin St. (at…   94102\n# … with 19 more rows\n\n\nJoining the branch_data dataset wih the main dataset based on branch name.\n\nlibrary_data <- left_join(library_data, branch_data, by=c(\"branch\"))\nlibrary_data\n\n# A tibble: 423,448 × 15\n   patron…¹ total…² total…³ age   branch activ…⁴ activ…⁵ notic…⁶ provi…⁷ year_…⁸\n   <chr>      <dbl>   <dbl> <chr> <chr>  <chr>   <chr>   <chr>   <lgl>     <dbl>\n 1 SENIOR        28      13 65 t… Main … Novemb… 2012    email   TRUE       2003\n 2 ADULT         21      10 55 t… Main … October 2015    email   TRUE       2003\n 3 ADULT        275     559 60 t… Main … January 2015    email   TRUE       2003\n 4 ADULT         73      38 45 t… Missi… Februa… 2016    email   TRUE       2003\n 5 ADULT        182      90 45 t… Main … July    2016    email   TRUE       2003\n 6 SENIOR         1       0 65 t… Main … July    2004    email   TRUE       2003\n 7 DIGITAL…      31       4 65 t… Potre… January 2012    email   TRUE       2003\n 8 ADULT        547     177 60 t… Main … July    2016    phone   FALSE      2003\n 9 ADULT         28       1 60 t… Main … Decemb… 2015    email   TRUE       2003\n10 ADULT         77       3 60 t… Main … July    2016    email   TRUE       2003\n# … with 423,438 more rows, 5 more variables: outside_of_county <lgl>,\n#   y_m <chr>, phone_number <chr>, address <chr>, pincode <dbl>, and\n#   abbreviated variable names ¹​patron_type, ²​total_checkouts, ³​total_renewals,\n#   ⁴​active_month, ⁵​active_year, ⁶​notice_preference, ⁷​provided_email,\n#   ⁸​year_registered\n\n\n\n\nData Exploration\nNow, we will look at the different groups present within the dataset based on the categories/columns.\n\nlibrary_data%>%\n  filter(age!='NA')%>%\n  ggplot(aes(x = age)) +\n  theme(axis.text.x = element_text(angle = 90))+\n  geom_bar(stat = \"count\",fill='lightblue') +\n  labs(title = \"Number of records by Age Range\",\n       x = \"Age Range\",\n       y = \"Number for Records\")\n\n\n\n\nThe above plot shows the distribution of records based on age range. We can observe that maximum records in the library system lies for the range of 25 to 34 years followed by 35 to 44 years.\n\nggplot(data = library_data, aes(x = patron_type)) +\n      theme(axis.text.x = element_text(angle = 90))+\n  geom_bar(stat = \"count\",fill='lightpink') +\n  labs(title = \"Number of records by Patron type\",\n       x = \"Patron Type\",\n       y = \"Number for Records\")\n\n\n\n\nThe most prominent types of patrons registered in SF Library system are : Adults, Juvenile, Senior, Welcome (guest) and Young Adults.\n\nggplot(data = library_data, aes(x = branch)) +\n    theme(axis.text.x = element_text(angle = 90))+\n  geom_bar(stat = \"count\",fill='lightyellow') +\n  labs(title = \"Number of records by branches\",\n       x = \"Branches\",\n       y = \"Number for Records\")\n\n\n\n\nThe maximum patrons where registered at the MAIN Library, which makes sense, as the main branch generally has most books and is most accessible to the people.\n\n\nData Statistics\nThe two main continuous columns of this dataset are : Total Renewals and Total Checkouts made by the patrons. The statistics for which are present below:\n\nsummary(library_data$total_checkouts)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       2      19     162     113   35907 \n\n\n\nsummary(library_data$total_renewals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    2.00   59.66   27.00 8965.00"
  },
  {
    "objectID": "posts/PrajaktiKapade_Final_Project.html#visualizations",
    "href": "posts/PrajaktiKapade_Final_Project.html#visualizations",
    "title": "SF Library Usage Analysis",
    "section": "VISUALIZATIONS",
    "text": "VISUALIZATIONS\n\nYear of Registration by Age Range\nFirstly, we will analyze the trend of “year when patrons registered” with the “Age Range”.\n\nlibrary_data%>%\n  filter(age!='NA')%>%\n  ggplot( aes(x=year_registered, group=age, fill=age)) +\n    geom_density(adjust=1.5, alpha=.4) +\n    labs(title = \"Year of Registration by Age Range\",\n       x = \"Year of Registry\",\n       y = \"Number of patrons registered\")\n\n\n\n\nAs expected patrons with ages 60 and higher have been registered since 2003. We can see that the young children from the age of 0-9 years have started registering since 2012, which shows they are curious readers or their parents would’ve enrolled them to read books and gain information.\n\n\nTotal Checkouts and Renewals by Age Range\nNow, we will look into the two important columns total_checkouts and total_renewals for each category of patrons and different age groups to get an idea of the circulation of the books.\n\nlibrary_data%>%\nfilter(age!='NA')%>%\nggplot(aes(x = age,y=total_checkouts)) +\n  geom_bar(stat = \"sum\",fill='lightpink') +\n  labs(title = \"Total Checkouts by Age Range\",\n       x = \"Age Range\",\n       y = \"Total number of checkouts\") +coord_flip()+theme(legend.position=\"none\")\n\n\n\n\nThe majority of checkouts in the SF libraries were made by ages: 65 to 74 years followed by 45-54 years and the young readers 10-19 years.This shows us that the every generation is interested in reading books.\n\nknitr::opts_chunk$set(echo = TRUE)\nggplot(data = library_data, aes(x = age,y=total_renewals)) +\n  geom_bar(stat = \"sum\",fill='lightblue') +\n  labs(title = \"Total Renewals by Age Range\",\n       x = \"Age Range\",\n       y = \"Total number of Renewals\") +coord_flip()+theme(legend.position=\"none\")\n\n\n\n\nThe majority of renewals on the other hand were made by 45-54 years followed by 65-74 years. We can observe using the above two trends that 45-54 and 65-74 years age gaps show maximum engagement and have issued and renewed majority of books as compared to others.\n\n\nTotal Checkouts and Renewals by Patron Types\n\nlibrary_data %>% \n  ggplot(aes(x=patron_type, y=total_checkouts)) + geom_boxplot(alpha = 0) +\n    geom_jitter(alpha = 0.3, color = \"tomato\")+\n    theme(axis.text.x = element_text(angle = 90))+\n   labs(title = \"Total Checkouts by Patron type\",\n       x = \"Patron type\",\n       y = \"Total number of Checkouts\")\n\n\n\n\nThe majority of checkouts were made by ADULTS and SENIORS as compared to other types of patrons, which validates our theory above.\n\nlibrary_data %>% \n  ggplot(aes(x=patron_type, y=total_renewals)) + geom_boxplot(alpha = 0) +\n    geom_jitter(alpha = 0.3, color = \"tomato\")+\n  theme(axis.text.x = element_text(angle = 90))+\n   labs(title = \"Total Renewals by Patron type\",\n       x = \"Patron type\",\n       y = \"Total number of Renewals\")\n\n\n\n\nThe renewals here show that even though ADULTS and SENIORS had maximum renewals, there were significant renewals by YOUNG ADULTS, JUVENILE and STAFF members as well.\n\n\nCheckouts and Renewals - A trend\nAs, seen the “Total Checkouts” and “Total Renewals” follow a similar trend, let us analyze them in with the year of registry.\n\nfiltered <- library_data %>%\n  group_by(year_registered) %>%                    # group by\n  summarise(check = mean(total_checkouts), rene = mean(total_renewals)) \n\nggplot() +\n  geom_line(data = filtered, aes(x=year_registered, y=check),arrow = arrow(), color=\"#69b3a2\",size=2)+\n  geom_point(size=2, color=\"#69b3a2\")+\n   labs(title = \"Average Checkouts by Year of Registry\",\n     x = \"Year Registered\",\n     y = \"Average number of Checkouts\")\n\n\n\nmyarrow=arrow(angle = 15, ends = \"both\", type = \"closed\")\n\n\nfiltered <- library_data %>%\n  group_by(year_registered) %>%                    # group by\n  summarise(check = mean(total_checkouts), rene = mean(total_renewals)) \n\nggplot() +\n  geom_line(data = filtered, aes(x=year_registered, y=rene),arrow = arrow(), color=\"lightblue\",size=2)+\n  geom_point(size=2, color=\"lightblue\")+\n   labs(title = \"Average Renewals by Year of Registry\",\n     x = \"Year Registered\",\n     y = \"Average number of Renewals\")\n\n\n\nmyarrow=arrow(angle = 15, ends = \"both\", type = \"closed\")\n\nThe values of checkouts and renewals have been declining over the years, having been at its highest in 2003 and being the lowest in value by 2016. There was a slight increase in 2007 but was not much significant. This totally makes sense, as the people who registered way before had more time, thus, more checkouts and subscriptions.\n\nlibrary_data %>%\n  filter(active_year!='None')%>%# data                 \n  group_by(active_year) %>%                   \n  summarise(y = mean(total_checkouts)) %>% \nggplot( aes(x=active_year, y=y, group=1)) +\n  geom_line(arrow = arrow(),color=\"#D55E00\", size=2)+\n  geom_point(color=\"#D55E00\", size=2)+\n   labs(title = \"Average Checkouts by Active Year\",\n     x = \"Active Year (Last year of activity)\",\n     y = \"Average number of Checkouts\")\n\n\n\nmyarrow=arrow(angle = 15, ends = \"both\", type = \"closed\")\n\n\nlibrary_data %>%\n  filter(active_year!='None')%>%# data                 \n  group_by(active_year) %>%                    # group by\n  summarise(y = mean(total_renewals)) %>% \nggplot( aes(x=active_year, y=y, group=1)) +\n  geom_line(arrow = arrow(),color='#E69F00',size=2)+\n  geom_point(color='#E69F00',size=2)+\n   labs(title = \"Average Renewals by Active Year\",\n     x = \"Active Year (Last year of activity)\",\n     y = \"Average number of Renewals\")\n\n\n\nmyarrow=arrow(angle = 15, ends = \"both\", type = \"closed\")\n\nThe Checkouts and renewals value is increasing significantly from year to year and the highest value was in 2016. This basically shows us the trend about how people are issuing more and more books moving forward.\nAs, seen Total renewals and Total Checkouts clearly follow similar trends, let us confirm their correlation using a plot.\n\nplot(library_data$total_checkouts, library_data$total_renewals, pch = 19, col = \"lightblue\",xlab=\"Total Checkouts\", ylab=\"Total Renewals\")\n\n# Regression line\nabline(lm( library_data$total_renewals ~ library_data$total_checkouts), col = \"red\", lwd = 3)\n\n# Pearson correlation\ntext(paste(\"Correlation:\", round(cor(library_data$total_checkouts,  library_data$total_renewals), 2)),x=30000,y=1000)\n\n\n\n\nThe plot suggests a strong correlation between total checkouts and total renewals with a pearson coefficient of 0.59 ~ 0.6.\n\n\nTotal Checkouts by Month for year 2016\nAs, this dataset provides information about monthly checkouts and renewals, we will look at monthly charts for last year in the records i.e. 2016, to understand what trends have been lately.\n\nlibrary_data %>%\n    filter(active_year=='2016')%>%\n    ggplot(aes(x = y_m, fill = total_checkouts)) +\n    geom_histogram( fill=\"#404080\", alpha=0.6, stat = \"count\") + theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = \"none\")+ labs(title=\"Total Checkouts by Month for year 2016\", y=\"Total Checkouts\",x=\"month\")+coord_flip()\n\n\n\n\n\n\nTrends of Age Range and Patron Type for July 2016\nThe histogram suggests the majority of checkouts occurred in month of July followed by June. Looking closely into July data, to understand the distribution of Total Checkouts by Age and Patron type:\n\nfilter_data <- library_data %>%\n  filter(y_m =='2016 July')\n\nfilter_data <- aggregate(filter_data$total_checkouts, by=list(Age_Range=filter_data$age), FUN=mean)\n\nfilter_data\n\n           Age_Range        x\n1       0 to 9 years 300.2186\n2     10 to 19 years 598.0791\n3     20 to 24 years 283.6111\n4     25 to 34 years 162.1240\n5     35 to 44 years 347.6696\n6     45 to 54 years 588.2905\n7     55 to 59 years 736.5017\n8     60 to 64 years 758.1687\n9     65 to 74 years 755.9043\n10 75 years and over 757.8021\n\nggplot(data = filter_data, aes(x = \"\", y = x, fill = Age_Range)) + \n  geom_bar(stat = \"identity\") + \n  coord_polar(\"y\")+\n  labs(title=\"Total Checkouts by Age Range\")+\n  scale_fill_brewer(palette = \"Paired\")+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid  = element_blank())+\n  labs(x=\"\", y=\"\")\n\n\n\n\nThe pie chart shows the major contributions of checkouts in July 2016 were made by ages 45 years and above.\n\nco <- c(\"#FFDB6D\", \"#C4961A\", \"#F4EDCA\", \n                \"#D16103\", \"#C3D7A4\", \"#52854C\", \"#4E84C4\", \"#293352\",\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n          \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\",\"#999999\")\nfilter_data <- library_data %>%\n  filter(y_m =='2016 July')\n\nfilter_data <- aggregate(filter_data$total_checkouts, by=list(Patron_Types=filter_data$patron_type), FUN=mean)\n\nfilter_data\n\n          Patron_Types           x\n1                ADULT  449.280391\n2        AT USER ADULT  641.735849\n3     AT USER JUVENILE  345.571429\n4       AT USER SENIOR  381.363636\n5         AT USER TEEN  740.000000\n6        BOOKS BY MAIL  891.720000\n7  DIGITAL ACCESS CARD    2.142857\n8     FRIENDS FOR LIFE  760.545455\n9             JUVENILE  369.018302\n10       RETIRED STAFF 1443.379747\n11              SENIOR  756.174221\n12             SPECIAL  655.713615\n13               STAFF 1226.017953\n14        TEACHER CARD  159.995708\n15             VISITOR   18.781250\n16             WELCOME    9.240132\n17         YOUNG ADULT  645.511378\n\nggplot(data = filter_data, aes(x = \"\", y = x, fill = Patron_Types)) + \n  geom_bar(stat = \"identity\") + \n  coord_polar(\"y\")+\n  labs(title=\"Total Checkouts by Patrons type\")+\n  scale_fill_manual(values=co)+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid  = element_blank())+\n  labs(x=\"\", y=\"\")\n\n\n\n\nThe pie chart suggests major contributions in July 2016 for checkout were made by Retired Staff and Staff members.\n\n\nNotice preference by location(pincode)\n\nlibrary_data %>% filter(pincode!='NA')%>%\n  select(notice_preference,pincode) %>% \n                group_by(notice_preference,pincode) %>% \n                summarise(n = n()) %>% \n                ggplot(aes(reorder(pincode, n), n, fill = notice_preference)) + \n                geom_bar(stat = \"identity\", position = position_dodge()) + \n                theme(axis.text.x = element_text(angle = 90))+\n                scale_fill_brewer(palette = \"Set2\")+\n                labs(title=\"Notification preference by Pincode\", fill = \"Notice Prefernce\",x='pincodes',y='Number of records') \n\n\n\n\nThis plot suggests, most patrons prefer emails as the notice preference, and most of the patrons are registered in pincode 94102. Examining the data based on the most popular pincode - “924102”, which will give us trends about area nearby.\n\n\nCheckouts for prominent pincode by Patron Types and Age Ranges\n\nlibrary_data %>%\n  filter(pincode =='94102' & age!='NA')%>%\nggplot( aes(x=age, y=patron_type, size = total_checkouts)) +\n    geom_point(alpha=0.7) +\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(title=\"Checkouts by Patron Types and Age Ranges\",x='Age Range',y='Patron Type') \n\n\n\n\nWe can observe major activity for pincode 94102, were made by ADULTS of age 45-54 years and 54-59 years,SENIORS of 64-74 years. There are also a few checkouts by STAFF members with ages 0-9 years, which looks suspicious.\n\n\nYear of Patron Registration by Branches\nThe plots show how many patrons registered in each year for each branch location.\n\n# Bottom Left\nggplot(library_data, aes(x=branch, y=year_registered, fill=branch)) + \n    geom_boxplot(alpha=0.3)+ coord_flip()  +  theme(axis.text.x = element_text(angle = 90),legend.position=\"none\") +\n   labs(title='Year of Patron Registration by Branches',x='Year Registered',y='Branch')\n\n\n\n\nThere is an interesting observation in Children’s Bookmobile Branch, while most registries of patrons were towards 2012, there was one registry in 2003 (which can be seen as an outlier).\n\n\nAnalysis for Residents and Non-Residents\nThere is also an important column “outside_of_county” which shows the patrons outside of San Franciso. Let us analyze some trends related to the column.\n\nlibrary_data %>%ggplot(aes(total_checkouts , outside_of_county)) +geom_boxplot(aes(color=outside_of_county))+labs(title=\"Total Checkouts by Patrongs Outside and In County\", x='Outside of County',y='Total Checkouts')\n\n\n\n\nThe plot suggests, most of the patrons are present in SF itself. Also, total checkouts for a few patrons in SF cross over 20k as well.\n\n\nDifferent Patron types by year of registery and their residence.\nThis shows density of patrons based on the year of registration of different types of patrons residing in and out of San Franciso.\n\n# Using Small multiple\n\nggplot(data=library_data, aes(x=year_registered, group=outside_of_county, fill=outside_of_county)) +\n    geom_density(adjust=1.5,alpha = 0.4) +\n    facet_wrap(~patron_type) +\n    theme(\n      legend.position = \"bottom\",\n      panel.spacing = unit(0.5, \"lines\"),\n      axis.ticks.x=element_blank(),\n    )+\n  labs(title=\"Patron density based on year of registration, patron type and residency\")\n\n\n\n\n\n\nDifferent Age Range by year of registery and their residence.\nThis shows density of patrons based on the year of registration for different age groups residing in and out of San Franciso.\n\n# Using Small multiple\n\nlibrary_data%>%\n  filter(age!='NA')%>%\n  ggplot( aes(x=year_registered, group=outside_of_county, fill=outside_of_county)) +\n    geom_density(adjust=1.5, alpha=.4) +\n    facet_wrap(~age) +\n    theme(\n    legend.position = \"bottom\",\n      panel.spacing = unit(0.9, \"lines\"),\n      axis.ticks.x=element_blank()\n    )+\n  labs(title=\"Patron density based on year of registration, age range and residency\")"
  },
  {
    "objectID": "posts/PrajaktiKapade_Final_Project.html#reflection",
    "href": "posts/PrajaktiKapade_Final_Project.html#reflection",
    "title": "SF Library Usage Analysis",
    "section": "REFLECTION",
    "text": "REFLECTION\nThis project and working with R has been a learning experience for me as I am a Computer Science student. While starting this project, I thought maybe the San Franciso Library Usage dataset would be very concise, but as I kept diving into the dataset, I realized it has so much information about patrons, the library branches, the age groups and their contributions to checkouts and renewals of books. The Integrated library management dataset gave me some interesting findings.\nMy strategy was to understand how each category(column) is affecting the checkouts and renewals, and then if any interesting observation was seen, I would move forward with digging in more in that direction. For eg: I looked into 2016’s data per month and found that July had the most checkouts. I dived in to the July 2016’s data to understand who made those checkouts? Which age range and patron type contributed the most to it? This approach made the visualization making process very intriguing for me.\nThe dataset had a lot of records, but to incorporate more information on branches I searched through the SF public library website and manually loaded the data to a csv, which I then incorporated in the main dataset. Challenging part was to understand what different fields mean, specially what different year values signify? Also, understanding from whose point of view is this data made and whats the timeline?\nI tried to learn, have fun with it, and incorporate all types of plots, specially histograms, density plots and facet_wrap which made it easier to see observations. If we had more information about the data from other parts of California or USA, I could have found trends for a much higher population using these visualizations. Also, we need more information about the branches, as only geographical information was available on the website. Apart from that, some information about the different categories of books, which age groups like which categories more, that would be an interesting research to do."
  },
  {
    "objectID": "posts/PrajaktiKapade_Final_Project.html#conclusion",
    "href": "posts/PrajaktiKapade_Final_Project.html#conclusion",
    "title": "SF Library Usage Analysis",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nStarting from the basics, there were 10 Age ranges varying over 17 Patron Types, 34 branches and over 4 lakh records. The dataset gave me insights about which Age Range is making the most checkouts and renewals - and it was inspiring to figure out that 10-19, 45-54 and 65-74 age ranges had the maximum checkouts, this implies all generations are interested in issuing books and reading them, which is a great habit. I also noticed that ADULTS and SENIORS had the highest checkouts and renewals, which means they take some time reading it or re-read it, by renewing the books. Comparatively, there are also high renewals for YOUNG ADULTS, students generally tend to issue some books for studying and end up renewing them.\nIt was very clear, that checkouts and renewals were strong correlated, which does make sense intuitively. I showed it using the pearson coefficient in our code above. The plots also reflected that the checkouts and renewals have increased over time, that means people are getting more and more interested in issuing and reading books. Another trend, I noted was related to “Year of Registry” and “Average Checkouts/Average Renewals”, which should intuitively follow a decreasing trend (as people who registered earlier will definitely have more checkouts due to more time), but there was a slight increase in 2007, so that was definitely an anomaly.\nFor finding a general trend, I looked into 2016’s data per month (as it was the last recorded year in the dataset) and found that July had the most checkouts. I dived into the July 2016 data and figured they were majorly made by patrons of age 45 and above, and most of them were Staff members or Retired staff. One reason for this could be the holiday season for students, as they do not contribute much towards the checkouts.\nI also figured that, most patrons prefer EMAIL as their preference of notice and based on the pincodes of branches, “94102” is the most popular pincode for checkouts and renewals. On looking in more detail about the pincode “94102”, I figured most checkouts in that area were made by ADULTS of age 45-54 years and 54-59 years and SENIORS of 64-74 years. There are also a few checkouts by STAFF members with ages 0-9 years, which is weird.\nThere are some patrons who reside outside the SF county, but they are lesser in numbers than the ones in SF. The registry with SF Public library over the years for people residing in and out of the county can be seen the graphs above, they have been grouped by Age Range and Patron Types.\nAfter so many observations, we still do not know how Age Ranges and Patron Types are related, and will need some more information about them. Also, how are different branches related to each other? Do they share some books? Some more information can lead to better observations and analysis, but all these visualizations on our dataset, were definitely insightful and enlightened me about the SF Integrated Library System."
  },
  {
    "objectID": "posts/PrajaktiKapade_Final_Project.html#bibliography",
    "href": "posts/PrajaktiKapade_Final_Project.html#bibliography",
    "title": "SF Library Usage Analysis",
    "section": "BIBLIOGRAPHY",
    "text": "BIBLIOGRAPHY\nRStudio Team (2022). RStudio: Integrated Development Environment for R. RStudio, PBC, Boston, MA, http://www.rstudio.com/.\nWickham, H., François, R., Henry, L., & Müller, K. (n.d.). Programming with dplyr. dplyr. https://dplyr.tidyverse.org/articles/programming.html\nWickham, H. & Grolemund, G. (n.d.). R for data science [eBook edition]. O’Reilly. https://r4ds.had.co.nz/index.html\nWickham et al. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686\nSource of Dataset : https://www.kaggle.com/datasets/datasf/sf-library-usage-data\nhttps://data.sfgov.org/Culture-and-Recreation/Library-Usage/qzz6-2jup\nSF Public Library Website : https://sfpl.org/\nBranch Information : https://sfpl.org/locations/#!/filters?sort_by=weight&sort_order=ASC&page=2"
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html",
    "href": "posts/SahasraIyer_FinalProject.html",
    "title": "601_Final_Project",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html#introduction",
    "href": "posts/SahasraIyer_FinalProject.html#introduction",
    "title": "601_Final_Project",
    "section": "Introduction",
    "text": "Introduction\nSpotify is perhaps one of, if not the leading, music streaming service available today. It boasts of a wide variety of music genres in its coffins, and presently has over 456 million monthly active users, which includes 195 paying subscribers (as of September 2022). Spotify allows its users to create their own playlists, and also generates daily and weekly playlists, basis the streaming numbers of a certain genre or geographic region.\nIn this project, I have chosen to study the Top 50 Playlists of 4 countries - India, USA, France and Brazil. The music from these countries vary extremely in terms of their artists, as well as genres. What I aim to research through this project, is to find some similarities (if they exist) amongst these playlists, as this would suggest some sort of symphony across music as an art form, or if they vary, as this would mean that certain features exist that make the music originating from these countries distinctive to their audiences."
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html#data",
    "href": "posts/SahasraIyer_FinalProject.html#data",
    "title": "601_Final_Project",
    "section": "Data",
    "text": "Data\nThe data being used in this project was not available beforehand and involved using Spotify’s API to source the current data of the Top 50 Songs from each of the countries we chose. To start with I used the spotifyr wrapper package, to get different tracks and the attributes associated with them, relating to the features of songs, as well as details of the artists that created them and the genres that they belong to. The first step in accessing this Spotify data is to get an API key, which I created on the Spotify Developer Dashboard. The following code was used to get the Spotify access token :\n\n# Use client_id and secret token from developer dashboard to get access token\nid <- '4949e892016b49a988d0ceb6db9c8152'\nsecret <- '191ad5c5e32d4552a83d9e8eb95f1456'\nSys.setenv(SPOTIFY_CLIENT_ID = id)\nSys.setenv(SPOTIFY_CLIENT_SECRET = secret)\naccess_token <- get_spotify_access_token()\n\nWith the Spotify access token now available, I then manually added the 4 playlists to my own account, in order to carry out a filtered analysis on the playlists present in my account.\nWe now fetch the playlists saved in my Spotify account, using the code chunk below :\n\n# Here I have used my unique Spotify user id to fetch the saved playlists on my Spotify account\noptions(httr_oauth_cache=T)\nuser_id <- 'u965216r0zoxby3bmbsxdsynm'\nuser_playlists <- get_user_playlists(user_id, limit = 20, offset = 0,\n  authorization = get_spotify_authorization_code(),\n  include_meta_info = FALSE)\n\nError in token$sign(req$method, req$url): attempt to apply non-function\n\n\nThe variable user_playlists now contains all the playlists saved to my account. We will now filter the playlist of the Top 50 songs from the 4 countries we aim to analyse.\n\n# Here, I have filtered the 4 playlist of interest\nfilter_user_playlists <- user_playlists %>%\n  filter(name %in% c('Top Songs - India','Top Songs - USA','Top Songs - Brazil','Top Songs - France'))\n\nError in filter(., name %in% c(\"Top Songs - India\", \"Top Songs - USA\", : object 'user_playlists' not found\n\n\nWe will now use the function get_track_data for fetching the tracks that are contained in each of the 4 playlists. This is done by using the unique playlist id, associated with each of the playlists. As we are constructing a dataset, we are also mapping each track with the playlist that it is retrieved from, in order to ensure that in the occasion of an overlap, we know which playlist the track originated from. Along with the tracks within the playlists, we also get the audio features associated with each track. This consists of features such as danceability, acousticness, track length, etc. which we will discuss in further sections.\n\n# get_track_data - Function defined to get all the tracks within a playlist\nget_track_data <- function(index) {\n  features <- data.frame()\n  tracks <- data.frame()\n  upper_lim = index+1\n  for(i in index:upper_lim) {\n    playlist_tracks <- get_playlist_tracks(filter_user_playlists[i,\"id\"], authorization = get_spotify_access_token())\n    playlist_tracks$playlist_name <- filter_user_playlists[i, \"name\"]\n    tracks <- rbind2(tracks, playlist_tracks)\n    playlist_features <- get_track_audio_features(tracks$track.id, \n                                                        authorization = get_spotify_access_token())\n    features <- rbind2(features, playlist_features)\n  }\n  \n  return (list(tracks, features))\n}\n\nThe code below fetches the tracks and associated audio features from the first 2 playlists from the filtered playlists, which are ‘Top Songs - India’ and ‘Top Songs - USA’. We require to do this piecewise, as the function get_playlist_tracks has a predefined limit of fetching only 100 tracks per fetch call (sourced from the Spotify API documentation).\n\ntracks_f <- data.frame()\nfeatures_f <- data.frame()\n\n# Retrieve data of first 2 playlists\ndata <- get_track_data(1)\n\nError in eval(parse(text = text, keep.source = FALSE), envir): object 'filter_user_playlists' not found\n\ntracks_f <- rbind2(tracks_f, data[[1]])\n\nError in (function (cond) : error in evaluating the argument 'y' in selecting a method for function 'rbind2': object of type 'closure' is not subsettable\n\nfeatures_f <- rbind2(features_f, data[[2]])\n\nError in (function (cond) : error in evaluating the argument 'y' in selecting a method for function 'rbind2': object of type 'closure' is not subsettable\n\n\nWe then fetch the tracks and audio features for the next 2 playlists, ‘Top Songs - Brazil’ and ‘Top Songs - France’.\n\n# Retrieve data of last 2 playlists\ndata <- get_track_data(3)\n\nError in eval(parse(text = text, keep.source = FALSE), envir): object 'filter_user_playlists' not found\n\ntracks_f <- rbind2(tracks_f, data[[1]])\n\nError in (function (cond) : error in evaluating the argument 'y' in selecting a method for function 'rbind2': object of type 'closure' is not subsettable\n\nfeatures_f <- rbind2(features_f, data[[2]])\n\nError in (function (cond) : error in evaluating the argument 'y' in selecting a method for function 'rbind2': object of type 'closure' is not subsettable\n\n# Rename uri column in features_f dataframe fo r subsequent join operation\nfeatures_f <- features_f %>%\n  rename(\"track.uri\" = \"uri\")\n\nError in `rename()`:\n! Can't rename columns that don't exist.\n✖ Column `uri` doesn't exist.\n\n\nWe now have the consolidated tracks and features dataframes, which contains the tracks from all 4 playlists.\n\n#tracks_f\n#features_f\n\nWe will now do a left join on the tracks_f and featues_f dataframes on the track.uri column, to get our final dataset.\n\n# Join operation to get final dataset\nall_tracks <- tracks_f%>%\n  left_join(features_f, by=\"track.uri\")\n\nError in `left_join()`:\n! Join columns must be present in data.\n✖ Problem with `track.uri`.\n\nload(\"all_track_sahasra.RData\")"
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html#analysis-and-visualization",
    "href": "posts/SahasraIyer_FinalProject.html#analysis-and-visualization",
    "title": "601_Final_Project",
    "section": "Analysis and Visualization",
    "text": "Analysis and Visualization\nWe will first look at the different acoustic features of the tracks, namely danceability, speechiness, acousticness, energy and loudness. Let us first look at what each of these features convey about a song :\n\nDanceability: Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.\nSpeechiness: This detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.\nAcousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic.\nEnergy: Represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.\nLoudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.\n\nWe only consider the above acoustic features, in the interest of looking at variables that show a degree of variation across the playlists, the visualizations of which are as below :\n\n# Boxplots for visualizing danceability, energy and loudness across the 4 playlists\nfig_danceability <- plot_ly(all_tracks, y=~danceability, color = ~playlist_name, type = \"box\") %>% \n  layout(yaxis = list(title = c(\"Danceability\")))\nfig_energy <- plot_ly(all_tracks, y=~energy, color = ~playlist_name, type = \"box\", showlegend=FALSE) %>% \n  layout(yaxis = list(title = c(\"Energy\")))\nfig_loudness <- plot_ly(all_tracks, y=~loudness, color = ~playlist_name, type = \"box\", showlegend=FALSE) %>%\n  layout(yaxis = list(title = c(\"Loudness\")))\n\n\nfig <- subplot(fig_danceability, fig_energy, fig_loudness, nrows=3, titleY=TRUE) %>%\n  layout(title=list(text=\"Feature comparison across playlists\"),\n  plot_bgcolor='#e5ecf6', \n         xaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'),\n         yaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'))\n\nfig\n\n\n\n\n\n\n# Boxplots for visualizing speechinessand acousticness across the 4 playlists\nfig_speechiness <- plot_ly(all_tracks, y=~speechiness, color = ~playlist_name, type = \"box\") %>%\n  layout(yaxis = list(title = c(\"Speechiness\")))\nfig_acousticness <- plot_ly(all_tracks, y=~acousticness, color = ~playlist_name, type = \"box\", showlegend=FALSE)  %>%\n  layout(yaxis = list(title = c(\"Acousticness\")))\n\nfig <- subplot(fig_speechiness, fig_acousticness, nrows=2, titleY=TRUE) %>%\n  layout(title=list(text=\"Feature comparison across playlists\"),\n  plot_bgcolor='#e5ecf6', \n         xaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'), \n         yaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'))\n\nfig\n\n\n\n\n\nFrom the boxplots above, we observe that the Brazil playlist has the highest number of danceable songs. Furthermore, Brazil, France and India have comparable energy coefficients for the tracks in their playlists, and USA has a slightly low track energy coefficient. In totality, however, France has the highest energy tracks (more compact boxplot). From the third boxplot, we observe that the same trend follows for loudness, which gives us the intuition that energy and loudness are perhaps correlated. Here however, Brazil has a more compact boxplot than France, which shows that the Brazil songs have a higher decibel level. We can perhaps anticipate some really upbeat songs in this playlist.\nThe speechiness plots vary across the 4 playlists, but we observe that India and USA have comparable medians, with France trailing behind closely. We can perhaps assume that the tracks here are composed of more music than words. I can’t completely solidify the thought on the same, as there exist many outliers in this boxplot. Finally, the acousticness boxplot shows us that France has the least acoustic songs in its playlist. Again, as this is confidence based, we cannot arrive at conclusive results.\nLet us now visualize a heatmap for these acoustic features, to get a numeric value for their correlatedness. We construct the heatmap by computing correlation between the features, which is done using Pearson’s method. The value ranges between -1 and 1. A value closer to 1 indicates a high correlation, and a value closer to -1 indicates that the 2 features are extremely uncorrelated.\n\n# Constructing the heatmap for checking correlation between acoustic features\ntrack_features <- all_tracks %>%\n  select(danceability, energy, loudness, speechiness, acousticness, instrumentalness, track.popularity, playlist_name) %>%\n  rename(\n    popularity = track.popularity\n  )\n\ncor_mat <- cor(track_features[sapply(track_features, is.numeric)])\n\nhm_data <- melt(cor_mat)\n\nhm_data <- hm_data %>%\n  rename(\n    Features_x = Var1,\n    Features_y = Var2,\n    Index = value\n  )\n\nhm_plot <- ggplot(hm_data,aes(x = Features_x, y = Features_y, fill = Index)) +\n  geom_tile() + \n  theme(axis.text.x=element_text(angle=45,hjust=1)) +\n  scale_fill_gradient(high = \"blue\", low = \"white\") \n\nggplotly(hm_plot)\n\n\n\n\n\nThe color density for each feature measured against the other, gives us an estimate of the relatedness (or otherwise) of 2 features.\nFor instance, the darkest color gradient is present in the cross-section between loudness and energy, an intuition we previously had. It has a correlation coefficient of 0.709. Let us look at the loudness vs energy distribution plot.\n\n# Scatterplot for Loudness vs Energy\noptions(dplyr.summarise.inform = FALSE)\n\ngroup_by_playlist <- all_tracks %>%\n  group_by(playlist_name) %>%\n  rename(\n    Playlist = playlist_name\n  )\n\nfig <- ggplot(group_by_playlist, aes(x=loudness, y=energy, color=Playlist, fill=Playlist, text=(paste(\"Loudness:\", loudness, \"<br>\", \"Energy:\", energy))), showlegend=FALSE)  +\n  geom_point() +\n  labs(\n    fill=\"Playlist\",\n    x=\"Loudness\",\n    y=\"Energy\"\n  ) + \n  facet_wrap(~Playlist) + \n  ggtitle(\"Loudness vs Energy\") \n\n\nggplotly(fig)\n\n\n\n\n\nFrom the above scatterplots, we can observe a similar behaviour across all 4 playlists. As the decibel of a track increases, so does its energy factor.\nTwo other features seem to have a higher color gradient that the rest, loudness and danceability. Their visualization is as below :\n\n# Scatterplot for Loudness vs Danceability\nfig <- ggplot(group_by_playlist, aes(x=loudness, y=energy, color=Playlist, fill=Playlist, text=(paste(\"Loudness:\", loudness, \"<br>\", \"Danceability:\", danceability))), showlegend=FALSE)  +\n  geom_point() +\n  labs(\n    fill=\"Playlist\",\n    x=\"Loudness\",\n    y=\"Danceability\"\n  ) + \n  facet_wrap(~Playlist) + \n  ggtitle(\"Loudness vs Danceability\") \n\n\nggplotly(fig)\n\n\n\n\n\nAgain, we see an upward trend for these 2 acoustic features. So we can conclude that the playlists have a general trend of tracks having high decibels being considered to be more danceable.\nLet us now observe the heatmap from the lower side of the index spectrum. We can observe that the 2 features acousticness and danceability are almost white, with a correlation factor of -0.349. This shows us that they are inversely related.\n\n# Scatterplot for Acousticness vs Danceability\nfig <- ggplot(group_by_playlist, aes(x=acousticness, y=energy, color=Playlist, fill=Playlist, text=(paste(\"Acousticness:\", acousticness, \"<br>\", \"Danceability:\", danceability))), showlegend=FALSE)  +\n  geom_point() +\n  labs(\n    fill=\"Playlist\",\n    x=\"Acousticness\",\n    y=\"Danceability\"\n  ) + \n  facet_wrap(~Playlist) + \n  ggtitle(\"Acousticness vs Danceability\") \n\n\nggplotly(fig)\n\n\n\n\n\nFrom the plots above, we do observe a tapering end at the lower right of each plot, which shows us that the two features do not follow a linear trend.\nFrom the plots so far, we see that danceability, energy and loudness are some key acoustic features within our playlists. Let us construct a distribution plot for each of these 3 features for our playlists.\n\n# Density plot for Danceability distribution\ngreen <- \"#1ed760\"\nyellow <- \"#e7e247\"\npink <- \"#ff6f59\"\nblue <- \"#17bebb\"\n\ndance_dist <- all_tracks %>%\n  rename(\n    Playlist = playlist_name\n  )\nfig <- ggplot(dance_dist, aes(x=danceability, fill=Playlist))+\n  geom_density(alpha=0.7, color=NA)+\n  scale_fill_manual(values=c(green, yellow, pink, blue))+\n  labs(x=\"Danceability\", y=\"Density\")+\n  theme_minimal()+\n  ggtitle(\"Distribution of Danceability\")\n\n\nggplotly(fig, tooltip=c(\"text\"), showlegend=TRUE)\n\n\n\n\n\nThis graph suggests that of the 4 playlist, the USA playlist has the widest range of danceability in its Top 50 playlist. Further, we can also see that France’s playlist consists of songs on the higher end of the danceability spectrum.\nWe will now plot the energy distribution across the 4 playlists :\n\n# Density plot for Energy distribution\nfig <- ggplot(dance_dist, aes(x=energy, fill=Playlist))+\n  geom_density(alpha=0.7, color=NA)+\n  scale_fill_manual(values=c(green, yellow, pink, blue))+\n  labs(x=\"Energy\", y=\"Density\")+\n  theme_minimal()+\n  ggtitle(\"Distribution of Energy\")\n\n\nggplotly(fig, tooltip=c(\"text\"), showlegend=TRUE)\n\n\n\n\n\nWe observe that India and US have the widest range of energy amongst the 4 playlists. US has extremely few tracks with high energy coefficient in its playlist, compared to the other 3, which was also what we observed in the boxplots plotted previously. Also, France’s Top 50 playlist is mostly made up of songs on the higher end of the energy spectrum, which was a similar observation in our boxplots.\nWe will now plot the loudness distribution across the 4 playlists :\n\n# Density plot for Loudness distribution\nfig <- ggplot(dance_dist, aes(x=loudness, fill=Playlist))+\n  geom_density(alpha=0.7, color=NA)+\n  scale_fill_manual(values=c(green, yellow, pink, blue))+\n  labs(x=\"Loudness\", y=\"Density\")+\n  theme_minimal()+\n  ggtitle(\"Distribution of Loudness\")\n\n\nggplotly(fig, tooltip=c(\"text\"), showlegend=TRUE)\n\n\n\n\n\nFrom the density plot, we observe that the US, Brazil and France playlists have some really high decibel songs, of which Brazil is the clear winner, which is very similar to our observation with the boxplots. While the Indian playlist has many high decibel songs towards the higher end of the spectrum, it doesn’t contain any with the decibel levels as high as the other 3.\nApart from these main acoustic features, I also analyzed the track popularity feature against danceability. This stemmed from the simple intuition that popular tracks could perhaps have danceable tunes. The visualization of the same is as below :\n\n# Scatterplot for Popularity vs Danceability\noptions(dplyr.summarise.inform = FALSE)\n\ngroup_by_danceability <- all_tracks %>%\n  group_by(playlist_name, track.popularity) %>%\n  summarise(\n    mean_d = mean(danceability)\n  ) %>%\n  rename(\n    Playlist = playlist_name\n  )\n\nfig <- ggplot(group_by_danceability, aes(x=track.popularity, y=mean_d, color=Playlist, text=(paste(\"Popularity:\", track.popularity, \"<br>\", \"Danceability:\", mean_d))))  +\n  geom_point() +\n  labs(\n    x=\"Popularity\",\n    y=\"Danceability\",\n    fill=\"Playlist\"\n  ) +\n  facet_wrap(~Playlist) + \n  ggtitle(\"Popularity vs Danceability\")\n\nggplotly(fig)\n\n\n\n\n\nPopularity is measured on a scale between 0 and 100, where 100 is the best. Per my intuition, the plots towards the right end of the scatter plots should have then had a higher danceability factor. However, the scatter plots we observe are completely random, exhibiting no significant relationship between the 2 variables.\nAn interesting acoustic feature that is unexplored so far is speechiness. As per the documentation, speechiness refers to the presence of spoken words in a song. Songs with a speechiness score between 0.33 and 0.66 contain both music and speech; they could be rap songs, for example. Based on this, we’re going to look at speechiness based on the difference between the speechiness score and 0.33. If the difference is above 0, it’s most likely a rap song. The farther below 0, the more instrumental the track is. We will first use mutate to create a new column that calculates the speechiness difference score by subtracting 0.33 from the speechiness column.\n\n# Mutate column for getting difference\nall_tracks <- all_tracks %>%\n  mutate(difference=speechiness - 0.33)\n\nWe will now plot the graph to observe how many bars go above or below zero, which will show us the speechiness of each track in the playlists.\n\n# Plot for checking speechiness bars for all tracks\nfig <- ggplot(all_tracks, aes(x=reorder(track.name, -difference), y=difference, fill=playlist_name, text=(paste(\"Track:\", track.name, \"<br>\",\"Speechiness:\", speechiness))))+\n  geom_col()+\n  scale_fill_manual(values=c(green, yellow, pink, blue)) +\n  theme_minimal()+\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.ticks.y=element_blank(),\n        panel.grid.major = element_blank())+\n  ylab(\"Speechiness Difference\")+\n  labs(\n    fill=\"Playlist\"\n  ) +\n  facet_wrap(~playlist_name)+\n  ggtitle(\"Speechiness Difference\")\n\nggplotly(fig, tooltip=c(\"text\"))\n\n\n\n\n\nBrazil has more bars above 0, than any of the other countries. Furthermore, France has 3 distinct tracks, with a significant speechiness index in them. “Baile No Morro” is the speechiest song in the Brazil playlist with a speechiness of 0.453 and France’s speechiest song is “Canette dans les mains” with a speechiness of 0.4. India and US have lesser bars above 0, and also a smaller speechiness index comapred to the other 2 countries.\nMoving on, we will now explore key, which describes the scale on which a song is based. This essentially means thta most of the notes in a song will come from the scale of that key.\nFor the purpose of representing this graphically, I created the below dataframe to find out how many songs from each playlist are in certain keys and the total number of songs in each key :\n\n# Dataframe for getting key composition in tracks\nkey_by_country <- all_tracks%>%\n  select(playlist_name, key)%>%\n  group_by(playlist_name, key)%>%\n  mutate(n=n())%>%\n  unique()%>%\n  group_by(key)%>%\n  mutate(total=sum(n))\n  #mutate(percent=round((n/total)*100))\n\nhead(key_by_country, 10)\n\n# A tibble: 10 × 4\n# Groups:   key [10]\n   playlist_name        key     n total\n   <chr>              <int> <int> <int>\n 1 Top Songs - France     6    14    31\n 2 Top Songs - France     8     2    21\n 3 Top Songs - France     9    16    40\n 4 Top Songs - France     2    15    46\n 5 Top Songs - France     4    12    25\n 6 Top Songs - France     7    17    39\n 7 Top Songs - France    11    10    34\n 8 Top Songs - France     3     2    16\n 9 Top Songs - France     5     4    18\n10 Top Songs - France    10     6    18\n\n\nWe now graph which keys are comprised in each of the playlists.\n\n# Renaming playlist_name for purpose of simplicity\nkey_by_country <- key_by_country %>%\n  rename(\n    Playlist = playlist_name\n  )\n\n\n# Stacked bar chart for musical key proportions\nfig <- ggplot(key_by_country, aes(x=key, fill=Playlist, y = n, color = Playlist, \n                                text = paste(\"Number of Songs: \", n, \"<br>\")))+\n  geom_bar(position=\"fill\", width=0.5, stat = \"identity\")+\n  labs(x=\"Key\", y=\"Number of Songs\", fill=\"Playlist\")+\n  guides(fill=guide_legend(title=\"Playlist\"))+\n  theme_minimal()+\n  ggtitle(\"Musical Key Proportions by Playlist\")\n  \n\nggplotly(fig, tooltip=c(\"text\"))\n\n\n\n\n\nFrom the stacked graph, we observe that no single key has an even distribution for all 4 playlists. Furthermore, we also observe that Indian tracks are dominated by the higher keys and songs from France use the middle-order keys extensively.\nWe will now try to incorporate and analyse one of the most widely associated attributes with music data - genre. Obtaining the genre data was relatively difficult for this dataset, as Spotify does not provide tags for genres for each individual track. The genre typically comes from the artist who composed the song, and the genres associated with them.\n\n# get_artist_name_id - Function to fetch artist name and artist id\nget_artist_name_id <- function(df){\n  for (i in 1:nrow(df)){\n        df[i, \"artist_name\"] <- list(df$track.artists[[i]]$name)\n        df[i, \"artist_id\"] <- list(df$track.artists[[i]]$id)\n    } \n  return(df)\n}\n\n# get_track_genre - Function to get genres associated with each track\nget_track_genre <- function(df){\n  for(i in 1:nrow(df)){\n    get_artists_op <- get_artists(df[i, \"artist_id\"], \n                                  authorization = get_spotify_access_token())\n    df[i, \"genre\"] <- stri_paste(unlist(get_artists_op$genre), collapse=',')\n  }\n  return(df)\n}\n\nAs a first step, we will fetch the artist name and artist_id, which will be required to further query and get the genre data. We then use this data to call the get_artists() API, which uses the artist’s id to get the genres associated with that artist. Some artists do not have any genres associated with them, we list them as “unlisted” in the interest of getting genre proportions.\n\n# Fetch data to get genres mapped to each track\nfilter_all_tracks <- all_tracks %>%\n  select(track.id, track.name, track.artists, duration_ms, track.uri, playlist_name)\nall_tracks_with_artist <- get_artist_name_id(filter_all_tracks)\n\nall_tracks_with_genres <- get_track_genre(all_tracks_with_artist)\nall_tracks_with_genres$genre <- ifelse(all_tracks_with_genres$genre==\"\", \"unlisted\", all_tracks_with_genres$genre)\n\n\n#head(all_tracks_with_genres)\n\nThe genres are now available as comma separated values. For plotting a pie chart, we require a mapped set of the frequency of occurrence of each genre across the 4 playlists. I’ve written the below code chunk for creating a list with frequency counts.\n\n# Create list for getting frequency count of genres across all tracks\ngenre_dict <- list()\n\nfor(i in 1:nrow(all_tracks_with_genres)){\n  if(!is.null(all_tracks_with_genres[i, \"genre\"])){\n    x <- str_split(all_tracks_with_genres[i, \"genre\"], \",\")\n    for(j in 1:length(x[[1]])){\n      if(!x[[1]][j] %in% names(genre_dict)){\n        genre_dict[[x[[1]][j]]] <- as.numeric(1)\n      } else {\n        vals <- genre_dict[[x[[1]][j]]]\n        genre_dict[[x[[1]][j]]] <- (as.numeric(vals)+1)\n      }\n    }\n  } else {\n    next\n  }\n}\n\nFinally, we create a dataframe in the interest of simplicity from the list above, to plot a pie chart.\n\n# Transform list to dataframe for plotting pie chart\ngenre_counts <- data.frame()\ngenre_counts <- data.frame(genre_dict)\ncol_names = names(genre_counts)\n\ngenre_counts <- genre_counts %>%\n  pivot_longer(\n    cols = col_names,\n    names_to = \"features\",\n    values_to = \"counts\",\n    values_drop_na = TRUE\n  )\n\nplot_ly(genre_counts,values=~counts,labels=~factor(features),marker=list(colors=c('#FF7F0E', '#1F77B4')),type=\"pie\")\n\n\n\n\n\nThis plot gives us a distribution of the genres for all tracks across all 4 playlists. We can see that ‘pop’ has the highest proportion amongst all tracks, with almost 10% representation, followed by ‘dance.pop’ with almost 6% tracks.\nThe initial effort was to create this pie chart grouped by playlist, however, implementing frequency counts and playlist name mapping became increasingly complex.\nOne last feature I analysed was track duration. The track duration was avialble in milliseconds, so I decided to first convert it to minutes to get a better idea for comparability. Below is the visualization of track durations across the 4 playlists.\n\n# Boxplot for track duration (in mins) for the 4 playlists\nall_tracks_min <- all_tracks %>%\n  mutate(duration_mins = duration_ms/60000)\n\nfig_time_duration <- plot_ly(all_tracks_min, y=~duration_mins, color = ~playlist_name, type = \"box\") %>% \n  layout(yaxis = list(title = c(\"Track Duration\")))\n\nfig <- subplot(fig_time_duration, nrows=1, titleY=TRUE) %>%\n  layout(title=list(text=\"Time Duration across Playlists\"),\n  plot_bgcolor='#e5ecf6', \n         xaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'),\n         yaxis = list( \n           zerolinecolor = '#ffff', \n           zerolinewidth = 2, \n           gridcolor = 'ffff'))\n\nfig\n\n\n\n\n\nWe can see that Indian songs in general have longer tracks, followed by USA. Brazil has a very compact range of track duration, ranging from 2 to ~3.5 minutes."
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html#reflection",
    "href": "posts/SahasraIyer_FinalProject.html#reflection",
    "title": "601_Final_Project",
    "section": "Reflection",
    "text": "Reflection\nProcuring, analysing, cleaning and visualizing data is a part of the day-to-day of any aspiring Data Scientist. This particular project was fascinating, yet challenging in terms of the data that it presented. Understanding the nuances of each acoustic feature, and the limitations of the Spotify wrapper was a whole new learning curve. The Spotify wrapper presents a wide array of data. I think, perhaps, sticking to analyzing global playlists, instead of certain curated playlists, could have perhaps brought down the interestingness of the visualizations. However, I made the decision to analyze globally curated playlists to be backed by numbers, and not simply go on analyzing data on the basis on intuition.\nAother decision that I made to primariliy focus on the acoustic features, and not on other aspects, such as album statistics, or artists that fetaure in multiple playlists, was done in the interest of maintaining the aesthetics of the visualizations.\nAs previously mentioned, the most challenging part about this project was understanding how to procure the data, dabble with and gain an understanding of the different acoustic features, and generate a variety of visualizations.\nIf I could carry on with this project, I would delve deeper into podcasts as a category, and try to analyse the acoustic features in podcast playlists. The Spotify wrapper also has API calls such as ‘get_recommendations()’, which essentially creates a playlist-style listening experience based on seed artists, tracks and genres. I would have wanted to experiment with recommendations, basis the Top 50 artists and tracks, and see what insights could be gained from the same.\nIn essence, I believe the Spotify package has some amazing API functions, and I would have liked to deep-dive into that documentation to have a more well-rounded project."
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html#conclusion",
    "href": "posts/SahasraIyer_FinalProject.html#conclusion",
    "title": "601_Final_Project",
    "section": "Conclusion",
    "text": "Conclusion\nThis project revolved around the Spotify data of the Top 50 tracks from 4 countries, namely, India, USA, Brazil and France. I chose these 4 countries very intuitively on the basis of the music that I personally have heard from these 4 countries, and know them to be extremely varied. Through the course of this project, I discovered that certain musical similarities do exists between the tracks from the different countries.\nOur initial analysis involved looking at the danceability, energy, loudness, acousticness and speechiness of the tracks. We observed that Brazil has the highest number of danceable songs, which was backed by the density distribution plot, France has a high energy factor in their Top 50 tracks, which was confirmed by the energy distribution plot and that the Brazilian songs have higher decibel levels than songs from the other playlists (which was also supported by the loudness density plot).\nFurther, our heatmap suggested that loudness and energy have a likely linear relationship, which we observed via the scatterplot. The heatmap also suggested an inverse relationship between acousticness and danceability. Both of these conclusions intuitively make sense, as higher decibel music typically requires a greater energy performance, thus inciting a similar response. Also, it is not typical to seek out soft, acoustic covers when one is looking for some numbers to dance on, which supplements the inverse relationship.\nWe also observed that popularity bears no significance on the danceability of a track. We also delved into the speechiness of a track, where we went with the assumption that any track having speechiness difference > 0.33 would typically be classified to be a rap song, and saw the representation of speechiness across the 4 playlists. We also looked at the musical key composition of all 4 playlists.\nFinally, we looked at the genres of the tracks present in these playlists, which required extensive coding. We saw that pop and dance pop were the most popular genres.\nIt must be noted that this data was relevant as of 16th December, 2022. Given the volatility of these charts, the above analysis stands good for the data as of above mentioned date. There is a possibility that the playlists have since seen changes in tracks, which could perhaps affect the results of this analysis."
  },
  {
    "objectID": "posts/SahasraIyer_FinalProject.html#bibliography",
    "href": "posts/SahasraIyer_FinalProject.html#bibliography",
    "title": "601_Final_Project",
    "section": "Bibliography",
    "text": "Bibliography\n\nhttps://cran.r-project.org/web/packages/tinyspotifyr/tinyspotifyr.pdf - The Spotifyr wrapper documentation\nhttps://towardsdatascience.com/what-makes-a-song-likeable-dbfdb7abe404 - Audio features definitions\nhttps://www.rcharlie.com/spotifyr/ - Spotifyr package usage website\nhttps://plotly.com/r/ - Plotly R Open Sourcing Graphing Library"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "",
    "text": "Cryptocurrencies, the word makes people curious about what it is, why is so important, what kind of role does it play in the society. But cryptocurrencies are nothing but money which stored in digital format and it is designed to be an exchange medium whose records are saved on that digital format on something called the blockchain and smart contracts. But we can talk about blockchain and smart contracts in a different topic altogether.\nThe term cryptocurrencies first came to be in the early 2010’s as a form of digital assets and has now caused an economic boom where people who had invested back in the day are present day billionaires now.\nThese digital assets are the talk of the town and this project we will find out how cryto currencies started out and developed in prices and market capitalization.\nThe dataset was extracted from Kaggle."
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#about-the-dataset",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#about-the-dataset",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "About the Dataset",
    "text": "About the Dataset\nIn this project we have a data set about with about 20 different crytpo currencies including bitcoin, ethereum, solana, tether, etc., and it has the following features as well:\n1.Currency 2.Date 3.Open (Opening Price) 4.High (Highest price of the crypto that day) 5.Low (Lowest price of the crypto that day) 6.Close (Closing Price) 7.Volume (Total Volume of coins traded by day) 8.Market Cap (Total Market Value of the coin by day)"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#my-research-question",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#my-research-question",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "My research question:",
    "text": "My research question:\nIn this project we want to vizualize the 23 different crypto-currenices in a time series, where we can see the respective crypto-currency along with its open, close, high and low prices and values based on the dates.\nAnd we will also compare Bitcoin with tether in terms of prices and volume and find out: - which coin dominated in which years in terms of market capitalization. - the different prices of the cryptocurrencies from the years 2017 to 2021 because thats when these digital assets became a super hit. - Why Bitcoin’s volume decreased during 2020-2021 while tether’s volume kept increasing?"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#the-full-dataset",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#the-full-dataset",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "The full Dataset",
    "text": "The full Dataset\n\ncrypto <- read_csv(\"_data/crypto_maa2.csv\")\n\nRows: 37082 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Currency, Date\ndbl (6): Open, High, Low, Close, Volume, Marketcap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrypto\n\n# A tibble: 37,082 × 8\n   Currency Date        Open  High   Low Close   Volume  Marketcap\n   <chr>    <chr>      <dbl> <dbl> <dbl> <dbl>    <dbl>      <dbl>\n 1 Aave     05/10/2020  52.7  55.1  49.8  53.2       0   89128129.\n 2 Aave     06/10/2020  53.3  53.4  40.7  42.4  583091.  71011441.\n 3 Aave     07/10/2020  42.4  42.4  36.0  40.1  682834.  67130037.\n 4 Aave     08/10/2020  39.9  44.9  36.7  43.8 1658817. 220265142.\n 5 Aave     09/10/2020  43.8  47.6  43.3  46.8  815538. 235632208.\n 6 Aave     10/10/2020  46.8  51.4  46.7  49.1 1074627. 247288429.\n 7 Aave     11/10/2020  49.1  51.5  48.7  49.7  692151. 249940843.\n 8 Aave     12/10/2020  49.7  54.4  48.8  52.2 1354836. 262915666.\n 9 Aave     13/10/2020  52.2  57.5  49.6  51.1 1386221. 257307050.\n10 Aave     14/10/2020  51.4  57.9  49.6  51.3 3132405. 258274392.\n# … with 37,072 more rows"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#summary-of-the-crypto-dataset",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#summary-of-the-crypto-dataset",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Summary of the Crypto Dataset",
    "text": "Summary of the Crypto Dataset\nHere we get the summary of the data set where we can check the full features of each and every column.\nWe can see that the highest price any cryptocurrency has is 64863.10 and the lowest is 62208.96.\nAnd the highest market cap is 1186360000000. Which is $1,186,360,000,000. That is a huge amount of money. This value would most likely be for bitcoin as its has the highest value in the market till date.\n\ncrypto$Date = as.Date(crypto$Date, format = \"%d/%m/%Y\")\ncrypto$Open = as.numeric(gsub(pattern = ',','',crypto$Open))\ncrypto$High = as.numeric(gsub(pattern = ',','',crypto$High))\ncrypto$Low = as.numeric(gsub(pattern = ',','',crypto$Low))\ncrypto$Close = as.numeric(gsub(pattern = ',','',crypto$Close))\ncrypto$Volume = as.numeric(gsub(pattern = ',','',crypto$Volume))\ncrypto$Marketcap = as.numeric(gsub(pattern = ',','',crypto$Marketcap))\ncrypto$Year = as.numeric(format(crypto$Date,\"%Y\"))\ncrypto$Currency = capitalize(crypto$Currency)\nsummary(crypto)\n\n   Currency              Date                 Open               High         \n Length:37082       Min.   :2013-04-29   Min.   :    0.00   Min.   :    0.00  \n Class :character   1st Qu.:2017-03-05   1st Qu.:    0.07   1st Qu.:    0.08  \n Mode  :character   Median :2019-01-09   Median :    1.00   Median :    1.01  \n                    Mean   :2018-08-15   Mean   :  985.32   Mean   : 1016.06  \n                    3rd Qu.:2020-05-13   3rd Qu.:   30.46   3rd Qu.:   31.92  \n                    Max.   :2021-07-06   Max.   :63523.75   Max.   :64863.10  \n      Low               Close              Volume            Marketcap        \n Min.   :    0.00   Min.   :    0.00   Min.   :0.000e+00   Min.   :0.000e+00  \n 1st Qu.:    0.07   1st Qu.:    0.07   1st Qu.:4.937e+06   1st Qu.:2.396e+08  \n Median :    1.00   Median :    1.00   Median :8.513e+07   Median :1.405e+09  \n Mean   :  952.99   Mean   :  987.12   Mean   :3.022e+09   Mean   :1.543e+10  \n 3rd Qu.:   29.00   3rd Qu.:   30.51   3rd Qu.:9.388e+08   3rd Qu.:5.159e+09  \n Max.   :62208.96   Max.   :63503.46   Max.   :3.510e+11   Max.   :1.190e+12  \n      Year     \n Min.   :2013  \n 1st Qu.:2017  \n Median :2019  \n Mean   :2018  \n 3rd Qu.:2020  \n Max.   :2021  \n\ncrypto\n\n# A tibble: 37,082 × 9\n   Currency Date        Open  High   Low Close   Volume  Marketcap  Year\n   <chr>    <date>     <dbl> <dbl> <dbl> <dbl>    <dbl>      <dbl> <dbl>\n 1 Aave     2020-10-05  52.7  55.1  49.8  53.2       0   89128129.  2020\n 2 Aave     2020-10-06  53.3  53.4  40.7  42.4  583091.  71011441.  2020\n 3 Aave     2020-10-07  42.4  42.4  36.0  40.1  682834.  67130037.  2020\n 4 Aave     2020-10-08  39.9  44.9  36.7  43.8 1658817. 220265142.  2020\n 5 Aave     2020-10-09  43.8  47.6  43.3  46.8  815538. 235632208.  2020\n 6 Aave     2020-10-10  46.8  51.4  46.7  49.1 1074627. 247288429.  2020\n 7 Aave     2020-10-11  49.1  51.5  48.7  49.7  692151. 249940843.  2020\n 8 Aave     2020-10-12  49.7  54.4  48.8  52.2 1354836. 262915666.  2020\n 9 Aave     2020-10-13  52.2  57.5  49.6  51.1 1386221. 257307050.  2020\n10 Aave     2020-10-14  51.4  57.9  49.6  51.3 3132405. 258274392.  2020\n# … with 37,072 more rows"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#vizualizing-the-cryptocurrencies-in-the-data-set.",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#vizualizing-the-cryptocurrencies-in-the-data-set.",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Vizualizing the cryptocurrencies in the data set.",
    "text": "Vizualizing the cryptocurrencies in the data set.\nWe need to check if the dataset has no NA values and we can check that be creating a piechart plot\n\nplot_ly(as.data.frame(crypto$Currency),labels = crypto$Currency,type = 'pie', hole = 0.3) %>%\n  layout(xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),\n         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#prices-market-values-and-volume-of-the-digital-assets.",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#prices-market-values-and-volume-of-the-digital-assets.",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Prices, Market Values and Volume of the digital assets.",
    "text": "Prices, Market Values and Volume of the digital assets.\nIn this visualization we will go ahead and check the opening prices and the closing prices in the plot. It will be clearly shown that the prices of all currencies didn’t start picking up until the later half of 2017. The were a few interesting coins that picked up in the later half of 2017 and died down quickly after the first two years that is was introduced to the financial markets because of this.\nThe cause of the sudden surge in interest in Bitcoin in the middle of 2017 is still a mystery, but it is wildly speculative that a single whale artificially inflated the price of Bitcoin, sparking a surge in demand for the currency on a massive scale. As a result, the financial world began to take an interest in digital currencies, and the prices of practically all currencies increased, with Bitcoin emerging as the strongest and most well-liked currency in the field.\nWe need to make a visualization to get the market values for all the coins to make it easier to navigate through the data. The data set will be divided into 23 subsets of each currency.\nAs The subsetted data is in the form ‘OHLC’ (open,High,Low,Close) it is in the ideal format to work with the Quantmod library and hence is converted to XTS type.\n\nBitcoin = subset(crypto,crypto$Currency == 'Bitcoin')\nBitcoin$Currency = NULL\ncolnames(Bitcoin) = c('BitcoinDate','BitcoinOpen','BitcoinHigh','BitcoinLow','BitcoinClose','BitcoinVolume','BitcoinMarketcap', 'BitcoinYear')\nBitcoin = xts(Bitcoin[,-1], order.by=as.Date(Bitcoin$BitcoinDate))\n\nLitecoin = subset(crypto,crypto$Currency == 'Litecoin')\nLitecoin$Currency = NULL\ncolnames(Litecoin) = c('LitecoinDate','LitecoinOpen','LitecoinHigh','LitecoinLow','LitecoinClose','LitecoinVolume','LitecoinMarketcap', 'LitecoinYear')\nLitecoin = xts(Litecoin[,-1], order.by=as.Date(Litecoin$LitecoinDate))\n\nXRP = subset(crypto,crypto$Currency == 'XRP')\nXRP$Currency = NULL\ncolnames(XRP) = c('XRPDate','XRPOpen','XRPHigh','XRPLow','XRPClose','XRPVolume','XRPMarketcap', 'XRPYear')\nXRP = xts(XRP[,-1], order.by=as.Date(XRP$XRPDate))\n\nDogecoin = subset(crypto,crypto$Currency == 'Dogecoin')\nDogecoin$Currency = NULL\ncolnames(Dogecoin) = c('DogecoinDate','DogecoinOpen','DogecoinHigh','DogecoinLow','DogecoinClose','DogecoinVolume','DogecoinMarketcap', 'DogecoinYear')\nDogecoin = xts(Dogecoin[,-1], order.by=as.Date(Dogecoin$DogecoinDate))\n\nMonero = subset(crypto,crypto$Currency == 'Monero')\nMonero$Currency = NULL\ncolnames(Monero) = c('MoneroDate','MoneroOpen','MoneroHigh','MoneroLow','MoneroClose','MoneroVolume','MoneroMarketcap', 'MoneroYear')\nMonero = xts(Monero[,-1], order.by=as.Date(Monero$MoneroDate))\n\nStellar = subset(crypto,crypto$Currency == 'Stellar')\nStellar$Currency = NULL\ncolnames(Stellar) = c('StellarDate','StellarOpen','StellarHigh','StellarLow','StellarClose','StellarVolume','StellarMarketcap', 'StellarYear')\nStellar = xts(Stellar[,-1], order.by=as.Date(Stellar$StellarDate))\n\nTether = subset(crypto,crypto$Currency == 'Tether')\nTether$Currency = NULL\ncolnames(Tether) = c('TetherDate','TetherOpen','TetherHigh','TetherLow','TetherClose','TetherVolume','TetherMarketcap', 'TetherYear')\nTether = xts(Tether[,-1], order.by=as.Date(Tether$TetherDate))\n\nNEM = subset(crypto,crypto$Currency == 'NEM')\nNEM$Currency = NULL\ncolnames(NEM) = c('NEMDate','NEMOpen','NEMHigh','NEMLow','NEMClose','NEMVolume','NEMMarketcap', 'NEMYear')\nNEM = xts(NEM[,-1], order.by=as.Date(NEM$NEMDate))\n\nEthereum = subset(crypto,crypto$Currency == 'Ethereum')\nEthereum$Currency = NULL\ncolnames(Ethereum) = c('EthereumDate','EthereumOpen','EthereumHigh','EthereumLow','EthereumClose','EthereumVolume','EthereumMarketcap', 'EthereumYear')\nEthereum = xts(Ethereum[,-1], order.by=as.Date(Ethereum$EthereumDate))\n\nIOTA = subset(crypto,crypto$Currency == 'IOTA')\nIOTA$Currency = NULL\ncolnames(IOTA) = c('IOTADate','IOTAOpen','IOTAHigh','IOTALow','IOTAClose','IOTAVolume','IOTAMarketcap', 'IOTAYear')\nIOTA = xts(IOTA[,-1], order.by=as.Date(IOTA$IOTADate))\n\nEOS = subset(crypto,crypto$Currency == 'EOS')\nEOS$Currency = NULL\ncolnames(EOS) = c('EOSDate','EOSOpen','EOSHigh','EOSLow','EOSClose','EOSVolume','EOSMarketcap', 'EOSYear')\nEOS = xts(EOS[,-1], order.by=as.Date(EOS$EOSDate))\n\nTRON = subset(crypto,crypto$Currency == 'TRON')\nTRON$Currency = NULL\ncolnames(TRON) = c('TRONDate','TRONOpen','TRONHigh','TRONLow','TRONClose','TRONVolume','TRONMarketcap', 'TRONYear')\nTRON = xts(TRON[,-1], order.by=as.Date(TRON$TRONDate))\n\nChainlink = subset(crypto,crypto$Currency == 'Chainlink')\nChainlink$Currency = NULL\ncolnames(Chainlink) = c('ChainlinkDate','ChainlinkOpen','ChainlinkHigh','ChainlinkLow','ChainlinkClose','ChainlinkVolume','ChainlinkMarketcap', 'ChainlinkYear')\nChainlink = xts(Chainlink[,-1], order.by=as.Date(Chainlink$ChainlinkDate))\n\nCardano = subset(crypto,crypto$Currency == 'Cardano')\nCardano$Currency = NULL\ncolnames(Cardano) = c('CardanoDate','CardanoOpen','CardanoHigh','CardanoLow','CardanoClose','CardanoVolume','CardanoMarketcap', 'CardanoYear')\nCardano = xts(Cardano[,-1], order.by=as.Date(Cardano$CardanoDate))\n\nCosmos = subset(crypto,crypto$Currency == 'Cosmos')\nCosmos$Currency = NULL\ncolnames(Cosmos) = c('CosmosDate','CosmosOpen','CosmosHigh','CosmosLow','CosmosClose','CosmosVolume','CosmosMarketcap', 'CosmosYear')\nCosmos = xts(Cosmos[,-1], order.by=as.Date(Cosmos$CosmosDate))\n\nSolana = subset(crypto,crypto$Currency == 'Solana')\nSolana$Currency = NULL\ncolnames(Solana) = c('SolanaDate','SolanaOpen','SolanaHigh','SolanaLow','SolanaClose','SolanaVolume','SolanaMarketcap', 'SolanaYear')\nSolana = xts(Solana[,-1], order.by=as.Date(Solana$SolanaDate))\n\nPolkadot = subset(crypto,crypto$Currency == 'Polkadot')\nPolkadot$Currency = NULL\ncolnames(Polkadot) = c('PolkadotDate','PolkadotOpen','PolkadotHigh','PolkadotLow','PolkadotClose','PolkadotVolume','PolkadotMarketcap', 'PolkadotYear')\nPolkadot = xts(Polkadot[,-1], order.by=as.Date(Polkadot$PolkadotDate))\n\nUniswap = subset(crypto,crypto$Currency == 'Uniswap')\nUniswap$Currency = NULL\ncolnames(Uniswap) = c('UniswapDate','UniswapOpen','UniswapHigh','UniswapLow','UniswapClose','UniswapVolume','UniswapMarketcap', 'UniswapYear')\nUniswap = xts(Uniswap[,-1], order.by=as.Date(Uniswap$UniswapDate))\n\nAave = subset(crypto,crypto$Currency == 'Aave')\nAave$Currency = NULL\ncolnames(Aave) = c('AaveDate','AaveOpen','AaveHigh','AaveLow','AaveClose','AaveVolume','AaveMarketcap', 'AaveYear')\nAave = xts(Aave[,-1], order.by=as.Date(Aave$AaveDate))\n\nNow we talk a look at the the price series of the cryptocurrencies.\nFirst we will combine the time-series data into one singe data frame for easier plotting.\n\ncombined_df <- merge(Bitcoin, Ethereum, by = \"Date\")\n\nWarning in merge.xts(Bitcoin, Ethereum, by = \"Date\"): NAs introduced by coercion\n\nWarning in merge.xts(Bitcoin, Ethereum, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, Aave, by = \"Date\")\n\nWarning in merge.xts(combined_df, Aave, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, Aave, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, Cardano, by = \"Date\")\n\nWarning in merge.xts(combined_df, Cardano, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Cardano, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, Chainlink, by = \"Date\")\n\nWarning in merge.xts(combined_df, Chainlink, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Chainlink, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, Cosmos, by = \"Date\")\n\nWarning in merge.xts(combined_df, Cosmos, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Cosmos, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, Dogecoin, by = \"Date\")\n\nWarning in merge.xts(combined_df, Dogecoin, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Dogecoin, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, EOS, by = \"Date\")\n\nWarning in merge.xts(combined_df, EOS, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, EOS, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, Ethereum, by = \"Date\")\n\nWarning in merge.xts(combined_df, Ethereum, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Ethereum, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, IOTA, by = \"Date\")\n\nWarning in merge.xts(combined_df, IOTA, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, IOTA, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, Litecoin, by = \"Date\")\n\nWarning in merge.xts(combined_df, Litecoin, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Litecoin, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, Monero, by = \"Date\")\n\nWarning in merge.xts(combined_df, Monero, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Monero, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, NEM, by = \"Date\")\n\nWarning in merge.xts(combined_df, NEM, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, NEM, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, XRP, by = \"Date\")\n\nWarning in merge.xts(combined_df, XRP, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, XRP, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, Polkadot, by = \"Date\")\n\nWarning in merge.xts(combined_df, Polkadot, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Polkadot, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, Stellar, by = \"Date\")\n\nWarning in merge.xts(combined_df, Stellar, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Stellar, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, Tether, by = \"Date\")\n\nWarning in merge.xts(combined_df, Tether, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Tether, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, TRON, by = \"Date\")\n\nWarning in merge.xts(combined_df, TRON, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, TRON, by = \"Date\"): NAs introduced by coercion\n\ncombined_df <- merge(combined_df, Uniswap, by = \"Date\")\n\nWarning in merge.xts(combined_df, Uniswap, by = \"Date\"): NAs introduced by\ncoercion\n\n\nWarning in merge.xts(combined_df, Uniswap, by = \"Date\"): NAs introduced by\ncoercion\n\ncombined_df <- merge(combined_df, XRP, by = \"Date\")\n\nWarning in merge.xts(combined_df, XRP, by = \"Date\"): NAs introduced by coercion\n\n\nWarning in merge.xts(combined_df, XRP, by = \"Date\"): NAs introduced by coercion\n\ncombined_df[is.na(combined_df)] <- 0\n\nAnd then we will plot the data in a line graph, we will reduce bitcoin’s opening price by 1000 to see the other prices more clearly.\n\nggplot(combined_df, aes(x = index(Bitcoin), y = BitcoinOpen/100, color = \"Bitcoin\")) +\n  geom_line() +\n  ggtitle(\"Crypto Opening price series\") + \n  theme(legend.position = \"top\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(\n  x = \"Date\",\n  y = \"Opening Prices\",\n  color = \"Currencies\",\n  caption = \"Sai\") +\n  scale_x_date(date_labels = \"%b %y\", date_breaks = \"6 months\") + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  theme(legend.key.width = unit(2,\"cm\"))+\n  geom_line(aes(y = EthereumOpen, color = \"Ethereum\"))+\ngeom_line(aes(y = AaveOpen, color = \"Aave\"))+\ngeom_line(aes(y = CardanoOpen, color = \"Cardano\"))+\ngeom_line(aes(y = ChainlinkOpen, color = \"Chainlink\"))+\ngeom_line(aes(y = CosmosOpen, color = \"Cosmos\"))+\ngeom_line(aes(y = DogecoinOpen, color = \"Dogecoin\"))+\ngeom_line(aes(y = EOSOpen, color = \"EOS\"))+\ngeom_line(aes(y = IOTAOpen, color = \"IOTA\"))+\ngeom_line(aes(y = LitecoinOpen, color = \"Litecoin\"))+\ngeom_line(aes(y = MoneroOpen, color = \"Monero\"))+\ngeom_line(aes(y = NEMOpen, color = \"NEM\"))+\ngeom_line(aes(y = PolkadotOpen, color = \"Polkadot\"))+\ngeom_line(aes(y = StellarOpen, color = \"Stellar\"))+\ngeom_line(aes(y = TetherOpen, color = \"Tether\"))+\ngeom_line(aes(y = TRONOpen, color = \"TRON\"))+\ngeom_line(aes(y = UniswapOpen, color = \"Uniswap\"))+\ngeom_line(aes(y = XRPOpen, color = \"XRP\"))\n\n\n\n\nFrom the line graph above we can see that the price of bitcoin rose in 2017 and the dropped and almost all the crytocurriences had the same trend until 2021 where they skyrocketted."
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#market-capitalization",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#market-capitalization",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Market Capitalization",
    "text": "Market Capitalization\nNow we will check the total market capitalization in a the same line graph format.\n\nggplot(combined_df, aes(x = index(Bitcoin), y = BitcoinMarketcap/100000000, color = \"Bitcoin\")) +\n  geom_line() +\n  ggtitle(\"Crypto Market Cap\") + \n  theme(legend.position = \"top\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(\n  x = \"Date\",\n  y = \"Total Mean Capital\",\n  color = \"Currencies\",\n  title = \"Crypto Market Capitalization\",\n  subtitle = \"in 100 millions\", \n  caption = \"Sai\") +\n  scale_x_date(date_labels = \"%b %y\", date_breaks = \"6 months\") + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  theme(legend.key.width = unit(2,\"cm\"))+\n  geom_line(aes(y = EthereumMarketcap/100000000, color = \"Ethereum\"))+\ngeom_line(aes(y = AaveMarketcap/100000000, color = \"Aave\"))+\ngeom_line(aes(y = CardanoMarketcap/100000000, color = \"Cardano\"))+\ngeom_line(aes(y = ChainlinkMarketcap/100000000, color = \"Chainlink\"))+\ngeom_line(aes(y = CosmosMarketcap/100000000, color = \"Cosmos\"))+\ngeom_line(aes(y = DogecoinMarketcap/100000000, color = \"Dogecoin\"))+\ngeom_line(aes(y = EOSMarketcap/100000000, color = \"EOS\"))+\ngeom_line(aes(y = IOTAMarketcap/100000000, color = \"IOTA\"))+\ngeom_line(aes(y = LitecoinMarketcap/100000000, color = \"Litecoin\"))+\ngeom_line(aes(y = MoneroMarketcap/100000000, color = \"Monero\"))+\ngeom_line(aes(y = NEMMarketcap/100000000, color = \"NEM\"))+\ngeom_line(aes(y = PolkadotMarketcap/100000000, color = \"Polkadot\"))+\ngeom_line(aes(y = StellarMarketcap/100000000, color = \"Stellar\"))+\ngeom_line(aes(y = TetherMarketcap/100000000, color = \"Tether\"))+\ngeom_line(aes(y = TRONMarketcap/100000000, color = \"TRON\"))+\ngeom_line(aes(y = UniswapMarketcap/100000000, color = \"Uniswap\"))+\ngeom_line(aes(y = XRPMarketcap/100000000, color = \"XRP\"))\n\n\n\n\nThe market capitalisation of Bitcoin throughout time is depicted in the plot below. It’s interesting to note that in the beginning, in terms of market capitalization, Bitcoin and Ethereum were practically equal. But as Bitcoin became more and more well-known over time, its value multiplied.\nAs previously noted, it is still somewhat unclear what caused the unexpected interest and growth in the value of digital currencies 10 years after their creation, including that of Bitcoin past 2017. However, some of the primary factors are as follows:\n\nThe regulation of digital currencies is not very strict: Digital currencies are unregulated, in contrast to fiat currencies like the US Dollar or the Euro, which means that no central bank or specific government of any country has as much authority over them as they do over other financial products.\n\nRemember that the world was experiencing the 2008 banking crises when “Satoshi Nakamoto,” the person who invented Bitcoin, wrote the code that enabled it to exist back in 2009. The need for an alternative to fiat currencies, which were subject to heavy regulation and the speculative dynamics of the financial markets, led to the creation of digital currencies.\n\nThey are very discrete: Compared to bank accounts or other forms of money storage, cryptocurrencies provide a lot more privacy. The alpha-numeric account numbers or “addresses” of cryptocurrencies are used as pointers to digital wallets and are unrelated to the personal data of the organization or person who owns the wallet. thereby offering a considerably stronger sense of protection against fraud and financial watchdog rules than traditional currencies.\nOnly twenty million Bitcoins are available at any given time, which is a fairly small and consistent supply for this particular currency. Unlike traditional currencies, Bitcoins cannot be produced indefinitely in a money mint after a major natural disaster or a financial crisis. This not only increases the value of the existing Bitcoins over time but also lends consistency to their monetary values."
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#price-indicators-of-all-the-currencies.",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#price-indicators-of-all-the-currencies.",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Price Indicators of all the currencies.",
    "text": "Price Indicators of all the currencies.\nHere in this visualization below we will check the prices of all the crypto currencies from 2017 to 2021\n\ncrypto$Year <- as.character(crypto$Year)\n\n  crypto %>%\n  dplyr::filter(Year == \"2017\" | Year == \"2018\" | Year == \"2019\" | Year == \"2020\" | Year == \"2021\") %>%\n  plot_ly(type = 'box',x = ~High,transforms = list(list(type = 'filter',\n                                                        target = ~Currency,\n                                                        operation = '=',\n              value = unique(crypto$Currency)[1])),name=\"High\") %>%\n  add_boxplot(x = ~Low,name = 'Low') %>%\n  add_boxplot(x = ~Open,name = 'Open') %>%\n  add_boxplot(x = ~Close,name = 'Close') %>%\n  \n  layout(title = 'All Price Indicators of all Currencies',\n         xaxis = list(title = 'Value'), \n         updatemenus = list(list(type = 'dropdown',buttons = list(\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[1]),\n         label = unique(crypto$Currency)[1]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[2]),\n         label = unique(crypto$Currency)[2]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[3]),\n         label = unique(crypto$Currency)[3]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[4]),\n         label = unique(crypto$Currency)[4]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[5]),\n         label = unique(crypto$Currency)[5]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[6]),\n         label = unique(crypto$Currency)[6]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[7]),\n         label = unique(crypto$Currency)[7]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[8]),\n         label = unique(crypto$Currency)[8]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[9]),\n         label = unique(crypto$Currency)[9]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[10]),\n         label = unique(crypto$Currency)[10]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[11]),\n         label = unique(crypto$Currency)[11]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[12]),\n         label = unique(crypto$Currency)[12]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[13]),\n         label = unique(crypto$Currency)[13]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[14]),\n         label = unique(crypto$Currency)[14]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[15]),\n         label = unique(crypto$Currency)[15]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[16]),\n         label = unique(crypto$Currency)[16]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[17]),\n         label = unique(crypto$Currency)[17]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[18]),\n         label = unique(crypto$Currency)[18]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[19]),\n         label = unique(crypto$Currency)[19]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[20]),\n         label = unique(crypto$Currency)[20]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[21]),\n         label = unique(crypto$Currency)[21]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[22]),\n         label = unique(crypto$Currency)[22]),\n    list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Currency)[23]),\n         label = unique(crypto$Currency)[23]))))) %>%\n  layout(annotations = list(list(text = \"For the years 2017-21\",  xref = \"paper\", yref = \"paper\",\n                            yanchor = \"bottom\",xanchor = \"center\", align = \"center\",\n                            x = 0.5, y = .97, showarrow = FALSE)))"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#yearly-capitalization-representation",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#yearly-capitalization-representation",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Yearly Capitalization Representation",
    "text": "Yearly Capitalization Representation\n\ncrypto %>%\n  plot_ly(labels = ~Currency,values = ~Marketcap, type = 'pie',\n          transforms = list(list(type = 'filter',target = ~Year,operation = '=',\n                          value = unique(crypto$Year)[1])),name=\"2013\") %>%\n  layout(title = \"Currency Yearly Market Capitalization Representation\",\n         xaxis = list(title = 'Value'), \nupdatemenus = list(list(type = 'dropdown',buttons = list(\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[1]),\n       label = unique(crypto$Year)[1]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[2]),\n       label = unique(crypto$Year)[2]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[3]),\n       label = unique(crypto$Year)[3]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[4]),\n       label = unique(crypto$Year)[4]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[5]),\n       label = unique(crypto$Year)[5]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[6]),\n       label = unique(crypto$Year)[6]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[7]),\n       label = unique(crypto$Year)[7]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[8]),\n       label = unique(crypto$Year)[8]),\n  list(method = \"restyle\",args = list(\"transforms[0].value\", unique(crypto$Year)[9]),\n       label = unique(crypto$Year)[9])))))"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#traded-crypto-visualization",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#traded-crypto-visualization",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Traded Crypto Visualization",
    "text": "Traded Crypto Visualization\nThe volume traded data from 2017 onward is shown here because there was scarcely any interest in digital currency before to that year. It is clear that after the middle of 2017, there was an increase in market interest, which led to significant volume being traded on international financial markets.\nAlthough Tether’s price range (96 cents to $1.04 vs. 4k to 18k for Bitcoin) and market capitalization are vastly different from those of Bitcoin, it is also extremely fascinating to notice that Tether rivals and occasionally surpasses Bitcoin in terms of trading volume.\n\n  ggplot(combined_df['2017-08/'], aes(x = index(Bitcoin['2017-08/']), y = BitcoinVolume['2017-08/']/100000000, color = \"Bitcoin\")) +\n  geom_line() +\n  ggtitle(\"Traded Crypto\") + \n  theme(legend.position = \"top\") + \n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  labs(\n  x = \"Date\",\n  y = \"Total Mean Volume\",\n  color = \"Currencies\",\n  title = \"All crypto traded\",\n  subtitle = \"in 100 millions\", \n  caption = \"Sai\") +\n  scale_x_date(date_labels = \"%b %y\", date_breaks = \"6 months\") + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  theme(legend.key.width = unit(2,\"cm\"))+\n  geom_line(aes(y = EthereumVolume/100000000, color = \"Ethereum\"))+\ngeom_line(aes(y = AaveVolume/100000000, color = \"Aave\"))+\ngeom_line(aes(y = CardanoVolume/100000000, color = \"Cardano\"))+\ngeom_line(aes(y = ChainlinkVolume/100000000, color = \"Chainlink\"))+\ngeom_line(aes(y = CosmosVolume/100000000, color = \"Cosmos\"))+\ngeom_line(aes(y = DogecoinVolume/100000000, color = \"Dogecoin\"))+\ngeom_line(aes(y = EOSVolume/100000000, color = \"EOS\"))+\ngeom_line(aes(y = IOTAVolume/100000000, color = \"IOTA\"))+\ngeom_line(aes(y = LitecoinVolume/100000000, color = \"Litecoin\"))+\ngeom_line(aes(y = MoneroVolume/100000000, color = \"Monero\"))+\ngeom_line(aes(y = NEMVolume/100000000, color = \"NEM\"))+\ngeom_line(aes(y = PolkadotVolume/100000000, color = \"Polkadot\"))+\ngeom_line(aes(y = StellarVolume/100000000, color = \"Stellar\"))+\ngeom_line(aes(y = TetherVolume/100000000, color = \"Tether\"))+\ngeom_line(aes(y = TRONVolume/100000000, color = \"TRON\"))+\ngeom_line(aes(y = UniswapVolume/100000000, color = \"Uniswap\"))+\ngeom_line(aes(y = XRPVolume/100000000, color = \"XRP\"))\n\nWarning: Removed 1436 rows containing missing values (`geom_line()`).\n\n\n\n\n\nAs expected, the graphical illustration below demonstrates that in 2020 and in 2021, Tether currency traded at a bigger volume on the financial markets than Bitcoin.\nThe project goes on to investigate the causes of this as well as the relationships between the market capitalization and volume of trading of the two currencies.\n\nBitcoin$Year = as.numeric(format(index(Bitcoin),\"%Y\"))\nTether$Year = as.numeric(format(index(Tether),\"%Y\"))\n\nVol_yrly_BT = aggregate(BitcoinVolume ~ Year, data=Bitcoin , FUN= mean)\nVol_yrly_TT = aggregate(TetherVolume ~ Year, data=Tether , FUN= mean)\n\nVol_yrly_combined = merge(Vol_yrly_BT, Vol_yrly_TT, by=\"Year\")\n\n# Merge the data for Bitcoin and Tether into a single data frame\nmerged_data <- merge(Vol_yrly_BT, Tether, by = \"Year\")\n\n# Create the plot using the merged data\nplot_ly(merged_data, x =~Year ,y = ~BitcoinVolume, type = 'bar',name = 'Bitcoin Volume',\n        marker = list(color = 'rgb(49,130,189)')) %>%\n  add_trace(y = ~TetherVolume, name = 'Tether Volume',\n            marker = list(color = 'rgb(204,204,204)')) %>%\n  layout(title = 'Yearly Volume Traded by Bitcoin & Tether',\n         xaxis = list(title = 'Years',tickangle = -45),\n         yaxis = list(title = 'Total Volume Traded'),\n         margin = list(b=100),\n         barmode = 'group')"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#distribution-and-correlation-of-the-volume",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#distribution-and-correlation-of-the-volume",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Distribution and Correlation of the Volume",
    "text": "Distribution and Correlation of the Volume\nThe distribution of the volume variable for the currencies Tether and Bitcoin is shown in the graphs below. The graph clearly shows that it is a right-skewed distribution. They are both positively skewed, in other words. The data’s means and standard deviations are also noted.\nThat mean is far lower than the median in both cases, as can be seen quite plainly. This may indicate that the left side of the data contains many outliers.\n\nBitcoin2021 = subset(Bitcoin,Year == '2020' | Year == '2021')\nTether2021 = subset(Tether,Year == '2020' | Year == '2021')\n\nggplot(Bitcoin2021,aes(x = BitcoinVolume/100000000, fill = cut(BitcoinVolume,30))) +\n  geom_histogram(show.legend = FALSE,bins = 50,color = 'black') + \n  scale_fill_discrete(h = c(240,10)) + \n  theme_minimal() +\n  labs(x = 'Bitcoin Volume', y = 'Frequency',title = 'Bitcoin Volume for 2020-21',\n  subtitle = 'in Hundred Millions',caption = \"Sai\" ) +\n  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  annotate(\"text\", x=2000, y=92, label= paste(\"Mean =\",round(mean(Bitcoin2021$BitcoinVolume/100000000),2))) +\n  annotate(\"text\", x=2000, y=97, label= paste(\"SD =\",round(sd(Bitcoin2021$BitcoinVolume/100000000),2))) +\n  annotate(\"text\", x=2000, y=87, label= paste(\"Median =\",round(median(Bitcoin2021$BitcoinVolume/100000000),2))) +\n  scale_x_continuous(breaks = seq(0, 3500, 100)) +\n  scale_y_continuous(limits = c(0, 100))+\n  geom_vline(aes(xintercept = mean(Bitcoin2021$BitcoinVolume/100000000)), linetype = \"dashed\")\n\n\n\n\n\nggplot(Tether2021,aes(TetherVolume/100000000, fill = cut(TetherVolume,30))) + \n  geom_histogram(show.legend = FALSE,bins = 50,color = 'black',linewidth = .001) + \n  scale_fill_discrete(h = c(240,10)) + \n  theme_minimal() +\n  labs(x = 'Tether Volume', y = 'Frequency',title = 'Tether Volume for 2020-21',\n  subtitle = 'in Hundred Millions' ) +\n  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  scale_x_continuous(breaks = seq(0, 3500, 100)) +\n  annotate(\"text\", x=452, y=95, label= paste(\"Mean =\",round(mean(Tether2021$TetherVolume/100000000),2))) +\n  annotate(\"text\", x=454, y=100, label= paste(\"SD =\",round(sd(Tether2021$TetherVolume/100000000),2))) +\n  annotate(\"text\", x=450, y=90, label= paste(\"Median =\",round(median(Tether2021$TetherVolume/100000000),2))) +\n  scale_y_continuous(limits = c(0, 100)) +\n  geom_vline(aes(xintercept = mean(Tether2021$TetherVolume/100000000)), linetype = \"dashed\")\n\n\n\n\nNow we check the correlation between the volume of tether and bitcoin.We investigate the relationship between market capitalization and volume traded for the currencies Tether and Bitcoin in the plot below. Both currencies display a positive correlation between their volume of trades and market capitalization, with Tether displaying a larger correlation of 0.64 and Bitcoin displaying a relatively smaller, but still very strong correlation of 0.56.\nIn essence, this indicates that the Market Capitalization of other currencies—more specifically, Tether than Bitcoin—directly affects the volume of trading. Which is not very surprising given that, among other factors, the level of public trust has a significant impact on a product’s or company’s market value. For instance, when news broke that Mordena and other pharmaceutical companies had perhaps discovered a vaccine for COVID-19, their stock values skyrocketed. despite the fact that there was initially no evidence or definitive tests.\nWe are still trying to figure out why Tether trades at volumes that are practically identical to those of Bitcoin, despite being so far behind on every other metric.\n\nggplot(Tether2021, aes(x=scale(TetherMarketcap), y=scale(TetherVolume))) + \n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, fullrange=TRUE,color = 'red') + \n  geom_rug()+\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  labs(x = 'Scaled Tether Market Capatalization ', y = 'Scaled Tether Volume',\n       title = 'Tether Volume and Market Capatalization for 2020-21',\n       subtitle = 'Scaled',caption = \"Sai\" ) +\n  theme(panel.background = element_rect(colour = \"orange\",size = 2,linetype = \"solid\")) +\n  annotate(\"text\", x= 0, y=4,size = 3.5, label= paste(\"Correlation =\",\n            round(cor(Tether2021$TetherVolume,Tether2021$TetherMarketcap),2)))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nggplot(Bitcoin2021, aes(x=scale(BitcoinMarketcap), y=scale(BitcoinVolume))) + \n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, fullrange=TRUE,color = 'red') + \n  geom_rug()+\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  labs(x = 'Scaled Bitcoin Market Capatalization ', y = 'Scaled Bitcoin Volume',\n       title = 'Bitcoin Volume and Market Capatalization for 2018-19',\n       subtitle = 'Scaled',caption = \"Sai\" ) +\n  theme(panel.background = element_rect(colour = 'orange',size = 2, \n                                        linetype = \"solid\"))+\n  annotate(\"text\", x=-0.5, y=7.5,size = 3.5, label= paste(\"Correlation =\",\n        round(cor(Bitcoin2021$BitcoinVolume,Bitcoin2021$BitcoinMarketcap),2)))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe closing prices of the two currencies, as depicted in the graph below, provide the answers to the aforementioned question.\nThe volatility of Bitcoin’s price is one of the main causes of the high levels of trading trust in Tether. As you can see from the graph below, Bitcoin’s price has decreased from more than 4000 to approximately upwards of 18000 in just two years. The price difference in the instance of Tether, however, has not exceeded a measly 6 cents.\nThe base price of the stock is another factor in Tether’s high trading volume. Tether’s pricing is so low that it allows both low risk mutual fund companies and those who aren’t big investors to enter the market for digital currencies without taking on too much risk. But compared to Bitcoin, this also results in very little reward because reward is always inversely correlated with risk.\nThis does raise the question of why Tether and not another currency that competes with Bitcoin and has a low base price and low price volatility. The only cryptocurrency that is backed by US dollars is Tether, which is the solution to this query. Investor confidence in the stock is increased because no other currency is backed by real money.\n\nbitcoin_close = plot_ly(as.data.frame(Bitcoin2021),y = ~BitcoinClose, x = ~index(Bitcoin2021), \n        type = 'scatter',mode = 'lines',name = 'Bitcoin Closing Price') %>%\nlayout(xaxis = list(title = 'Years',tickangle = -45,tickfont = list(size = 10)),\n       yaxis = list(title = 'Price Range',tickfont = list(size = 10)))\n\ntether_close = plot_ly(as.data.frame(Tether2021),y = ~TetherClose, x = ~index(Tether2021), \n        type = 'scatter',mode = 'lines',name = 'Tether Closing Price') %>%\n    layout(xaxis = list(title = 'Years',tickangle = 45,tickfont = list(size = 10)),\n           yaxis = list(title = 'Price Range',tickfont = list(size = 10)))\n\nsubplot(bitcoin_close, tether_close) %>% layout(title=\"Closing Prices of Bitcoin and Tether\",font = 10)"
  },
  {
    "objectID": "posts/SaisrinivasAmbatipudi_FinalProject.html#conclusion",
    "href": "posts/SaisrinivasAmbatipudi_FinalProject.html#conclusion",
    "title": "Saisrinivas_Ambatipudi_Final_Project - Analysis of Cryptocurrency Data",
    "section": "Conclusion:",
    "text": "Conclusion:\nWe have successfully analyzed the crytocurrencies and to conclude my analysis:\n\nWe have found out which coin dominated the market based on the year. 2013 - There were only 4 digital assets on the market, so BITCOIN dominated by 93.4%. 2014 - The was an addition of 2 more digital assets to the market but BITCOIN still dominated by 93.7%. A 0.4 increase in market capitalization mean the volume of the coins were increasing. 2015 - BITCOIN still dominated but there was an addition of 3 more digital assets which let to market cap of BITCOIN dropping to 89.7% and XRP having 6.94% of the market share 2016 - No new cryptocurrencies were added but the market capitalization of BITCOIN decreased to 87% and we can see other digital assets climbing up like ethereum taking 7.82% of the market. 2017 - This year is where the crypto boom happened and we can see that new digital assets were added and ethereum took 20.1% of the market and bitcoin dropped to 63.2% and XRP having 7.47 % 2018 - More coins added and BITCOIN dropped to 53% and ethereum had close to 20% while XRP had 10%. 2019 - BITCOIN made a comeback with taking 70% of the market share back from the other digital assets by taking 70% of the market. 2020 - Around 8-10 cryptocurrencies were introduced to the market but BITCOIN still maintained 71% market share. 2021 - BITCOIN had 60% of the market share, 17% was ethereum’s and there were more crypto currencies made that year.\nUsing the Price vizualization above with the drop down of different cryptocurrencies we see that:\n\nWe see that BITCOIN’s prices are mostly outliers due to the heavy rise from the years 2017 - 2021\nEthereum also competes with the market capitalization but not as much as BITCOIN.\nOther Crypto currencies had less market share but due to their small prices and sudden gains and drops people gained and lost a lot of money, hence they did have major volatility making it uninvestable.\nUsually crypto currencies depend on the popularity for it price to increase.\n\nWhy Bitcoin’s volume decreased during 2020-2021 while tether’s volume kept increasing? Mostly due to the high volatility in the prices the volume decreased whereas tether had a stable prices hence the volume kept increasing."
  },
  {
    "objectID": "posts/SanjanaJhaveri_FinalPaper.html",
    "href": "posts/SanjanaJhaveri_FinalPaper.html",
    "title": "Final Paper",
    "section": "",
    "text": "library(readr)\nnfl <- read_csv(\"_data/nfl2019.csv\")\nView(nfl)\n\n\nnfl <- read_csv(\"_data/nfl2019.csv\",\n                          skip = 1,\n                          col_names = c(\"delete\", \"Opponent\", \"Home_Ranking\", \"Opponent_Ranking\", \"Home_1st_Downs\", \"Home_Total_Yards\", \"Home_Passing_Yards\", \"Home_Rushing_Yards\", \"Home_Turnovers\", \"Opponent_1st_Downs\", \"Opponent_Total_Yards\",\"Opponent_Passing_Yards\", \"Opponent_Rushing_Yards\", \"Opponent_Turnovers\", \"Home_Offensive_Ranking\", \"Home_Defensive_Ranking\", \"Home_Special_Teams\", \"Home\", \"Year\", \"Winner\")) %>%\n  select(!starts_with(\"delete\")) %>%\n  na_if(\"Skipped\")\n\n\nmean(nfl$Home_Total_Yards)\n\n[1] 348.048\n\nmean(nfl$Opponent_Total_Yards)\n\n[1] 346.2134\n\n\n\nmean(nfl$Home_Passing_Yards)\n\n[1] 231.8302\n\nmean(nfl$Opponent_Passing_Yards)\n\n[1] 230.5542\n\n\n\nmean(nfl$Home_Rushing_Yards)\n\n[1] 116.2179\n\nmean(nfl$Opponent_Rushing_Yards)\n\n[1] 115.6592\n\n\n\nnfl%>%\n  ggplot(aes(x=`Home_Passing_Yards`, y=`Year`)) +\n  geom_point()\n\n\n\n\n\nnfl%>%\n  ggplot(aes(x=`Opponent_Passing_Yards`, y=`Year`)) +\n  geom_point()\n\n\n\n\n\nnfl$home_winner <- with(nfl, ifelse(Home == Winner, 'Win', 'Loss'))\n\nview(nfl)\n\n\nnrow(filter(nfl,home_winner == \"Win\"))\n\n[1] 451\n\n\n\nsum(nfl$Home_Passing_Yards > nfl$Opponent_Passing_Yards & nfl$home_winner == \"Win\") \n\n[1] 262\n\n\n\nsum(nfl$Home_Rushing_Yards > nfl$Opponent_Rushing_Yards & nfl$home_winner == \"Win\") \n\n[1] 311\n\n\n\nlibrary(ggplot2)\n\nscatter <- nfl %>%\n  ggplot(mapping=aes(x = Year, y = `Home_Passing_Yards`))+ \n  geom_point(aes(color=home_winner))\nscatter\n\n\n\n\n\nlibrary(ggplot2)\n\nscatter <- nfl %>%\n  ggplot(mapping=aes(x = Year, y = `Home_Rushing_Yards`))+ \n  geom_point(aes(color=home_winner))\nscatter\n\n\n\n\n##Final Paper\nMy final project was focused on an NFL (National Football League) dataset. I chose this particular dataset because I like to watch the NFL games every Sunday and I think I understand the game pretty well. However, during the games, I seem to have a couple of questions. My research question focuses on if each team’s stats in yardage (passing and rushing yards), correlate to each other and to the win. First, I wanted to see if the home team has more passing yards than the opponent then would it correlate to an automatic win? If that answer was yes that the two correlate directly to each other, it would tell us that rushing yards don’t matter as much in the game and that teams should focus on dominating in the passing game. It might even affect what kind of quarterback the teams draft. Do you pick a quarterback that is fast and can run or do you pick one that has a good arm and can throw for many yards. If that answer was no that the two don’t correlate directly to each other it would suggest two other hypotheses. The first would be that since the passing game doesn’t matter to the win, maybe the rushing game does, so the logical next step would be to look at whichever team has the most rushing yards and see how many times they pulled off the win. If we see that the number of times the team with the most rushing yards ends up winning, then we have our answer that rushing yards in the game matter more than passing yards. Again, I imagine that would affect draft picks in focusing more on running backs and quarterbacks that can run. If the answer is no there is no direct correlation, we have to look at the second hypothesis. The second would be that neither the passing yards nor the rushing yards in the game matter. We cannot predict the winner by just looking at the number of passing yards or the number of rushing yards. I think that the more passing yards a team has the more likely chance they have of winning. Obviously, the team with the most passing yards or rushing yards will win every game so arbitrarily I will say the team with the most passing yards or rushing yards has to win 65%+ of the games in order to say that either yardage has a significant impact. I think this because every team has a quarterback that can throw really well and throw for many yards but not every team has a quarterback that can run a lot or successfully in a consistent manner. I think that rushing yards do not matter as much as passing yards and maybe that’s why the average number of rushing yards in a game tends to be less than the passing yards. I think this research question can not only help teams select quarterbacks, wide receivers and running backs during the draft, but also identify weak points in the team’s roster to see where the team needs to practice more. Additionally, it can also help predict wins/losses within the league. The dataset I chose was the NFL team stats and outcomes from 2019-2022. It includes stats on the defense, offense and special teams units for the home team. It also includes information on team standings (rankings) and the outcomes of each game. When looking at the dataset, I renamed each column into something that made more sense to me, using opponent or home at the beginning of each name to specify which team. I deleted the first column because I did not think that the week of the NFL season that the game was played mattered to me to answer my research question. The next column is the opponent’s name. For the next column, instead of Tm I renamed that column to the Home_Ranking because this is what the home team ranked as during the week that the game was played and Tm is a very vague name. The next column is the Opponent_Ranking which is what the opponent team ranked as during the week the game was played, which I renamed from the nonspecific name Opp.1. Next, I renamed 1stD and TotYd into home 1st downs and home total yards respectively. I kept 1st downs because I feel like it’s a good stat to have. It won’t necessarily be a good predictor for who wins the game, but the more first downs you have the more likely you are going to be closer to the touchdowns and to make a field goal. Basically, the more first downs you have you probably are closer to getting more points on the board (either 3 points or 7). The total yards field is extremely important because this the sum of the passing yards and the rushing yards. The team with the win is automatically going to have the most total yards. Next couple of columns we have home passing yards and home rushing yards. These two columns are the main columns that I’ll be looking at to answer my research question. In the next column we have home turnovers, which is not a great indicator of whether or not a team wins that game but a lot of turnovers means that the other team got an additional chance to put more points on the board. The next couple of columns are the same as before but for the opponent: opponent 1st downs, opponent total yards, opponent passing yards, opponent rushing yards and opponent turnovers, which all now have opponent leading the name instead of a .1 differentiating these columns from the home team stats. These columns have the same importance as the parallel columns do for the home team. I will be looking closely as the opponent passing yards and opponent rushing yards columns to answer my question. The next couple of columns describe the different unit rankings for the home team. We have the home offensive ranking, the home defensive ranking and the home special teams ranking. Next, I have listed the home team name and the year that the game was played. This dataset only shows games that were played in the 2019-2022 regular season, meaning that playoff games and the Superbowl game stats are not counted here. I kept the column naming the winner, but this column lists the whole team name, which is why I have created another column called home_winner. This column looks at the team name of the winner and shows if it is the same team name listed as the home team. If it is the same team name, we put ‘Win’ in the home_winner column and if it is not equal, which means that the opponent won, we put a ‘Loss’ in the home_winner column. The home_winner column is relative to the home team. This is how I renamed the columns and cleaned the data. I needed to clean the data by renaming the columns because the original dataset had column names that were not descriptive of what team (opponent or home) we were looking at. A lot of the numeric valued columns like passing yards or rushing yards had names like PassY and RushY for the home team and PassY.1 and RushY.1 for the opponent, which gets confusing after a while. This is why I decided to rename all the columns as something descriptive and make sure that they all have opponent or home attached to the column name. After cleaning the dataset, we have 20 total columns and 895 rows. For my visualizations, I am focusing on 6 columns: Home_Passing_Yards, Home_Rushing_Yards, Opponent_Passing_Yards, Opponent_Rushing_Yards, Year and home_winner. I have run the basic statistics operations on Home/Opponent total yards, Home/Opponent passing yards and Home/Opponent rushing yards by calculating the mean of each of these columns. This is just to establish some sort of arbitrary baseline. For example, the mean of Home_Passing_Yards and Opponent_Passing_Yards is 231.83 and 230.55 respectively. Although these values are close, this operation tells me that the home team edged out the opponent team most of the time in passing yards and that in most games the passing yard total should be somewhere around 230 yards to be significant. This tells me that if for example the home team has 175 yards in the passing game, the opponent team probably won because 175 yards is significantly less than the average 230 yards. The baseline for the rushing yards is home at 116.22 and the opponent at 115.66. Again, the values are so close that it is not really significant. I expect the mean for the home and opponent total yards to be very close too, especially because both the passing yards and the rushing yards averages were very close. Calculating the average of these two comes out to home total yards at 348.05 and the opponent total yards at 346.21. Again, too close to be significantly leaning towards either team but this establishes a nice baseline for very value we will be looking at. I decided to not calculate the maximum or minimum values of each team’s passing, rushing or total yards because I feel like that information does not do anything to further answer my research question nor does it provide any additional missing information and I can adequately deduce that information from the data visualizations I have created. I decided to plot the Home Passing Yards vs the year to see how over the 4 years, how the passing yards have evolved. This visualization allows me to see with an overall view what is the average range of passing yards by the home team and which data points are outliers. I next decided to plot the Opponent Passing Yards vs the year so I can compare the two Home_Passing_Yards and Opponent_Passing_Yards in the big picture. Looking at the big picture, I don’t see much of a difference. The overall range of the average each year looks about the same and the outliers also look around the same which seems accurate since the average of each were so close to each other. From there, I decided to draw a real conclusion on my research question of whether the team’s passing yards correlate to a win if they have more passing yards than the opposing team. I needed to have a visual depicting the wins and losses with the home passing yards to see if the home team won more games with a lot of passing yards or not. So, I had to create a new field called home_winner. To get the values in this new column, I created a simple conditional statement. Basically, if the home team name is equal to the winner team name, then put the value ‘Win’ in the home_winner column. Else, if the opponent team name equals the winning team name, then put the value ‘Loss’ in the home_winner column. The home_winner column is all relative to the home team. Next, I decided to filter for the ‘Win’ value in home_winner to see how many times the home team won. They have 451 wins over 4 years. To determine the answer to my research question, I must count how many times the home team passing yards were greater than the opposing teams passing yards and that the home_winner has the value ‘Win’. Meeting that conditional came out to 262 times. I also counted how many times the home team rushing yards were greater than the opposing teams rushing yards to answer the question of whether a team’s rushing yards correlate to the win if they rushed more than the other team. Again, I used the conditional of Home_Rushing_Yards are greater than Opposing_Rushing_Yards and home_winning is equal to ‘Win’. Meeting than conditional came out to 311. Looking at the visual I created, the loss is shown as the color red and the win depicted as blue, we plot the year against the home passing yards. We see that the different colors dots are kind of all over the place. In 2019 and 2020, we see that the home passing yards are very high around 500 and 450 respectively but both of those come out as a loss. On the other hand, in 2021 and 2022 we see that the highest home passing yard around 450 comes out to wins. We can also see that when the home passing yards is at a minimum in all 4 years, anywhere from around 0 to 50, all of those show as a loss. However, instead of seeing a lot of blue congregated towards the top of the y axis and the red towards the bottom, we see no real divisive line between the two colors. I created the same kind of visual to look at the rushing yards for the home team. Again, I have red depicting a loss and blue showing a win and plotted the year against the home rushing yards. With the rushing yards, we can see a little more division already. We can see in all the years except for maybe 2021, the highest number of rushing yards clearly are home team wins. Additionally, we also see that the lowest number of rushing yards are clearly home team losses. We see more division with the rushing yards in that the blue or wins are mostly concentrated towards the top of the y-axis as more rushing yards for the home team and that conversely the red are more concentrated towards the bottom of the y-axis as fewer rushing yards for the home team. From my visualizations I learned that looking at the bigger picture in visuals tells you a lot of information. I can see when depicting wins and losses with color where wins and losses are more concentrated and if a divisive line exists. I enjoyed my experience with this project. It allowed me to see NFL games in a new light, a more logical statistical light. After I started this project, I started paying more attention to the passing yardage and rushing yardage that each team put up at the end of the game. I started noticing that certain teams usually have more passing yardage than rushing yardage because they are a more pass reliant team or vice versa with a more run heavy team. Looking back at my analysis in this project, I realize that I am only looking at the offensive side of each team and not accounting for the defense at all, even though some teams win based on defense alone. I now realize that some teams are more equipped to handle a pass heavy team with their defense and vice versa and defense definitely plays a role in whether a team wins or loses. I think if I was continuing this project, I would have tried to incorporate a team’s defensive strategy into my analysis somehow. Maybe see if the defensive ranking affects the win or loss. Or compare each team’s defensive and offensive rankings and try to see how those correlates to the win. It would be interesting to see if a team’s offensive or defensive ranking holds more weight to the win or if it’s a tossup and it really doesn’t matter. The most challenging aspect of this project definitely was figuring out how to visualize the home passing yardage and home rushing yardage and how to see which team won at the same time. Once I figured out that I have to base everything off the home team and create a new column win/loss for the home team, creating the visual became way easier. I originally was thinking of assigning each team a number and instead of a team name being displayed a number would be displayed, which would basically be the new win/loss column. However, since there are 32 NFL teams, I would have to assign 32 numbers and remember the number for each team. In addition to that, I would be plotting based on 32 teams for each graph which would further complicate matters and would make reading the graph impossibly difficult. I also did not think about the fact that these numbers are spread over 4 years and the quarterbacks, wide receivers and running backs are not the same for all 4 years. Some quarterbacks are more pass reliant and some quarterbacks are running quarterbacks which means their rushing yardage is usually higher whether they win or lose. If I was continuing this project, I would have to look at the quarterback rating and whether there are more running quarterbacks or pass heavy quarterbacks within the league. I think I would somehow incorporate what kind of quarterback the winning team had so that I can see whether a running quarterback or a pass heavy quarterback tends to do better within the league. Other areas of growth in this project could be to look at whether the team’s ranking as you go further through the weeks of the regular season plays an impact on which team wins or loses. For example, if the San Francisco 49ers are ranked at #9 in week 1 vs. ranked at #9 in week 15 of a regular 18-week season, is it more likely they will win in week 1 or week 15 when in both weeks they are ranked in the top 10. Another area I would consider looking at if I were to continue with this project would be to see if covid 19 played a big impact on the team’s not putting in as much passing or rushing yardage in 2020. If the fans were not allowed to be there, would the teams still have enough of a push to keep pushing the numbers up in each category. In regards to fans, it would be interesting to see how big the stadium of the home team was and how large of a fan base of each team came out to see them play and if that affected the win. I know the atmosphere of the game is a big thing in college football, like how big the stadium is, how many fans come out to see their team, how loud it can get, etc. but is it as big of a factor in the NFL? In continuing this project, another idea I had would be to continue looking at the rest of the 2019-2022 seasons. Like not only looking at the statistics of the regular 18-week season but looking at the numbers from the post season of the playoffs and the Superbowl. Do the playoffs and the Superbowl automatically assure bigger passing and rushing yardage? Do these numbers matter more or less in the postseason? Does rushing yardage matter more in the playoffs and the Superbowl than passing yardage or vice versa? Another area to look at would be if penalties had a significant impact on any of the games. Since yards given or taken away during penalties are not counted in passing or rushing yards or even the total yardage, a team might have won because of more penalties against the other team giving one team more advantages and more yards that are not technically accounted for in any statistics. My final thoughts on this are yes, a win is a win no matter how big or small, by 1 point or 50 points but if the home team and opposing team have a small deficit in passing yardage but a big deficit in rushing yardage does that directly link to a smaller deficit in points or a bigger deficit or does it not matter? What about the opposite scenario (a big deficit in passing yardage but a small deficit in rushing yardage)? In conclusion, I can answer both my research questions using the data I have gathered and from the visuals I have created using that data. My first research question was if one team has more passing yards than the other team, does that directly link to a win? The answer to that is no, not necessarily since the data doesn’t show that happens most of the time. I arrived to this answer by seeing that in the 4-year time frame of my data, there are 451 times that the home team has claimed the win. In those 451 times, if you apply the conditional that the home team has more passing yards than the opposing team and that the home team claims the win, then you get 262 times those conditions were met. This means that only 58% of the home wins also had the home team cashing in more passing yards than the opposing team. 58% is a majority but it is not the arbitrary 65% I defined in my introduction where the number of wins is significant enough that it cannot be disputed that the passing yards played a role in the win. With 58%, you also cannot accurately predict the winner of the game by only looking at passing yardage numbers. The visual depicting the home passing yards vs year with colors showing a win or loss corroborates this answer as both blue and red are shown all over the graph with no real separating line between the wins and losses. My second research question was if one team has more rushing yards than the other team, does that directly link to a win? The answer to that is yes, I think we can see the data shows this happens most of the time. Again, we have 451 wins with the home team. Looking at the conditional we have for the rushing yards, if the home team has more rushing yards than the opposing team and the home team has the win, we see that condition is met 311 times. This means that 69% of the home wins also had the home team cashing in more rushing yards than the opposing team. 69% is definitely a majority and is larger than the 65% that I defined in my introduction. In my opinion, it is a big enough majority that you can usually know the winner by only comparing the rushing yards of the two teams and 69% is significant enough that you can definitely say that rushing yards plays a big role in determining which team gets the W. Also, if you look at the visual for home rushing yards and look at the blue and red colors, you can see they are a little more isolated from each other than the passing yard graph. The red is more towards the bottom and means a loss and the blue is more towards the top, meaning more rushing yards and a win. These answers are opposite of my hypothesis since I originally thought that more passing yards would equivalate a win and more rushing yards would not matter. The only real question I have that is left unanswered is that do these findings really change a general manager’s outlook on how to draft a quarterback.\n##Bibliography Finnstats. “COUNTIF Function in R: R-Bloggers.” R, 18 July 2021, https://www.r-bloggers.com/2021/07/countif-function-in-r/. Grolemund, Hadley Wickham and Garrett. “R For Data Science.” Welcome, https://r4ds.had.co.nz/index.html. “NFL Teams Stats and Outcomes.” Kaggle, https://www.kaggle.com/datasets/thedevastator/nfl-team-stats-and-outcomes. “The R Project for Statistical Computing.” R, https://www.r-project.org/."
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html",
    "href": "posts/SarahMcAlpine_finalproject.html",
    "title": "Sarah McAlpine - Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(viridisLite)\nlibrary(readxl)\nlibrary(summarytools)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(tm)\nlibrary(SnowballC)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#data-summary",
    "href": "posts/SarahMcAlpine_finalproject.html#data-summary",
    "title": "Sarah McAlpine - Final Project",
    "section": "Data Summary",
    "text": "Data Summary\nA software company has gathered qualitative and quantitative feedback from 24 client organizations using three surveys at different stages of the client relationship: purchase decision, launch, and early results. At most, three individual people from each institution responded to at least one survey, two people responded to two surveys, and no one person responded to all three surveys. Given the small sample size of only 31 respondents across the 3 surveys, and given the overlap in feedback across the three surveys, this analysis will combine all available data into a single set for an initial analysis.\nAt the outset, it should be noted that the conclusions drawn from this data will be limited due to the small sample size. The value of this analysis, therefore, aims to find strong trends and to determine what changes, if any, to the surveys would yield more business intelligence."
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#read-in-join",
    "href": "posts/SarahMcAlpine_finalproject.html#read-in-join",
    "title": "Sarah McAlpine - Final Project",
    "section": "Read In & Join",
    "text": "Read In & Join\nThe data has been sanitized to protect the privacy of those involved. Client email addresses are replaced with a new variable, UserID, where the letter corresponds to an organization and the number corresponds to the respondent from that organization. To illustrate, R1, R2, R3 are three different people from the same organization. This will allow analysis of individuals and organization-based feedback.\nSince there is some overlap in organizations, respondents, and questions across the three surveys, the first step will be a full_join, with a calculation of x/2 to align the 10-point ratings with the 5-point ratings. A full join will allow analysis of as much data as possible, while requiring careful handling of NA values.\n\n\nCode\n# assign x/2 function\nmydivide <- function(x){x/2}\n\n# read in each sheet\nsurvey1_orig <- read_xlsx(\"_data/ClientSurveys.xlsx\", sheet =1)\nsurvey2_orig <- read_xlsx(\"_data/ClientSurveys.xlsx\", sheet =2) %>%\n  mutate(across(starts_with(\"How would\"), funs(mydivide)),\n            .keep = \"unused\")\nsurvey3_orig <- read_xlsx(\"_data/ClientSurveys.xlsx\", sheet =3) %>%\n    mutate(across(starts_with(c(\"How\", \"Overall\")), funs(mydivide)),\n            .keep = \"unused\")\n\n# full join to keep all data, key = UserID\njoinedsurveys_orig <- survey2_orig %>%\n  full_join(x = survey2_orig, y=survey1_orig, by = \"UserID\") %>%\n  full_join(survey3_orig, by = \"UserID\")"
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#cleaning",
    "href": "posts/SarahMcAlpine_finalproject.html#cleaning",
    "title": "Sarah McAlpine - Final Project",
    "section": "Cleaning",
    "text": "Cleaning\nAs with most surveys, the questions are too long for column names, so each is renamed with the simplest phrasing possible. The new names contain prefixes and suffixes that will help with sorting later. For instance, rating questions begin with “qr_”, free text questions begin with “qf_”, and selection questions begin with “qs_”. The full text of each question is listed in the appendix at the end of this document.\n\n\nCode\n# look at colnames to rename them\n# colnames(joinedsurveys_orig)\n\n# assign clean data frame\nsurveys_renamed <- joinedsurveys_orig\n#rename columns\ncolnames(surveys_renamed) <- c(\"Timestamp1\",\n                               \"UserID\",\n                               \"delete\",\n                               \"delete2\",\n                               \"qs_prompted_adoption\",\n                               \"qf_goal_constituents\",\n                               \"qf_launcheffort_staffrolestime\",\n                               \"qr_launcheffort_retaskstaff\",\n                               \"qr_launcheffort_generatecontent\",\n                               \"qr_launcheffort_techIT\",\n                               \"qr_launcheffort_staffadoption\",\n                               \"qf_launcheffort_detail\",\n                               \"qr_benefit_donorengagement\",\n                               \"qr_benefit_increasedgifts\",\n                               \"qr_benefit_costsavings\",\n                               \"qr_benefit_timesavings\",\n                               \"qr_benefit_GOproductivity\",\n                               \"qf_benefit_detail\",\n                               \"qf_donorcommsbefore\",\n                               \"qf_process_content\",\n                               \"qf_donor_relationship\",\n                               \"qf_donor_interaction\",\n                               \"qf_donor_response\",\n                               \"qf_donor_analytics\",\n                               \"qf_team_analytics\",\n                               \"qf_support_detail\",\n                               \"qf_Olacking\",\n                               \"qf_biggest_success\",\n                               \"qf_reality_vs_expectations\",\n                               \"qf_client_advice\",\n                               \"qf_Oimprove\",\n                               \"qr_implementationsupport\",\n                               \"Timestamp2\",\n                               \"qs_prompted_adoption\",\n                               \"qs_choice_over_competitors\",\n                               \"qf_feature_detail\",\n                               \"qr_benefit_donorengagement\",\n                               \"qr_benefit_increasedgifts\",\n                               \"qr_benefit_costsavings\",\n                               \"qr_benefit_timesavings\",\n                               \"qr_benefit_GOproductivity\",\n                               \"qf_choice_factors\",\n                               \"Timestamp3\",\n                               \"qs_onboard_lacking\",\n                               \"qf_onboard_lacking_detail\",\n                               \"qs_onboard_wentwell\",\n                               \"qf_onboard_wentwell_detail\",\n                               \"qf_onboard_communication_detail\",\n                               \"qf_onboard_interpretneeds_detail\",\n                               \"delete3\",\n                               \"qr_onboard_communication\",\n                               \"qr_onboard_interpretneeds\",\n                               \"qr_onboard_overall\")\n# view new column names\n# colnames(surveys_renamed)\n\n\nAfter renaming, pivoting and grouping is used to combine the repeated questions into the same variable. For instance, surveys 1 and 2 both ask, “What prompted your adoption of [the software product]?” This step gets those responses into the same column while maintaining the distinct timestamps and UserIDs for each instance. At first, the dataset includes 53 columns, and after de-duplicating the 6 repeated questions, it has 47 columns.\n\n\nCode\n# combine repeated questions\ndeduped_questions <- surveys_renamed %>%\n  # result is 53 variables\n  pivot_longer(cols = (starts_with(\"q\")),\n               names_to = \"question\",\n               values_to = \"response\",\n               values_transform = list(response = as.character),\n               values_drop_na = TRUE) %>%\n  group_by(question) %>%\n  pivot_wider(names_from = \"question\",\n              names_sort = TRUE,\n              values_from = \"response\") \n# confirmed reduction in columns and same number of rows\ndim(deduped_questions)\n\n\n[1] 32 47\n\n\nWith the names abbreviated and the repeated questions regrouped, further cleaning can be done. First, the columns are sorted alphabetically to make use of the naming conventions. Two unneeded questions about sizing and shipping for a gift are removed as well as one row with a test response. Various spellings of “n/a”, “N/A”, and “???” are mutated to NA values. The fields that had been mutated from numeric to character for the pivot are converted back to numeric values, and then those numeric columns are moved to the front of the data set after UserID.\nThe result is a tidy data set where each of 31 respondents and all of their responses are on the same row beginning with the UserID, and 44 variables are organized into separate columns for analysis.\n\n\nCode\n# cleaning steps\nsurveys_clean <- deduped_questions %>%\n    # alpha-order columns\n    select(order(colnames(deduped_questions))) %>%\n    # remove deletes\n    select(-starts_with(\"delete\")) %>%\n    # remove test row, should get 31 x 45\n    filter(is.na(qf_goal_constituents) | qf_goal_constituents != \"test\") %>%\n    # deal with NAs\n    mutate(across(where(is.character), ~na_if(.,\"n/a\"))) %>%\n    mutate(across(where(is.character), ~na_if(.,\"N/A\"))) %>%\n    mutate(across(where(is.character), ~na_if(.,\"N/a\"))) %>%\n    mutate(across(where(is.character), ~na_if(.,\"???\"))) %>%\n    # group\n    group_by(sort(\"UserID\")) %>%\n    # arrange by client\n    arrange(\"UserID\") %>%\n    # convert back to numeric\n    mutate(across(starts_with(\"qr\"), ~as.numeric(.))) %>%\n      # move numeric values to front\n      relocate(where(is.numeric)) %>%\n      # bring UserID to the leftmost\n      relocate(\"UserID\")\ndim(surveys_clean)\n\n\n[1] 31 45"
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#data-questions",
    "href": "posts/SarahMcAlpine_finalproject.html#data-questions",
    "title": "Sarah McAlpine - Final Project",
    "section": "Data Questions",
    "text": "Data Questions\nWith a richer data set, it’s possible that significant correlations could be found between client characteristics and the feedback they give, such as length of time with the software, staff size, usage categories, etc. However, since this analysis seeks to identify broad trends across aspects of performance and the effort required to implement the software, the primary focus will be on those performance and effort variables rather than trends among organizations or respondents.\nA total of 13 questions asked respondents to rate feedback on a scale of 1-5 (as noted in the cleaning steps above, those on a scale of 1-10 were recalculated). A simple boxplot with a point overlay of individual ratings gives a general idea of the quantitative data. A “jitter” positioning on the geom_point helps make it easier to see the quantity of responses and generally where they were.\n\n\nCode\n# set up rating data, 177 x 3\n ratings <- surveys_clean %>%\n  ungroup() %>%\n  select(starts_with(\"qr\") | \"UserID\") %>%\n  pivot_longer(cols = starts_with(\"qr\"),\n               names_to = \"question\",\n               values_to = \"rating\",\n               values_drop_na = TRUE) \n\n# boxplot with jitter point \nratings %>% \n  ggplot(aes(question, rating)) +\n  geom_boxplot(fill = \"#fde725\") +\n  coord_flip() +\n      geom_point(alpha = .4,\n             size = .2,   \n             position = \"jitter\") +\n  labs(title = \"Exploratory Visualization of All Ratings\")\n\n\n\n\n\nIn the plot above, the range and density of responses are easy to see. With this view, it is clear that only one person responded to the “interpret needs” question, about 5-6 responded to “launch effort” questions, and a much larger sample responded to “benefit” questions. The one person who responded regarding how well the software company interpreted their needs during setup responded with the most positive possible rating. The launch effort and benefit questions require further plotting to analyze, as neither tells a clear story from this noisy plot. It is also not very helpful to see benefits ranked against effort."
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#quantitative-analysis",
    "href": "posts/SarahMcAlpine_finalproject.html#quantitative-analysis",
    "title": "Sarah McAlpine - Final Project",
    "section": "Quantitative Analysis",
    "text": "Quantitative Analysis\n\nEarly Benefits of the Software\nFrom the boxplot below, arranged by decreasing medians, it is clear that improved donor engagement and time savings are the benefits most strongly reported. Cost savings has the broadest span with a mean in the center of the rating options. Both increased gifts and gift officer productivity are on the lower end of benefits experienced, since both have means greater than the minimum rating, it is safe to interpret that these benefits have been seen to some degree. It is notable that cost savings, increased gifts, and gift officer productivity all have significant responses at the minimum rating; the software company should pay attention to those who are reporting none of these benefits.\n\n\nCode\n# set up benefits data \nbenefits <- surveys_clean %>%\n  ungroup() %>%\n  select(starts_with(\"qr_ben\") | \"UserID\") %>%\n  rename(\"cost savings\" = \"qr_benefit_costsavings\", \n         \"improved donor engagement\" = \"qr_benefit_donorengagement\", \n         \"gift officer productivity\" = \"qr_benefit_GOproductivity\", \n         \"increased gifts\" = \"qr_benefit_increasedgifts\", \n         \"time savings\" = \"qr_benefit_timesavings\") %>%   \n  pivot_longer(cols = !UserID,\n               names_to = \"question\",\n               values_to = \"rating\",\n               values_drop_na = TRUE) \n\n# boxplot\nbenefits %>%\n  group_by(question) %>% \n  ggplot(aes(rating, question )) +\n  geom_boxplot(fill = \"#5ec962\",\n                xlim= c(0,5)) +\n  scale_y_discrete(limits = c(\"gift officer productivity\", \n                              \"increased gifts\", \n                              \"cost savings\", \n                              \"time savings\", \n                              \"improved donor engagement\")) +\n  labs(title = \"Degree of Product Benefits Experienced\", x = \"Client Rating (low to high)\" , y = NULL) +\n  theme_minimal()\n\n\n\n\n\nNotably, one qualitative question on the survey regarding internal staff metrics (nearly synonymous with gift officer productivity), all four respondents to that free text question indicated they had not looked at the internal analytics. Therefore, it is possible that these benefits may be going unnoticed. The software company may wish to increase attention to that feature. It should be noted that no person gave feedback on both the gift officer productivity rating AND the internal analytics question. Nonetheless, the strong trends of each group indicate a possible gap that the software company should note.\n\nAlternate Views\nOn the way to creating the boxplot above, other plots were attempted that were not as helpful.\n\n\nCode\n# geom_jtter\nbenefits %>% \n  ggplot(aes(question, rating)) +\n  geom_jitter(width = .1,\n              height = .1,\n              alpha = .2,\n              size = 10,) +\n  scale_color_viridis_d() +\n  scale_x_discrete(limits = c(\"gift officer productivity\", \n                              \"increased gifts\", \n                              \"cost savings\", \n                              \"time savings\", \n                              \"improved donor engagement\")) +\n  coord_flip() +\n  labs(title = \"Density of Benefit Ratings: View A\") +\n  theme_minimal()\n\n\n\n\n\nCode\n# geom_count\nbenefits %>% \n  ggplot(aes(question, rating)) +\n  geom_count(color = \"#440154FF\") +\n  scale_size_area(max_size = 10) +\n  coord_flip() +\n  scale_x_discrete(limits = c(\"gift officer productivity\", \n                              \"increased gifts\", \n                              \"cost savings\", \n                              \"time savings\", \n                              \"improved donor engagement\")) +\n  theme_minimal() +\n  labs(title = \"Density of Benefit Ratings: View B\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nCode\n# density of ratings for each benefit\nbenefits %>%\n   ggplot(aes(rating, group = question, fill = question)) +\n           geom_density(alpha = .9) +\n  facet_wrap(\"question\") +\n  scale_fill_viridis_d() +\n  theme_minimal() +\n  labs(title = \"Density of Benefit Ratings: View C\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nAbove is a jitter plot that gives a general, but ultimately unhelpful, indication of the density of response ratings for each benefit, as does a geom_count plot where the size of dots represents frequency of responses. There is also a density plot that illustrated the frequency with which each rating was received for each benefit, and there is limited knowledge to gain here. If these were plotting average ratings over time, that would be much more interesting. Still, it can be noted that some benefits had greater variability in ratings than others, such as improved donor engagement and cost savings.\nFurther analysis could group ratings to take another view. For instance it could be argued that a rating of 1 = low, ratings of 2-3 = moderate, and ratings of 4-5 = high. New trends could emerge with this approach, but perhaps not.\n\n\n\nEffort Required for Launch\nThe next group of ratings for investigation are those regarding the degree of effort required for launching the software product, across four aspects: the involvement of IT/technical staff, the work of retasking existing staff for a new model of work, the effort required for staff to be trained on and adopt the software, and the work of generating the content for the ultimate output. See below for how these were rated in relation to one another.\n\n\nCode\n#set up launch data \nlaunch <- surveys_clean %>%\n  ungroup() %>%\n  select(starts_with(\"qr_launch\") | \"UserID\") %>%\n  rename(\"generate content\" = \"qr_launcheffort_generatecontent\",\n         \"retasking staff\" = \"qr_launcheffort_retaskstaff\",\n         \"staff adoption\" = \"qr_launcheffort_staffadoption\",\n         \"IT/technical staff\" = \"qr_launcheffort_techIT\" ) %>%   \n  pivot_longer(cols = !UserID,\n               names_to = \"question\",\n               values_to = \"rating\",\n               values_drop_na = TRUE) \n\n# plot on boxplot, in increasing order\nlaunch %>% \n  arrange() %>%\n  ggplot(aes(question, rating)) +\n  geom_boxplot(fill = \"#2D708EFF\") +\n  coord_flip() +\n  scale_x_discrete(limits = c(\"generate content\" , \"staff adoption\" , \"retasking staff\" , \"IT/technical staff\")) +\n  labs(title = \"Degree of Effort Required for Launch\", \n       subtitle = \"*based on only 6 responses\",\n       y = \"Client Rating (low to high)\",\n       x = NULL) +\n  theme_minimal()\n\n\n\n\n\nThis visualization makes it easy to see which aspects were the easiest, and which were the hardest. The lowest is the effort required by IT/technical staff. Next, the effort of retasking staff to make internal team adjustments to roles of work, followed closely by the effort of staff learning to use the software, shown as staff adoption. The greatest degree of effort reported was for generating content within the system. Given the context, this last is an excellent finding.\nTo explain further, the greatest amount of effort required is the substantive work that makes use of the product. On the other hand, if staff adoption took the most work, the software company would certainly want to improve the onboarding and training process. It’s also great that the technical staff assistance needed is minimal, and that the staff who will be using the product take more effort to learn it. To add context, this is a specialized product for domain experts, and so it is expected that there be substantial effort involved in training them to use it. The amount of effort required to generate content is the point of the product and projects anyway, so that makes sense to take the most effort.\nThat said, it would be interesting to compare the degree of effort required to generate content with and without the software: is it decreased, even if it still requires the most effort? Furthermore, it would be interesting to compare the effort ratings over time for the same organization: is year two easier than year one, and year three easier still?\n\n\nFactors for Adoption\nAnother facet the surveys explored were the factors that led the organizations to adopt the software in the first place. For this question, which was present in two of the surveys, the answers were in a multi-selection format, where respondents could select any number of the options listed. The following visualization is based on the responses only, and so there may have been other options provided that no one selected.\nTo set up this for plotting, NA values were omitted so that the percentage of respondents who selected a given option could be calculated from an accurate total. It was a challenge to figure out how to get a single column of 28 responses into a rectangular format for plotting. Mutate allowed the addition of sum columns to the data, and later the full text column was dropped. Once regular expressions based on distinct keywords were identified, the addition of summed columns was straightforward. At that point, the data was quite repetitive, and so the next step was to take a single row and pivot it to a vertical format. What results is a table including just the keyword of each factor and the number of times it appeared in the data.\nThis was ideal for visualizing the part-to-whole relationship of what proportion of respondents selected each option. The final step was to add a column with the percentage as a function of the sum over the total responses, and then to reformat decimal results into percentages rounded to a single decimal place.\n\n\nCode\n# set up adoption selection question data\nadoption <- surveys_clean %>%\n  ungroup() %>%\n  select(contains(\"prompted_adoption\")) %>%\n  na.omit()\n\n# add count and percentage of occurences to df\nadoption_df <- adoption %>%\n  mutate(personalization = sum(str_count(.,regex(\"personalization\")))) %>%\n  mutate(campaign = sum(str_count(.,regex(\"campaign\")))) %>%\n  mutate(reporting = sum(str_count(.,regex(\"reporting\")))) %>%\n  mutate(digitize = sum(str_count(.,regex(\"digitize\")))) %>%\n  mutate(flexibility = sum(str_count(.,regex(\"Flexibility\")))) %>%\n    select(!starts_with(\"q\")) %>%\n    slice(1) %>%\n    # pivot\n  pivot_longer(c(\"personalization\",\n                 \"campaign\", \n                 \"reporting\", \n                 \"digitize\", \n                 \"flexibility\"),\n               names_to = \"feature\",\n               values_to = \"votes\") %>%\n  # add column for percentages\n  mutate(per = votes / 28) %>%\n  mutate(per = round (per*100, digits = 1)) %>%\n  arrange(desc(votes)) \n\n# plot\nadoption_df %>%\n  arrange(desc(per)) %>%\n  ggplot(aes(x = feature, y = per)) +\n           geom_bar(stat = \"identity\",\n                    fill = \"#7ad151\") +\n  scale_y_continuous(limits = c(0,80)) +\n  scale_x_discrete(limits = c(\"flexibility\",\"campaign\",\"reporting\",\n                              \"digitize\", \"personalization\")) +\n  labs(title = \"Factors that Prompted Adoption\", \n       x = \"reason\", \n       y = \"% adopted the software for this reason\") +\n  theme_classic() +\n  theme(axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  coord_flip() +\n    geom_text(data = filter(adoption_df, feature == \"personalization\"), aes(x=feature, y=0, \n                                 label = paste0(\"Desire for increased personalization in routine donor relationship-building                  \", per ,\"%\")), hjust = -0.01) +\n    geom_text(data = filter(adoption_df, feature == \"digitize\"), aes(x=feature, y=0, \n                                 label = paste0(\"Digitize/modernize the advancement office                                               \", per ,\"%\")), hjust = -0.01) +\n    geom_text(data = filter(adoption_df, feature == \"reporting\"), aes(x=feature, y=0, \n                                 label = paste0(\"Solve the burden of donor stewardship reporting                  \", per ,\"%\")), hjust = -0.01) +\n    geom_text(data = filter(adoption_df, feature == \"campaign\"), aes(x=feature, y=0, \n                                 label = paste0(\"Increased demands of a campaign                    \", per ,\"%\")), hjust = -0.01) +\n    geom_text(data = filter(adoption_df, feature == \"flexibility\"), aes(x=feature, y=0, \n                                 label = paste0(per ,\"%     Flexibility vs. print for campaign materials\")), hjust = 0) \n\n\n\n\n\nThe above plot required manual labeling to place the many-word factors onto the value bars themselves. This is much easier to read than a legend or separate axis labels. With more time, it would have been better to place the percentage labels individually rather than with many spaces added to the x labels. This may not render well in all cases.\nHowever, despite its shortfalls, this is a really clear visual of how these factors ranked for the respondents in adopting the software. Personalization of donor communications is a clear priority, followed by a desire to digitize and modernize. At about the same interval is the desire to solve the administrative burden of producing and sharing stewardship reports, followed again at about the same interval by increased demands of a fundraising campaign. The least favored option, at less than 10% as popular as the next factor and only 5% as popular as the most selected option, is the flexibility offered by the platform over print campaign materials.\nThe software company should note these rankings as they approach the market and try to win new clients. They are likely to benefit from emphasizing the possibility for personalization and modernization. Perhaps the flexibility of digital vs. print campaign materials would be a greater focus for those organizations that report increased campaign demands."
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#qualitative-analysis",
    "href": "posts/SarahMcAlpine_finalproject.html#qualitative-analysis",
    "title": "Sarah McAlpine - Final Project",
    "section": "Qualitative Analysis",
    "text": "Qualitative Analysis\nOf course, not all of the questions on the survey asked respondents to rate categories. Several questions were free text questions that allowed the clients to mention the features, issues, and processes that were most pressing for them. A closer look at their text-as-data could yield interesting findings for the software company, or in the least, could help define further questions for future surveys and research.\n\nWord Cloud\nA word cloud provides a general sense of the frequency that words appear in a body, or corpus, of text. Most of the time, the words in the center and in larger sizes are used more frequently. The survey responses were first cleaned by identifying and removing NA values, and then by using text mining tools in the tm package: remove punctuation, change all characters to lowercase, ignore common English words that would add little value (like “the”, “and”, “to”, etc.), and finally to group words with the same root or “stem” together. This last part is helpful so that words like “report,” “reports,” and “reporting” are grouped together rather than being counted separately. These functions add value that are especially useful when analyzing large corpora of text, but are also helpful in this case.\nAfter the text was cleaned, the frequency of 6 instances was used as the minimum parameter for what would appear in the word cloud. This was a subjective decision based on observation of several options, and this seemed the clearest and most helpful view of the data. The resulting word cloud is below.\n\n\nCode\n# isolate and prepare freetext responses\nfreetext <- surveys_clean %>%\n  ungroup() %>%\n  # select only the freetext variables\n  select(starts_with(\"qf_\") | \"UserID\") %>%\n  # change \"did not select\" responses to NA\n  mutate(qf_feature_detail = if_else(str_detect(qf_feature_detail, \"select\"), \n                                     NA_character_, qf_feature_detail)) %>%\n  mutate(across(na.rm = TRUE))\n\n  # set up free text corpus\n  ftcorpus <- Corpus(VectorSource(freetext))\n  # inspect(ftcorpus)\n  \n  # replace symbols and special characters with spaces\n  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\n  ftcorpus <- tm_map(ftcorpus, toSpace, \"/\")\n  ftcorpus <- tm_map(ftcorpus, toSpace, \"@\")\n  ftcorpus <- tm_map(ftcorpus, toSpace, \"\\\\|\")\n  ftcorpus <- tm_map(ftcorpus, toSpace, \"NA\")\n  ftcorpus <- tm_map(ftcorpus, toSpace, \"\\n\")\n  ftcorpus <- tm_map(ftcorpus, toSpace, \"c\\\\(\")  \n\n  # clean corpus for useful words only\n  ftclean <- tm_map(ftcorpus, tolower)\n  ftclean <- tm_map(ftclean, removeNumbers)\n  ftclean <- tm_map(ftclean, removePunctuation)\n  ftclean <- tm_map(ftclean, removeWords, stopwords(\"english\"))\n  ftclean <- tm_map(ftclean, stemDocument)\n\n  # check the result\n  # inspect(ftclean)\n  \n  #build wordcloud\n  wordcloud(ftclean,\n            min.freq = 6,\n            colors = viridis(24),\n            random.order = F)\n\n\n\n\n\nThis visualization makes it easy to identify the top five wordstems respondents used: donor, report, use, content, and platform. To begin to interpret this outcome, it is unsurprising that these are the top five words. After all, the software is a platform for sharing reports and content with donors, and the questions are aimed at the staff who use that platform. No big surprises there. One strange output is the wordstem, “realli”, which upon investigation turns out to be many uses of the word really. It would be interesting to compare these words respondents used most to the frequency with which the same words appear in the prompts.\n\n\nBarplot\nAfter the top five, the rest are more difficult to rank visually in a word cloud, so a barplot will be helpful here. Since the barplot is more orderly and therefore easier to read, the minimum frequency used in this plot is increased from 6 to 8 occurrences. The result is below in descending order.\n\n\nCode\n# set up ordered list of most frequent stem words\ndtm <- DocumentTermMatrix(ftclean)\nfreq <- colSums(as.matrix(dtm))\nfreq <- freq[order(freq, decreasing=TRUE)]\n\n# create data set with words and its frequency\nwf <- data.frame(word=names(freq), freq=freq) \n\n# plot word frequency\nggplot(subset(wf, freq > 8), \n       aes(x = reorder(word, freq), freq)) +\ngeom_bar(stat=\"identity\", fill = \"#21918c\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Most Frequently Used Words in Survey Responses\", y = \"times used\", x = \"root word\")\n\n\n\n\n\nA barplot stacks the words neatly, and with them sorted by frequency, it is very easy to identify their frequency in relation to one another. It is also much easier to read the words themselves. After the top five, it is now possible to identify the next group of most frequently used words: time, office/officer, get, data, gift, and communicate/communication. We now know that time is a high concern for this group, as well as (possibly) “getting data”, (possibly) “gift officers”, and various aspects of communication. It could be interesting to follow up with an analysis of two-word pairs to see if these theories play out."
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#conclusion",
    "href": "posts/SarahMcAlpine_finalproject.html#conclusion",
    "title": "Sarah McAlpine - Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nDespite the relatively small sample size, several strong trends emerge in the results of these surveys. The software company now has a sense of which factors were most powerful in prompting its adoption, which aspects of launching the software required the most effort, which benefits were being experienced most at the time of the survey, and what concepts the respondents are considering most when given the chance to respond in their own words. These are valuable findings that can help shape the company’s onboarding program, business development, and product refinement.\nThe use of R in this analysis was critical to its success and replicability. The “viridis” color palette was selected for its high visual accessibility. The analysis includes both quantitative analysis and qualitative analysis and approaches many aspects of data cleaning and tidying that were unique to each grouping of survey questions.\nAcknowledgment and much gratitude is due to Professors Meredith Rolfe and Dr. Rosemary Pang; DACSS tutors Maddie Hertz, Sathvik Thogaru, and Leah Dion; and classmates who helped along the way. Thanks as well to the online R coding community on GitHub, StackOverflow, and R-Bloggers.com. This global community gave guidance and training, asked interesting questions, infused the learning process with humanity and good humor, and was boldly vulnerable. Many thanks to all of you."
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#selected-sources",
    "href": "posts/SarahMcAlpine_finalproject.html#selected-sources",
    "title": "Sarah McAlpine - Final Project",
    "section": "Selected Sources",
    "text": "Selected Sources\n\nhttps://adv-r.hadley.nz/index.html\nhttps://byrneslab.net/classes/biol607/readings/wickham_layered-grammar.pdf\nhttps://www.data-to-viz.com/\nhttps://datasciencelk.com/creating-a-word-cloud-using-r/\nhttps://ggplot2.tidyverse.org/index.html\nhttps://idc9.github.io/stor390/notes/custom_viz/custom_viz.html\nhttps://posit.co/resources/cheatsheets/\nhttps://r-pkgs.org/\nhttps://r-graph-gallery.com/\nhttps://r-graphics.org/\nhttps://r4ds.had.co.nz/index.html\nhttps://rstudio-education.github.io/hopr/\nhttps://www.tidytextmining.com/tidytext.html"
  },
  {
    "objectID": "posts/SarahMcAlpine_finalproject.html#appendix",
    "href": "posts/SarahMcAlpine_finalproject.html#appendix",
    "title": "Sarah McAlpine - Final Project",
    "section": "Appendix",
    "text": "Appendix\nComplete Survey Questions and Renaming Notes\n\n\nCode\n#  [1] \"Timestamp.x\"                                             \n\n#  [2] \"UserID\"                                               \n\n#  [3] \"What size sweatshirt would you like (we are out of mediums, sorry!)?.x\"             \n#  renamed: delete                                                         \n\n#  [4] \"What address should we ship the sweatshirt to?.x\"        \n#  renamed: delete                                                         \n\n#  [5] \"What prompted your adoption of O?.x\"                     \n# renamed:prompted_adoption                                              \n\n#  [6] \"What is your annual fundraising goal, and how many constituents do you reach out to with O?.x\"\n# renamed:goal_constituents                                              \n\n#  [7] \"Outside of your user base, how many individuals support processes within O and what are their individual roles? If possible, please estimate what percentage of their time is O focused..x\"  \n# renamed: launcheffort_staffrolestime                                    \n\n#  [8] \"Please indicate the scale of effort required for launching the various aspects of O (1 = Low level of effort; 5 = High level of effort). [Retasking of staff].x\"           \n# renamed: launcheffort_retaskstaff                                       \n\n#  [9] \"Please indicate the scale of effort required for launching the various aspects of O (1 = Low level of effort; 5 = High level of effort). [Generating content].x\"           \n# renamed: launcheffort_generatecontent                                   \n\n# [10] \"Please indicate the scale of effort required for launching the various aspects of O (1 = Low level of effort; 5 = High level of effort). [Tech/Interface with IT].x\"       \n# renamed: launcheffort_techIT                                            \n\n# [11] \"Please indicate the scale of effort required for launching the various aspects of O (1 = Low level of effort; 5 = High level of effort). [Challenges with staff adoption and comfort].x\"\n# renamed: launcheffort_staffadoption                                     \n\n# [12] \"Please add any comments or details that would help us understand your ratings better....12.x\"                        \n# renamed: launcheffort_detail                                            \n\n# [13] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Improved number or quality of donor engagement touches (whether measured or anecdotal)].x\" \n# renamed: benefit_donorengagement                                        \n\n# [14] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Increase in financial contributions].x\"\n# renamed: benefit_increasedgifts\n\n# [15] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Cost savings].x\"                    \n# renamed: benefit_costsavings                                            \n\n# [16] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Time savings].x\"                    \n# renamed: benefit_timesavings                                         \n\n# [17] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Increased gift officer productivity].x\" \n# renamed: benefit_GOproductivity   \n\n# [18] \"Please add any comments or details that would help us understand your ratings better....18.x\"                        \n# renamed: benefit_detail           \n\n# [19] \"How did you manage philanthropic communications before O?.x\"                        \n# renamed: donor_commsbefore \n\n# [20] \"What is your process for developing content, and how do you know what content to feed to each donor as youâ€™re personalizing sites/reports? To what extent is this decided/produced by gift officers versus other team members?.x\"\n# renamed: process_content \n\n# [21] \"What challenges of donor relationship-building is O helping to solve for you?.x\"    \n# renamed: donor_relationship                                             \n\n# [22] \"How has O changed, facilitated, or limited your interaction with donors?.x\"         \n# renamed: donor_interaction                                              \n\n# [23] \"What has been the donor response to hyper-personalized communications?.x\"          \n# renamed: donor_response                                                 \n\n# [24] \"What have you learned from looking at the end-donor use data O provides?.x\"         \n# renamed: donor_analytics                                               \n\n# [25] \"What have you learned about your internal team through the aggregate analytics module?.x\" \n# renamed: team_analytics                                                \n\n# [26] \"How would you rate SA/Oâ€™s implementation and support?.x\"                          \n# renamed: rateO_implementationsupport \n\n# [27] \"Is there anything you would like to expand upon regarding your selection above?.x\"                \n# renamed: rateO_support_detail \n\n# [28] \"What could have been handled better?.x\"\n# renamed: O_lacking\n\n# [29] \"What is the biggest O-facilitated success your organization can point to (either with an individual donor relationship or in the aggregate)?.x\"                            \n# renamed: biggest_success\n\n# [30] \"How was the reality of O different from your expectations (both positive and negative)?.x\"                                 \n# renamed: reality_vs_expectations\n\n# [31] \"What advice would you have for future O users?.x\"        \n# renamed: client_advice\n\n# [32] \"How can O improve (in big and small ways)? Your complete candor would be much appreciated..x\"                \n# renamed: O_improve\n\n# [33] \"Timestamp.y\"                                             \n# renamed: reorder to beginning\n\n# [34] \"What prompted your adoption of O?.y\"                     \n# renamed: prompted_adoption2\n\n# [35] \"Why did you choose O over other platforms you were exploring?\"\n# renamed: O_over_competitors\n\n# [36] \"If \\\"unique product features\\\" was selected in the above question, can you please elaborate?\" \n# renamed: feature_detail \n\n# [37] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Improved number or quality of donor engagement touches (whether measured or anecdotal)].y\"\n# renamed: benefit_donorengagement2\n\n# [38] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Increase in financial contributions].y\"  \n# renamed: benefit_increasedgifts2\n\n# [39] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Cost savings].y\"\n# renamed: benefit_costsavings2 \n\n# [40] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Time savings].y\"                    \n# renamed: benefit_timesavings2  \n\n# [41] \"Please indicate the degree to which you have experienced the following benefits from O (1 = To a small degree; 5 = To a great degree). [Increased gift officer productivity].y\"\n# renamed: benefit_GOproductivity2\n\n# [42] \"Please share any other factors your team considered before selecting O (if applicable).\"               \n# renamed: choice_factors              \n\n# [43] \"Timestamp\"                                               \n\n# [44] \"Overall, how did the onboarding process go for you?\"     \n# renamed: onboard_overall \n\n# [45] \"What part of the onboarding process could O have handled better?\"                   \n# renamed: onboard_lacking\n\n# [46] \"Please expand on your selections above:...5\"             \n# renamed: onboard_lacking_detail\n\n# [47] \"What part of the onboarding process did O handle especially well?\"                  \n# renamed: onboard_wentwell\n\n# [48] \"Please expand on your selections above:...7\"             \n# renamed: onboard_wentwell_detail\n\n# [49] \"How effectively did we communicate during the onboarding process?\"\n# renamed: onboard_communication \n\n# [50] \"Please expand on your selections above:...9\"          \n# renamed: onboard_communication_detail\n\n# [51] \"How effectively did we interpret your needs during the onboarding process?\"         \n# renamed: onboard_interpretneeds\n\n# [52] \"Please expand on your selections above:...11\"  \n# renamed: onboard_interpretneeds_detail \n\n# [53] \"Are you willing to talk about your onboarding experience with future O clients?\"\n# renamed: onboard_shareexperience"
  },
  {
    "objectID": "posts/shelton_final.html",
    "href": "posts/shelton_final.html",
    "title": "DACSS 601: Florida Homelessness",
    "section": "",
    "text": "florida_1820.csv contains population figures, homelessness counts, poverty counts and other demographic indicators3 at the county level from 2018 to 2020. All 67 Florida counties have data for the 3 years, resulting in 201 observations of 15 variables. Each observation provides a count (or rate) for all variables from a single county for a year. The variables selected closely mirror the stressors mentioned in the reference study (Zugazaga), along with supplementary variables in an attempt to completely capture the circumstances.\nHomelessness is a complex living situation with several qualifying conditions; at its simplest state, the U.S Dept. of Housing and Urban Development defines it as lacking a fixed, regular nighttime residence (not a shelter) or having a nighttime residence not designed for human accommodation1.\nOn a single night in 2020, over 500,0002 people experienced homelessness in the United States. Florida - the third largest state by population - had the fourth largest homeless population of 2020 with 27,4872.\nFlorida counties represent a wide range of demographic profiles; the state is a hub to a variety of industries including tourism, defense, agriculture, and information technology. Investigating homelessness in Florida counties with robust data can lead to several conclusions about who is being impacted where, and how state policy is failing (or aiding) groups of a diverse population."
  },
  {
    "objectID": "posts/shelton_final.html#introduction",
    "href": "posts/shelton_final.html#introduction",
    "title": "DACSS 601: Florida Homelessness",
    "section": "Introduction",
    "text": "Introduction\n\nResearch QuestionHypothesisData Read-in and Tidying\n\n\nCarole Zugazaga’s 2004 study of 54 single homeless men, 54 single homeless women, and 54 homeless women with children in the Central Florida area investigated stressful life events common among homeless people. Interview responses revealed that women were more likely to have been sexually or physically assaulted, while men were more likely to have been incarcerated or abuse drugs/alcohol. Homeless women with children were more likely to be in foster care as a youth.\nNearly a decade later, county-level data can be used to investigate the relationship between Zugazaga’s reported stressful life events (incarceration, drug arrests, poverty, forcible sex, foster care)3 and homelessness rates.\n\n\n\n\n\n\nResearch Question\n\n\n\nDo particular life stressors increase a population’s vulnerability to homelessness?\n\n\n\n\nHomelessness is not a new issue in the United States, yet homeless policy targets elimination via criminalization rather than prevention. A 2019 article from Homeless Voice provides a brief description of homeless policy in major citites across Florida. Despite state and federal governments being aware of the circumstances that increase vulnerability to homelessness for decades, I anticipate at least one Zugazaga’s five stressors to remain significant in a model relating stressors to Florida homelessness counts 2018-2020.\n\n\n\n\n\n\nResearch Hypothesis\n\n\n\nH0: All stressors are insignificant in predicting homelessness counts ( Bi = 0 for i=0,1,2,…n )\nHA: At least one stressor Bi is significant in predicting homelessness counts\n\n\n\n\nThe data were collected from the Florida Department of Health. Variable names3 were used as search indicators to produce counts for Florida counties. Unfortunately, we cannot accurately analyze the effect of COVID-19 as data is incomplete for the majority of counties in 2021.\nRead in and tidying were done in a separate file. The process began as laborious, but was lightened by the discovery of 10 year tables. Before this discovery, I would enter a variable name into the search bar of Florida Department of Health, select 2018, and download the .xlsx file to my personal data folder. readxl was used to bring in the file, and mutate created a date column filled with 2018. Once the data was appropriate, I saved it in the environment under variable2018. This was repeated for years 2019 and 2020. Then all three tibbles were full _join by County to provide a dataset with 201 observations (67 counties by 3 years) and three variables, County, Year, and the variable measurement itself . This was written as a .csv back to the same personal data folder.\n\nOnce learning that I could draw 10-year tables from the website rather than having to download three individual .xlsx files, the process became smoother. Now, I would only rename excess years as “delete” to get a table of measurements for only 2018, 2019, and 2020. pivot_longer then moved the years to a self-titled column and transferred the values to a column of my naming. This was written as a .csv.\n\nI then merged all the tables and wrote this as a .csv florida_full. I completed a sanity check using distinct county names to ensure I had 201 observations of 15 variables as desired.\n\n\n\n\n\n\n\n\nIntro to Data Tables\n\n\n\n\n\n\n\n\n\n  \n\n\n\n    County               Year      Homeless (Count)   Population     \n Length:201         Min.   :2018   Min.   :   0.0   Min.   :   8367  \n Class :character   1st Qu.:2018   1st Qu.:  11.0   1st Qu.:  28089  \n Mode  :character   Median :2019   Median : 151.0   Median : 130642  \n                    Mean   :2019   Mean   : 427.8   Mean   : 317746  \n                    3rd Qu.:2020   3rd Qu.: 563.0   3rd Qu.: 367471  \n                    Max.   :2020   Max.   :3516.0   Max.   :2864600  \n                                                                     \n Unemployment Rate   Median Inc    Incarceration (Rateper1000) Poverty (Count) \n Min.   : 2.100    Min.   :34583   Min.   : 0.60               Min.   :   906  \n 1st Qu.: 3.400    1st Qu.:41401   1st Qu.: 2.50               1st Qu.:  4901  \n Median : 4.000    Median :50640   Median : 3.40               Median : 16210  \n Mean   : 4.697    Mean   :51116   Mean   : 3.84               Mean   : 42922  \n 3rd Qu.: 5.600    3rd Qu.:58093   3rd Qu.: 4.50               3rd Qu.: 46034  \n Max.   :13.500    Max.   :83803   Max.   :18.60               Max.   :482656  \n                                                                               \n Drug Arrests (Count) Relocated (Rate) Sub Abuse Enrollment (Count)\n Min.   :   13        Min.   : 4.689   Min.   :   5.0              \n 1st Qu.:  225        1st Qu.:11.244   1st Qu.:  76.0              \n Median :  729        Median :12.700   Median : 250.0              \n Mean   : 1558        Mean   :13.288   Mean   : 877.6              \n 3rd Qu.: 1903        3rd Qu.:14.544   3rd Qu.:1030.0              \n Max.   :13038        Max.   :22.553   Max.   :6272.0              \n                                                                   \n Adult Psych Beds (Count) Severe Housing Problems (Rate) Forcible Sex (Count)\n Min.   :  0.00           Min.   : 9.6                   Min.   :   0.0      \n 1st Qu.:  0.00           1st Qu.:13.3                   1st Qu.:  14.0      \n Median :  0.00           Median :15.4                   Median :  45.0      \n Mean   : 66.26           Mean   :15.8                   Mean   : 170.5      \n 3rd Qu.: 84.00           3rd Qu.:17.3                   3rd Qu.: 225.0      \n Max.   :778.00           Max.   :29.8                   Max.   :1408.0      \n                          NA's   :134                                        \n Foster Care (Count)\n Min.   :   3.0     \n 1st Qu.:  33.0     \n Median : 153.0     \n Mean   : 326.1     \n 3rd Qu.: 353.0     \n Max.   :2289.0     \n                    \n\n\n\n\n  \n\n\n\n\n\n\nExpanding Intro to Data exposes summary statistics including mean, range, quantiles, and standard deviation for all 15 variables. The table below the summaries provides arranged figures for basic parameters of interest grouped by county. The data was nearly tidy, with complete observations for each variable aside from one - Severe Housing Issues rates for 2019 and 2020 (unrecorded) - these are later filled in with 2018’s value for the regression analysis."
  },
  {
    "objectID": "posts/shelton_final.html#visualizations-and-analysis",
    "href": "posts/shelton_final.html#visualizations-and-analysis",
    "title": "DACSS 601: Florida Homelessness",
    "section": "Visualizations and Analysis",
    "text": "Visualizations and Analysis\n\nVisualizationRegression, Diagnostics, and Model Selection\n\n\nggplot2 is used to visualize important relationships between homeless counts and Zugazaga’s stressors. The Florida counties have been categorized into 4 Regions and 3 Income Levels:\n\n\n\n\n\n\nRegion\n\n\n\n\nNorthwest: Escambia County to Madison County; cities include Pensacola, Panama City Beach, and Tallahassee\nNorth: Hamilton County to Marion County; cities include Jacksonville, Gainseville,and Ocala, St. Augustine\nCentral: Lake County to Okeechobee County; cities include Orlando, Kissimmee, Tampa, St. Petersburg\nSouth: Sarasota County to Miami-Dade County; cities include Ft. Lauderdale, Ft. Myers, Miami, Boca Raton, West Palm Beach\n\n\n\n\n\n\n\n\n\n\nIncome Level\n\n\n\n\nHigh: Median Income >= 60000\nMedium: Median Income >= 40000\nLow: Median Income < 40000\n\n\n\n\n\nCode\n\n# Categorize by regions... conditional better perhaps?\n\n\nflorida_og_plot <- florida_og_rates %>%\n                    mutate('Region' = case_when(County == 'Escambia' |\n                                              County == 'Santa Rosa'| \n                                              County == 'Okaloosa'| \n                                              County == 'Walton'| \n                                              County == 'Holmes'| \n                                              County == 'Washington'| \n                                              County == 'Bay'| \n                                              County == 'Jackson'| \n                                              County == 'Calhoun'| \n                                              County == 'Gulf'| \n                                              County == 'Gadsden'| \n                                              County == 'Escambia'| \n                                              County == 'Liberty'| \n                                              County == 'Leon'| \n                                              County == 'Wakulla'| \n                                              County == 'Franklin' | \n                                              County == 'Jefferson'| \n                                              County == 'Madison'| \n                                              County == 'Taylor' ~ \n'Northwest',\n                                              County == 'Hamilton'| \n                                              County == 'Suwannee'| \n                                              County == 'Lafayette'| \n                                              County == 'Dixie'| \n                                              County == 'Gilchrist'| \n                                              County == 'Union'| \n                                              County == 'Baker'| \n                                              County == 'Columbia'| \n                                              County == 'Nassau'| \n                                              County == 'Levy'| \n                                              County == 'Bradford'| \n                                              County == 'Alachua'| \n                                              County == 'Nassau'| \n                                              County == 'Duval'| \n                                              County == 'Putnam'| \n                                              County == 'Marion'| \n                                              County == 'Volusia'| \n                                              County == 'Flagler'| \n                                              County == 'Citrus'| \n                                              County == 'Clay'| \n                                              County == 'St. Johns' ~ \n'North',\n                                              County == 'Lake'|\n                                              County == 'Sumter'|\n                                              County == 'Seminole'|\n                                              County == 'Orange'|\n                                              County == 'Hernando'|\n                                              County == 'Pasco'|\n                                              County == 'Brevard'|\n                                              County == 'Indian River'|\n                                              County == 'Pinellas'|\n                                              County == 'Hillsborough'|\n                                              County == 'Polk'|\n                                              County == 'Osceola'|\n                                              County == 'Hardee'|\n                                              County == 'Manatee'|\n                                              County == 'Okeechobee'|\n                                              County == 'Highlands' ~ \n'Central',\n                                              County == 'St. Lucie'|\n                                              County == 'Sarasota'|\n                                              County == 'Martin'|\n                                              County == 'Palm Beach'|\n                                              County == 'Collier'|\n                                              County == 'Broward'|\n                                              County == 'Lee'|\n                                              County == 'DeSoto'|\n                                              County == 'Charlotte'|\n                                              County == 'Hendry'|\n                                              County == 'Monroe'|\n                                              County == 'Miami-Dade'|\n                                              County == 'Glades'|\n                                              County == 'Hendry' \n~ 'South'))\n\n# Categorize by Median Income Level\n\nflorida_og_plot <- florida_og_plot %>%\n                    mutate('Income Level' = case_when(\n                    `Median Inc` >= 60000 ~ 'High',\n                    `Median Inc` < 60000 &\n                    `Median Inc` >= 40000 ~ 'Medium',\n                    `Median Inc` < 40000 ~ 'Low'))\n\n\n\n\n\n\n\n\na) Interactive Summary\n\n\n\n\n\n\n\nCode\n# Plot 1 \n# Group by and Summarize to extract averages\nflorida_int <- florida_og_plot%>%\n                group_by(County, Region)%>%\n                  summarize('Homeless' = mean(`Homeless (Count)`),\n                              'Population' = mean(Population))%>%\n\n# Arrange before applying factor levels to retain ordering in plot\n              arrange(desc(Population))%>%\n                mutate(County = factor(County, County))%>%\n# Prep Text for tooltip \n              mutate(text = paste(\"County: \", County, \"\\nRegion: \", Region, \"\\nPopulation: \", Population, \"\\nHomeless: \", Homeless, sep=\"\"))\n\n# Making interactive plot\n\nflorida_interactive <- ggplot(data = florida_int, aes(x=Population, y=Homeless, size = Population, color = Region, text=text)) +\n    geom_point(alpha=0.7)+\n      scale_size(range = c(1.0, 10.0), name= \"\") \n       # scale_fill_brewer(palette = 'Set2')\n          \n          \n\n# Complete interactive plot \nflorida_interactive_full <- ggplotly(florida_interactive, tooltip=\"text\")\n\n    \nflorida_interactive_full\n\n\n\n\n\n\n\nUse the interactive plot to gain an idea of the size of counties in Florida and their homeless counts.\nHover over a point to see the details, and double-click an item in the legend to isolate the region.\n\n\n\n\n\n\n\n\n\n\nb) Homeless Rate by Region\n\n\n\n\n\n\n\nCode\n# Plot 2\n\nflorida_box <- florida_og_plot %>%\n    mutate(`Homeless (Rate)` = `Homeless (Rate)`*100 )%>%\n      mutate(`Region`=fct_relevel(`Region`, \"Northwest\", \"North\", \"Central\", \"South\"))%>%\n      ggplot(aes(y=`Homeless (Rate)`, x=`Region`, fill=`Region`)) +\n        geom_boxplot(alpha=0.7)+\n  #Scale of y axis\n          scale_y_continuous(breaks=(seq(0,1.5,by=.25)))+\n  # Dimensions of graph\n            coord_cartesian(ylim=c(0,1.5)) +\n              coord_flip()+\n                scale_fill_brewer(palette = 'Set2')+\n                                   theme_grey()+\n                                     theme(legend.position = \"none\")+\n                                        labs(title= \"Florida Homeless Rates\", \n                                             subtitle=\"2018-2020\", \n                                              x= \" \", \n                                                y= \"Homeless Rate (%)\", \n                                            caption = \"Visualized by Region\")\n\nflorida_box\n\n\n\n\n\n\nA look at the distributions of homeless rates across Florida counties displays where the highest rates in the state exists.\nThe state is generally uniform, with the bulk of each region’s distribution sitting below 0.25% of county populations.\n\nThe largest difference is between the distributions of Northwest Florida and South Florida. This can be attributed to the stark contrast in where the population in these regions are living.\nSouth Florida is the most urbanized region in the state, with millions living in Miami, Ft. Lauderdale, and West Palm Beach; Northwest Florida is quite the opposite, with small coastal towns and rural inland towns reminiscent of Southern Alabama or Georgia.\n\n\n\n\n\n\n\n\n\n\n\nc) Homeless Rate by Income Level\n\n\n\n\n\n\n\nCode\n\n# Plot 3\n#Relevel Income, Mutate Response \nflorida_income <- florida_og_plot %>%\n    mutate(`Homeless (Rate)` = `Homeless (Rate)`*100)%>%\n      mutate(`Income Level` = fct_relevel(`Income Level`, \"Low\", \"Medium\", \"High\"))%>%\n      ggplot(aes(y=`Homeless (Rate)`, x=`Region`,\n                  fill=`Income Level`, \n                    )) +\n      geom_bar(position='dodge', stat='identity')+\n          \n              scale_fill_brewer(palette = 'Set2')+\n                                   theme_grey()+\n                                     \n                    labs(title= \"Barplot: Income Level x Homeless Rate\", \n                           subtitle=\"2018-2020\", \n                              x= \"Region\", \n                                y= \"Homeless Rate (%)\", \n                                  caption = \"Visualized by Income Level\")\n\nflorida_income\n\n\n\n\n\n\nThe barplot further details the differences mentioned in Plot 2. Regions where the populations are less urbanized show Low Income counties reflecting a higher homeless rate as one would assume.\nOnce entering South Florida, wealth disparities in urbanized areas breaks this trend -counties with high income now report high homeless rates\n\n\n\n\n\n\n\n\n\n\nd) Homeless Rate x Incarceration Rate per1000\n\n\n\n\n\n\n\nCode\n#Plot 4\n\nflorida_incarc <- florida_og_plot %>%\n    mutate(`Homeless (Rate)` = `Homeless (Rate)`*100,\n           `Incarceration (Rateper1000)`= `Incarceration (Rateper1000)`/10)%>%\n      ggplot(aes(y=`Homeless (Rate)`, \n                  x=`Incarceration (Rateper1000)`, \n                    color=`Region`, \n                      label=`County`)) +\n      geom_point(alpha=0.7)+\n  # Must include because of label aesthetic\n      ggrepel::geom_text_repel(show.legend = FALSE, \n                  max.overlaps = 15, \n                  alpha=0.7,\n                    size=2.5, \n                      nudge_x = -.05, \n                        nudge_y =.05)+\n          \n              scale_fill_brewer(palette = 'Set2')+\n                                   theme_grey()+\n              # Titles, Axes, Caption                   \n                    labs(title= \"Scatterplot: Incarceration x Homeless Rate\", \n                           subtitle=\"2018-2020\", \n                              x= \"Incarceration Rate (per100)\", \n                                y= \"Homeless Rate (%)\", \n                                  caption = \"Visualized by Region\")\n\nflorida_incarc\n\n\n\n\n\n\nOne of Zugazaga’s male stressors Incarceration Rate is illustrated with Homeless Rate; a positive trend exists relating incarceration rates and homelessness rates across Florida counties.\n\nMany state correctional facilities are located in rural counties, explaining both Baker and Monroe observations’ large influence on the plot.\n\n\n\n\n\n\n\n\n\n\n\ne) Homeless Rate x Drug Arrest Rate\n\n\n\n\n\n\n\nCode\n#Plot 4\n\nflorida_drug <- florida_og_plot %>%\n    mutate(`Homeless (Rate)` = `Homeless (Rate)`*100,\n           `Drug Arrests (Rate)`= `Drug Arrests (Rate)`*100)%>%\n      ggplot(aes(y=`Homeless (Rate)`, \n                  x=`Drug Arrests (Rate)`, \n                    color=`Region`, \n                      label=`County`)) +\n      geom_point(alpha=0.7)+\n  # Must include because of label aesthetic\n      ggrepel::geom_text_repel(show.legend = FALSE, \n                  max.overlaps = 15, \n                  alpha=0.7,\n                    size=2.5, \n                      nudge_x = -.05, \n                        nudge_y =.05)+\n          \n              scale_fill_brewer(palette = 'Set2')+\n                                   theme_grey()+\n                                     \n                    labs(title= \"Scatterplot: Drug Arrest Rate x Homeless Rate\", \n                           subtitle=\"2018-2020\", \n                              x= \"Drug Arrests (Rate)\", \n                                y= \"Homeless Rate (%)\", \n                                  caption = \"Visualized by Region\")\n\nflorida_drug\n\n\n\n\n\n\nA similar positive association is seen comparing the homeless rate of a county with its Drug Arrest (Rate).\n\nThe high influence of Northwest Florida is likely due to stricter drug policies held by police in less urbanized areas.\n\n\n\n\n\n\n\n\non Assumption of Validity\nWhile over 10 variables are predicting Homeless (Rate) across Florida counties, there are still limitations when attempting to comment on the magnitude of an individual stressor. Stressors influence homelessness by driving those in severe situations out of their home or away from their place of origin. Homeless (Rate) is not an ideal measure of magnitude as the homeless population migrating to escape or avoid certain stressors would result in counties with low stressor values having a higher homeless population; this effect is left unexplained by the following models.\n\nThe variable Relocated (Rate) is included as an attempt to control for new movement, however this doesn’t completely capture county-to-county migration.\nFL Charts has data that records Population Who Lived in a Different County One Year Earlier, however with the data spanning 2009-2014, using values recorded 4 years prior to our data isn’t desirable either.\nThe most appropriate data to accurately capture county-to-county migration is here via the US Census Bureau. The -In, -Out, -Net... spreadsheet provides totals for each county in the United States and movement to all other US counties; unfortunately, this data is too complex to wrangle into the simple data set florida_1820.csv.\n\n\n\non Assumption of Linearity\n\n\nCode\n# Fit 1: A Linear Regression Model With All Vars\n\n# Checking Linearity of variables not supported by our literature\n# Correlation Matrix\nflorida_matrix <- florida_og_rates %>%\n                    select(-c(contains(\"Count\"), \n                              'Year', \n                              'Poverty (Rate)', \n                              'Severe Housing Problems (Rate)',\n                              'Incarceration (Rateper1000)',\n                              'Sub Abuse Enrollment (Rate)',\n                              'Drug Arrests (Rate)',\n                              'Adult Psych Beds (Rate)',\n                              'Foster Care (Rate)',\n                              'Forcible Sex (Rate)' ))%>%\n                      pairs()\n\n\n\n\n\nCode\nflorida_matrix\n\n\nNULL\n\n\nA quick look at stressors with a relationship to homelessness not mentioned in Zugazaga’s study, or those that needed further investigation are shown here to confirm linearity with the response, Homeless (Rate). Checking the bottom row,the associations are weak, but a linear approximation is appropriate.\n\n\nLinear Regression Models\n\n\n\n\n\n\nFit 1: All Variables (No Transformations)\n\n\n\n\n\n\n\nCode\n# Linear relationship appears appropriate for all, possibly attempt log transformation on UE Rate?\n\n# Creating A Linear Model with all variables included: No Transformations\n\n# County Removed as too many levels; improvement: NWFL, NFL, CFL, SWFL, SOFLO categories?\n\n# Fit 1 - OLS with all variables predicting homeless rate\nfit1 <- florida_og_rates %>% \n          select(-'County')%>%\n            \n            lm(formula=`Homeless (Rate)` ~.)\n\nsummary(fit1)\n\n\n\nCall:\nlm(formula = `Homeless (Rate)` ~ ., data = .)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0024414 -0.0004897 -0.0001599  0.0004384  0.0034222 \n\nCoefficients: (1 not defined because of singularities)\n                                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      -4.128e-03  2.556e-03  -1.615 0.112968    \nYear                                     NA         NA      NA       NA    \n`Homeless (Count)`                5.430e-06  8.739e-07   6.214 1.28e-07 ***\nPopulation                       -1.913e-09  3.885e-09  -0.492 0.624751    \n`Unemployment Rate`              -6.668e-05  2.953e-04  -0.226 0.822334    \n`Median Inc`                      3.952e-08  2.799e-08   1.412 0.164626    \n`Incarceration (Rateper1000)`    -2.753e-05  7.492e-05  -0.367 0.714928    \n`Poverty (Count)`                 2.761e-08  1.873e-08   1.474 0.147123    \n`Drug Arrests (Count)`           -6.460e-08  2.561e-07  -0.252 0.801977    \n`Relocated (Rate)`               -7.993e-05  5.790e-05  -1.380 0.174016    \n`Sub Abuse Enrollment (Count)`    9.462e-08  3.900e-07   0.243 0.809368    \n`Adult Psych Beds (Count)`       -2.652e-05  7.325e-06  -3.620 0.000719 ***\n`Severe Housing Problems (Rate)`  1.813e-04  7.569e-05   2.395 0.020672 *  \n`Forcible Sex (Count)`           -1.431e-06  2.445e-06  -0.585 0.561202    \n`Foster Care (Count)`            -3.979e-06  1.511e-06  -2.633 0.011413 *  \n`Poverty (Rate)`                 -2.976e-04  6.024e-03  -0.049 0.960809    \n`Drug Arrests (Rate)`             7.704e-02  6.911e-02   1.115 0.270682    \n`Sub Abuse Enrollment (Rate)`     6.409e-02  1.431e-01   0.448 0.656276    \n`Adult Psych Beds (Rate)`         4.202e+00  2.208e+00   1.903 0.063122 .  \n`Forcible Sex (Rate)`             3.869e-01  9.245e-01   0.418 0.677501    \n`Foster Care (Rate)`              6.961e-01  2.996e-01   2.323 0.024549 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001111 on 47 degrees of freedom\n  (134 observations deleted due to missingness)\nMultiple R-squared:  0.7193,    Adjusted R-squared:  0.6058 \nF-statistic: 6.338 on 19 and 47 DF,  p-value: 1.329e-07\n\n\nCode\nrss1 <- deviance(fit1)\nprint(c('RSS Fit 1', rss1))\n\n\n[1] \"RSS Fit 1\"            \"5.80154282170776e-05\"\n\n\n\nThe first model predicts Homeless (Rate) using all variables, without any transformations or interactions. This causes 134 observations to removed as they are missing values for Severe Housing Problems (Rate).\nOnly 1 variable - Severe Housing Problems (Rate) - is deemed significant at alpha = 0.05; those without a star (see output) are deemed inconsequential in predicting Homeless (Rate) by this model.\nEffect of Relocated (Rate) is negative, indicating that migration can ‘help reduce’ homelessness by county, as predicted in ‘Assumptions on Validity’ (above)\nLooking at the signs and magnitude of the predicted (insignificant) variables, they seem plausible - Increases in variables like Drug Arrests (Rate) or Sub Abuse Enrollment (Rate) increase response Homeless (Rate) substantially.\n\nSub Abuse Enrollement (Rate) can be interpreted here an an indication of how many people in the area are suffering from addiction/abuse problems, rather than a suggestion that substance abuse programs increase homelessness.\n\n\n\n\n\n\n\n\n\n\n\nFit 1: Diagnostics\n\n\n\n\n\n\n\n\n\n\n\nFit 1 does a poor job of obeying the assumptions regarding residuals of linear regression.\nResiduals vs Fitted shows a negative trend the greater the fitted value is, violating the linearity and independence assumption.\n\nScale - Location confirms this as the standardized residuals increase in magnitude the greater the fitted value is.\n\nQ-Q Plot shows a deviation from the diagonal, violating the assumption that residuals follow an approximately Normal distribution\nThere are several points that could be considered outliers due to their residual or leverage value, how greatly they influence the points around them in the model.\n\nMonroe County (130), Hardee County (73), and Columbia County (34) all have large positive residuals, indicating our model greatly under-estimated the number of homeless people in this county.\nBaker County (4) has worryingly high leverage, its explanatory values have great influence on the data\nAll of these outliers represent sparsely populated, rural counties, typically outside of more urbanized areas; hence large values for stressors will command great influence on the model.\n\n\n\n\n\n\n\n\n\n\n\nFit 2: All Variables, All Observations (Fill Severe Housing Rate)\n\n\n\n\n\n\n\nCode\n# fit 2\nfit2 <- florida_og_rates %>% \n          select(-c('County','Year'))%>%\n            #mutate(`Unemployment Rate` = log(`Unemployment Rate`))%>%\n              fill('Severe Housing Problems (Rate)', .direction=\"down\")%>%\n                 lm(formula=`Homeless (Rate)` ~ . - `Median Inc`)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = `Homeless (Rate)` ~ . - `Median Inc`, data = .)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0023982 -0.0005760 -0.0001193  0.0003527  0.0054020 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      -1.170e-03  6.202e-04  -1.887  0.06079 .  \n`Homeless (Count)`                4.028e-06  4.080e-07   9.875  < 2e-16 ***\nPopulation                       -3.923e-09  1.516e-09  -2.589  0.01041 *  \n`Unemployment Rate`               2.761e-05  4.930e-05   0.560  0.57608    \n`Incarceration (Rateper1000)`    -1.655e-05  3.571e-05  -0.463  0.64364    \n`Poverty (Count)`                 2.058e-08  8.721e-09   2.360  0.01931 *  \n`Drug Arrests (Count)`           -1.844e-07  1.044e-07  -1.766  0.07902 .  \n`Relocated (Rate)`               -6.520e-05  2.649e-05  -2.461  0.01477 *  \n`Sub Abuse Enrollment (Count)`    1.853e-07  1.731e-07   1.070  0.28588    \n`Adult Psych Beds (Count)`       -1.895e-05  3.380e-06  -5.606 7.58e-08 ***\n`Severe Housing Problems (Rate)`  1.822e-04  3.177e-05   5.733 4.03e-08 ***\n`Forcible Sex (Count)`            1.474e-06  1.257e-06   1.172  0.24257    \n`Foster Care (Count)`            -1.477e-06  5.711e-07  -2.586  0.01049 *  \n`Poverty (Rate)`                 -5.887e-03  2.224e-03  -2.647  0.00884 ** \n`Drug Arrests (Rate)`             1.065e-01  2.599e-02   4.099 6.24e-05 ***\n`Sub Abuse Enrollment (Rate)`    -5.298e-02  6.067e-02  -0.873  0.38366    \n`Adult Psych Beds (Rate)`         4.170e+00  9.278e-01   4.494 1.24e-05 ***\n`Forcible Sex (Rate)`            -1.535e-01  3.972e-01  -0.386  0.69967    \n`Foster Care (Rate)`              4.293e-01  1.371e-01   3.131  0.00203 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0009618 on 182 degrees of freedom\nMultiple R-squared:  0.5964,    Adjusted R-squared:  0.5565 \nF-statistic: 14.94 on 18 and 182 DF,  p-value: < 2.2e-16\n\n\nCode\nbic2 <- BIC(fit2)\nprint(c('BIC Fit 2:', bic2))\n\n\n[1] \"BIC Fit 2:\"        \"-2136.04637375734\"\n\n\nCode\nrss2 <- deviance(fit2)\nprint(c('RSS Fit 2', rss2))\n\n\n[1] \"RSS Fit 2\"            \"0.000168367650150255\"\n\n\n\nIn Fit 2, values from Severe Housing Problems (Rate) were filled down to restore all observations for use in the model.\n\nExample: Alachua County has the same Severe Housing Problems (Rate) for 2018-2020\n\nSeveral key stressors were deemed significant, with Adult Psych Beds (Rate) having the largest magnitude\n\nStressors from Zugazaga’s study that were found significant by this model include Drug Arrests (Rate) and Foster Care (Rate)\n\n\n\n\n\n\n\n\n\n\n\nFit 2: Diagnostics\n\n\n\n\n\n\n\n\n\n\n\nUsing all observations in Fit 2has not improved the diagnostic plots, but including 2019 and 2020 values has revealed new outliers.\n2020 produced Liberty County (117) as an outlier, a small, rural county in the Panhandle of Florida.\n\nIn late 2018, Hurricane Michael devastated the area; the influence of this observation is likely a direct result of measurements being altered greatly or even unaccounted during 2019 due to the population being “in a transitional state”\nLiberty’s records in 2020 will show a vast difference to the incomplete measures of 2019\n\n\n\n\n\n\n\n\n\n\n\nFit 3: Random Effects Model - Controlling for County over Time\n\n\n\n\n\n\n\nCode\n#tranform to panel data\nflorida_panel <-  pdata.frame(florida_og_rates, index=c('County','Year'))\n\n#random effects model\n#Exclude Median Inc because of large numeric difference\nfit3 <- florida_panel %>%\n          plm(formula = Homeless..Rate. ~ \n         Unemployment.Rate +             \n         Incarceration..Rateper1000. +\n         Relocated..Rate. +           \n         Poverty..Rate. +        \n         Drug.Arrests..Rate. +          \n         Sub.Abuse.Enrollment..Rate. +  \n         Adult.Psych.Beds..Rate. +      \n         Forcible.Sex..Rate. +           \n         Foster.Care..Rate., \n            model= 'random')\n                   \nsummary(fit3)\n\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = Homeless..Rate. ~ Unemployment.Rate + Incarceration..Rateper1000. + \n    Relocated..Rate. + Poverty..Rate. + Drug.Arrests..Rate. + \n    Sub.Abuse.Enrollment..Rate. + Adult.Psych.Beds..Rate. + Forcible.Sex..Rate. + \n    Foster.Care..Rate., data = ., model = \"random\")\n\nBalanced Panel: n = 67, T = 3, N = 201\n\nEffects:\n                    var   std.dev share\nidiosyncratic 4.793e-07 6.923e-04 0.246\nindividual    1.467e-06 1.211e-03 0.754\ntheta: 0.6866\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-0.00193525 -0.00026946 -0.00010248  0.00017024  0.00612304 \n\nCoefficients:\n                               Estimate  Std. Error z-value Pr(>|z|)   \n(Intercept)                  4.2668e-04  8.3580e-04  0.5105 0.609699   \nUnemployment.Rate            9.5178e-06  4.1582e-05  0.2289 0.818953   \nIncarceration..Rateper1000.  2.1187e-05  5.9881e-05  0.3538 0.723478   \nRelocated..Rate.            -2.3556e-05  4.4214e-05 -0.5328 0.594191   \nPoverty..Rate.               1.1404e-03  3.2398e-03  0.3520 0.724835   \nDrug.Arrests..Rate.          7.2622e-02  2.7264e-02  2.6637 0.007729 **\nSub.Abuse.Enrollment..Rate. -1.4647e-02  5.4401e-02 -0.2693 0.787736   \nAdult.Psych.Beds..Rate.      2.7492e+00  9.9533e-01  2.7621 0.005743 **\nForcible.Sex..Rate.         -3.1645e-01  4.0769e-01 -0.7762 0.437628   \nFoster.Care..Rate.           2.3889e-01  1.7728e-01  1.3475 0.177804   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    0.00010218\nResidual Sum of Squares: 9.3309e-05\nR-Squared:      0.086778\nAdj. R-Squared: 0.043746\nChisq: 18.1495 on 9 DF, p-value: 0.033478\n\n\nCode\n#bic3 <- BIC(fit3)\n#print(c('BIC Fit 3:', bic3))\n\nrss3 <- deviance(fit3)\nprint(c('RSS Fit 3', rss3))\n\n\n[1] \"RSS Fit 3\"            \"9.33086018067315e-05\"\n\n\n\nEvaluating the model with a random effects model allows us to control for unmeasureable differences between counties.\n\nEach county receives its own intercept, drawn from a collection of possible intercepts\n\nOnly variables Drug Arrests (Rate) and Adult Psych Beds (Rate) retained their significance\n\nDrug Arrests (Rate) saw a slight decrease in magnitude whereas Adult Psych Beds (Rate) increased.\nBoth are positive, indicating that increases in either of these rates result in an increase in Homeless Rate\n\n\n\n\n\n\n\n\n\n\n\nFit 4: Random Effects Model - Zugazaga’s variables\n\n\n\n\n\n\n\nCode\n# fit 4 - random efffects 2                   \nfit4 <- plm(formula = Homeless..Rate. ~ \n              Drug.Arrests..Rate. + \n              lag(Sub.Abuse.Enrollment..Rate.,1) +\n              Forcible.Sex..Rate. +\n              Foster.Care..Rate.,   \n            data = florida_panel,\n            model= 'random')\n                   \nsummary(fit4)\n\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = Homeless..Rate. ~ Drug.Arrests..Rate. + lag(Sub.Abuse.Enrollment..Rate., \n    1) + Forcible.Sex..Rate. + Foster.Care..Rate., data = florida_panel, \n    model = \"random\")\n\nBalanced Panel: n = 67, T = 2, N = 134\n\nEffects:\n                    var   std.dev share\nidiosyncratic 1.848e-07 4.298e-04  0.12\nindividual    1.350e-06 1.162e-03  0.88\ntheta: 0.7469\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-0.00193151 -0.00022543 -0.00002260  0.00019457  0.00192708 \n\nCoefficients:\n                                       Estimate  Std. Error z-value Pr(>|z|)\n(Intercept)                          0.00089264  0.00037360  2.3893  0.01688\nDrug.Arrests..Rate.                  0.09055443  0.02068601  4.3776  1.2e-05\nlag(Sub.Abuse.Enrollment..Rate., 1) -0.08876300  0.06165290 -1.4397  0.14995\nForcible.Sex..Rate.                 -0.32816661  0.33634980 -0.9757  0.32923\nFoster.Care..Rate.                   0.25476909  0.15898288  1.6025  0.10905\n                                       \n(Intercept)                         *  \nDrug.Arrests..Rate.                 ***\nlag(Sub.Abuse.Enrollment..Rate., 1)    \nForcible.Sex..Rate.                    \nFoster.Care..Rate.                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2.8828e-05\nResidual Sum of Squares: 2.4526e-05\nR-Squared:      0.14921\nAdj. R-Squared: 0.12283\nChisq: 22.6247 on 4 DF, p-value: 0.00015047\n\n\nCode\n#bic4 <- BIC(fit4)\n#print(c('BIC Fit 4:', bic4))\n\nrss4 <- deviance(fit4)\nprint(c('RSS Fit 4', rss4))\n\n\n[1] \"RSS Fit 4\"            \"2.45264002390625e-05\"\n\n\n\nAgain using a random effects model with only stressors mentioned in Zugazaga’s study, we see Drug Arrests (Rate) as a significant predictor of homelessness at the alpha = 0.05 level.\nForcible Sex may not capture the circumstances suggested in Zugazaga’s study, as an arrest rate for forcible sex crimes captures the perpetrators within a county, rather than the victims.\n\nMany sex crimes go unreported or unpunished, due to familial or power relationships between the victim and offender\n\nFoster Care and Sub Abuse Enrollement (Rate) have signs and values that correspond well with Zugazaga’s results\n\nLagging Sub Abuse Enrollement (Rate) shows an increase of citizens involved in substance abuse programs can lead to a decrease in homelessness in the following year\n\n\n\n\n\n\n\nModel Selection\n\n\n\n\n\n\nStargazer Plot\n\n\n\n\n\n\n## \n## Homelessness in Florida\n## =======================================================================================================\n##                                                             Dependent variable:                        \n##                                     -------------------------------------------------------------------\n##                                                      Homeless Rate                    Homeless..Rate.  \n##                                                           OLS                              panel       \n##                                                                                           linear       \n##                                              (1)                     (2)               (3)       (4)   \n## -------------------------------------------------------------------------------------------------------\n## Year                                                                                                   \n##                                                                                                        \n##                                                                                                        \n## `Homeless (Count)`                        0.00001***              0.00000***                           \n##                                           (0.00000)               (0.00000)                            \n##                                                                                                        \n## Population                                  -0.000                 -0.000**                            \n##                                            (0.000)                 (0.000)                             \n##                                                                                                        \n## `Unemployment Rate`                        -0.0001                 0.00003                             \n##                                            (0.0003)               (0.00005)                            \n##                                                                                                        \n## `Median Inc`                               0.00000                                                     \n##                                           (0.00000)                                                    \n##                                                                                                        \n## `Incarceration (Rateper1000)`              -0.00003                -0.00002                            \n##                                            (0.0001)               (0.00004)                            \n##                                                                                                        \n## `Poverty (Count)`                          0.00000                0.00000**                            \n##                                           (0.00000)                (0.000)                             \n##                                                                                                        \n## `Drug Arrests (Count)`                     -0.00000               -0.00000*                            \n##                                           (0.00000)               (0.00000)                            \n##                                                                                                        \n## `Relocated (Rate)`                         -0.0001                -0.0001**                            \n##                                            (0.0001)               (0.00003)                            \n##                                                                                                        \n## `Sub Abuse Enrollment (Count)`             0.00000                 0.00000                             \n##                                           (0.00000)               (0.00000)                            \n##                                                                                                        \n## `Adult Psych Beds (Count)`               -0.00003***             -0.00002***                           \n##                                           (0.00001)               (0.00000)                            \n##                                                                                                        \n## `Severe Housing Problems (Rate)`           0.0002**               0.0002***                            \n##                                            (0.0001)               (0.00003)                            \n##                                                                                                        \n## `Forcible Sex (Count)`                     -0.00000                0.00000                             \n##                                           (0.00000)               (0.00000)                            \n##                                                                                                        \n## `Foster Care (Count)`                     -0.00000**              -0.00000**                           \n##                                           (0.00000)               (0.00000)                            \n##                                                                                                        \n## `Poverty (Rate)`                           -0.0003                -0.006***                            \n##                                            (0.006)                 (0.002)                             \n##                                                                                                        \n## `Drug Arrests (Rate)`                       0.077                  0.107***                            \n##                                            (0.069)                 (0.026)                             \n##                                                                                                        \n## `Sub Abuse Enrollment (Rate)`               0.064                   -0.053                             \n##                                            (0.143)                 (0.061)                             \n##                                                                                                        \n## `Adult Psych Beds (Rate)`                   4.202*                 4.170***                            \n##                                            (2.208)                 (0.928)                             \n##                                                                                                        \n## `Forcible Sex (Rate)`                       0.387                   -0.153                             \n##                                            (0.924)                 (0.397)                             \n##                                                                                                        \n## `Foster Care (Rate)`                       0.696**                 0.429***                            \n##                                            (0.300)                 (0.137)                             \n##                                                                                                        \n## Unemployment.Rate                                                                    0.00001           \n##                                                                                     (0.00004)          \n##                                                                                                        \n## Incarceration..Rateper1000.                                                          0.00002           \n##                                                                                     (0.0001)           \n##                                                                                                        \n## Relocated..Rate.                                                                    -0.00002           \n##                                                                                     (0.00004)          \n##                                                                                                        \n## Poverty..Rate.                                                                        0.001            \n##                                                                                      (0.003)           \n##                                                                                                        \n## Drug.Arrests..Rate.                                                                 0.073***  0.091*** \n##                                                                                      (0.027)   (0.021) \n##                                                                                                        \n## Sub.Abuse.Enrollment..Rate.                                                          -0.015            \n##                                                                                      (0.054)           \n##                                                                                                        \n## Adult.Psych.Beds..Rate.                                                             2.749***           \n##                                                                                      (0.995)           \n##                                                                                                        \n## lag(Sub.Abuse.Enrollment..Rate., 1)                                                            -0.089  \n##                                                                                                (0.062) \n##                                                                                                        \n## Forcible.Sex..Rate.                                                                  -0.316    -0.328  \n##                                                                                      (0.408)   (0.336) \n##                                                                                                        \n## Foster.Care..Rate.                                                                    0.239     0.255  \n##                                                                                      (0.177)   (0.159) \n##                                                                                                        \n## Constant                                    -0.004                 -0.001*           0.0004    0.001** \n##                                            (0.003)                 (0.001)           (0.001)  (0.0004) \n##                                                                                                        \n## -------------------------------------------------------------------------------------------------------\n## Observations                                  67                     201               201       134   \n## R2                                          0.719                   0.596             0.087     0.149  \n## Adjusted R2                                 0.606                   0.556             0.044     0.123  \n## Residual Std. Error                    0.001 (df = 47)         0.001 (df = 182)                        \n## F Statistic                         6.338*** (df = 19; 47) 14.940*** (df = 18; 182) 18.150**  22.625***\n## =======================================================================================================\n## Note:                                                                       *p<0.1; **p<0.05; ***p<0.01\n\n\n\n\n\nComparing Residuals Sum Squared and R2 Fit 2, Fit 3, and Fit 4 I would select Fit 3 for inference. Although Fit 4 (with lag) had a lower Residual Sum Squared value, I appreciate the completeness of Fit 3 and believe the extra variables provide a better picture of how stressors impact the homeless population in Florida.\n\nTransforming the data into panel data produces more accurate coefficients, as rather than 201 individual observations, the model considers 3 years of 67 individual observations. This results in smaller standard error. The R^2 is only considered in passing, as the goal of the study is inference not prediction.\nResearch Question:\n\nAll of Zugazaga’s effects had plausible signs demonstrating their influence on homelessness in Fit 3, but only Drug Arrests (Rate) was significant at the 0.05 level as hypothesized. This significance is a comment on the mathematical properties of the model rather than on the real-life effect of the stressors, which all are influential situations that can contribute to homelessness.\nDrug Arrests (Rate) positive slope indicated that as the rate of arrests made for drug abuse/possession is in a county increases, so does the homeless rate in the county.\n\nThis is a comment on the availability of drugs in Florida counties, and how insufficient addiction treatment can contribute to other socioeconomic issues in a community\nCriminalization does not solve the problem, it relocates it; it is likely many returning citizens will be caught in a cycle of drug abuse, incarceration, and homelessness."
  },
  {
    "objectID": "posts/shelton_final.html#reflection",
    "href": "posts/shelton_final.html#reflection",
    "title": "DACSS 601: Florida Homelessness",
    "section": "Reflection",
    "text": "Reflection\n\nConclusionsImprovements\n\n\n\n\n\n\n\n\nOn the Project Itself\n\n\n\nR-Programming\n\nMuch of the tedious cleaning and read-in work could easily be cleaned up with applications of loops and functions\n\nA fun winter project could be to “optimize” the file that created florida_1820.csv\n\nA key to becoming an efficient coder is not only a solid understanding of syntax, but being able to troubleshoot errors using online resources\n\nFamiliarity comes with frequent use and searching for solutions via online forums, books, blogs…\n\nIt is important (and difficult) to create informative, readable code\n\nComment as though you’re guiding a stranger through the script.\n\nTechnical Skills: Github, R-Programming\n\nResearch\n\nIt was a very simple regression analysis, however I feel as I painted too broad a scope with my research question.\n\nIt was very difficult to feel satisfied with study as the ambiguous question left much to be desired with regression results.\n\nBe thorough in explanation and assumptions taken when conducting research, report details\n\n\n\n\n\n\n\n\n\nWas the Research Question Answered?\n\n\n\n\nAs hypothesized, the model proved several stressors to be significant in predicting Homeless Rates across Florida\n\nThe significance of Drug Arrests (Rate) in the 3 models using all of the observations allows us to reject H0: No stressors are significant in predicting Homeless (Rate).\n\nUnfortunately, the study is unable to make a substantial comment on which stressors most increased vulnerability to Homelessness, evaluating magnitude. To do this, deeper demographic variables would need to be included, as well as improvements in controlling for stressors as a push factor in homeless migration.\n\nWe would also need to incorporate variables that aren’t strictly associated with homelessness (loss of a loved one, severe bodily injury, chronic illness…) for an improved comparison.\n\nBecause of limitations in the data, and the broad scope of the research question, the study isn’t able to make any new comments on the status of homelessness in Florida, it instead confirms the relevance of Zugazaga’s stressors to homeless life two decades later.\n\n\n\n\n\n\n\n\n\nPrediction vs Inference\n\n\n\n\nThe goal of this brief study was to make inferences regarding stressors’ impact on Homelessness (Rate) in Florida.\nIf prediction was our focus, I would use new 2021 data from FL Charts without the Homeless (Rate) column to test the efficacy of Fit 2 as a predictive tool.\n\n\n\n\n\nWhile the data is quick illustration of homelessness in Florida by county, there are improvements that could be made to both data collection and the research question itself to further the study.\n\n\n\n\n\n\nData\n\n\n\n\nUnfortunately, FL Health Charts did not provide demographic breakdown for the homeless population (Age, Sex, Race), which would drastically widen the scope of the analysis, leading to far more interesting conclusions.\nThere is only have data for a three year period; this is too small of a range to make a strong statement about the impact of homeless policy on Florida counties or how the relevance of certain stressors has changed over time. For a more in depth study I would begin with a 10 year range.\n\n\n\n\n\n\n\n\n\nResearch Question\n\n\n\n\nDemographic breakdown of stressors’ impact (Age, Sex, Race)\nModernize Zugazaga’s interviews to adjust variables for homelessness in 2022\n\nconduct interviews with groups of single men, single women, and women with children\n\nReduce noise; once controlling for county, hone in on 1 or 2 certain stressors and their accompanying variables to view their impact on a population instead of viewing a broad range of stressors as a whole\nIncluded life stressors not associated with homelessness for comparison\nExtend the question to the entire country, providing a breakdown by state\nCompare to foreign countries to contrast governments’ approaches to homelessness and leading causes of homelessness around the world."
  },
  {
    "objectID": "posts/shelton_final.html#references",
    "href": "posts/shelton_final.html#references",
    "title": "DACSS 601: Florida Homelessness",
    "section": "References",
    "text": "References\nChang, W. (2022). R Graphics Cookbook, 2nd Edition. O’Reilly Media.\nGrolemund, G., & Wickham, H. (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\nR Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.https://www.r-project.org.\nWickham, H. (2019). Advanced R, Second Edition (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9781351201315\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.\nZugazaga, C. (2004). Stressful life event experiences of homeless adults: A comparison of single men, single women, and women with children. J. Community Psychol., 32: 643-654. https://doi.org/10.1002/jcop.20025"
  },
  {
    "objectID": "posts/shelton_final.html#codebook",
    "href": "posts/shelton_final.html#codebook",
    "title": "DACSS 601: Florida Homelessness",
    "section": "Codebook",
    "text": "Codebook\n\nCounty - Florida county (67 total), divided into Region - Northwest Florida, Northeast Florida, Central Florida, and South Florida for visualizations\nPopulation - Yearly population count for county, used as denomintor of all rate variables unless specified.\nYear - Years 2018, 2019, 2020 included in this study\nHomeless (Rate) - Yearly homeless count of a county divided by county population\nUnemployment Rate - The ratio of unemployed to the civilian labor force, expressed as a percent\nMedian Inc - Median household income is the amount which divides the income distribution into two equal groups\nIncarceration Rate per 1000 - Number of incarcerated people per 1000 (within county)\nPoverty Rate - Number of people living below poverty line divided by population\nDrug Arrests (Rate) - Arrests attributed to possession or sale of illegal drugs divided by population\nRelocated (Rate) - The number of people over age 1 who lived in a different county the previous year\nSub Abuse Enrollment (Rate) - The number of beds indicates the number of adults (age 18 and over) who may receive substance abuse treatment on an in-patient basis\nAdult Psych Beds (Rate) - When adults psychiatric distress are uninsured, charged with crimes or meet state criteria for civil commitment because they are violent/dangerous to themselves or others, psychiatric beds are where they are admitted for treatment. The number of beds indicates the number of people who may potentially receive adult (age 18 and over) psychiatric care on an in-patient basis. Divided by population\nSevere Housing Problems (Rate) - The percentage of households with at least one or more of the following housing problems: lack of kitchen facilities; lack of plumbing facilities; more than 1.5 persons per room, severe cost burden (monthly housing costs including utlities exceed 50% of monthly income).\nForcible Sex (Rate) - Any sexual act or attempt involving force is classified as a forcible sex offense regardless of the age of the victim or the relationship of the victim to the offender, divided by population\nFoster Care (Rate) - Foster care provides a safe and stable environment for children when the cannot be with their parents for some reason, divided by population :::\n\n\nFootnotes\n1.) Homeless Definition\n2.) US Interagency Council on Homelessness\n3.) Explanation of variables and collection method in Codebook tab"
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html",
    "href": "posts/ShriyaSehgal_FinalProject.html",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "",
    "text": "library(tibble)\nlibrary(DT)\nlibrary(knitr)\nlibrary(tm)\nlibrary(ggrepel)\nlibrary(ggplot2)\nlibrary(wordcloud)\nlibrary(dplyr)\nlibrary(fitdistrplus)\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(formattable)\nlibrary(GGally)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html#introduction",
    "href": "posts/ShriyaSehgal_FinalProject.html#introduction",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "Introduction",
    "text": "Introduction\nThe movie industry is one of the largest commercial institutions today. It has made a significant contribution to the global economy of the countries around the world. Hundreds of thousands of movies are being released worldwide every year in the hopes that it would be a box office hit and win the hearts of the public audience. With the massive amount of data on movies available today, I believe it would be really fascinating to examine and understand the factors that lead to the success of a movie. Such analysis would be crucial for the film production houses and sponsors to invest their time and money to the kind of movies that are positively acclaimed by the critics and the public. It would also be extremely beneficial to understand the historical trends and maximize the overall profit of the movie.\nSo the main objective of the paper will be to:\n\nExplore the distribution of the movies wrt. some important feature.\nExplore the movie’s IMDb score wrt. some important features.\nUnderstand how the budget and the revenue of the movie vary wrt some important features."
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html#data",
    "href": "posts/ShriyaSehgal_FinalProject.html#data",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "Data",
    "text": "Data\nThe 5000 IMDb Movie Dataset (https://www.kaggle.com/datasets/carolzhangdc/imdb-5000-movie-dataset) has been taken from Kaggle which contains information about 5000 movies that have been released between 1960-2016.The dataset holds interesting and valuable features like the movie’s genre, IMDb score, budget, revenue, actors/director names, facebook likes, critics/user reviews and many more which would crucial in visualizing and understanding how a movie’s success is a factor of these attributes. The detailed description of the data has been added after the process of data cleaning and preparation.\n\nmovie <-  read_csv(\"_data/movie_metadata.csv\", show_col_types = FALSE)\n\nhead(movie, 20)\n\n# A tibble: 20 × 28\n   color direct…¹ num_c…² durat…³ direc…⁴ actor…⁵ actor…⁶ actor…⁷   gross genres\n   <chr> <chr>      <dbl>   <dbl>   <dbl>   <dbl> <chr>     <dbl>   <dbl> <chr> \n 1 Color James C…     723     178       0     855 Joel D…    1000  7.61e8 Actio…\n 2 Color Gore Ve…     302     169     563    1000 Orland…   40000  3.09e8 Actio…\n 3 Color Sam Men…     602     148       0     161 Rory K…   11000  2.00e8 Actio…\n 4 Color Christo…     813     164   22000   23000 Christ…   27000  4.48e8 Actio…\n 5 <NA>  Doug Wa…      NA      NA     131      NA Rob Wa…     131 NA      Docum…\n 6 Color Andrew …     462     132     475     530 Samant…     640  7.31e7 Actio…\n 7 Color Sam Rai…     392     156       0    4000 James …   24000  3.37e8 Actio…\n 8 Color Nathan …     324     100      15     284 Donna …     799  2.01e8 Adven…\n 9 Color Joss Wh…     635     141       0   19000 Robert…   26000  4.59e8 Actio…\n10 Color David Y…     375     153     282   10000 Daniel…   25000  3.02e8 Adven…\n11 Color Zack Sn…     673     183       0    2000 Lauren…   15000  3.30e8 Actio…\n12 Color Bryan S…     434     169       0     903 Marlon…   18000  2.00e8 Actio…\n13 Color Marc Fo…     403     106     395     393 Mathie…     451  1.68e8 Actio…\n14 Color Gore Ve…     313     151     563    1000 Orland…   40000  4.23e8 Actio…\n15 Color Gore Ve…     450     150     563    1000 Ruth W…   40000  8.93e7 Actio…\n16 Color Zack Sn…     733     143       0     748 Christ…   15000  2.91e8 Actio…\n17 Color Andrew …     258     150      80     201 Pierfr…   22000  1.42e8 Actio…\n18 Color Joss Wh…     703     173       0   19000 Robert…   26000  6.23e8 Actio…\n19 Color Rob Mar…     448     136     252    1000 Sam Cl…   40000  2.41e8 Actio…\n20 Color Barry S…     451     106     188     718 Michae…   10000  1.79e8 Actio…\n# … with 18 more variables: actor_1_name <chr>, movie_title <chr>,\n#   num_voted_users <dbl>, cast_total_facebook_likes <dbl>, actor_3_name <chr>,\n#   facenumber_in_poster <dbl>, plot_keywords <chr>, movie_imdb_link <chr>,\n#   num_user_for_reviews <dbl>, language <chr>, country <chr>,\n#   content_rating <chr>, budget <dbl>, title_year <dbl>,\n#   actor_2_facebook_likes <dbl>, imdb_score <dbl>, aspect_ratio <dbl>,\n#   movie_facebook_likes <dbl>, and abbreviated variable names …\n\n\n\nData Cleaning\nThe first process of cleaning the data includes removing any spurious characters (�) and white spaces from the different features like title and genre that could lead to an invalid analysis. Such character can occur in the raw data due to the internet scrapping process. The code given below will take care of the same.\n\nmovie$movie_title <- (sapply(movie$movie_title,gsub,pattern=\"\\\\�\",replacement=\"\"))\nmovie$genres <- (sapply(movie$genres,gsub,pattern=\"\\\\|\",replacement=\" \"))\n\nAfter reading that data I realized that the dataset contains duplicate movies that should be removed. The code given below will take care of the same.\n\nmovie = movie[!duplicated(movie$movie_title),]\n\nI also notices that the currency in ‘Budget’ and ‘Gross’ of all the movies are not consistent with each other. For instance, Japan had all of it’s movies information in Yen which makes it difficult to compare it’s financial success with respect to a movie in USA. To make it easier to compare the movies with each other irrespective of their origin, I manually converted the currency into USD dollars.\n\nmovie <- transform(movie, budget = ifelse(country == \"South Korea\", budget/1173.49, budget))\nmovie <- transform(movie, budget = ifelse(country == \"Japan\", budget/115.33, budget))\nmovie <- transform(movie, budget = ifelse(country == \"Turkey\", budget/3.49, budget))\nmovie <- transform(movie, budget = ifelse(country == \"Hungary\", budget/298.17, budget))\nmovie <- transform(movie, budget = ifelse(country == \"Thailand\", budget/35.67, budget))\n\nmovie <- transform(movie, gross = ifelse(country == \"South Korea\", gross/1173.49, gross))\nmovie <- transform(movie, gross = ifelse(country == \"Japan\", gross/115.33, gross))\nmovie <- transform(movie, gross = ifelse(country == \"Turkey\", gross/3.49, gross))\nmovie <- transform(movie, gross = ifelse(country == \"Hungary\", gross/298.17, gross))\nmovie <- transform(movie, gross = ifelse(country == \"Thailand\", gross/35.67, gross))\n\nTo make the analysis more comprehensible, I converted the budget and revenue into the factor of millions and Facebook likes of the director and actors into a factor of thousands.\n\n#Convert budget and revenue in millions\nmovie$budget <- movie$budget/1000000\nmovie$gross <- movie$gross/1000000\nmovie$cast_total_facebook_likes <- movie$cast_total_facebook_likes/1000\nmovie$movie_facebook_likes <- movie$movie_facebook_likes/1000\nmovie$actor_1_facebook_likes <- movie$actor_1_facebook_likes/1000\nmovie$director_facebook_likes <- movie$director_facebook_likes/1000\nmovie$actor_2_facebook_likes <- movie$actor_2_facebook_likes/1000\nmovie$actor_3_facebook_likes <- movie$actor_3_facebook_likes/1000\n\nThe Gross income earned by the movie along with the budget are 2 of the most important factors in deciding the financial success of the movie. So I mutated the data and found 2 new important features based on the budget and gross, ie. the profit earned by the movie and its rate of investment percentage.\n\nmovie <- movie %>% \n  mutate(profit = gross - budget,\n         ROI_perctentage = (profit/budget)*100)\n\n\n\nFinal Updated Data Description\nThe final data description of the updated data has been shown below along with its datatype (character/numerical)\n\ndescription.type <- lapply(movie,class)\ndescription.desc <- c(\"Tells if the Movie was colored or black/white\",\n\"DIrector's Name\",\n\"Number of critics who reviewed\",\n\"Runtime of the movie in minutes\",\n\"Number of director's facebook page likes\",\n\"Number of 3rd actor's facebook page likes\",\n\"Name of second actor\",\n\"Number of 1st actor's facebook page likes\",\n\"Movie's Gross Earning in million in $\",\n\"Movie's Genre\",\n\"First actor's Name\",\n\"Movie's Title\",\n\"Number of IMDb User's votes\",\n\"Cast member's total facebook likes\",\n\"Name of the third actor\",\n\"Number of the actor who featured in the movie poster\",\n\"Movie plot describing Keywords\",\n\"IMDb's link\",\n\"Number of User's reviews\",\n\"Movie's Language\",\n\"Country\",\n\"Content rating \",\n\"Budget in millions in $\",\n\"Release Year\",\n\"Actor 2 facebook likes\",\n\"IMDB score\",\n\"Aspect ratio of the movie\",\n\"Number of facebook likes\",\n\"Genre\",\n\"Keywords\",\n\"Profit in millions in $\",\n\"Return Of Investment in Percentage\")\ndescription.name1 <- colnames(movie)\ndata.desc <- as_data_frame(cbind(description.name1,description.type,description.desc))\ncolnames(data.desc) <- c(\"Factors\",\"DataType\",\"Factor Description\")\nlibrary(knitr)\nkable(data.desc)\n\n\n\n\n\n\n\n\n\nFactors\nDataType\nFactor Description\n\n\n\n\ncolor\ncharacter\nTells if the Movie was colored or black/white\n\n\ndirector_name\ncharacter\nDIrector’s Name\n\n\nnum_critic_for_reviews\nnumeric\nNumber of critics who reviewed\n\n\nduration\nnumeric\nRuntime of the movie in minutes\n\n\ndirector_facebook_likes\nnumeric\nNumber of director’s facebook page likes\n\n\nactor_3_facebook_likes\nnumeric\nNumber of 3rd actor’s facebook page likes\n\n\nactor_2_name\ncharacter\nName of second actor\n\n\nactor_1_facebook_likes\nnumeric\nNumber of 1st actor’s facebook page likes\n\n\ngross\nnumeric\nMovie’s Gross Earning in million in $\n\n\ngenres\ncharacter\nMovie’s Genre\n\n\nactor_1_name\ncharacter\nFirst actor’s Name\n\n\nmovie_title\ncharacter\nMovie’s Title\n\n\nnum_voted_users\nnumeric\nNumber of IMDb User’s votes\n\n\ncast_total_facebook_likes\nnumeric\nCast member’s total facebook likes\n\n\nactor_3_name\ncharacter\nName of the third actor\n\n\nfacenumber_in_poster\nnumeric\nNumber of the actor who featured in the movie poster\n\n\nplot_keywords\ncharacter\nMovie plot describing Keywords\n\n\nmovie_imdb_link\ncharacter\nIMDb’s link\n\n\nnum_user_for_reviews\nnumeric\nNumber of User’s reviews\n\n\nlanguage\ncharacter\nMovie’s Language\n\n\ncountry\ncharacter\nCountry\n\n\ncontent_rating\ncharacter\nContent rating\n\n\nbudget\nnumeric\nBudget in millions in $\n\n\ntitle_year\nnumeric\nRelease Year\n\n\nactor_2_facebook_likes\nnumeric\nActor 2 facebook likes\n\n\nimdb_score\nnumeric\nIMDB score\n\n\naspect_ratio\nnumeric\nAspect ratio of the movie\n\n\nmovie_facebook_likes\nnumeric\nNumber of facebook likes\n\n\nprofit\nnumeric\nGenre\n\n\nROI_perctentage\nnumeric\nKeywords\n\n\ncolor\ncharacter\nProfit in millions in $\n\n\ndirector_name\ncharacter\nReturn Of Investment in Percentage"
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html#exploratory-data-analysis",
    "href": "posts/ShriyaSehgal_FinalProject.html#exploratory-data-analysis",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nIn this section, I used various packages and graphical methods to explore the 5000 IMDB movie data set.\n\nDistribution of Some Important Variables\nThis section explores the frequency of the movies with respect to many important features. The profits of the movie, its imdb score as well as its run time give close to a normal distribution. We can see that both the budget and gross income earned are right skewed; majority of the movies have their budget and revenue ranging from $0 million to $20 million. On the other hand the profit, which is normally distributed around $0, portrays that almost an equal number of movies have seen financial success and failure. The graph that explores the distribution of the movie release year shows there has been a significant increase in the movies produced every year and we can see a considerable upward trend since 1990.\n\npar(mfrow=c(2,3))\nhist(movie$gross,col = 'red',breaks=500,main='Movie gross',xlab = 'Gross (in million $)',xlim = c(0,100))\nhist(movie$budget,col = 'blue',breaks=500,main='Movie budget',xlab = 'budget (in million $)',xlim = c(0,100))\nhist(movie$profit,col = 'red',breaks=200,main='Profits of the Movie',xlab = 'Profit (in  million $)', xlim = c(-200,200))\nhist(movie$title_year,col = 'blue',breaks=70,main='Movie Release Year',xlab = 'Movie Release Year',xlim = c(1960,2016))\nhist(movie$imdb_score,col = 'red',breaks=70,main='Movie IMDb Score ',xlab = 'IMDb Score',xlim = c(0,10))\nhist(movie$duration,col = 'blue',breaks=100,main='Movie Runtime',xlab = 'Movie Runtime',xlim = c(0,200))"
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html#movie-genre-analysis",
    "href": "posts/ShriyaSehgal_FinalProject.html#movie-genre-analysis",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "Movie Genre Analysis",
    "text": "Movie Genre Analysis\nThe genre of the movie is an important attribute that helps distinguish them into different categories and analyse which of them do the best in terms of financial success and user/critics reviews. In this dataset, each movie has more than one genre, so the separation of each genre in this column was required to explore all of them separately. The ‘TM’ package was used for this purpose. I converted the genre variable to corpus and further analysed them. A word cloud is a graphical representation for the word frequency that gives highest importance to the words/titles that appear more frequently in a text. This is a great way to understand which are the top movie genre, at a cursory glance. We can see that Drama, Comedy and Thriller are the top movie genres in the word cloud.\n\ngenre <- Corpus(VectorSource(movie$genres))\ngenre_dtm <- DocumentTermMatrix(genre)\ngenre_freq <- colSums(as.matrix(genre_dtm))\nfreq <- sort(colSums(as.matrix(genre_dtm)), decreasing=TRUE)\ngenre_wf <- data.frame(word=names(genre_freq), freq=genre_freq)\npal2 <- brewer.pal(10,\"Dark2\")\nwordcloud(genre_wf$word,genre_wf$freq,random.order=FALSE,\n          rot.per=.15, colors=pal2,scale=c(5,.9))"
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html#movie-imdbs-analysis-on-various-factors",
    "href": "posts/ShriyaSehgal_FinalProject.html#movie-imdbs-analysis-on-various-factors",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "Movie IMDB’s Analysis on Various Factors",
    "text": "Movie IMDB’s Analysis on Various Factors\n\nRelationship of Genre with the IMDB Score\nThe next graph explores how the IMDB Score varies for all the genres. For this I have created a new dataframe that contains all the genres as different columns along with its final imdb score. As we go through each movie’s genre, we keep adding 1 to the respective genre column that is present in the main movie_dataset attribute. Once that is done I calculated the mean of all the imdb scores of each genre. We can see that almost all the genre’s movies have similar Imdb rating and there is no significant trend that shows that one kind of genre does better than the other which is a very interesting to see.\n\ngenres.df <- as.data.frame(movie[,c(\"genres\", \"imdb_score\")])\n\ngenres.df$Action <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Action\") 1 else 0)\ngenres.df$Adventure <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Adventure\") 1 else 0)\ngenres.df$Animation <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Animation\") 1 else 0)\ngenres.df$Biography <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Biography\") 1 else 0)\ngenres.df$Comedy <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Comedy\") 1 else 0)\ngenres.df$Crime <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Crime\") 1 else 0)\ngenres.df$Documentary <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Documentary\") 1 else 0)\ngenres.df$Drama <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Drama\") 1 else 0)\ngenres.df$Family <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Family\") 1 else 0)\ngenres.df$Fantasy <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Fantasy\") 1 else 0)\ngenres.df$`Film-Noir` <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Film-Noir\") 1 else 0)\ngenres.df$History <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"History\") 1 else 0)\ngenres.df$Horror <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Horror\") 1 else 0)\ngenres.df$Musical <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Musical\") 1 else 0)\ngenres.df$Mystery <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Mystery\") 1 else 0)\ngenres.df$News <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"News\") 1 else 0)\ngenres.df$Romance <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Romance\") 1 else 0)\ngenres.df$`Sci-Fi` <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Sci-Fi\") 1 else 0)\ngenres.df$Short <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Short\") 1 else 0)\ngenres.df$Sport <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Sport\") 1 else 0)\ngenres.df$Thriller <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Thriller\") 1 else 0)\ngenres.df$War <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"War\") 1 else 0)\ngenres.df$Western <- sapply(1:length(genres.df$genres), function(x) if (genres.df[x,1] %like% \"Western\") 1 else 0)\n\nmeans <- rep(0,23)\nfor (i in 1:23) {\n  means[i] <- mean(genres.df$imdb_score[genres.df[i+2]==1])\n}\nbarplot(means, main = \"Average imdb scores for different genres\")\n\n\n\n\n\n\nActor’s with the Highest Average IMDB Score\nThe table format shows the top 20 actors which give the highest IMDB rating. Actors are what give life to the characters and drive a movie to its success or failure. Hence, this is an important factor that allows the production house to hire the actors who do generally well and give a highly acclaimed movie. This table shows that Krystyna Janda’s movies average IMDB rating is 9.10. This of course in not a complete analysis because we should also consider the number of movies done by each actor, but it still shows an approximate trend\n\nmovie %>%\n  group_by(actor_1_name) %>%\n  summarise(avg_imdb = mean(imdb_score)) %>%\n  arrange(desc(avg_imdb)) %>%\n  top_n(20, avg_imdb) %>%\n  formattable(list(avg_imdb = color_bar(\"orange\")), align = 'l')\n\n\n\n\n\n\nactor_1_name\n\n\navg_imdb\n\n\n\n\n\n\nKrystyna Janda\n\n\n9.10\n\n\n\n\nJack Warden\n\n\n8.90\n\n\n\n\nRob McElhenney\n\n\n8.80\n\n\n\n\nAbigail Evans\n\n\n8.70\n\n\n\n\nElina Abai Kyzy\n\n\n8.70\n\n\n\n\nJackie Gleason\n\n\n8.70\n\n\n\n\nKimberley Crossman\n\n\n8.70\n\n\n\n\nMaria Pia Calzone\n\n\n8.70\n\n\n\n\nTakashi Shimura\n\n\n8.70\n\n\n\n\nBunta Sugawara\n\n\n8.60\n\n\n\n\nClaudia Cardinale\n\n\n8.60\n\n\n\n\nDavid Raizor\n\n\n8.60\n\n\n\n\nDonna Reed\n\n\n8.60\n\n\n\n\nPaulette Goddard\n\n\n8.60\n\n\n\n\nRuth Wilson\n\n\n8.60\n\n\n\n\nScatman Crothers\n\n\n8.55\n\n\n\n\nBahare Seddiqi\n\n\n8.50\n\n\n\n\nCollin Alfredo St. Dic\n\n\n8.50\n\n\n\n\nDebi Mazar\n\n\n8.50\n\n\n\n\nEmilia Fox\n\n\n8.50\n\n\n\n\nNimrat Kaur\n\n\n8.50\n\n\n\n\nTobias Menzies\n\n\n8.50\n\n\n\n\n\n\n\n\n\nDirector’s with the Highest Average IMDB Score\nSimilar to the above table, this one shows the top 20 director which gives the highest IMDB ratings. This table shows that John Blanchard’s movies have the highest average rating of 9.5.\n\nmovie %>%\n  group_by(director_name) %>%\n  summarise(avg_imdb = mean(imdb_score)) %>%\n  arrange(desc(avg_imdb)) %>%\n  top_n(20, avg_imdb) %>%\n  formattable(list(avg_imdb = color_bar(\"orange\")), align = 'l')\n\n\n\n\n\n\ndirector_name\n\n\navg_imdb\n\n\n\n\n\n\nJohn Blanchard\n\n\n9.500\n\n\n\n\nCary Bell\n\n\n8.700\n\n\n\n\nMitchell Altieri\n\n\n8.700\n\n\n\n\nSadyk Sher-Niyaz\n\n\n8.700\n\n\n\n\nCharles Chaplin\n\n\n8.600\n\n\n\n\nMike Mayhall\n\n\n8.600\n\n\n\n\nDamien Chazelle\n\n\n8.500\n\n\n\n\nMajid Majidi\n\n\n8.500\n\n\n\n\nRaja Menon\n\n\n8.500\n\n\n\n\nRon Fricke\n\n\n8.500\n\n\n\n\nSergio Leone\n\n\n8.475\n\n\n\n\nChristopher Nolan\n\n\n8.425\n\n\n\n\nAsghar Farhadi\n\n\n8.400\n\n\n\n\nBill Melendez\n\n\n8.400\n\n\n\n\nCatherine Owens\n\n\n8.400\n\n\n\n\nJay Oliva\n\n\n8.400\n\n\n\n\nMarius A. Markevicius\n\n\n8.400\n\n\n\n\nMoustapha Akkad\n\n\n8.400\n\n\n\n\nRakeysh Omprakash Mehra\n\n\n8.400\n\n\n\n\nRichard Marquand\n\n\n8.400\n\n\n\n\nRobert Mulligan\n\n\n8.400\n\n\n\n\nS.S. Rajamouli\n\n\n8.400\n\n\n\n\n\n\n\n\n\nTop 40 movies based on their profits\nThis particular graph shows profit earned by the top 40 movies that. We can see with this plot that the trend is almost linear showing that usually the movies with high budget tend to be financially more successful and earn more profit.\n\nmovie %>%\n  arrange(desc(profit)) %>%\n  top_n(40, profit) %>%\n  filter(title_year %in% c(2000:2016)) %>%\n  ggplot(aes(x=budget, y=profit)) +\n  geom_point() +\n  geom_smooth() + \n  geom_text_repel(aes(label=movie_title)) +\n  labs(x = \"Budget $million\", y = \"Profit $million\", title = \"Top 40 Profitable Movies\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nAverage Movie Budget by Countries\nThis part of the analysis explores how the budget of the movies vary with each country. Surprisingly, India spends the most amount of money on their movies. It is also interesting to see that the trend is not very linear, i.e. the top countries have a significantly higher movie budget than the countries that have lower average budget of the movies.\n\ncountry_movie <- movie %>%\n  subset(country != \"\") %>%\n  subset(country != \"New Line\") %>%\n  group_by(country) %>%\n  summarise(count=n(),\n            avg_budget = mean(budget,na.rm=\"true\"),\n            avg_gross = mean(gross,na.rm=\"true\"))\ncountry_with_multiple_movies <- subset(country_movie,count>1)[1]\n\nggplot(country_movie[complete.cases(country_movie), ],\n       aes(x=reorder(country,-avg_budget),avg_budget))+\n  geom_bar(stat = \"identity\",fill = 'red')+\n  theme(axis.text.x=element_text(angle=90, hjust=1))+\n  ylab(\"Average Movie Budget in Million $\")+\n  xlab(\"\")+\n  ggtitle(\"Average Movie Budget by Country\")\n\n\n\n\n\n\nAverage Movie Gross by Country\nThis part of the analysis explores how the revenue earned by the movies vary with each country in millions. We can see that the top 5 countries have a considerably higher revenue(more than $50 Million ) than the rest of the countries.\n\nggplot(country_movie[complete.cases(country_movie), ],\n       aes(x=reorder(country,-avg_gross),avg_gross))+\n  geom_bar(stat = \"identity\",fill = 'blue')+\n  theme(axis.text.x=element_text(angle=90, hjust=1))+\n  ylab(\"Average Movie Gross in Million $\")+\n  xlab(\"\")+\n  ggtitle(\"Average Movie Gross by Country\")\n\n\n\n\n\n\nAverge Movie Budget by Language\nSimilar to the above 2 plots, these 2 graph explores how the budget spent and the revenue earned by the movies vary with the language of the movie produced.\n\nlanguage_movie <- movie %>%\n  group_by(language) %>%\n  summarise(count=n(),\n            avg_budget = mean(budget,na.rm=\"true\"),\n            avg_gross = mean(gross,na.rm=\"true\"))\ndirector_with_multiple_movies <- subset(language_movie,count>1)[1]\n\nggplot(language_movie[complete.cases(language_movie), ],\n       aes(x=reorder(language,-avg_budget),avg_budget))+\n  geom_bar(stat = \"identity\",fill = 'red')+\n  theme(axis.text.x=element_text(angle=90, hjust=1))+\n  ylab(\"Average Movie Budget in Million\")+\n  xlab(\"\")+\n  ggtitle(\"Average Movie Budget by the Language\")"
  },
  {
    "objectID": "posts/ShriyaSehgal_FinalProject.html#average-movie-gross-by-language",
    "href": "posts/ShriyaSehgal_FinalProject.html#average-movie-gross-by-language",
    "title": "Movie Exploratoy Analysis: Final Project",
    "section": "Average Movie Gross by Language",
    "text": "Average Movie Gross by Language\n\nggplot(language_movie[complete.cases(language_movie), ],\n       aes(x=reorder(language,-avg_gross),avg_gross))+\n  geom_bar(stat = \"identity\",fill = 'blue')+\n  theme(axis.text.x=element_text(angle=90, hjust=1))+\n  ylab(\"Average Movie Gross in Million\")+\n  xlab(\"\")+\n  ggtitle(\"Average Movie Gross by the Language\")\n\n\n\n\n\nFinal Correlation Map\nThe last plot is a heatmap that shows how various attributes of a movie are correlated to each other. We can see that there is a very high correlation between the gross income and the profit earned by the movies with a factor of 0.74. There is also a high correlation between reviews and movie FB likes.\n\nggcorr(movie, label = TRUE, label_round = 2, label_size = 2, size =2 , hjust = .85) +\n  ggtitle(\"Correlation Heatmap\") +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html",
    "href": "posts/theresaSzczepanski_final.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(hrbrthemes)\nlibrary(viridis)\nlibrary(ggpubr)\nlibrary(purrr)\nlibrary(plotly)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#introduction",
    "href": "posts/theresaSzczepanski_final.html#introduction",
    "title": "Final Project",
    "section": "Introduction",
    "text": "Introduction\nMassachusetts Comprehensive Assessment System (MCAS) tests were introduced as part of the Massachusetts Education Reform Act in 1993 with the goal of providing all students with the skills and knowledge to thrive in a “complex and changing society” (Papay et. al, 2020 pp, 1). The MCAS tests are a significant tool for educational equity. Scores on the Grade 10 Math MCAS test “predict longer-term educational attainments and labor market success, above and beyond typical markers of student advantage. For example, among demographically similar students who attended the same high school and have the same level of ultimate educational attainment, those with higher MCAS mathematics scores go on to have much higher average earnings than those with lower scores.” (Papay et. al, 2020 pp 7-10)\nIn this report, I will analyze the Spring 2022 MCAS Results for students completing the High School Introductory Physics MCAS at Rising Tide Charter Public School.\nThe MCAS_2022 data frame contains performance results from 495 students from Rising Tide on the Spring 2022 Massachusetts Comprehensive Assessment System (MCAS) tests.\nFor each student, there are values reported for 256 different variables which consist of information from four broad categories\n\nDemographic characteristics of the students themselves (e.g., race, gender, date of birth, town, grade level, years in school, years in Massachusetts, and low income, title1, IEP, 504, and EL status ).\nKey assessment features including subject, test format, and accommodations provided\nPerformance metrics: This includes a student’s score on individual item strands, e.g.,sitem1-sitem42.\n\nSee the MCAS_2022 data frame summary and codebook in the appendix for further details.\nThe second data set, SG9_Item, is \\(42 \\times 9\\) and consists of 9 variables with information pertaining to the 42 questions on the 2022 HS Introductory Physics Item Report. The variables can be broken down into 2 categories:\nDetails about the content of a given test item:\nThis includes the content Reporting Category (MF (motion and forces) WA (waves), and EN (energy)), the Standard from the 2016 STE Massachusetts Curriculum Framework, the Item Description providing the details of what specifically was asked of students, and the points available for a given question, item Possible Points.\nSummary Performance Metrics:\n\nFor each item, the state reports the percentage of points earned by students at Rising Tide, RT Percent Points, the percentage of available points earned by students in the state, State Percent Points, and the difference between the percentage of points earned by Rising Tide students and the percentage of points earned by students in the state, RT-State Diff.\nLastly, SG9_CU306Dis and SG9_CU306NonDis are \\(3 \\times 5\\) dataframes consisting of summary performance data by Reporting Category for students with disabilities and without disabilities; most importantly including RT Percent Points and State Percent Pointsby disability status.\n\nWhen considering our student performance data, we hope to address the following broad questions:\n\n\n\nWhat adjustments (if any) should be made at the Tier 1 level, i.e., curricular adjustments for all students in the General Education setting?\nWhat would be the most beneficial areas of focus for a targeted intervention course for students struggling to meet or exceed performance expectations?\nAre there notable differences in student performance for students with and without disabilities?"
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#function-library",
    "href": "posts/theresaSzczepanski_final.html#function-library",
    "title": "Final Project",
    "section": "Function Library",
    "text": "Function Library\nTo read in, tidy, and join our data frames for each content area we will use functions. In this library. I am also drafting some functions that I would use to scale up this project. There is still work to be done here.\n\nItem analysis Read in FunctionFunction to Read in MCAS Preliminary ResultsFunctions for Item Report/Exam StructureFunction to Join Student Performance to Item ReportFunction Student Performance by KeyWord\n\n\n\n\nCode\n#Item analysis Read in Function: Input: sheet_name, subject, grade; return: student item report for a given grade level and subject.\n\n#subject must be: \"math\", \"ela\", or \"science\"\nread_item<-function(sheet_name, subject, grade){\n  subject_item<-case_when(\n    subject == \"science\"~\"sitem\",\n    subject == \"math\"~\"mitem\",\n    subject == \"ela\"~\"eitem\"\n  )\n  if(subject == \"science\"){\n  read_excel(\"_data/2022MCASDepartmentalAnalysis.xlsx\", sheet = sheet_name, \n             skip = 1, col_names= c(subject_item, \"Type\", \"Reporting Category\", \"Standard\", \"item Desc\", \"delete\", \"item Possible Points\",\"RT Percent Points\", \"State Percent Points\", \"RT-State Diff\")) %>%\n                  select(!contains(\"delete\"))%>%\n                  filter(!str_detect(sitem,\"Legend|legend\"))%>%\n    mutate(sitem= as.character(sitem))%>%\n    separate(c(1), c(\"sitem\", \"delete\"))%>%\n  select(!contains(\"delete\"))%>%\n   mutate(sitem =\n            str_c(subject_item, sitem))\n   }\n  else if(subject == \"math\" && grade < 10){\n     read_excel(\"_data/2022MCASDepartmentalAnalysis.xlsx\", sheet = sheet_name, \n              skip = 1, col_names= c(subject_item, \"Type\", \"Reporting Category\", \"Standard\", \"item Desc\", \"delete\", \"item Possible Points\",\"delete\",\"RT Percent Points\", \"State Percent Points\", \"RT-State Diff\"))%>%\n                   select(!contains(\"delete\"))%>%\n                   filter(!str_detect(mitem,\"Legend|legend\"))%>%\n     mutate(mitem = as.character(mitem))%>%\n     separate(c(1), c(\"mitem\", \"delete\"))%>%\n   select(!contains(\"delete\"))%>%\n    mutate(mitem =\n             str_c(subject_item, mitem))\n     \n  }\n  else if(subject == \"math\" && grade == 10){\n     read_excel(\"_data/2022MCASDepartmentalAnalysis.xlsx\", sheet = sheet_name, \n              skip = 1, col_names= c(subject_item, \"Type\", \"Reporting Category\", \"Standard\", \"item Desc\", \"delete\", \"item Possible Points\",\"RT Percent Points\", \"State Percent Points\", \"RT-State Diff\"))%>%\n                   select(!contains(\"delete\"))%>%\n                   filter(!str_detect(mitem,\"Legend|legend\"))%>%\n     mutate(mitem = as.character(mitem))%>%\n     separate(c(1), c(\"mitem\", \"delete\"))%>%\n   select(!contains(\"delete\"))%>%\n    mutate(mitem =\n             str_c(subject_item, mitem))\n     \n   }\n    \n}\n\n\n\n\n\n\nCode\n## MCAS Preliminary Results Read In\n## Input file_path where the results csv file is stored, and the \"year\" the exam was administered\nread_MCAS_Prelim<-function(file_path, year){read_csv(file_path,\n                    skip=1)%>%\n  select(-c(\"sprp_dis\", \"sprp_sch\", \"sprp_dis_name\", \"sprp_sch_name\", \"sprp_orgtype\",\n  \"schtype\", \"testschoolname\", \"yrsindis\", \"conenr_dis\"))%>%\n\n#Recode all nominal variables as characters\n  \n \n  mutate(testschoolcode = as.character(testschoolcode))%>%\n  #Include this line when using the non-private dataframe\n  # mutate(sasid = as.character(sasid))%>%\n  mutate(highneeds = as.character(highneeds))%>%\n  mutate(lowincome = as.character(lowincome))%>%\n  mutate(title1 = as.character(title1))%>%\n  mutate(ever_EL = as.character(ever_EL))%>%\n  mutate(EL = as.character(EL))%>%\n  mutate(EL_FormerEL = as.character(EL_FormerEL))%>%\n  mutate(FormerEL = as.character(FormerEL))%>%\n  mutate(ELfirstyear = as.character(ELfirstyear))%>%\n  mutate(IEP = as.character(IEP))%>%\n  mutate(plan504 = as.character(plan504))%>%\n  mutate(firstlanguage = as.character(firstlanguage))%>%\n  mutate(nature0fdis = as.character(natureofdis))%>%\n  mutate(spedplacement = as.character(spedplacement))%>%\n  mutate(town = as.character(town))%>%\n  mutate(ssubject = as.character(ssubject))%>%\n\n\n#Recode all ordinal variable as factors\n\n  mutate(grade = as.factor(grade))%>%\n  mutate(levelofneed = as.factor(levelofneed))%>%\n  mutate(eperf2 = recode_factor(eperf2,\n                                   \"E\" = \"Exceeding\",\n                                   \"M\" = \"Meeting\",\n                                   \"PM\" = \"Partially Meeting\",\n                                   \"NM\"= \"Not Meeting\",\n                                  .ordered = TRUE))%>%\n  mutate(eperflev = recode_factor(eperflev,\n                                   \"E\" = \"E\",\n                                   \"M\" = \"M\",\n                                   \"PM\" = \"PM\",\n                                   \"NM\"= \"NM\",\n                                   \"DNT\" = \"DNT\",\n                                   \"ABS\" = \"ABS\",\n                                  .ordered = TRUE))%>%\n    mutate(mperf2 = recode_factor(mperf2,\n                                   \"E\" = \"Exceeding\",\n                                   \"M\" = \"Meeting\",\n                                   \"PM\" = \"Partially Meeting\",\n                                   \"NM\"= \"Not Meeting\",\n                                  .ordered = TRUE))%>%\n  mutate(mperflev = recode_factor(mperflev,\n                                   \"E\" = \"E\",\n                                   \"M\" = \"M\",\n                                   \"PM\" = \"PM\",\n                                   \"NM\"= \"NM\",\n                                   \"INV\" = \"INV\",\n                                   \"ABS\" = \"ABS\",\n                                  .ordered = TRUE))%>%\n\n  # The science variables contain a mixture of legacy performance levels and\n  # next generation performance levels which needs to be addressed in the ordering\n  # of these factors.\n  mutate(sperf2 = recode_factor(sperflev,\n                                   \"E\" = \"Exceeding\",\n                                   \"M\" = \"Meeting\",\n                                   \"PM\" = \"Partially Meeting\",\n                                   \"NM\"= \"Not Meeting\",\n                                  .ordered = TRUE))%>%\n  mutate(sperflev = recode_factor(sperf2,\n                                   \"E\" = \"E\",\n                                   \"M\" = \"M\",\n                                   \"PM\" = \"PM\",\n                                   \"NM\"= \"NM\",\n                                   \"INV\" = \"INV\",\n                                   \"ABS\" = \"ABS\",\n                                  .ordered = TRUE))%>%\n  #recode DOB using lubridate\n  mutate(dob = mdy(dob,\n  quiet = FALSE,\n  tz = NULL,\n  locale = Sys.getlocale(\"LC_TIME\"),\n  truncated = 0\n))%>%\n  mutate(IEP = case_when(\n   IEP == \"1\" ~ \"Disabled\",\n    IEP == \"0\" ~ \"NonDisabled\"\n  ))%>%\n    mutate(year = year)\n}\n\n\n\n\n\n\nCode\n##Function for number of items table and graph\n\n##ToDo Should a Function Produce Table and Graph?\n##ToDo, Adjust the caption for test and year?\n##ToDo, the Data Files need to be Updated to Include ELA reports\n\nSubject_Cat_Total<-function(subject, subjectItemDF){\n    if(subject == \"science\"){subjectItemDF%>%\n        select(`sitem`, `item Possible Points`, `Reporting Category`)%>%\n        group_by(`Reporting Category`)%>%\n        summarise(available_points = sum(`item Possible Points`, na.rm=TRUE))%>%\n        mutate(percent_available_points = available_points/(sum(available_points, na.rm = TRUE)))%>%\n        ggplot(aes(x='',fill = `Reporting Category`, y = `available_points`)) +\n    geom_bar(position=\"fill\", stat = \"identity\") + coord_flip()+\n labs(subtitle =\"All Students\" ,\n       y = \"% Points Available\",\n       x= \"Reporting Category\",\n       title = \"Percentage of Exam Points Available by Reporting Category\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))\n    } \n  else if (subject == \"math\"){subjectItemDF%>%\n      select(`mitem`, `item Possible Points`, `Reporting Category`)%>%\n      group_by(`Reporting Category`)%>%\n      summarise(available_points = sum(`item Possible Points`, na.rm=TRUE))%>%\n      mutate(percent_available_points = available_points/(sum(available_points, na.rm = TRUE)))%>%\n      ggplot(aes(x='',fill = `Reporting Category`, y = `available_points`)) +\n    geom_bar(position=\"fill\", stat = \"identity\") + coord_flip()+\n labs(subtitle =\"All Students\" ,\n       y = \"% Points Available\",\n       x= \"Reporting Category\",\n       title = \"Percentage of Exam Points Available by Reporting Category\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))\n  \n} else if (subject == \"ELA\"){subjectItemDF%>%\n    select(`eitem`, `item Possible Points`, `Reporting Category`)%>%\n    group_by(`Reporting Category`)%>%\n    summarise(available_points = sum(`item Possible Points`, na.rm=TRUE))%>%\n    mutate(percent_available_points = available_points/(sum(available_points, na.rm = TRUE)))%>%\n    ggplot(aes(x='',fill = `Reporting Category`, y = `available_points`)) +\n    geom_bar(position=\"fill\", stat = \"identity\") + coord_flip()+\n labs(subtitle =\"All Students\" ,\n       y = \"% Points Available\",\n       x= \"Reporting Category\",\n       title = \"Percentage of Exam Points Available by Reporting Category\",\n      caption = \"2022 ELA MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))\n}\n    }\n# testDF<-read_item(\"SG9Physics\", \"science\")\n# #view(testDF)\n# Subject_Cat_Total(\"science\", testDF)\n\n\n\n\n\n\nCode\nStudent_Item<-function(subject, gradeLevel, subjectItemDF, studentPerfDF){\n if(subject == \"science\"){\n   select( studentPerfDF, contains(\"sitem\"), gender, grade, yrsinsch,\n                             race, IEP, `plan504`, sattempt, sperflev, sperf2, sscaleds)%>%\n                            filter((grade == gradeLevel) & sattempt != \"N\")%>%\n     pivot_longer(contains(\"sitem\"), names_to = \"sitem\", values_to = \"sitem_score\")%>%\n     left_join(subjectItemDF, \"sitem\")\n }\nif(subject == \"math\"){\n   select( studentPerfDF, contains(\"mitem\"), gender, grade, yrsinsch,\n                             race, IEP, `plan504`, mattempt, mperflev, mperf2, mscaleds)%>%\n                            filter((grade == gradeLevel) & mattempt != \"N\")%>%\n     pivot_longer(contains(\"mitem\"), names_to = \"mitem\", values_to = \"mitem_score\")%>%\n     left_join(subjectItemDF, \"mitem\")\n}\n  ####ToDo, update departmental analysis data to include ELA item reports\n  if(subject == \"ela\"){\n   select( studentPerfDF, contains(\"eitem\"), gender, grade, yrsinsch,\n                             race, IEP, `plan504`, eattempt, eperflev, eperf2, escaleds)%>%\n                            filter((grade == gradeLevel) & eattempt != \"N\")%>%\n     pivot_longer(contains(\"eitem\"), names_to = \"eitem\", values_to = \"eitem_score\")%>%\n     left_join(subjectItemDF, \"eitem\")\n}\n}\n# TestMCAS_2022<-read_MCAS_Prelim(\"_data/PrivateSpring2022_MCAS_full_preliminary_results_04830305.csv\",2022)\n# SG5_Item<-read_item(\"SG5\", \"science\", 5)\n# SG5_Student_Item<-Student_Item(\"science\", 5, SG5_Item, TestMCAS_2022)\n# SG5_Student_Item\n# TestMCAS_2022<-read_MCAS_Prelim(\"_data/PrivateSpring2022_MCAS_full_preliminary_results_04830305.csv\",2022)\n# MG5_Item<-read_item(\"MG5\", \"math\", 5)\n# MG5_Student_Item<-Student_Item(\"math\", 5, MG5_Item, TestMCAS_2022)\n# MG5_Student_Item\n\n\n\n\n\n\nCode\nkeyWord<-function(subjectItemDF, subject, keyWord){\n  keyWord<-str_to_lower(keyWord)\n  keyWordFirst<-str_to_upper(str_sub(keyWord, 1L,1L))\n  keyWordEnd<-str_sub(keyWord, 1L+1, -1L)\n  keyWordCap<-str_c(keyWordFirst, keyWordEnd)\n  if (subject == \"science\"){\n  select(subjectItemDF,`sitem`, `item Desc`,`item Possible Points`, `Reporting Category`, `State Percent Points`, `RT-State Diff`)%>%\n   mutate( key_word = case_when(\n     !(str_detect(`item Desc`, keyWord)|str_detect(`item Desc`,keyWordCap)) ~ str_c(\"Non-\", keyWordCap),\n    str_detect(`item Desc`, keyWord)|str_detect(`item Desc`,keyWordCap) ~ keyWordCap))\n  }\n   else if (subject == \"math\"){\n  select(subjectItemDF, `mitem`, `item Desc`,`item Possible Points`, `Reporting Category`, `State Percent Points`, `RT-State Diff`)%>%\n  mutate( key_word = case_when(\n     !(str_detect(`item Desc`, keyWord)|str_detect(`item Desc`,keyWordCap)) ~ str_c(\"Non-\", keyWordCap),\n    str_detect(`item Desc`, keyWord)|str_detect(`item Desc`,keyWordCap) ~ keyWordCap))\n  }\n}\n#view(SG9_Calc)\n# MG8_Item<-read_item(\"MG8\", \"math\", 8)\n# MG5_Item\n# MG8_Describe<-keyWord(MG8_Item, \"math\", \"determine\")\n# MG8_Describe\n# SG8_Item<-read_item(\"SG8\", \"science\", 8)\n# SG8_Item\n# SG8_Calc<-keyWord(SG8_Item, \"science\", \"calculate\")\n# SG8_Calc"
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#data-read-in-tidy",
    "href": "posts/theresaSzczepanski_final.html#data-read-in-tidy",
    "title": "Final Project",
    "section": "Data Read-In Tidy",
    "text": "Data Read-In Tidy\n\nRead in Student Performance and Item Description DataWorkflow SummaryTidy DataJoin and Sanity Checks\n\n\n\n\nCode\n#Filter, rename variables, and mutate values of variables on read-in\n\nMCAS_2022<-read_MCAS_Prelim(\"_data/PrivateSpring2022_MCAS_full_preliminary_results_04830305.csv\",2022)\n\n#view(MCAS_2022)\nhead(MCAS_2022)\n\n\n\n\n  \n\n\n\nIntroductory Physics, SG9_Item Read-In\n\n\nCode\n# G9 Science Item analysis\n\n\nSG9_Item<-read_item(\"SG9Physics\", \"science\")%>%\n  mutate(`Reporting Category` = case_when(\n    `Reporting Category` == \"EN\" ~ \"Energy\",\n    `Reporting Category` == \"MF\" ~ \"Motion and Forces\",\n    `Reporting Category` == \"WA\" ~ \"Waves\"\n    ))\n                   \nhead(SG9_Item)\n\n\n\n\n  \n\n\n\nCode\n#view(SG9_Item)\n\n\nIntroductory Physics, SG9_CU306Dis Read-In\n\n\nCode\nSG9_CU306Dis<-read_excel(\"_data/MCAS CU306 2022/CU306MCAS2022PhysicsGrade9ByDisability.xlsm\", \n  sheet = \"Disabled Students\", \n   col_names = c(\"Reporting Category\", \"Possible Points\", \"RT%Points\",\n                \"State%Points\", \"RT-State Diff\"))%>%\n  filter(`Reporting Category` == \"Energy\"|`Reporting Category`== \"Motion, Forces, and Interactions\"| `Reporting Category` == \"Waves\" )\n\n#view(SG9_CU306Dis)\n\n\nSG9_CU306Dis\n\n\n\n\n  \n\n\n\nIntroductory Physics, SG9_CU306NonDis Read-In\n\n\nCode\nSG9_CU306NonDis<-read_excel(\"_data/MCAS CU306 2022/CU306MCAS2022PhysicsGrade9ByDisability.xlsm\", \n  sheet = \"Non-Disabled Students\", \n   col_names = c(\"Reporting Category\", \"Possible Points\", \"RT%Points\",\n                \"State%Points\", \"RT-State Diff\"))%>%\n  filter(`Reporting Category` == \"Energy\"|`Reporting Category`== \"Motion, Forces, and Interactions\"| `Reporting Category` == \"Waves\" )\n\nSG9_CU306NonDis\n\n\n\n\n  \n\n\n\nCode\n#view(SG9_CU306NonDis)\n\n\n\n\nAfter examining the summary of MCAS_2022 (see appendix), I chose to\nFilter:\n\nSchoolID : There are several variables that identify our school, I removed all but one, testschoolcode.\nStudentPrivacy: I left the sasid variable which is a student identifier number, but eliminated all values corresponding to students’ names.\ndis: We are a charter school within our own unique district, therefore any “district level” data is identical to our “school level” data.\n\nRename\nI currently have not renamed variables, but there are some trends to note:\n\nan e before most ELA MCAS student item performance metric variables\nan m before most Math MCAS student item performance metric variables\nan s before most Science MCAS student item performance metric variables\n\nMutate\nI left as doubles\n\nvariables that measured scores on specific MCAS items e.g., mitem1\nvariables that measured student growth percentiles (sgp)\nvariables that counted a student’s years in the school system or state.\n\nRecode to char\n\nvariables that are nominal but have numeric values, e.g., town\n\nRefactor as ord\n\nvariables that are ordinal, e.g., mperflev.\n\nRecode to date\n\ndob using lubridate.\n\n\n\nI am interested in analyzing the 9th Grade Science Performance. To do this, I will select a subset of our MCAS_2022 data frame which includes:\n\n9th Grade students who took the Introductory Physics test\nScores on the 42 Science Items\npoints available on the\nPerformance level on the test sperflev.\nDemographic characteristics of the students.\n\n\n\nCode\nSG9_MCAS_2022 <- select(MCAS_2022, contains(\"sitem\"), gender, grade, yrsinsch,\n                             race, IEP, `plan504`, sattempt, sperflev, sperf2, sscaleds)%>%\n                            filter((grade == 9) & sattempt != \"N\")\n\nSG9_MCAS_2022<-select(SG9_MCAS_2022, !(contains(\"43\")|contains(\"44\")|contains(\"45\")))\n\n\n#view(SG9_MCAS_2022)\nhead(SG9_MCAS_2022)\n\n\n\n\n  \n\n\n\nWhen I compared this data frame to the State reported analysis, the state analysis only contains 68 students. Notably, my data frame has 69 entries while the state is reporting data on only 68 students. I will have to investigate this further.\nSince I will join this data frame with the SG9_Item, using sitem as the key, I need to pivot this data set longer.\n\n\nCode\nSG9_MCAS_2022<- pivot_longer(SG9_MCAS_2022, contains(\"sitem\"), names_to = \"sitem\", values_to = \"sitem_score\")\n\n#view(SG9_MCAS_2022)\nhead(SG9_MCAS_2022)\n\n\n\n\n  \n\n\n\nAs expected, we now have 42 X 69 = 2898 rows.\n\n\nNow, we should be ready to join our data sets using sitem as the key. We should have a 2,898 by (10 + 8) = 2,898 by 18 data frame. We will also check our raw data against the performance data reported by the state in the item report by calculating percent_earned by Rising Tide students and comparing it to the figure RT Percent Points and storing the difference in earned_diff\n\n\nCode\nSG9_StudentItem <- SG9_MCAS_2022 %>%\n  left_join(SG9_Item, \"sitem\")\n\n\nhead(SG9_StudentItem)\n\n\n\n\n  \n\n\n\nCode\nSG9_StudentItem\n\n\n\n\n  \n\n\n\nCode\nSG9_StudentItem%>%\n   group_by(sitem)%>%\n   summarise(percent_earned = round(sum(sitem_score, na.rm=TRUE)/sum(`item Possible Points`, na.rm=TRUE),2) )%>%\n   left_join(SG9_Item, \"sitem\")%>%\n   mutate(earned_diff = percent_earned-`RT Percent Points`)\n\n\n\n\n  \n\n\n\nAs expected, we now have a 2,898 X 18 data frame and the earned_diff values all round to 0."
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#g9-science-performance-analysis",
    "href": "posts/theresaSzczepanski_final.html#g9-science-performance-analysis",
    "title": "Final Project",
    "section": "G9 Science Performance Analysis",
    "text": "G9 Science Performance Analysis\nNow we can examine the content of the exam itself and our students’ performance relative to the state.\n\nStructure of the ExamPerformance by Content StrandsStudent Performance by Item DifficultyStudent Performance Key WordsStudent Performance and Disability\n\n\n\n\nWhat reporting categories were emphasized by the state?\n\nWe can see from our summary that 50% of the exam points (30 of the available 60) come from questions from the Motion and Forces Reporting Category, followed by 30% from Energy, and 20% from Waves.\n\n\nCode\nSG9_Cat_Total<-SG9_Item%>%\n  select(`sitem`, `item Possible Points`, `Reporting Category`)%>%\n  group_by(`Reporting Category`)%>%\n  summarise(available_points = sum(`item Possible Points`, na.rm=TRUE))%>%\n  mutate(percent_available_points = available_points/(sum(available_points, na.rm = TRUE)))\n\nSG9_Cat_Total\n\n\n\n\n  \n\n\n\n\n\nCode\nggplot(SG9_Cat_Total, aes(x='',fill = `Reporting Category`, y = `available_points`)) +\n    geom_bar(position=\"fill\", stat = \"identity\") + coord_flip()+\n labs(subtitle =\"All Students\" ,\n       y = \"% Points Available\",\n       x= \"Reporting Category\",\n       title = \"Percentage of Exam Points Available by Reporting Category\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))\n\n\n\n\n\n\n\n\n\nWhere did Rising Tide students lose most of their points?\n\nThe proportion of points lost by Rising Tide students corresponds to the proportion of points available for each Reporting Category of the the exam. This suggests that our students are prepared consistently across the units in the Reporting Categories.\n\n\nCode\nSG9_Cat_Loss<-SG9_StudentItem%>%\n  select(`sitem`, `Reporting Category`, `item Possible Points`, `sitem_score`)%>%\n  group_by(`Reporting Category`)%>%\n  summarise(sum_points_lost = sum(`item Possible Points`-`sitem_score`, na.rm=TRUE),\n            available_points = sum(`item Possible Points`, na.rm=TRUE))%>%\n              mutate(percent_points_lost = round(sum_points_lost/sum(sum_points_lost,na.rm=TRUE),2))%>%\n            mutate(percent_available_points = available_points/(sum(available_points, na.rm = TRUE)))\nSG9_Cat_Loss<-SG9_Cat_Loss%>%\n  select(`Reporting Category`, `percent_available_points`, `percent_points_lost`)\n\nSG9_Cat_Loss\n\n\n\n\n  \n\n\n\n\n\nCode\nSG9_Percent_Loss<-SG9_StudentItem%>%\n  select(`sitem`, `Reporting Category`, `item Possible Points`, `sitem_score`)%>%\n  mutate(`points_lost` = `item Possible Points` - `sitem_score`)%>%\n  #ggplot(df, aes(x='', fill=option)) + geom_bar(position = \"fill\") \n  ggplot( aes(x='',fill = `Reporting Category`, y = `points_lost`)) +\n    geom_bar(position=\"fill\", stat = \"identity\") + coord_flip()+\n labs(subtitle =\"All Rising Tide Students\" ,\n       y = \"% Points Loints\",\n       x= \"Reporting Category\",\n       title = \"Percentage of Points Lost by Reporting Category\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))\n   \n\nSG9_Percent_Loss\n\n\n\n\n\n\n\nDid Rising Tide students’ performance relative to the state vary by content reporting categories?\n\nWe can see from our table that on average our students earned between 4 and 5 percent fewer of the available points relative to their peers in the state for items in each of the three reporting Categories.\n\n\nCode\nSG9_Cat_RTState<-SG9_Item%>%\n  select(`sitem`, `item Possible Points`, `Reporting Category`, `State Percent Points`, `RT Percent Points`, `RT-State Diff`)%>%\n  group_by(`Reporting Category`)%>%\n  summarise(available_points = sum(`item Possible Points`, na.rm=TRUE),\n            RT_points = sum(`RT Percent Points`*`item Possible Points`, na.rm = TRUE),\n            RT_Percent_Points = 100*round(RT_points/available_points,2),\n            State_Percent_Points = 100*round(sum(`State Percent Points`*`item Possible Points`/available_points, na.rm = TRUE),2))%>%\n  mutate(`RT-State Diff` = round(RT_Percent_Points - State_Percent_Points, 2))%>%\n ggplot( aes(fill = `Reporting Category`, y=`RT-State Diff`, x=`Reporting Category`)) +\n    geom_bar(position=\"dodge\", stat=\"identity\") +\n  labs(subtitle =\"All Students\" ,\n       y = \"RT-State Diff\",\n       x= \"Reporting Category\",\n       title = \"Difference in RT vs State Percent Points Earned by Reporting Category\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))+\n   geom_text(aes(label = `RT-State Diff`), vjust = -1., colour = \"white\", position = position_dodge(.9))\n\nSG9_Cat_RTState\n\n\n\n\n\nHere we see the distribution of RT-State Diff (difference between the percentage of points earned on a given item by Rising Tide students and percentage of points earned on the same item by their peers in the State) by sitem and content Reporting Category. We can see generally that items in the Motion and Forces Reporting Category seems to display the most concerning variability in student performance relative to the state. It would be worth looking at the specific question strands with the Physics Teachers. (It would be helpful to add item labels to the dots using ggplotly, however I did not find a way to have that render on the class blog)\n\n\nCode\nSG9_Cat_Box <-SG9_Item%>%\n  select(`sitem`, `Reporting Category`, `State Percent Points`, `RT-State Diff`)%>%\n  group_by(`Reporting Category`)%>%\n  ggplot( aes(x=`Reporting Category`, y=`RT-State Diff`, fill=`Reporting Category`)) +\n    geom_boxplot() +\n    scale_fill_viridis(discrete = TRUE, alpha=0.6) +\n    geom_jitter(color=\"black\", size=0.1, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) +\n    ggtitle(\"G9 Introductory Physics School State Difference by Item\") +\n    xlab(\"\")\n\nSG9_Cat_Box\n\n\n\n\n\nCode\n#ggplotly(SG9_Cat_Box)\n\n\n\n\n\n\nCan differences in Rising Tide student performance on an item and State performance on an item be explained by the difficulty level of an item?\n\nWhen considering RT-State Diff against State Percent Points for each sitem on the MCAS, this does not seem to generally be the case. Although the regression line shows RT-State Diff more likely to be negative on items where students in the State earned fewer points, the p-value is not significant.\n\n\nCode\nG9Sci_Diff_Dot<-SG9_Item%>%\n  select(`State Percent Points`, `RT-State Diff`, `Reporting Category`)%>%\n ggplot( aes(x=`State Percent Points`, y=`RT-State Diff`)) +\n     geom_point(size = 1, color=\"#69b3a2\")+\n geom_smooth(method=\"lm\",color=\"grey\", size =.5 )+\n  labs(title = \"RT-State Diff by Difficulty Level\", y = \"RT-State Diff\",\n       x = \"State Percent Points\") +\n  stat_cor(method = \"pearson\")#+facet(vars(`Reporting Category`)) +#label.x = 450, label.y = 550)\n\nG9Sci_Diff_Dot\n\n\n\n\n\n\n\n\n\nHow did students perform based on key words?\n\nWhen scanning the item Desc entries in the SG9_Item data frame, there are several questions containing the word “Calculate” in their description.\n\n\nHow much is calculation emphasized on this exam and how did Rising Tide students perform relative to their peers in the state on items containing “calculate” in their description?\n\n\n\nCode\nSG9_Calc<-SG9_Item%>%\n  select(`sitem`, `item Desc`,`item Possible Points`, `Reporting Category`, `State Percent Points`, `RT-State Diff`)%>%\n   mutate( key_word = case_when(\n     !str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Non-Calc\",\n     str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Calc\"))\n#view(SG9_Calc)\n\nSG9_Calc\n\n\n\n\n  \n\n\n\nNow, we can see that by the Waves and Energy categories half of the available points come from questions with calculate and half do not. In the Motion and Forces category, 40% of points are associated with questions that ask students to “calculate”.\n\n\nCode\nSG9_Calc%>%\n  group_by(`Reporting Category`, `key_word`)%>%\n  summarise(avg_RT_State_Diff = mean(`RT-State Diff`, na.rm=TRUE),\n            med_RT_State_Diff = median(`RT-State Diff`, na.rm =TRUE),\n            #sum_RT_State_Diff = sum(`RT-State Diff`, na.rm=TRUE),\n            sum_sitem_Possible_Points = sum(`item Possible Points`, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n\nCode\nSG9_Calc_PointsAvail<-SG9_Calc%>%\n  group_by(`Reporting Category`, `key_word`)%>%\n  summarise(avg_RT_State_Diff = mean(`RT-State Diff`, na.rm=TRUE),\n            med_RT_State_Diff = median(`RT-State Diff`, na.rm =TRUE),\n            sum_RT_State_Diff = sum(`RT-State Diff`, na.rm=TRUE),\n            sum_item_Possible_Points = sum(`item Possible Points`, na.rm = TRUE))%>%\n   ggplot(aes(fill=`key_word`, y=sum_item_Possible_Points, x=`Reporting Category`)) + geom_bar(position=\"dodge\", stat=\"identity\")+ \n  labs(subtitle =\"Calculate\" ,\n       y = \"Available Points\",\n       x= \"Reporting Category\",\n       title = \"Available points by Key Word\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n       theme(axis.text.x=element_text(angle=60,hjust=1))+\n   geom_text(aes(label = `sum_item_Possible_Points`), vjust = 1.5, colour = \"white\", position = position_dodge(.9))\n\nSG9_Calc_PointsAvail\n\n\n\n\n\nWhen we compare the median RT-State Diff for items containing the word “calculate” in their description vs. items that do not, we can see that across all of the Reporting Categories Rising Tide students performed significantly weaker relative to their peers in the state on questions that asked them to “calculate”.\n\n\nCode\nSG9_Calc_MedDiffBar<-SG9_Calc%>%\n  group_by(`Reporting Category`, `key_word`)%>%\n  summarise(mean_RT_State_Diff = round(mean(`RT-State Diff`, na.rm=TRUE),2),\n            med_RT_State_Diff = median(`RT-State Diff`, na.rm =TRUE),\n            sum_RT_State_Diff = sum(`RT-State Diff`, na.rm=TRUE))%>%\n   ggplot(aes(fill=`key_word`, y=med_RT_State_Diff, x=`Reporting Category`)) + geom_bar(position=\"dodge\", stat=\"identity\") + coord_flip()+\n   labs(subtitle =\"Calculate\" ,\n       y = \"Median RT-State-Diff\",\n       x= \"Reporting Category\",\n       title = \"Median RT-State-Diff by Key Word\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n    theme(axis.text.y=element_text(angle=40,hjust=.5))+\n   geom_text(aes(label = `med_RT_State_Diff`), hjust = 1, vjust = .75, colour = \"black\", position = position_dodge(.8))\nSG9_Calc_MedDiffBar\n\n\n\n\n\nHere we can see the distribution of RT-State Diff by sitem and Reporting Category and the disparity in RT-State Diff when we consider items asking students to “Calculate” vs. those that do not.\n\n\nCode\nSG9_Calc_Box <-SG9_Calc%>%\n  group_by(`key_word`, `Reporting Category`)%>%\n  ggplot( aes(x=`key_word`, y=`RT-State Diff`, fill=`Reporting Category`)) +\n    geom_boxplot() +\n    scale_fill_viridis(discrete = TRUE, alpha=0.6) +\n    geom_jitter(color=\"black\", size=0.1, alpha=0.9) +\n    theme_ipsum() +\n    theme(\n      #legend.position=\"none\",\n      plot.title = element_text(size=11)\n    ) + labs(subtitle =\"Calculate\" ,\n       y = \"RT-State-Diff\",\n       x= \"Calculate vs. Non-Calculate\",\n       title = \"RT-State-Diff by Key Word\",\n      caption = \"2022 HS Introductory Physics MCAS\")\n   # ggtitle(\"RT-State-Diff by Key Word\") +\n  #  xlab(\"\")\n\nSG9_Calc_Box\n\n\n\n\n\n\n\nDid RT students perform worse relative to their peers in the state on more “challenging” calculation items?\n\nIf we consider the difficulty of items containing the word calculate for students as reflected in the state-wide performance (State Percent Points) for a given item, the gap between Rising Tide students’ performance to their peers in the state RT-State Diff does not seem to increase significantly with the difficulty .\n\n\nCode\n#view(SG9_Calc)\nSG9_Calc_Dot<- SG9_Calc%>%\n  select(`State Percent Points`, `RT-State Diff`, `key_word`)%>%\n  filter(key_word == \"Calc\")%>%\n  ggplot( aes(x=`State Percent Points`, y=`RT-State Diff`)) +\n     geom_point(size = 1, color=\"#69b3a2\")+\n geom_smooth(method=\"lm\",color=\"grey\", size =.5 )+\n  labs(title = \"RT State Diff vs. State Percent Points\", y = \"RT State Diff\",\n       x = \"State Percent Points\")+\n    stat_cor(method = \"pearson\")\n\nSG9_Calc_Dot\n\n\n\n\n\n\n\nIs the “calculation gap” consistent across performance levels?\n\nHere we can see that students with a higher performance level lost a greater proportion of their points on questions involving “Calculate”. I.e., the higher a student’s performance level, the greater the percentage of their points were lost to items asking them to “calculate”. This suggests that in the general classroom to raise student performance, students should spend a higher proportion of time on calculation based activities.\n\n\nCode\n# G9 Points Lost\nG9Sci_StudentCalcPerflev<-SG9_StudentItem%>%\n  select(gender, sitem, sitem_score, `item Desc`, `item Possible Points`, `State Percent Points`, IEP, `RT-State Diff`, `Reporting Category`, `sperflev`)%>%\n  mutate( key_word = case_when(\n     !str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Non-Calc\",\n     str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Calc\"))%>%\n  group_by(`sperflev`, `key_word`)%>%\n  summarise(total_points_lost = sum(`sitem_score`-`item Possible Points`, na.rm = TRUE),\n            med_RT_State_Diff = median(`RT-State Diff`, na.rm=TRUE))\nG9Sci_StudentCalcPerflev\n\n\n\n\n  \n\n\n\nCode\n#view(SG9_StudentItem)\n\n\n\n\n\nG9Sci_StudentCalcPerflev%>%\n ggplot(aes(fill=`key_word`, y=total_points_lost, x=`sperflev`)) + geom_bar(position=\"fill\", stat=\"identity\") +\n   labs(subtitle =\"Calculate\" ,\n       y = \"Percentage Points Lost\",\n       x= \"Performance Level\",\n       title = \"Percentage of Points Lost by Key Word and Performance Level\",\n      caption = \"2022 HS Introductory Physics MCAS\")\n\n\n\n\n\nCode\n#G9Sci_StudentCalcPerflev\n\n\n\n\n\n\nAre there differences in the performance of non-disabled and disabled students relative to their academic peers in the state?\n\nWe can see from our CU306 reports that our students with disabilities performed better relative to their peers in the state, RT-State Diff, across all Reporting Categories, while our non-disabled students performed worse relative to their peers in the state across all Reporting Categories. This suggest that more attention needs to be paid to the needs of the non-disabled students in the General Education setting.\n\n\nCode\nSG9_CU306Dis%>%\n  select(`RT-State Diff`, `Reporting Category`)%>%\n  mutate(`Disability Satus` = \"Disabled\")\n\n\n\n\n  \n\n\n\nCode\nSG9_CU306NonDis%>%\n  select(`RT-State Diff`, `Reporting Category`)%>%\n  mutate(`Disability Satus` = \"Non-Disabled\")\n\n\n\n\n  \n\n\n\nWhen we examine the points lost by reporting category and disability status, there does not seem to be a significant difference in performance between disabled and non-disabled students across Reporting Categories.\n\n\nCode\nG9Sci_StudentCalcDis<-SG9_StudentItem%>%\n  select(gender, sitem, sitem_score, `item Desc`, `item Possible Points`, `State Percent Points`, IEP, `RT-State Diff`, `Reporting Category`, `sperflev`)%>%\n  mutate( key_word = case_when(\n     !str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Non-Calc\",\n     str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Calc\"))%>%\n  group_by(`Reporting Category`, `key_word`, `IEP`)%>%\n  summarise(total_points_lost = sum(`sitem_score`-`item Possible Points`, na.rm = TRUE))%>%\n ggplot(aes(fill=`key_word`, y=total_points_lost, x=`Reporting Category`)) + geom_bar(position=\"dodge\", stat=\"identity\")+\n  facet_wrap(vars(IEP))+ coord_flip()+\n   labs(subtitle =\"Calculate\" ,\n       y = \"Sum Points Lost\",\n       x= \"Reporting Category\",\n       title = \"Sum Points Lost by Key Word Non-Disabled vs. Disabled\",\n      caption = \"2022 HS Introductory Physics MCAS\")+\n  geom_text(aes(label = `total_points_lost`), vjust = 1.5, colour = \"black\", position = position_dodge(.95))\n\n#G9Sci_StudentCalcDis\n\n\n\n\nCode\nG9Sci_StudentCalcDis<-SG9_StudentItem%>%\n  select(gender, sitem, sitem_score, `item Desc`, `item Possible Points`, `State Percent Points`, IEP, `RT-State Diff`, `Reporting Category`, `sperflev`)%>%\n  mutate( key_word = case_when(\n     !str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Non-Calc\",\n     str_detect(`item Desc`, \"calculate|Calculate\") ~ \"Calc\"))%>%\n  group_by(`Reporting Category`, `key_word`, `IEP`)%>%\n  summarise(sum_points_lost = sum(`sitem_score`-`item Possible Points`, na.rm = TRUE))%>%\n ggplot(aes(fill=`key_word`, y=sum_points_lost, x=`Reporting Category`)) + geom_bar(position=\"fill\", stat=\"identity\")+\n  facet_wrap(vars(IEP))+ coord_flip()+\n   labs(subtitle =\"Calculate\" ,\n       y = \"Percent Points Lost\",\n       x= \"Reporting Category\",\n       title = \"Percent Points Lost by Key Word and Disability Status\",\n      caption = \"2022 HS Introductory Physics MCAS\")\n\nG9Sci_StudentCalcDis"
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#conclusion",
    "href": "posts/theresaSzczepanski_final.html#conclusion",
    "title": "Final Project",
    "section": "Conclusion",
    "text": "Conclusion\nA student’s performance on their 9th Grade Introductory Physics MCAS is strongly associated with their performance on their 8th Grade Math MCAS exam. This suggests that the use of prior Math MCAS and current STAR Math testing data can identify students in need of extra support.\n\n\nCode\nSG9_Math<-MCAS_2022%>%\n  select(sscaleds, mscaleds2021,sscaleds_prior, grade, sattempt)%>%\n  filter((grade == 9) & sattempt != \"N\")%>%\n  ggplot(aes(x=`mscaleds2021`, y =`sscaleds`))+ \n     geom_point(size = 1, color=\"#69b3a2\")+\n geom_smooth(method=\"lm\",color=\"grey\", size =.5 )+\n  labs(title = \"2022 HS Introductory Physics vs. 2021 Math MCAS\", y = \"Physics Scaled Score\",\n       x = \"Math Scaled Score\") + \n  stat_cor(method = \"pearson\", label.x = 450, label.y = 550)\n\nSG9_Math\n\n\n\n\n\nRising Tide students as a whole performed slightly weaker relative to the state in all content reporting areas; however, students classified as disabled performed better relative to their peers in the state. The performance gap between Rising Tide students and students in the state on the HS Introductroy Physics exam is accounted for by the performance of the non-disabled students in the general classroom setting.\nAll Rising Tide students, regardless of disability status, performed significantly weaker relative to students in the State on items including the key word “Calculate” in their item description. This suggests that we should dedicate more classroom instructional time to problem solving with calculation. Notably, the higher a student’s performance level, the higher the percentage of points a student lost for calculation items. The largest area of growth for students across all performance categories is on calculation based items; evidence based math interventions include small group, differentiated problem sets.\nThe discrepancy in performance by Rising Tide students with and without disabilities relative to their associated academic peers in the state, suggest that our non-disabled students would benefit from some of the practices and supports currently provided to our students on IEPs. Differentiated, tiered, small group problem sets in the general classroom setting could potentially address the “calculation gap”."
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#reflection-limitationsareas-for-improvement",
    "href": "posts/theresaSzczepanski_final.html#reflection-limitationsareas-for-improvement",
    "title": "Final Project",
    "section": "Reflection: Limitations/Areas for Improvement",
    "text": "Reflection: Limitations/Areas for Improvement\nI was inspired to work on this report after years of experience working at a public school. Public education is a sector that is filled with passion and positive intentions but also divisive discussions. There exist a plethora of simplistic “one-trick fixes” that are marketed to students, teachers, and families. The use of data is the best tool we have against pressing forward and investing our precious time and money with initiatives that do not improve student outcomes.\nOver the years, I’ve noticed that teachers and leaders are given annual data reports yet, most lack the time, capacity, or resources to identify evidence based, actionable measures to enact in the classroom or at the organizational level. When presented with all of the questions from an assessment individually and the performance of all of one’s students on paper, it is difficult to identify trends. Anecdotally, I have noticed every year the majority of teachers gravitating to the scores and performance of individual students that they previously taught and ascribing mistakes or successes to specific experiences with an individual or one word in a question prompt. While relationship building and teaching to a child are hallmarks to student-teacher relationships, a narrow lens like this will not allow a teacher to identify classroom level changes or curriculum level changes that could impact all students and future students. In one’s compassionate focus on individuals, a great opportunity to promote the learning for all students is lost.\nWith the use of R, and the MCAS reports, I decided to focus on ways to identify trends at the classroom or curricular level. I found it challenging to limit the scope of my work for this project. Also, I struggled with discerning when to use sum vs. when to use averages or medians. To improve a student’s performance on a test, we are concerned with total points lost and relative weight of a content category; to identify curricular weaknesses we are also interested in relative performance to the state by content area.\nI only completed the analysis of the Introductory Physics Exam for High school students. I have ELA, Math, and Science results for grades 5-8 as well as grade 10. I am still working on building a general function library to generate similar graphics and tables for other content areas and grade levels and I would like to complete a similar report for each grade level and subject area assessment for teachers to use.\nGiven access to historical data, I think it would be beneficial to examine these trends over time to discern the performance gaps attributable to changes in the population of students (a factor which we cannot control or change) vs. those attributable to curriculum and teaching (an area we can influence and effect change).\nI also have access to reports that include the teacher a student had and the grades they earned from their teacher in the year they were assessed on the MCAS. I would like to examine the relationship between a student’s performance as measured by their teachers compared to their performance level as measured by the state. Are their patterns to the groups of students with the largest discrepancy between these two metrics? This would be important data to support the teaching and learning at our school.\nOn a broader scale, I think that I need to develop a stronger sense for what summary statistics are the most meaningful for a given variable to identify potential trends or insights and subsequently what visualizations best convey these insights to a reader. I would also like to develop a tool-kit of best practices for “checking against my own biases”. What set of metrics can I perform to best control for my potential mistakes as a human being with a limited perspective?\n\n\n\n\n\n\nNote\n\n\n\nI did not cite the source for the MCAS Preliminary Results because it is not a publicly available data set as it contains students’ personal information. I did use the raw csv. file retrievable from the DESE portal title “MCAS Full Preliminary Results”."
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#references",
    "href": "posts/theresaSzczepanski_final.html#references",
    "title": "Final Project",
    "section": "References",
    "text": "References\nChang, W. (2022). R Graphics Cookbook, 2nd Edition. O’Reilly Media.\nGrolemund, G., & Wickham, H. (2016). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\nHighSchool Introductory Physics Item Report [Data] https://profiles.doe.mass.edu/mcas/mcasitems2.aspx?grade=HS&subjectcode=PHY&linkid=23&orgcode=04830000&fycode=2022&orgtypecode=5&\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.\nPapay, J. P., Mantil, A., McDonough, A., Donahue, K., An, L., & Murnane, R. J. (n.d.). Lifting all boats? Accomplishments and Challenges from 20 Years of Education Reform in Massachusetts. Retrieved December 2, 2022, from https://annenberg.brown.edu/sites/default/files/LiftingAllBoats_FINAL.pdf\nR Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.https://www.r-project.org.\nRStudio Team. (2019). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA. https://www.rstudio.com.\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "posts/theresaSzczepanski_final.html#appendix",
    "href": "posts/theresaSzczepanski_final.html#appendix",
    "title": "Final Project",
    "section": "Appendix",
    "text": "Appendix\n\nCodebook MCAS_2022 VariablesMCAS 2022 Data Summary\n\n\nFor more information about the MCAS, see the Department of Elementary and Secondary Education’s (DESE) page.\n\n\n\nvariable\nMeasurement Level\nValues\n\n\n\n\ngender\nNominal\nthe reported gender identify of the student. Female: F, Male: M, Non-binary: N\n\n\nitem Description\nNominal\ndetails of assessment question\n\n\nitem Possible Points\nDiscrete\nThe number of points available for a given sitem\n\n\nReporting Category\nNominal\ncontent area of sitem\n\n\n\n\nMotion and Forces\n\n\n\n\nWaves\n\n\n\n\nEnergy\n\n\nRT Percent Points\nContinuous\nPercent of points earned by Rising Tide Students for a given sitem\n\n\nRT-State Diff\nDiscrete\nDifference between percent of points earned by Rising Tide Students and Students in the State for a given sitem\n\n\nsitem\nNominal\nThe question number the MCAS exam\n\n\nsitem_score\nDiscrete\nThe number of points a student earned on a given sitem\n\n\nsperflev\nOrdinal\nThe student’s performance level\n\n\n\n\nExceeds Expectations\n\n\n\n\nMeets Expectations\n\n\n\n\nPartially Meets Expectations\n\n\n\n\nDoes Not Meet Expectations\n\n\nsscaleds\nDiscrete\nThe student’s scaled score by subject area (e: English, m: Math, s: Science)\n\n\nssgp\nContinuous\nThe student’s growth percentile by subject area (e: English, m: Math, s: Science)\n\n\nState Percent Points\nContinuous\nPercent of points earned by Massachusetts students for a given sitem\n\n\n\n\n\n\n\nCode\n# examine the summary to decide how to best set up our data frame\n\n print(summarytools::dfSummary(MCAS_2022,\n                         varnumbers = FALSE,\n                         plain.ascii  = FALSE,\n                         style        = \"grid\",\n                         graph.magnif = 0.70,\n                        valid.col    = FALSE),\n       method = 'render',\n       table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nMCAS_2022\nDimensions: 495 x 256\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      adminyear\n[numeric]\n      1 distinct value\n      2022:495(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      testschoolcode\n[character]\n      1. 4830305\n      495(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      grade\n[factor]\n      1. 52. 63. 74. 85. 96. 10\n      89(18.0%)91(18.4%)92(18.6%)91(18.4%)69(13.9%)63(12.7%)\n      \n      0\n(0.0%)\n    \n    \n      gradesims\n[numeric]\n      Mean (sd) : 7.3 (1.6)min ≤ med ≤ max:5 ≤ 7 ≤ 10IQR (CV) : 3 (0.2)\n      5:89(18.0%)6:91(18.4%)7:92(18.6%)8:91(18.4%)9:69(13.9%)10:63(12.7%)\n      \n      0\n(0.0%)\n    \n    \n      dob\n[Date]\n      min : 2005-02-08med : 2008-11-29max : 2011-10-17range : 6y 8m 9d\n      427 distinct values\n      \n      0\n(0.0%)\n    \n    \n      gender\n[character]\n      1. F2. M3. N\n      242(48.9%)251(50.7%)2(0.4%)\n      \n      0\n(0.0%)\n    \n    \n      race\n[character]\n      1. A2. B3. H4. M5. N6. W\n      8(1.6%)6(1.2%)25(5.1%)41(8.3%)5(1.0%)410(82.8%)\n      \n      0\n(0.0%)\n    \n    \n      yrsinmass\n[character]\n      1. 12. 23. 34. 45. 5+\n      11(2.2%)18(3.6%)19(3.8%)16(3.2%)431(87.1%)\n      \n      0\n(0.0%)\n    \n    \n      yrsinmass_num\n[numeric]\n      Mean (sd) : 7.3 (2.4)min ≤ med ≤ max:1 ≤ 8 ≤ 12IQR (CV) : 3 (0.3)\n      12 distinct values\n      \n      0\n(0.0%)\n    \n    \n      yrsinsch\n[numeric]\n      Mean (sd) : 2.6 (1.5)min ≤ med ≤ max:1 ≤ 2 ≤ 6IQR (CV) : 3 (0.6)\n      1:159(32.1%)2:116(23.4%)3:80(16.2%)4:77(15.6%)5:31(6.3%)6:32(6.5%)\n      \n      0\n(0.0%)\n    \n    \n      highneeds\n[character]\n      1. 02. 1\n      290(58.6%)205(41.4%)\n      \n      0\n(0.0%)\n    \n    \n      lowincome\n[character]\n      1. 02. 1\n      369(74.5%)126(25.5%)\n      \n      0\n(0.0%)\n    \n    \n      title1\n[character]\n      1. 02. 1\n      393(79.4%)102(20.6%)\n      \n      0\n(0.0%)\n    \n    \n      ever_EL\n[character]\n      1. 1\n      20(100.0%)\n      \n      475\n(96.0%)\n    \n    \n      EL\n[character]\n      1. 02. 1\n      488(98.6%)7(1.4%)\n      \n      0\n(0.0%)\n    \n    \n      EL_FormerEL\n[character]\n      1. 02. 1\n      480(97.0%)15(3.0%)\n      \n      0\n(0.0%)\n    \n    \n      FormerEL\n[character]\n      1. 02. 1\n      487(98.4%)8(1.6%)\n      \n      0\n(0.0%)\n    \n    \n      ELfirstyear\n[character]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      IEP\n[character]\n      1. Disabled2. NonDisabled\n      114(23.0%)381(77.0%)\n      \n      0\n(0.0%)\n    \n    \n      plan504\n[character]\n      1. 02. 1\n      443(89.5%)52(10.5%)\n      \n      0\n(0.0%)\n    \n    \n      firstlanguage\n[character]\n      1. 22. 2673. 4154. 65. 6306. 77. 759\n      1(0.2%)481(97.2%)2(0.4%)8(1.6%)1(0.2%)1(0.2%)1(0.2%)\n      \n      0\n(0.0%)\n    \n    \n      natureofdis\n[numeric]\n      Mean (sd) : 6.9 (1.9)min ≤ med ≤ max:2 ≤ 7 ≤ 12IQR (CV) : 3 (0.3)\n      2:1(0.9%)3:9(7.8%)4:1(0.9%)5:19(16.5%)7:40(34.8%)8:38(33.0%)11:5(4.3%)12:2(1.7%)\n      \n      380\n(76.8%)\n    \n    \n      levelofneed\n[factor]\n      1. 12. 23. 34. 4\n      3(2.6%)14(12.2%)97(84.3%)1(0.9%)\n      \n      380\n(76.8%)\n    \n    \n      spedplacement\n[character]\n      1. 02. 13. 104. 20\n      380(76.8%)1(0.2%)104(21.0%)10(2.0%)\n      \n      0\n(0.0%)\n    \n    \n      town\n[character]\n      1. 2392. 3103. 524. 1455. 1826. 367. 208. 2619. 17110. 231[ 11 others ]\n      257(51.9%)54(10.9%)33(6.7%)30(6.1%)23(4.6%)20(4.0%)18(3.6%)12(2.4%)11(2.2%)8(1.6%)29(5.9%)\n      \n      0\n(0.0%)\n    \n    \n      county\n[character]\n      1. Barnstable2. Plymouth\n      56(11.3%)439(88.7%)\n      \n      0\n(0.0%)\n    \n    \n      octenr\n[numeric]\n      Min  : 0Mean : 1Max  : 1\n      0:13(2.6%)1:482(97.4%)\n      \n      0\n(0.0%)\n    \n    \n      conenr_sch\n[numeric]\n      1 distinct value\n      1:55(100.0%)\n      \n      440\n(88.9%)\n    \n    \n      conenr_sta\n[numeric]\n      1 distinct value\n      1:61(100.0%)\n      \n      434\n(87.7%)\n    \n    \n      access_part\n[numeric]\n      1 distinct value\n      1:7(100.0%)\n      \n      488\n(98.6%)\n    \n    \n      ealt\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      ecomplexity\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      emode\n[character]\n      1. O\n      422(100.0%)\n      \n      73\n(14.7%)\n    \n    \n      eteststat\n[character]\n      1. NTA2. NTO3. T\n      4(0.9%)1(0.2%)421(98.8%)\n      \n      69\n(13.9%)\n    \n    \n      wptopdev\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      wpcompconv\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem1\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:95(22.6%)1:326(77.4%)\n      \n      74\n(14.9%)\n    \n    \n      eitem2\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:132(31.4%)1:289(68.6%)\n      \n      74\n(14.9%)\n    \n    \n      eitem3\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:91(21.6%)1:330(78.4%)\n      \n      74\n(14.9%)\n    \n    \n      eitem4\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:79(18.8%)1:342(81.2%)\n      \n      74\n(14.9%)\n    \n    \n      eitem5\n[numeric]\n      Mean (sd) : 0.9 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.7)\n      0:109(25.9%)1:246(58.4%)2:66(15.7%)\n      \n      74\n(14.9%)\n    \n    \n      eitem6\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:97(23.0%)1:324(77.0%)\n      \n      74\n(14.9%)\n    \n    \n      eitem7\n[numeric]\n      Mean (sd) : 0.8 (0.5)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:95(22.6%)1:307(72.9%)2:19(4.5%)\n      \n      74\n(14.9%)\n    \n    \n      eitem8\n[numeric]\n      Mean (sd) : 0.8 (0.5)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:102(24.2%)1:292(69.4%)2:27(6.4%)\n      \n      74\n(14.9%)\n    \n    \n      eitem9\n[numeric]\n      Mean (sd) : 1.3 (1.5)min ≤ med ≤ max:0 ≤ 1 ≤ 7IQR (CV) : 0 (1.2)\n      0:79(18.8%)1:285(67.7%)2:10(2.4%)4:20(4.8%)6:20(4.8%)7:7(1.7%)\n      \n      74\n(14.9%)\n    \n    \n      eitem10\n[numeric]\n      Mean (sd) : 1.2 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 2 (0.7)\n      0:107(25.4%)1:124(29.5%)2:190(45.1%)\n      \n      74\n(14.9%)\n    \n    \n      eitem11\n[numeric]\n      Mean (sd) : 1.2 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.5)\n      0:54(12.8%)1:208(49.4%)2:159(37.8%)\n      \n      74\n(14.9%)\n    \n    \n      eitem12\n[numeric]\n      Mean (sd) : 2.5 (2.3)min ≤ med ≤ max:0 ≤ 1 ≤ 8IQR (CV) : 3 (0.9)\n      0:69(16.4%)1:152(36.1%)2:33(7.8%)3:6(1.4%)4:80(19.0%)5:7(1.7%)6:50(11.9%)7:18(4.3%)8:6(1.4%)\n      \n      74\n(14.9%)\n    \n    \n      eitem13\n[numeric]\n      Mean (sd) : 1.4 (1.5)min ≤ med ≤ max:0 ≤ 1 ≤ 7IQR (CV) : 1 (1)\n      0:88(21.0%)1:218(51.9%)2:56(13.3%)3:8(1.9%)4:27(6.4%)5:3(0.7%)6:18(4.3%)7:2(0.5%)\n      \n      75\n(15.2%)\n    \n    \n      eitem14\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:104(24.6%)1:318(75.4%)\n      \n      73\n(14.7%)\n    \n    \n      eitem15\n[numeric]\n      Mean (sd) : 0.9 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.7)\n      0:101(23.9%)1:260(61.6%)2:61(14.5%)\n      \n      73\n(14.7%)\n    \n    \n      eitem16\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:76(18.0%)1:346(82.0%)\n      \n      73\n(14.7%)\n    \n    \n      eitem17\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:122(28.9%)1:300(71.1%)\n      \n      73\n(14.7%)\n    \n    \n      eitem18\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:110(26.1%)1:312(73.9%)\n      \n      73\n(14.7%)\n    \n    \n      eitem19\n[numeric]\n      Mean (sd) : 0.9 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.7)\n      0:110(26.1%)1:234(55.5%)2:78(18.5%)\n      \n      73\n(14.7%)\n    \n    \n      eitem20\n[numeric]\n      Mean (sd) : 1 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:61(14.5%)1:281(66.6%)2:80(19.0%)\n      \n      73\n(14.7%)\n    \n    \n      eitem21\n[numeric]\n      Mean (sd) : 1 (0.5)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.5)\n      0:64(15.2%)1:309(73.2%)2:49(11.6%)\n      \n      73\n(14.7%)\n    \n    \n      eitem22\n[numeric]\n      Mean (sd) : 1.4 (1.5)min ≤ med ≤ max:0 ≤ 1 ≤ 7IQR (CV) : 0 (1.1)\n      0:51(12.1%)1:310(73.5%)2:10(2.4%)4:23(5.5%)6:19(4.5%)7:9(2.1%)\n      \n      73\n(14.7%)\n    \n    \n      eitem23\n[numeric]\n      Mean (sd) : 0.8 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.7)\n      0:124(29.4%)1:252(59.7%)2:46(10.9%)\n      \n      73\n(14.7%)\n    \n    \n      eitem24\n[numeric]\n      Mean (sd) : 0.9 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:81(19.2%)1:287(68.0%)2:54(12.8%)\n      \n      73\n(14.7%)\n    \n    \n      eitem25\n[numeric]\n      Mean (sd) : 0.9 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:84(19.9%)1:285(67.5%)2:53(12.6%)\n      \n      73\n(14.7%)\n    \n    \n      eitem26\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:121(28.7%)1:301(71.3%)\n      \n      73\n(14.7%)\n    \n    \n      eitem27\n[numeric]\n      Mean (sd) : 0.9 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:89(21.1%)1:272(64.5%)2:61(14.5%)\n      \n      73\n(14.7%)\n    \n    \n      eitem28\n[numeric]\n      Mean (sd) : 0.9 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:86(20.4%)1:283(67.1%)2:53(12.6%)\n      \n      73\n(14.7%)\n    \n    \n      eitem29\n[numeric]\n      Mean (sd) : 0.8 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.7)\n      0:123(29.1%)1:256(60.7%)2:43(10.2%)\n      \n      73\n(14.7%)\n    \n    \n      eitem30\n[numeric]\n      Mean (sd) : 1.2 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.6)\n      0:67(15.9%)1:219(51.9%)2:136(32.2%)\n      \n      73\n(14.7%)\n    \n    \n      eitem31\n[numeric]\n      Mean (sd) : 3.2 (2.2)min ≤ med ≤ max:0 ≤ 3 ≤ 8IQR (CV) : 4 (0.7)\n      0:25(6.9%)1:70(19.4%)2:81(22.5%)3:21(5.8%)4:69(19.2%)5:14(3.9%)6:55(15.3%)7:17(4.7%)8:8(2.2%)\n      \n      135\n(27.3%)\n    \n    \n      eitem32\n[numeric]\n      Mean (sd) : 3.2 (1.7)min ≤ med ≤ max:0 ≤ 3.5 ≤ 8IQR (CV) : 2 (0.5)\n      0:5(5.4%)1:5(5.4%)2:32(34.8%)3:4(4.3%)4:34(37.0%)5:1(1.1%)6:10(10.9%)8:1(1.1%)\n      \n      403\n(81.4%)\n    \n    \n      eitem33\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem34\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem35\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem36\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem37\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem38\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem39\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eitem40\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      erawsc\n[numeric]\n      Mean (sd) : 33 (8.2)min ≤ med ≤ max:6 ≤ 34 ≤ 47IQR (CV) : 10 (0.2)\n      39 distinct values\n      \n      73\n(14.7%)\n    \n    \n      emcpts\n[numeric]\n      Mean (sd) : 18.3 (4.1)min ≤ med ≤ max:3 ≤ 19 ≤ 26IQR (CV) : 5 (0.2)\n      24 distinct values\n      \n      73\n(14.7%)\n    \n    \n      eorpts\n[numeric]\n      Mean (sd) : 14.7 (5.4)min ≤ med ≤ max:1 ≤ 15 ≤ 28IQR (CV) : 8 (0.4)\n      28 distinct values\n      \n      73\n(14.7%)\n    \n    \n      eperpospts\n[numeric]\n      Mean (sd) : 66.3 (16.3)min ≤ med ≤ max:12 ≤ 69 ≤ 94IQR (CV) : 20 (0.2)\n      63 distinct values\n      \n      73\n(14.7%)\n    \n    \n      escaleds\n[numeric]\n      Mean (sd) : 501.3 (18.5)min ≤ med ≤ max:442 ≤ 502 ≤ 545IQR (CV) : 25 (0)\n      74 distinct values\n      \n      74\n(14.9%)\n    \n    \n      eperflev\n[ordered, factor]\n      1. E2. M3. PM4. NM5. DNT6. ABS\n      24(5.6%)206(48.4%)169(39.7%)22(5.2%)1(0.2%)4(0.9%)\n      \n      69\n(13.9%)\n    \n    \n      eperf2\n[ordered, factor]\n      1. Exceeding2. Meeting3. Partially Meeting4. Not Meeting\n      24(5.7%)206(48.9%)169(40.1%)22(5.2%)\n      \n      74\n(14.9%)\n    \n    \n      enumin\n[numeric]\n      1 distinct value\n      1:421(100.0%)\n      \n      74\n(14.9%)\n    \n    \n      eassess\n[numeric]\n      Min  : 0Mean : 1Max  : 1\n      0:4(0.9%)1:421(99.1%)\n      \n      70\n(14.1%)\n    \n    \n      esgp\n[numeric]\n      Mean (sd) : 52.6 (29.6)min ≤ med ≤ max:1 ≤ 54 ≤ 99IQR (CV) : 48.5 (0.6)\n      96 distinct values\n      \n      109\n(22.0%)\n    \n    \n      idea1\n[character]\n      1. 02. 13. 24. 35. 46. 57. BL8. OT\n      70(16.4%)79(18.5%)138(32.4%)97(22.8%)27(6.3%)6(1.4%)7(1.6%)2(0.5%)\n      \n      69\n(13.9%)\n    \n    \n      conv1\n[character]\n      1. 02. 13. 24. 35. BL6. OT\n      34(8.0%)121(28.4%)140(32.9%)122(28.6%)7(1.6%)2(0.5%)\n      \n      69\n(13.9%)\n    \n    \n      idea2\n[character]\n      1. 02. 13. 24. 35. 46. 57. BL8. OT\n      21(4.9%)121(28.4%)146(34.3%)96(22.5%)27(6.3%)9(2.1%)4(0.9%)2(0.5%)\n      \n      69\n(13.9%)\n    \n    \n      conv2\n[character]\n      1. 02. 13. 24. 35. BL6. OT\n      33(7.7%)121(28.4%)145(34.0%)121(28.4%)4(0.9%)2(0.5%)\n      \n      69\n(13.9%)\n    \n    \n      idea3\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      conv3\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      eattempt\n[character]\n      1. F2. N3. P\n      421(98.8%)4(0.9%)1(0.2%)\n      \n      69\n(13.9%)\n    \n    \n      malt\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      mcomplexity\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      mmode\n[character]\n      1. O\n      424(100.0%)\n      \n      71\n(14.3%)\n    \n    \n      mteststat\n[character]\n      1. NTA2. NTO3. T\n      2(0.5%)1(0.2%)423(99.3%)\n      \n      69\n(13.9%)\n    \n    \n      mitem1\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:94(22.3%)1:328(77.7%)\n      \n      73\n(14.7%)\n    \n    \n      mitem2\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:127(30.1%)1:295(69.9%)\n      \n      73\n(14.7%)\n    \n    \n      mitem3\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:174(41.2%)1:248(58.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem4\n[numeric]\n      Mean (sd) : 1.1 (1.1)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 2 (1)\n      0:156(37.1%)1:148(35.2%)2:55(13.1%)3:42(10.0%)4:19(4.5%)\n      \n      75\n(15.2%)\n    \n    \n      mitem5\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:237(56.3%)1:184(43.7%)\n      \n      74\n(14.9%)\n    \n    \n      mitem6\n[numeric]\n      Mean (sd) : 0.9 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1)\n      0:151(35.8%)1:219(51.9%)2:19(4.5%)3:22(5.2%)4:11(2.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem7\n[numeric]\n      Mean (sd) : 0.6 (0.7)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 1 (1.1)\n      0:213(50.5%)1:159(37.7%)2:50(11.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem8\n[numeric]\n      Mean (sd) : 0.8 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.1)\n      0:182(43.4%)1:167(39.9%)2:54(12.9%)3:7(1.7%)4:9(2.1%)\n      \n      76\n(15.4%)\n    \n    \n      mitem9\n[numeric]\n      Mean (sd) : 0.8 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1)\n      0:150(35.5%)1:225(53.3%)2:27(6.4%)3:8(1.9%)4:12(2.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem10\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:183(43.4%)1:239(56.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem11\n[numeric]\n      Mean (sd) : 0.7 (0.5)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.7)\n      0:123(29.1%)1:288(68.2%)2:11(2.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem12\n[numeric]\n      Mean (sd) : 0.8 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1)\n      0:161(38.2%)1:222(52.6%)2:23(5.5%)3:9(2.1%)4:7(1.7%)\n      \n      73\n(14.7%)\n    \n    \n      mitem13\n[numeric]\n      Mean (sd) : 1.2 (1.3)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.1)\n      0:156(37.0%)1:164(38.9%)2:24(5.7%)3:34(8.1%)4:44(10.4%)\n      \n      73\n(14.7%)\n    \n    \n      mitem14\n[numeric]\n      Mean (sd) : 1.1 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 0 (0.9)\n      0:102(24.2%)1:229(54.3%)2:47(11.1%)3:16(3.8%)4:28(6.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem15\n[numeric]\n      Mean (sd) : 0.5 (0.6)min ≤ med ≤ max:0 ≤ 0 ≤ 3IQR (CV) : 1 (1.3)\n      0:242(57.8%)1:153(36.5%)2:20(4.8%)3:4(1.0%)\n      \n      76\n(15.4%)\n    \n    \n      mitem16\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:223(53.0%)1:198(47.0%)\n      \n      74\n(14.9%)\n    \n    \n      mitem17\n[numeric]\n      Mean (sd) : 0.5 (0.6)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 1 (1.1)\n      0:219(52.0%)1:187(44.4%)2:15(3.6%)\n      \n      74\n(14.9%)\n    \n    \n      mitem18\n[numeric]\n      Mean (sd) : 0.5 (0.6)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 1 (1.1)\n      0:221(52.4%)1:186(44.1%)2:15(3.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem19\n[numeric]\n      Min  : 0Mean : 0.3Max  : 1\n      0:285(67.7%)1:136(32.3%)\n      \n      74\n(14.9%)\n    \n    \n      mitem20\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:242(57.3%)1:180(42.7%)\n      \n      73\n(14.7%)\n    \n    \n      mitem21\n[numeric]\n      Min  : 0Mean : 0.8Max  : 1\n      0:82(19.4%)1:340(80.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem22\n[numeric]\n      Mean (sd) : 1 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 0 (0.8)\n      0:81(19.2%)1:291(69.1%)2:19(4.5%)3:20(4.8%)4:10(2.4%)\n      \n      74\n(14.9%)\n    \n    \n      mitem23\n[numeric]\n      Mean (sd) : 0.8 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.1)\n      0:157(37.2%)1:223(52.8%)2:16(3.8%)3:6(1.4%)4:20(4.7%)\n      \n      73\n(14.7%)\n    \n    \n      mitem24\n[numeric]\n      Mean (sd) : 0.9 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.1)\n      0:165(39.1%)1:187(44.3%)2:46(10.9%)3:12(2.8%)4:12(2.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem25\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:179(42.6%)1:241(57.4%)\n      \n      75\n(15.2%)\n    \n    \n      mitem26\n[numeric]\n      Mean (sd) : 1 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1)\n      0:158(37.4%)1:172(40.7%)2:58(13.7%)3:24(5.7%)4:11(2.6%)\n      \n      72\n(14.5%)\n    \n    \n      mitem27\n[numeric]\n      Mean (sd) : 0.8 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.3)\n      0:194(46.1%)1:181(43.0%)2:16(3.8%)3:14(3.3%)4:16(3.8%)\n      \n      74\n(14.9%)\n    \n    \n      mitem28\n[numeric]\n      Mean (sd) : 0.7 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (1)\n      0:182(43.2%)1:190(45.1%)2:49(11.6%)\n      \n      74\n(14.9%)\n    \n    \n      mitem29\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:208(49.4%)1:213(50.6%)\n      \n      74\n(14.9%)\n    \n    \n      mitem30\n[numeric]\n      Mean (sd) : 0.6 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (1)\n      0:192(45.5%)1:195(46.2%)2:35(8.3%)\n      \n      73\n(14.7%)\n    \n    \n      mitem31\n[numeric]\n      Mean (sd) : 0.9 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1)\n      0:133(31.6%)1:241(57.2%)2:19(4.5%)3:17(4.0%)4:11(2.6%)\n      \n      74\n(14.9%)\n    \n    \n      mitem32\n[numeric]\n      Mean (sd) : 0.5 (0.6)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 1 (1.2)\n      0:240(56.9%)1:170(40.3%)2:12(2.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem33\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:216(51.2%)1:206(48.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem34\n[numeric]\n      Mean (sd) : 0.7 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.2)\n      0:190(45.1%)1:191(45.4%)2:20(4.8%)3:15(3.6%)4:5(1.2%)\n      \n      74\n(14.9%)\n    \n    \n      mitem35\n[numeric]\n      Mean (sd) : 0.8 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.1)\n      0:168(39.8%)1:200(47.4%)2:33(7.8%)3:15(3.6%)4:6(1.4%)\n      \n      73\n(14.7%)\n    \n    \n      mitem36\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:238(56.5%)1:183(43.5%)\n      \n      74\n(14.9%)\n    \n    \n      mitem37\n[numeric]\n      Mean (sd) : 1.1 (1.2)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1.1)\n      0:153(36.3%)1:187(44.3%)2:13(3.1%)3:36(8.5%)4:33(7.8%)\n      \n      73\n(14.7%)\n    \n    \n      mitem38\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:216(51.3%)1:205(48.7%)\n      \n      74\n(14.9%)\n    \n    \n      mitem39\n[numeric]\n      Mean (sd) : 0.3 (0.6)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 1 (1.6)\n      0:296(70.1%)1:106(25.1%)2:20(4.7%)\n      \n      73\n(14.7%)\n    \n    \n      mitem40\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:221(52.4%)1:201(47.6%)\n      \n      73\n(14.7%)\n    \n    \n      mitem41\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:31(49.2%)1:32(50.8%)\n      \n      432\n(87.3%)\n    \n    \n      mitem42\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:31(49.2%)1:32(50.8%)\n      \n      432\n(87.3%)\n    \n    \n      mrawsc\n[numeric]\n      Mean (sd) : 27.6 (11.2)min ≤ med ≤ max:0 ≤ 27 ≤ 58IQR (CV) : 15 (0.4)\n      51 distinct values\n      \n      72\n(14.5%)\n    \n    \n      mmcpts\n[numeric]\n      Mean (sd) : 10.5 (4)min ≤ med ≤ max:0 ≤ 10 ≤ 21IQR (CV) : 5 (0.4)\n      22 distinct values\n      \n      72\n(14.5%)\n    \n    \n      morpts\n[numeric]\n      Mean (sd) : 17.2 (8.1)min ≤ med ≤ max:0 ≤ 16 ≤ 38IQR (CV) : 12 (0.5)\n      38 distinct values\n      \n      72\n(14.5%)\n    \n    \n      mperpospts\n[numeric]\n      Mean (sd) : 50.3 (20.3)min ≤ med ≤ max:0 ≤ 50 ≤ 97IQR (CV) : 28 (0.4)\n      67 distinct values\n      \n      72\n(14.5%)\n    \n    \n      mscaleds\n[numeric]\n      Mean (sd) : 497.3 (17.6)min ≤ med ≤ max:440 ≤ 498 ≤ 555IQR (CV) : 20 (0)\n      80 distinct values\n      \n      72\n(14.5%)\n    \n    \n      mperflev\n[ordered, factor]\n      1. E2. M3. PM4. NM5. INV6. ABS\n      13(3.1%)168(39.4%)209(49.1%)33(7.7%)1(0.2%)2(0.5%)\n      \n      69\n(13.9%)\n    \n    \n      mperf2\n[ordered, factor]\n      1. Exceeding2. Meeting3. Partially Meeting4. Not Meeting\n      13(3.1%)168(39.7%)209(49.4%)33(7.8%)\n      \n      72\n(14.5%)\n    \n    \n      mnumin\n[numeric]\n      1 distinct value\n      1:423(100.0%)\n      \n      72\n(14.5%)\n    \n    \n      massess\n[numeric]\n      Min  : 0Mean : 1Max  : 1\n      0:2(0.5%)1:423(99.5%)\n      \n      70\n(14.1%)\n    \n    \n      msgp\n[numeric]\n      Mean (sd) : 43.7 (27.6)min ≤ med ≤ max:1 ≤ 40 ≤ 99IQR (CV) : 46 (0.6)\n      97 distinct values\n      \n      107\n(21.6%)\n    \n    \n      mattempt\n[character]\n      1. F2. N\n      424(99.5%)2(0.5%)\n      \n      69\n(13.9%)\n    \n    \n      salt\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      scomplexity\n[logical]\n      All NA's\n      \n      \n      495\n(100.0%)\n    \n    \n      smode\n[character]\n      1. O2. P\n      248(96.9%)8(3.1%)\n      \n      239\n(48.3%)\n    \n    \n      steststat\n[character]\n      1. NTA2. NTO3. T4. TR\n      2(0.6%)54(17.3%)250(80.1%)6(1.9%)\n      \n      183\n(37.0%)\n    \n    \n      ssubject\n[character]\n      1. 12. 23. 34. 6\n      3(2.3%)8(6.1%)51(38.6%)70(53.0%)\n      \n      363\n(73.3%)\n    \n    \n      sitem1\n[numeric]\n      Min  : 0Mean : 0.9Max  : 1\n      0:36(14.1%)1:220(85.9%)\n      \n      239\n(48.3%)\n    \n    \n      sitem2\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:109(42.6%)1:147(57.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem3\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:110(43.0%)1:146(57.0%)\n      \n      239\n(48.3%)\n    \n    \n      sitem4\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:102(40.0%)1:153(60.0%)\n      \n      240\n(48.5%)\n    \n    \n      sitem5\n[numeric]\n      Mean (sd) : 1 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 2 (0.7)\n      0:66(25.8%)1:125(48.8%)2:65(25.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem6\n[numeric]\n      Mean (sd) : 0.9 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.8)\n      0:77(30.1%)1:119(46.5%)2:60(23.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem7\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:113(44.1%)1:143(55.9%)\n      \n      239\n(48.3%)\n    \n    \n      sitem8\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:131(51.2%)1:125(48.8%)\n      \n      239\n(48.3%)\n    \n    \n      sitem9\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:65(25.4%)1:191(74.6%)\n      \n      239\n(48.3%)\n    \n    \n      sitem10\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:85(33.2%)1:171(66.8%)\n      \n      239\n(48.3%)\n    \n    \n      sitem11\n[numeric]\n      Mean (sd) : 0.6 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (1)\n      0:113(44.1%)1:139(54.3%)2:2(0.8%)3:1(0.4%)4:1(0.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem12\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:102(40.0%)1:153(60.0%)\n      \n      240\n(48.5%)\n    \n    \n      sitem13\n[numeric]\n      Mean (sd) : 0.9 (0.5)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:42(16.4%)1:186(72.7%)2:28(10.9%)\n      \n      239\n(48.3%)\n    \n    \n      sitem14\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:101(39.5%)1:155(60.5%)\n      \n      239\n(48.3%)\n    \n    \n      sitem15\n[numeric]\n      Mean (sd) : 1.4 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 1 (0.6)\n      0:45(17.6%)1:86(33.6%)2:100(39.1%)3:25(9.8%)\n      \n      239\n(48.3%)\n    \n    \n      sitem16\n[numeric]\n      Mean (sd) : 1.1 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 2 (0.7)\n      0:65(25.7%)1:110(43.5%)2:72(28.5%)3:6(2.4%)\n      \n      242\n(48.9%)\n    \n    \n      sitem17\n[numeric]\n      Mean (sd) : 1 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 1 (0.8)\n      0:68(26.7%)1:126(49.4%)2:49(19.2%)3:12(4.7%)\n      \n      240\n(48.5%)\n    \n    \n      sitem18\n[numeric]\n      Mean (sd) : 0.9 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.7)\n      0:70(27.3%)1:133(52.0%)2:53(20.7%)\n      \n      239\n(48.3%)\n    \n    \n      sitem19\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:110(43.0%)1:146(57.0%)\n      \n      239\n(48.3%)\n    \n    \n      sitem20\n[numeric]\n      Mean (sd) : 1 (0.9)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 1 (0.9)\n      0:78(30.6%)1:132(51.8%)2:24(9.4%)3:17(6.7%)4:4(1.6%)\n      \n      240\n(48.5%)\n    \n    \n      sitem21\n[numeric]\n      Mean (sd) : 0.8 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 0 (0.7)\n      0:62(24.6%)1:175(69.4%)2:11(4.4%)3:4(1.6%)\n      \n      243\n(49.1%)\n    \n    \n      sitem22\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:76(29.7%)1:180(70.3%)\n      \n      239\n(48.3%)\n    \n    \n      sitem23\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:95(37.3%)1:160(62.7%)\n      \n      240\n(48.5%)\n    \n    \n      sitem24\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:73(28.5%)1:183(71.5%)\n      \n      239\n(48.3%)\n    \n    \n      sitem25\n[numeric]\n      Mean (sd) : 0.7 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.9)\n      0:105(41.0%)1:127(49.6%)2:24(9.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem26\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:104(40.6%)1:152(59.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem27\n[numeric]\n      Mean (sd) : 1.5 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 1 (0.6)\n      0:24(9.4%)1:112(43.8%)2:90(35.2%)3:30(11.7%)\n      \n      239\n(48.3%)\n    \n    \n      sitem28\n[numeric]\n      Mean (sd) : 1.2 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 2 (0.9)\n      0:78(30.6%)1:83(32.5%)2:61(23.9%)3:33(12.9%)\n      \n      240\n(48.5%)\n    \n    \n      sitem29\n[numeric]\n      Mean (sd) : 1 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1.2 (0.7)\n      0:64(25.0%)1:124(48.4%)2:68(26.6%)\n      \n      239\n(48.3%)\n    \n    \n      sitem30\n[numeric]\n      Mean (sd) : 0.6 (0.5)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.9)\n      0:108(42.2%)1:147(57.4%)2:1(0.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem31\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:95(37.1%)1:161(62.9%)\n      \n      239\n(48.3%)\n    \n    \n      sitem32\n[numeric]\n      Min  : 0Mean : 0.7Max  : 1\n      0:88(34.4%)1:168(65.6%)\n      \n      239\n(48.3%)\n    \n    \n      sitem33\n[numeric]\n      Mean (sd) : 0.8 (0.4)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 0 (0.6)\n      0:58(22.7%)1:194(76.1%)2:3(1.2%)\n      \n      240\n(48.5%)\n    \n    \n      sitem34\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:137(53.5%)1:119(46.5%)\n      \n      239\n(48.3%)\n    \n    \n      sitem35\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:141(55.1%)1:115(44.9%)\n      \n      239\n(48.3%)\n    \n    \n      sitem36\n[numeric]\n      Mean (sd) : 0.9 (0.7)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.8)\n      0:75(29.4%)1:135(52.9%)2:45(17.6%)\n      \n      240\n(48.5%)\n    \n    \n      sitem37\n[numeric]\n      Mean (sd) : 0.7 (0.8)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 1 (1.1)\n      0:112(43.8%)1:109(42.6%)2:26(10.2%)3:9(3.5%)\n      \n      239\n(48.3%)\n    \n    \n      sitem38\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:107(41.8%)1:149(58.2%)\n      \n      239\n(48.3%)\n    \n    \n      sitem39\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:109(42.6%)1:147(57.4%)\n      \n      239\n(48.3%)\n    \n    \n      sitem40\n[numeric]\n      Min  : 0Mean : 0.6Max  : 1\n      0:90(35.2%)1:166(64.8%)\n      \n      239\n(48.3%)\n    \n    \n      sitem41\n[numeric]\n      Mean (sd) : 0.7 (0.6)min ≤ med ≤ max:0 ≤ 1 ≤ 2IQR (CV) : 1 (0.9)\n      0:95(37.1%)1:133(52.0%)2:28(10.9%)\n      \n      239\n(48.3%)\n    \n    \n      sitem42\n[numeric]\n      Mean (sd) : 1.2 (1)min ≤ med ≤ max:0 ≤ 1 ≤ 4IQR (CV) : 2 (0.8)\n      0:22(28.6%)1:27(35.1%)2:24(31.2%)3:2(2.6%)4:2(2.6%)\n      \n      418\n(84.4%)\n    \n    \n      sitem43\n[numeric]\n      Min  : 0Mean : 0.1Max  : 1\n      0:7(87.5%)1:1(12.5%)\n      \n      487\n(98.4%)\n    \n    \n      sitem44\n[numeric]\n      Mean (sd) : 1.3 (1.4)min ≤ med ≤ max:0 ≤ 1 ≤ 3IQR (CV) : 2.5 (1.1)\n      0:3(42.9%)1:1(14.3%)2:1(14.3%)3:2(28.6%)\n      \n      488\n(98.6%)\n    \n    \n      sitem45\n[numeric]\n      Mean (sd) : 0.7 (1)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 1.5 (1.3)\n      0:4(57.1%)1:1(14.3%)2:2(28.6%)\n      \n      488\n(98.6%)\n    \n    \n      srawsc\n[numeric]\n      Mean (sd) : 31.6 (9.4)min ≤ med ≤ max:8 ≤ 32.5 ≤ 57IQR (CV) : 14 (0.3)\n      43 distinct values\n      \n      239\n(48.3%)\n    \n    \n      smcpts\n[numeric]\n      Mean (sd) : 14.1 (4.9)min ≤ med ≤ max:2 ≤ 14 ≤ 29IQR (CV) : 6.2 (0.3)\n      26 distinct values\n      \n      239\n(48.3%)\n    \n    \n      sorpts\n[numeric]\n      Mean (sd) : 17.6 (6.4)min ≤ med ≤ max:0 ≤ 18 ≤ 32IQR (CV) : 9 (0.4)\n      33 distinct values\n      \n      239\n(48.3%)\n    \n    \n      sperpospts\n[numeric]\n      Mean (sd) : 56.9 (17.3)min ≤ med ≤ max:13 ≤ 57 ≤ 95IQR (CV) : 26 (0.3)\n      59 distinct values\n      \n      239\n(48.3%)\n    \n    \n      sscaleds\n[numeric]\n      Mean (sd) : 447.9 (105.2)min ≤ med ≤ max:214 ≤ 493 ≤ 558IQR (CV) : 41 (0.2)\n      91 distinct values\n      \n      185\n(37.4%)\n    \n    \n      sperflev\n[ordered, factor]\n      1. ABS2. Exceeding3. Meeting4. Partially Meeting5. Not Meeting6. F7. PAS8. NI9. P\n      2(0.6%)17(5.4%)102(32.7%)112(35.9%)17(5.4%)3(1.0%)54(17.3%)3(1.0%)2(0.6%)\n      \n      183\n(37.0%)\n    \n    \n      sperf2\n[ordered, factor]\n      1. Exceeding2. Meeting3. Partially Meeting4. Not Meeting5. ABS6. F7. PAS8. NI9. P\n      17(5.4%)102(32.7%)112(35.9%)17(5.4%)2(0.6%)3(1.0%)54(17.3%)3(1.0%)2(0.6%)\n      \n      183\n(37.0%)\n    \n    \n      snumin\n[numeric]\n      1 distinct value\n      1:241(100.0%)\n      \n      254\n(51.3%)\n    \n    \n      sassess\n[numeric]\n      Min  : 0Mean : 1Max  : 1\n      0:2(0.8%)1:241(99.2%)\n      \n      252\n(50.9%)\n    \n    \n      sattempt\n[character]\n      1. F2. N\n      256(82.1%)56(17.9%)\n      \n      183\n(37.0%)\n    \n    \n      ela_cd\n[numeric]\n      Min  : 0Mean : 0.9Max  : 2\n      0:71(53.8%)2:61(46.2%)\n      \n      363\n(73.3%)\n    \n    \n      math_cd\n[numeric]\n      Mean (sd) : 0.9 (1)min ≤ med ≤ max:0 ≤ 0 ≤ 2IQR (CV) : 2 (1.1)\n      0:71(53.8%)1:6(4.5%)2:55(41.7%)\n      \n      363\n(73.3%)\n    \n    \n      sci_cd\n[numeric]\n      Min  : 0Mean : 0.9Max  : 1\n      0:10(7.6%)1:122(92.4%)\n      \n      363\n(73.3%)\n    \n    \n      accom_e\n[numeric]\n      1 distinct value\n      1:76(100.0%)\n      \n      419\n(84.6%)\n    \n    \n      accom_m\n[numeric]\n      1 distinct value\n      1:78(100.0%)\n      \n      417\n(84.2%)\n    \n    \n      accom_s\n[numeric]\n      1 distinct value\n      1:47(100.0%)\n      \n      448\n(90.5%)\n    \n    \n      accom_readaloud\n[character]\n      1. H2. T\n      1(33.3%)2(66.7%)\n      \n      492\n(99.4%)\n    \n    \n      accom_scribe\n[character]\n      1. H\n      2(100.0%)\n      \n      493\n(99.6%)\n    \n    \n      accom_calculator\n[numeric]\n      1 distinct value\n      1:2(100.0%)\n      \n      493\n(99.6%)\n    \n    \n      grade2018\n[numeric]\n      Mean (sd) : 4.3 (1.1)min ≤ med ≤ max:3 ≤ 4 ≤ 7IQR (CV) : 2 (0.3)\n      3:77(28.4%)4:80(29.5%)5:62(22.9%)6:51(18.8%)7:1(0.4%)\n      \n      224\n(45.3%)\n    \n    \n      grade2019\n[numeric]\n      Mean (sd) : 4.8 (1.3)min ≤ med ≤ max:3 ≤ 5 ≤ 8IQR (CV) : 2 (0.3)\n      3:74(20.5%)4:79(21.9%)5:90(24.9%)6:65(18.0%)7:52(14.4%)8:1(0.3%)\n      \n      134\n(27.1%)\n    \n    \n      grade2021\n[numeric]\n      Mean (sd) : 5.9 (1.3)min ≤ med ≤ max:4 ≤ 6 ≤ 8IQR (CV) : 2 (0.2)\n      4:74(18.5%)5:87(21.7%)6:90(22.4%)7:88(21.9%)8:62(15.5%)\n      \n      94\n(19.0%)\n    \n    \n      escaleds2018\n[numeric]\n      Mean (sd) : 504.3 (18.2)min ≤ med ≤ max:442 ≤ 504 ≤ 560IQR (CV) : 23 (0)\n      61 distinct values\n      \n      229\n(46.3%)\n    \n    \n      escaleds2019\n[numeric]\n      Mean (sd) : 503.4 (18.4)min ≤ med ≤ max:443 ≤ 503 ≤ 555IQR (CV) : 22 (0)\n      71 distinct values\n      \n      138\n(27.9%)\n    \n    \n      escaleds2021\n[numeric]\n      Mean (sd) : 502.8 (21.1)min ≤ med ≤ max:441 ≤ 503 ≤ 560IQR (CV) : 26 (0)\n      83 distinct values\n      \n      96\n(19.4%)\n    \n    \n      mscaleds2018\n[numeric]\n      Mean (sd) : 502.9 (19.2)min ≤ med ≤ max:440 ≤ 503.5 ≤ 560IQR (CV) : 27 (0)\n      71 distinct values\n      \n      229\n(46.3%)\n    \n    \n      mscaleds2019\n[numeric]\n      Mean (sd) : 502.8 (18.2)min ≤ med ≤ max:450 ≤ 501 ≤ 559IQR (CV) : 25 (0)\n      77 distinct values\n      \n      138\n(27.9%)\n    \n    \n      mscaleds2021\n[numeric]\n      Mean (sd) : 495 (19.2)min ≤ med ≤ max:440 ≤ 495 ≤ 560IQR (CV) : 23 (0)\n      83 distinct values\n      \n      95\n(19.2%)\n    \n    \n      esgp2018\n[numeric]\n      Mean (sd) : 48.9 (29.1)min ≤ med ≤ max:1 ≤ 48 ≤ 99IQR (CV) : 53.5 (0.6)\n      81 distinct values\n      \n      316\n(63.8%)\n    \n    \n      esgp2019\n[numeric]\n      Mean (sd) : 43.2 (27.9)min ≤ med ≤ max:1 ≤ 39.5 ≤ 99IQR (CV) : 48.2 (0.6)\n      91 distinct values\n      \n      231\n(46.7%)\n    \n    \n      esgp2021\n[numeric]\n      Mean (sd) : 41.6 (30.7)min ≤ med ≤ max:1 ≤ 34.5 ≤ 99IQR (CV) : 51.5 (0.7)\n      88 distinct values\n      \n      201\n(40.6%)\n    \n    \n      msgp2018\n[numeric]\n      Mean (sd) : 52.9 (26.9)min ≤ med ≤ max:1 ≤ 55 ≤ 99IQR (CV) : 45.5 (0.5)\n      85 distinct values\n      \n      316\n(63.8%)\n    \n    \n      msgp2019\n[numeric]\n      Mean (sd) : 49.6 (27.3)min ≤ med ≤ max:1 ≤ 52 ≤ 98IQR (CV) : 46.2 (0.6)\n      92 distinct values\n      \n      231\n(46.7%)\n    \n    \n      msgp2021\n[numeric]\n      Mean (sd) : 28.7 (24.1)min ≤ med ≤ max:1 ≤ 23 ≤ 99IQR (CV) : 33.5 (0.8)\n      82 distinct values\n      \n      200\n(40.4%)\n    \n    \n      summarize\n[numeric]\n      Min  : 0Mean : 0.9Max  : 1\n      0:69(13.9%)1:426(86.1%)\n      \n      0\n(0.0%)\n    \n    \n      amend\n[character]\n      1. M\n      1(100.0%)\n      \n      494\n(99.8%)\n    \n    \n      datachanged\n[numeric]\n      1 distinct value\n      8:1(100.0%)\n      \n      494\n(99.8%)\n    \n    \n      eScaleForm\n[numeric]\n      1 distinct value\n      1:426(100.0%)\n      \n      69\n(13.9%)\n    \n    \n      mScaleForm\n[numeric]\n      1 distinct value\n      1:426(100.0%)\n      \n      69\n(13.9%)\n    \n    \n      sScaleForm\n[numeric]\n      1 distinct value\n      1:188(100.0%)\n      \n      307\n(62.0%)\n    \n    \n      eFormType\n[character]\n      1. C\n      426(100.0%)\n      \n      69\n(13.9%)\n    \n    \n      mFormType\n[character]\n      1. C\n      426(100.0%)\n      \n      69\n(13.9%)\n    \n    \n      sFormType\n[character]\n      1. C2. P\n      304(97.4%)8(2.6%)\n      \n      183\n(37.0%)\n    \n    \n      days_in_person\n[numeric]\n      Mean (sd) : 164.5 (12.3)min ≤ med ≤ max:86 ≤ 167 ≤ 179IQR (CV) : 10 (0.1)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      member\n[numeric]\n      Mean (sd) : 175.6 (8.5)min ≤ med ≤ max:101 ≤ 176 ≤ 180IQR (CV) : 4 (0)\n      22 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ssubject_prior\n[numeric]\n      Min  : 1Mean : 2.9Max  : 3\n      1:3(5.0%)3:57(95.0%)\n      \n      435\n(87.9%)\n    \n    \n      sscaleds_prior\n[numeric]\n      Mean (sd) : 240.1 (16.6)min ≤ med ≤ max:200 ≤ 240 ≤ 266IQR (CV) : 26 (0.1)\n      24 distinct values\n      \n      435\n(87.9%)\n    \n    \n      escaleds.legacy.equivalent\n[numeric]\n      Mean (sd) : 254.7 (9.6)min ≤ med ≤ max:206 ≤ 260 ≤ 268IQR (CV) : 14 (0)\n      14 distinct values\n      \n      433\n(87.5%)\n    \n    \n      mscaleds.legacy.equivalent\n[numeric]\n      Mean (sd) : 251.5 (14.2)min ≤ med ≤ max:212 ≤ 256 ≤ 278IQR (CV) : 18 (0.1)\n      24 distinct values\n      \n      432\n(87.3%)\n    \n    \n      sscaleds.legacy.equivalent\n[numeric]\n      Mean (sd) : 240.7 (14.3)min ≤ med ≤ max:204 ≤ 240 ≤ 276IQR (CV) : 18 (0.1)\n      26 distinct values\n      \n      425\n(85.9%)\n    \n    \n      sscaleds.highest.on.legacy.scale\n[numeric]\n      Mean (sd) : 240.9 (14.4)min ≤ med ≤ max:204 ≤ 240 ≤ 276IQR (CV) : 20.5 (0.1)\n      30 distinct values\n      \n      363\n(73.3%)\n    \n    \n      scpi\n[numeric]\n      Mean (sd) : 82.5 (23.2)min ≤ med ≤ max:25 ≤ 100 ≤ 100IQR (CV) : 25 (0.3)\n      25:3(4.8%)50:11(17.5%)75:13(20.6%)100:36(57.1%)\n      \n      432\n(87.3%)\n    \n    \n      sscaleds.highest.on.nextGen.scale\n[numeric]\n      Mean (sd) : 495.4 (19.2)min ≤ med ≤ max:461 ≤ 492 ≤ 531IQR (CV) : 33.5 (0)\n      24 distinct values\n      \n      432\n(87.3%)\n    \n    \n      sperf2.highest.on.nextGen.scale\n[character]\n      1. E2. M3. NM4. PM\n      2(3.2%)20(31.7%)3(4.8%)38(60.3%)\n      \n      432\n(87.3%)\n    \n    \n      nature0fdis\n[character]\n      1. 112. 123. 24. 35. 46. 57. 78. 8\n      5(4.3%)2(1.7%)1(0.9%)9(7.8%)1(0.9%)19(16.5%)40(34.8%)38(33.0%)\n      \n      380\n(76.8%)\n    \n    \n      year\n[numeric]\n      1 distinct value\n      2022:495(100.0%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23"
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html",
    "href": "posts/VinithaMaheswaran_FinalProject.html",
    "title": "DACSS 601: Final Paper",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#introduction",
    "href": "posts/VinithaMaheswaran_FinalProject.html#introduction",
    "title": "DACSS 601: Final Paper",
    "section": "Introduction",
    "text": "Introduction\nWith the FIFA World Cup 2022 currently happening, I was fascinated to take a dataset related to Sports. That is when I came across the Olympic Games dataset. The Olympic Games are an international sports festival, held every four years. The ultimate goals of the Olympics are to cultivate human beings, through sport, and contribute to world peace. Before the 1970s the Games were officially limited to competitors with amateur status, but in the 1980s many events were opened to professional athletes. The history of the Olympics began some 2,300 years ago. Their origin lays in the Olympian Games, which were held in the Olympia area of ancient Greece. Based on theories, the Games have been said to have started as a festival of art and sport, to worship gods. The five-ring emblem of Olympics is familiar to most people as the Games’ symbol, which represents the unity of the five continents.\nIt would be interesting to analyze the Olympic Games dataset and answer questions about how the Olympics have evolved over time, performance of different regions, and the representation of female athletes in the Olympics."
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#read-in-data",
    "href": "posts/VinithaMaheswaran_FinalProject.html#read-in-data",
    "title": "DACSS 601: Final Paper",
    "section": "Read in data",
    "text": "Read in data\nFor this project, I will be working with the 120 years of Olympic history: athletes and results dataset. The Olympics data has two csv files - “athlete_events.csv” and “noc_regions.csv”. This historical dataset contains information on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. This data was scraped from www.sports-reference.com in May 2018 and is available on Kaggle.\nThe variables in the “athlete_events.csv”: 1) ID - Unique number for each athlete 2) Name - Athlete’s name 3) Sex - Male or Female (M/F) 4) Age - Age of the athlete 5) Height - Height of the athlete in centimeters 6) Weight - Weight of the athlete in kilograms 7) Team - Name of the team the athlete is representing 8) NOC - National Olympic Committee 3-letter code 9) Games - Year and season of the Olympic Games 10) Year - Year the Olympics Games was held 11) Season - Summer or Winter Olympics 12) City - Name of the city hosting the Olympics 13) Sport - Sport 14) Event - Event in which the athlete participated 15) Medal - Medal won by the athlete - Gold, Silver, Bronze, or NA\nThe variables in the “noc_regions.csv”: 1) NOC (National Olympic Committee 3 letter code) 2) region (the country name) 3) Notes\n\n\nCode\n# Reading the \"athlete_events.csv\" and \"noc_regions.csv\" files\n\nathlete_data <- read_csv(\"_data/athlete_events.csv\")\nnoc_data <- read_csv(\"_data/noc_regions.csv\")\n\n\n\n\nCode\n# Displaying athlete_data dataset\n\nathlete_data\n\n\n\n\n  \n\n\n\n\n\nCode\n# Displaying noc_data dataset\n\nnoc_data\n\n\n\n\n  \n\n\n\n\n\nCode\n# Finding dimension of both datasets\n\ndim(athlete_data)\n\n\n[1] 271116     15\n\n\nCode\ndim(noc_data)\n\n\n[1] 230   3\n\n\n\n\nCode\n# Structure of athlete_data dataset\n\nstr(athlete_data)\n\n\nspc_tbl_ [271,116 × 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ID    : num [1:271116] 1 2 3 4 5 5 5 5 5 5 ...\n $ Name  : chr [1:271116] \"A Dijiang\" \"A Lamusi\" \"Gunnar Nielsen Aaby\" \"Edgar Lindenau Aabye\" ...\n $ Sex   : chr [1:271116] \"M\" \"M\" \"M\" \"M\" ...\n $ Age   : num [1:271116] 24 23 24 34 21 21 25 25 27 27 ...\n $ Height: num [1:271116] 180 170 NA NA 185 185 185 185 185 185 ...\n $ Weight: num [1:271116] 80 60 NA NA 82 82 82 82 82 82 ...\n $ Team  : chr [1:271116] \"China\" \"China\" \"Denmark\" \"Denmark/Sweden\" ...\n $ NOC   : chr [1:271116] \"CHN\" \"CHN\" \"DEN\" \"DEN\" ...\n $ Games : chr [1:271116] \"1992 Summer\" \"2012 Summer\" \"1920 Summer\" \"1900 Summer\" ...\n $ Year  : num [1:271116] 1992 2012 1920 1900 1988 ...\n $ Season: chr [1:271116] \"Summer\" \"Summer\" \"Summer\" \"Summer\" ...\n $ City  : chr [1:271116] \"Barcelona\" \"London\" \"Antwerpen\" \"Paris\" ...\n $ Sport : chr [1:271116] \"Basketball\" \"Judo\" \"Football\" \"Tug-Of-War\" ...\n $ Event : chr [1:271116] \"Basketball Men's Basketball\" \"Judo Men's Extra-Lightweight\" \"Football Men's Football\" \"Tug-Of-War Men's Tug-Of-War\" ...\n $ Medal : chr [1:271116] NA NA NA \"Gold\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ID = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   Height = col_double(),\n  ..   Weight = col_double(),\n  ..   Team = col_character(),\n  ..   NOC = col_character(),\n  ..   Games = col_character(),\n  ..   Year = col_double(),\n  ..   Season = col_character(),\n  ..   City = col_character(),\n  ..   Sport = col_character(),\n  ..   Event = col_character(),\n  ..   Medal = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\n\nCode\n# Structure of noc_data dataset\n\nstr(noc_data)\n\n\nspc_tbl_ [230 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ NOC   : chr [1:230] \"AFG\" \"AHO\" \"ALB\" \"ALG\" ...\n $ region: chr [1:230] \"Afghanistan\" \"Curacao\" \"Albania\" \"Algeria\" ...\n $ notes : chr [1:230] NA \"Netherlands Antilles\" NA NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   NOC = col_character(),\n  ..   region = col_character(),\n  ..   notes = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\n\nCode\n#Summary of athlete_data\n\nlibrary(summarytools)\nprint(summarytools::dfSummary(athlete_data,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.60, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nathlete_data\nDimensions: 271116 x 15\n  Duplicates: 1385\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      ID\n[numeric]\n      Mean (sd) : 68249 (39022.3)min ≤ med ≤ max:1 ≤ 68205 ≤ 135571IQR (CV) : 67454.2 (0.6)\n      135571 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Name\n[character]\n      1. Robert Tait McKenzie2. Heikki Ilmari Savolainen3. Joseph \"Josy\" Stoffel4. Ioannis Theofilakis5. Takashi Ono6. Alexandros Theofilakis7. Alfrd (Arnold-) Hajs (Gut8. Andreas Wecker9. Jean Lucien Nicolas Jacob10. Alfred August \"Al\" Jochim[ 134721 others ]\n      58(0.0%)39(0.0%)38(0.0%)36(0.0%)33(0.0%)32(0.0%)32(0.0%)32(0.0%)32(0.0%)31(0.0%)270753(99.9%)\n      \n      0\n(0.0%)\n    \n    \n      Sex\n[character]\n      1. F2. M\n      74522(27.5%)196594(72.5%)\n      \n      0\n(0.0%)\n    \n    \n      Age\n[numeric]\n      Mean (sd) : 25.6 (6.4)min ≤ med ≤ max:10 ≤ 24 ≤ 97IQR (CV) : 7 (0.3)\n      74 distinct values\n      \n      9474\n(3.5%)\n    \n    \n      Height\n[numeric]\n      Mean (sd) : 175.3 (10.5)min ≤ med ≤ max:127 ≤ 175 ≤ 226IQR (CV) : 15 (0.1)\n      95 distinct values\n      \n      60171\n(22.2%)\n    \n    \n      Weight\n[numeric]\n      Mean (sd) : 70.7 (14.3)min ≤ med ≤ max:25 ≤ 70 ≤ 214IQR (CV) : 19 (0.2)\n      220 distinct values\n      \n      62875\n(23.2%)\n    \n    \n      Team\n[character]\n      1. United States2. France3. Great Britain4. Italy5. Germany6. Canada7. Japan8. Sweden9. Australia10. Hungary[ 1174 others ]\n      17847(6.6%)11988(4.4%)11404(4.2%)10260(3.8%)9326(3.4%)9279(3.4%)8289(3.1%)8052(3.0%)7513(2.8%)6547(2.4%)170611(62.9%)\n      \n      0\n(0.0%)\n    \n    \n      NOC\n[character]\n      1. USA2. FRA3. GBR4. ITA5. GER6. CAN7. JPN8. SWE9. AUS10. HUN[ 220 others ]\n      18853(7.0%)12758(4.7%)12256(4.5%)10715(4.0%)9830(3.6%)9733(3.6%)8444(3.1%)8339(3.1%)7638(2.8%)6607(2.4%)165943(61.2%)\n      \n      0\n(0.0%)\n    \n    \n      Games\n[character]\n      1. 2000 Summer2. 1996 Summer3. 2016 Summer4. 2008 Summer5. 2004 Summer6. 1992 Summer7. 2012 Summer8. 1988 Summer9. 1972 Summer10. 1984 Summer[ 41 others ]\n      13821(5.1%)13780(5.1%)13688(5.0%)13602(5.0%)13443(5.0%)12977(4.8%)12920(4.8%)12037(4.4%)10304(3.8%)9454(3.5%)145090(53.5%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1978.4 (29.9)min ≤ med ≤ max:1896 ≤ 1988 ≤ 2016IQR (CV) : 42 (0)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Season\n[character]\n      1. Summer2. Winter\n      222552(82.1%)48564(17.9%)\n      \n      0\n(0.0%)\n    \n    \n      City\n[character]\n      1. London2. Athina3. Sydney4. Atlanta5. Rio de Janeiro6. Beijing7. Barcelona8. Los Angeles9. Seoul10. Munich[ 32 others ]\n      22426(8.3%)15556(5.7%)13821(5.1%)13780(5.1%)13688(5.0%)13602(5.0%)12977(4.8%)12423(4.6%)12037(4.4%)10304(3.8%)130502(48.1%)\n      \n      0\n(0.0%)\n    \n    \n      Sport\n[character]\n      1. Athletics2. Gymnastics3. Swimming4. Shooting5. Cycling6. Fencing7. Rowing8. Cross Country Skiing9. Alpine Skiing10. Wrestling[ 56 others ]\n      38624(14.2%)26707(9.9%)23195(8.6%)11448(4.2%)10859(4.0%)10735(4.0%)10595(3.9%)9133(3.4%)8829(3.3%)7154(2.6%)113837(42.0%)\n      \n      0\n(0.0%)\n    \n    \n      Event\n[character]\n      1. Football Men's Football2. Ice Hockey Men's Ice Hock3. Hockey Men's Hockey4. Water Polo Men's Water Po5. Basketball Men's Basketba6. Cycling Men's Road Race, 7. Gymnastics Men's Individu8. Rowing Men's Coxed Eights9. Gymnastics Men's Team All10. Handball Men's Handball[ 755 others ]\n      5733(2.1%)4762(1.8%)3958(1.5%)3358(1.2%)3280(1.2%)2947(1.1%)2500(0.9%)2423(0.9%)2411(0.9%)2264(0.8%)237480(87.6%)\n      \n      0\n(0.0%)\n    \n    \n      Medal\n[character]\n      1. Bronze2. Gold3. Silver\n      13295(33.4%)13372(33.6%)13116(33.0%)\n      \n      231333\n(85.3%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\n\n\nCode\n#Summary of noc_data\n\nlibrary(summarytools)\nprint(summarytools::dfSummary(noc_data,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.60, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nnoc_data\nDimensions: 230 x 3\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      NOC\n[character]\n      1. AFG2. AHO3. ALB4. ALG5. AND6. ANG7. ANT8. ANZ9. ARG10. ARM[ 220 others ]\n      1(0.4%)1(0.4%)1(0.4%)1(0.4%)1(0.4%)1(0.4%)1(0.4%)1(0.4%)1(0.4%)1(0.4%)220(95.7%)\n      \n      0\n(0.0%)\n    \n    \n      region\n[character]\n      1. Germany2. Czech Republic3. Malaysia4. Russia5. Serbia6. Yemen7. Australia8. Canada9. China10. Greece[ 196 others ]\n      4(1.8%)3(1.3%)3(1.3%)3(1.3%)3(1.3%)3(1.3%)2(0.9%)2(0.9%)2(0.9%)2(0.9%)200(88.1%)\n      \n      3\n(1.3%)\n    \n    \n      notes\n[character]\n      1. Antigua and Barbuda2. Australasia3. Bohemia4. Crete5. Hong Kong6. Individual Olympic Athlet7. Netherlands Antilles8. Newfoundland9. North Borneo10. North Yemen[ 11 others ]\n      1(4.8%)1(4.8%)1(4.8%)1(4.8%)1(4.8%)1(4.8%)1(4.8%)1(4.8%)1(4.8%)1(4.8%)11(52.4%)\n      \n      209\n(90.9%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-23\n\n\n\n\nBriefly describe the data\nThe dataset contains information on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016 (the past 120 years). The Winter and Summer Games were held in the same year until 1992. After that, they were staggered such that the Winter Games occur once every 4 years starting with 1994, and the Summer Games occur once every 4 years starting with 1996. The “athlete_events.csv” file has 271,116 observations and 15 variables/attributes. Each row in this csv file corresponds to an individual athlete competing in an individual Olympic event (athlete-events). It includes information about the athlete’s name, gender, age, height (in cm), weight (in kg), team/country they represent, National Olympic Committee (3-letter code) they are representing, year and season participated, Olympic games host city for that year and season, sport, athlete event and medal won. Each athlete will have multiple observations in the data as they would have participated in multiple events and during different seasons. This csv file has 1385 duplicates which I will be investigating in the next steps. The “noc_regions.csv” file has 230 observations and 3 variables/attributes. This file contains information about the ‘NOC’ National Olympic Committee which is a 3-letter code, the corresponding region and notes. The file has 230 unique codes for the NOC variable. Few of the regions have same NOC code which in some cases is distinguished using the notes. The notes has missing value for 209 observations. The NOC variable is present in both the files and can be used as a key to join both the files into a single dataset.\n\n\nLooking into duplicate data\n\n\nCode\n# Displaying the duplicate observations in \"athlete_events.csv\" file\n\nduplicate_athlete_data <- athlete_data[duplicated(athlete_data),]\nduplicate_athlete_data\n\n\n\n\n  \n\n\n\n\n\nCode\ntable(duplicate_athlete_data$Sport)\n\n\n\nArt Competitions          Cycling    Equestrianism          Sailing \n            1315               32                1               37 \n\n\nThe “athlete_events.csv” file has 1385 duplicates as shown above. The table() shows that more than 90% of the duplicate observations are for the Sport ‘Art Competitions’. These duplicates could have been introduced during the data collection while performing scraping. The duplicates can be removed from the athlete_data during the data cleaning process and before joining the datasets."
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#tidy-data-and-mutate-variables-as-needed",
    "href": "posts/VinithaMaheswaran_FinalProject.html#tidy-data-and-mutate-variables-as-needed",
    "title": "DACSS 601: Final Paper",
    "section": "Tidy Data and Mutate Variables (as needed)",
    "text": "Tidy Data and Mutate Variables (as needed)\nThe noc_data has some missing value in 212 observations. Hence, I start by cleaning the noc_data.\n\n\nCode\n#Check for missing/null data in the noc_data\n\nsum(is.na(noc_data))\n\n\n[1] 212\n\n\nCode\nsum(is.null(noc_data))\n\n\n[1] 0\n\n\n\n\nCode\n# Checking which columns have NA values in noc_data\n\ncol <- colnames(noc_data)\nfor (c in col){\n  print(paste0(\"NA values in \", c, \": \", sum(is.na(noc_data[,c]))))\n}\n\n\n[1] \"NA values in NOC: 0\"\n[1] \"NA values in region: 3\"\n[1] \"NA values in notes: 209\"\n\n\nThe ‘region’ variable in noc_data has missing values for 3 observations. The corresponding NOC code for these 3 observations are ROT, TUV, and UNK. I have displayed the 3 observations below.\n\n\nCode\n# Displaying the observations with missing value in 'region' variable\n\nnoc_data%>%filter(is.na(region))\n\n\n\n\n  \n\n\n\n\n\nCode\n# Displaying the observations with same value for both 'region' and 'notes' variables\n\nnoc_data%>%filter(region==notes)\n\n\n\n\n  \n\n\n\nAlthough the ‘region’ value is missing for these observations, we have the ‘notes’ for them. From the notes it is evident that ROT stands for Refugee Olympic Team, TUV stands for Tuvalu and UNK stands for Unknown. I further analyzed whether there are any observations in noc_data with the same value for both ‘region’ and ‘notes’ variables and found 1 observation. For the NOC code ‘IOA’, the region and notes is given the value ‘Individual Olympic Athletes’. Hence, for the NOC codes ‘ROT’, ‘TUV’ and ‘UNK’ I decided to impute the missing ‘region’ values with the corresponding ‘notes’ values.\n\n\nCode\n# Imputing the missing 'region' values with the corresponding 'notes' values in noc_data\n\nnoc_data <- noc_data%>%\n  mutate(region = coalesce(region,notes))\n\n# Sanity Check: Checking that the 3 observations no longer have missing 'region' values\n\nnoc_data%>%filter(is.na(region))\n\n\n\n\n  \n\n\n\nThe ‘notes’ variable in noc_data has missing values for 209 observations. Since, this is more than 90% I decided to drop the ‘notes’ variable. After dropping the ‘notes’ variable from the noc_data, it is left with 230 observations and 2 variables.\n\n\nCode\n# Dropping the 'notes' variable from noc_data\nnoc_data <- noc_data%>%\n  select(-c(3))\n\n# Displaying the noc_data after tidying\n\nnoc_data\n\n\n\n\n  \n\n\n\nNext, I cleaned the athlete_data. As the first step of cleaning the athlete_data, I dropped the 1385 duplicate observations which I had identified earlier while exploring the data. After dropping the duplicate observations, the athlete_data has 269,731 observations and 15 variables.\n\n\nCode\n# Dropping the 1385 duplicate observations from athlete_data\n\nathlete_data <- athlete_data%>%\n  distinct()\n\n\nThe athlete_data has 359615 instances of missing values.\n\n\nCode\n#Check for missing/null data in the athlete_data\n\nsum(is.na(athlete_data))\n\n\n[1] 359615\n\n\nCode\nsum(is.null(athlete_data))\n\n\n[1] 0\n\n\nThe variables ‘Age’, ‘Height’, ‘Weight’ and ‘Medal’ have missing values in the athlete_data.\n\n\nCode\n# Checking which columns have NA values in athlete_data\n\ncol <- colnames(athlete_data)\nfor (c in col){\n  print(paste0(\"NA values in \", c, \": \", sum(is.na(athlete_data[,c]))))\n}\n\n\n[1] \"NA values in ID: 0\"\n[1] \"NA values in Name: 0\"\n[1] \"NA values in Sex: 0\"\n[1] \"NA values in Age: 9315\"\n[1] \"NA values in Height: 58814\"\n[1] \"NA values in Weight: 61527\"\n[1] \"NA values in Team: 0\"\n[1] \"NA values in NOC: 0\"\n[1] \"NA values in Games: 0\"\n[1] \"NA values in Year: 0\"\n[1] \"NA values in Season: 0\"\n[1] \"NA values in City: 0\"\n[1] \"NA values in Sport: 0\"\n[1] \"NA values in Event: 0\"\n[1] \"NA values in Medal: 229959\"\n\n\nThe ‘Medal’ variable has 13295 observations with value Bronze, 13108 observations with value Silver, and 13369 observations with value Gold. The remaining values are missing for ‘Medal’ variable. The missing values indicate that the athlete did not win a medal for that sport event during that year and season.\n\n\nCode\ntable(athlete_data$Medal)\n\n\n\nBronze   Gold Silver \n 13295  13369  13108 \n\n\nI handled the missing data in ‘Medal’ variable by imputing the missing values with ‘No Medal’ as the athlete did not win a medal.\n\n\nCode\n# Handling missing data in 'Medal' variable\n\nathlete_data <- athlete_data%>%\n  mutate(Medal = replace(Medal, is.na(Medal), \"No Medal\"))\n\n#Sanity Check: Checking that the 'Medal' variable has no missing values after data imputation\n\nsum(is.na(athlete_data$Medal))\n\n\n[1] 0\n\n\nCode\ntable(athlete_data$Medal)\n\n\n\n  Bronze     Gold No Medal   Silver \n   13295    13369   229959    13108 \n\n\nThe variables ‘Age’, ‘Height’, and ‘Weight’ have 9315, 58814, and 61527 missing values respectively. This is equivalent to 0.03%, 0.22% and 0.23% of missing values. This is a significantly large number and I performed data imputation for these variables. I imputed the missing values with the average Age, Height and Weight of the athletes grouped by Sex, Season, Year, and Event. I grouped based on those variables as the athletes participating in the various events are usually in the same age, height and weight range. For example, the male athletes participating in the heavy weight wrestling belong to weight categories like 55kg/60kg/etc.\n\n\nCode\n# Finding the percentage of missing values for the variables 'Age', 'Height', and 'Weight'\n\nathlete_data %>% summarize_all(funs(sum(is.na(.)) / length(.)))\n\n\n\n\n  \n\n\n\n\n\nCode\n# Handling the missing data in 'Age', 'Height', and 'Weight' variables using data imputation\n\n# Storing the average age, height, and weight for each group\naverage_athlete_data <- athlete_data%>%\n  group_by(Sex, Season, Year, Event)%>%\n  summarise(average_Age = mean(Age, na.rm = TRUE),\n            average_Height = mean(Height, na.rm = TRUE),\n            average_Weight = mean(Weight, na.rm = TRUE))\n\n# Joining the athlete_data and average_athlete_data using Sex, Season, Year and Event as the key\ncleaned_athlete_data = merge(x=athlete_data, y=average_athlete_data, by=c(\"Sex\", \"Season\", \"Year\", \"Event\"), all.x=TRUE)\ncleaned_athlete_data <- tibble(cleaned_athlete_data)\n\n# Replacing the missing values in 'Age', 'Height', and 'Weight' variables with the corresponding values in 'Average_Age', 'Average_Height', and 'Average_Weight' variables\ncleaned_athlete_data <- cleaned_athlete_data%>%\n    mutate(Age = coalesce(Age, average_Age),\n           Height = coalesce(Height, average_Height),\n           Weight = coalesce(Weight, average_Weight))\n\n# Dropping the variables 'Average_Age', 'Average_Height', and 'Average_Weight' from cleaned_athlete_data as they are no longer needed\n\ncleaned_athlete_data <- cleaned_athlete_data%>%\n  select(-c(16,17,18))\n\n# Rounded off the Age', 'Height', and 'Weight' variables to the nearest integer\n\ncleaned_athlete_data <- cleaned_athlete_data%>%\n  mutate(Age = round(Age, digits = 0),\n         Height = round(Height, digits = 0),\n         Weight = round(Weight, digits = 1))\n\n# Finding the percentage of missing values for the variables 'Age', 'Height', and 'Weight' to check whether the percentage of missing values has decreased\n\ncleaned_athlete_data %>% summarize_all(funs(sum(is.na(.)) / length(.)))\n\n\n\n\n  \n\n\n\n\n\nCode\n# Displaying the count of missing values in cleaned_athlete_data for each variable\n\ncol <- colnames(cleaned_athlete_data)\nfor (c in col){\n  print(paste0(\"NA values in \", c, \": \", sum(is.na(cleaned_athlete_data[,c]))))\n}\n\n\n[1] \"NA values in Sex: 0\"\n[1] \"NA values in Season: 0\"\n[1] \"NA values in Year: 0\"\n[1] \"NA values in Event: 0\"\n[1] \"NA values in ID: 0\"\n[1] \"NA values in Name: 0\"\n[1] \"NA values in Age: 149\"\n[1] \"NA values in Height: 5586\"\n[1] \"NA values in Weight: 12185\"\n[1] \"NA values in Team: 0\"\n[1] \"NA values in NOC: 0\"\n[1] \"NA values in Games: 0\"\n[1] \"NA values in City: 0\"\n[1] \"NA values in Sport: 0\"\n[1] \"NA values in Medal: 0\"\n\n\nThe percentage of missing values for the variables ‘Age’, ‘Height’, and ‘Weight’ has reduced from 0.03%, 0.22% and 0.23% to 0.00056%, 0.02% and 0.046 % respectively which is a significant improvement. The remaining missing values could not be imputed as all the observations in the groups (grouped by Sex, Season, Year and Event) had missing values for ‘Age’/‘Height’/‘Weight’ which makes it impossible to get the mean values. One possible solution is to remove all the observations with missing values in any of the variables. This would result in 12,792 observations being dropped which is about 5% of the total data. For now, I am keeping the observations with missing values. However, I can remove the 12,792 observations and store it in another tibble for performing visualization in the future.\nThe ‘Games’ variable is redundant as it contains information about the year and season of the Olympic games which is already present in the ‘Year’ and ‘Season’ variables. Hence, I dropped the ‘Games’ variable.\n\n\nCode\n# Dropping the 'Games' variable from cleaned_athlete_data\n\ncleaned_athlete_data <- cleaned_athlete_data%>%\n  select(-c(12))\n\n\nThe cleaned_athlete_data is left with 269731 observations and 14 variables after cleaning."
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#join-data",
    "href": "posts/VinithaMaheswaran_FinalProject.html#join-data",
    "title": "DACSS 601: Final Paper",
    "section": "Join Data",
    "text": "Join Data\nAs the next step after tidying the datasets, I joined the cleaned_athlete_data and noc_data using ‘NOC’ as the key, into a single dataset. The joined dataset has 269731 observations and 15 variables which makes sense as the cleaned_athlete_data had 269731 observations and cleaned_athlete_data and noc_data datasets had 14 and 2 attributes respectively. Since, the “NOC” attribute is common in both datasets we count it only once.\n\n\nCode\n# performed left join for cleaned_athlete_data and noc_data datasets.\nolympic_data = merge(x=cleaned_athlete_data, y=noc_data, by=\"NOC\", all.x=TRUE)\nolympic_data <- tibble(olympic_data)\nolympic_data\n\n\n\n\n  \n\n\n\nI rearranged the order of variables in olympic_data to make the data more understandable and easier for analyzing. I also sorted the olympic_data in ascending order based on ‘Season’ and ‘Year’.\n\n\nCode\n# Rearranging the columns in olympic_data\n\nolympic_data <- olympic_data%>%\n  select(c(\"Season\", \"Year\", \"ID\", \"Name\", \"Sex\", \"Age\", \"Height\", \"Weight\", \"Team\", \"NOC\", \"region\", \"City\", \"Sport\", \"Event\", \"Medal\"))\n\n# Sorting the olympic_data in ascending order based on 'Season' and 'Year'\n\nolympic_data <- olympic_data%>%\n  arrange(Season, Year)\n\n\nThe olympic_data is cleaned and can be used for exploratory data analysis using descriptive statistics and visualizations. The cleaned olympic_data is displayed below.\n\n\nCode\n# Displaying the cleaned olympic_data\nolympic_data"
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#descriptive-statistics",
    "href": "posts/VinithaMaheswaran_FinalProject.html#descriptive-statistics",
    "title": "DACSS 601: Final Paper",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI performed the summary() for the olympic_data to see the statistics for the numerical variables ‘Year’, ‘Age’, ‘Height’, and ‘Weight’. From the summary we can see confirm that the dataset has data for Olympic games ranging from years 1896 - 2016.\n\n\nCode\n# Summary for olympic_data\n\nsummary(olympic_data)\n\n\n    Season               Year            ID             Name          \n Length:269731      Min.   :1896   Min.   :     1   Length:269731     \n Class :character   1st Qu.:1960   1st Qu.: 34656   Class :character  \n Mode  :character   Median :1988   Median : 68233   Mode  :character  \n                    Mean   :1979   Mean   : 68265                     \n                    3rd Qu.:2002   3rd Qu.:102111                     \n                    Max.   :2016   Max.   :135571                     \n                                                                      \n     Sex                 Age            Height          Weight      \n Length:269731      Min.   :10.00   Min.   :127.0   Min.   : 25.00  \n Class :character   1st Qu.:22.00   1st Qu.:168.0   1st Qu.: 62.00  \n Mode  :character   Median :25.00   Median :175.0   Median : 70.00  \n                    Mean   :25.53   Mean   :175.1   Mean   : 70.86  \n                    3rd Qu.:28.00   3rd Qu.:182.0   3rd Qu.: 78.80  \n                    Max.   :97.00   Max.   :226.0   Max.   :214.00  \n                    NA's   :149     NA's   :5586    NA's   :12185   \n     Team               NOC               region              City          \n Length:269731      Length:269731      Length:269731      Length:269731     \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    Sport              Event              Medal          \n Length:269731      Length:269731      Length:269731     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n\n\nI plotted a kernel density plot for variables ‘Age’, ‘Height’ and ‘Weight’. Kernel density estimation is a nonparametric method for estimating the probability density function of a continuous random variable.\n\n\nCode\n# Plotting a kernel density graph for 'Age' variable\n\nggplot(olympic_data, aes(x = Age, fill = Sex)) +\n  geom_density(adjust = 2, alpha = 0.6) + \n  labs(title = \"Athletes by age\")\n\n\n\n\n\n\n\nCode\n# Plotting a kernel density graph for 'Height' variable\n\nggplot(olympic_data, aes(x = Height, fill = Sex)) +\n  geom_density(adjust = 2, alpha = 0.6) + \n  labs(title = \"Athletes by height\")\n\n\n\n\n\n\n\nCode\n# Plotting a kernel density graph for 'Weight' variable\n\nggplot(olympic_data, aes(x = Weight, fill = Sex)) +\n  geom_density(adjust = 2, alpha = 0.6) + \n  labs(title = \"Athletes by weight\")\n\n\n\n\n\nFrom the above plots, we can infer that the distribution of Height for both male and female athletes is symmetric which is little surprising. The distribution of Weight and Age for both male and female athletes are right-skewed indicating the mean>median.\nThe quantile statistics we get from the summary() for the variables ‘Age’, ‘Height’, and ‘Weight’ is not intuitive as the statistics are based on Male and Female athletes, and Summer and Winter games combined. I grouped the data based on ‘Season’ and ‘Sex’ and summarized the mean and median for the variables ‘Age’, ‘Height’, and ‘Weight’. The mean height and weight (175cm and 70kg respectively) obtained from the summary() on entire olympic_data is quite different from the mean height and weight (178cm and 75.4kg for male and 167cm and 59kg for female athletes) obtained from grouping the data based on season and sex. Hence, it is better to summarize the statistics for numerical variables after grouping the data as male and female quantile statistics will definitely be different.\n\n\nCode\n# Grouping the data based on 'Season' and 'Sex' and summarizing the mean and median for the variables 'Age', 'Height', and 'Weight'\n\nolympic_data%>%\n  group_by(Season, Sex)%>%\n  summarise(mean_Age = round(mean(Age, na.rm = TRUE),digits =1),\n            median_Age = median(Age, na.rm = TRUE),\n            mean_Height = round(mean(Height, na.rm = TRUE),digits =1),\n            median_Height = median(Height, na.rm = TRUE),\n            mean_Weight = round(mean(Weight, na.rm = TRUE),digits =1),\n            median_Weight = median(Weight, na.rm = TRUE))\n\n\n\n\n  \n\n\n\nI also grouped the data based on ‘Sex’ and summarized the mean and median for the variables ‘Age’, ‘Height’, and ‘Weight’. I did this to see how the mean and median of ‘Age’, ‘Height’, and ‘Weight’ variables changes with respect to grouping the data based on ‘Season’ and ‘Sex’. From the mean and median values below we can say that grouping the data based on season and sex or just by sex of the athletes does not change the statistics significantly. Therefore, for the remaining homework I will be focusing on the data grouped by ‘Sex’ variable.\n\n\nCode\n# Grouping the data based on 'Sex' and summarizing the mean and median for the variables 'Age', 'Height', and 'Weight'\n\nolympic_data%>%\n  group_by(Sex)%>%\n  summarise(mean_Age = round(mean(Age, na.rm = TRUE),digits =1),\n            median_Age = median(Age, na.rm = TRUE),\n            mean_Height = round(mean(Height, na.rm = TRUE),digits =1),\n            median_Height = median(Height, na.rm = TRUE),\n            mean_Weight = round(mean(Weight, na.rm = TRUE),digits =1),\n            median_Weight = median(Weight, na.rm = TRUE))\n\n\n\n\n  \n\n\n\nI have found the descriptive statistics for the numerical variables. Next, I will find the mode/frequency/distribution for the categorical variables like ‘Season’, ‘Sex’, ‘region’, ‘City’, ‘Sport’, ‘Event’, and ‘Medal’.\nI found the frequency of values for variable ‘Season’ using the table(), the proportion of values using prop.table() and plotted a bar plot to represent the distribution of data observations for Season. I am using bar plots as I want to visualize the distribution of categorical variables. From the output of table(), prop.table() and the bar plot we know that we have 4 times more data observations for the Olympics Summer games compared to the Olympics Winter games. 82% of the data observations are for the Summer games and the remaining 18% is for the Winter games. One reason for this is that Winter Olympics has lesser teams/athletes and events (119 distinct events) compared to the Summer Olympics (651 distinct events).\n\n\nCode\n# Frequency/Proportion for variable 'Season'\n\ntable(olympic_data$Season)\n\n\n\nSummer Winter \n221167  48564 \n\n\nCode\nprop.table(table(olympic_data$Season))\n\n\n\n  Summer   Winter \n0.819954 0.180046 \n\n\n\n\nCode\n# Bar graph representing the distribution of data observations for Season.\n\nlibrary(ggplot2)\n\nggplot(olympic_data, aes(y = Season)) + \n  geom_bar(fill=\"#F8766D\", width = 0.5) +\n  labs(title = \"Distribution of data observations for Season\", \n       x = \"Count\", y = \"Season\")\n\n\n\n\n\n\n\nCode\n# Displaying the count of distinct events in Winter Olympics\n\nolympic_data%>%\n  filter(Season==\"Winter\")%>%\n  summarise(Distinct_Event = n_distinct(Event))\n\n\n\n\n  \n\n\n\n\n\nCode\n# Displaying the count of distinct events in Summer Olympics\n\nolympic_data%>%\n  filter(Season==\"Summer\")%>%\n  summarise(Distinct_Event = n_distinct(Event))\n\n\n\n\n  \n\n\n\nIn order to further understand the reason behind lesser number of data observations for “Winter”, I decided to analyze the distribution of years and the distribution of Season over the years. I found the frequency of values for variable ‘Year’ using the table() and plotted a bar plot to represent the distribution of data observations for years 1896 - 2016.\n\n\nCode\n# Frequency for variable 'Year'\n\ntable(olympic_data$Year)\n\n\n\n 1896  1900  1904  1906  1908  1912  1920  1924  1928  1932  1936  1948  1952 \n  380  1898  1301  1733  3069  4040  4292  5570  5238  2817  7146  7383  9358 \n 1956  1960  1964  1968  1972  1976  1980  1984  1988  1992  1994  1996  1998 \n 6434  9235  9480 10479 11959 10502  8937 11588 14676 16413  3160 13780  3605 \n 2000  2002  2004  2006  2008  2010  2012  2014  2016 \n13821  4109 13443  4382 13602  4402 12920  4891 13688 \n\n\n\n\nCode\n# Bar graph representing the distribution of data observations over Year\n\nlibrary(ggplot2)\n\nggplot(olympic_data, aes(x = Year)) + \n  geom_bar(fill=\"#00BFC4\") +\n  ylim(0,20000) +\n  labs(title = \"Distribution of data observations over Year\", \n       x = \"Year\", y = \"Count\") +\n  geom_text(stat='count', aes(label=..count..), hjust = -0.3, size = 3, angle = 90)\n\n\n\n\n\nFrom the output of table() function and the bar plot we can observe that the year 1896 has the least number of data observations and the year 1992 has the most number of data observations. This is because the year 1896 had the least number of events (43) and the year 1992 had the most number of events (314) in the Olympic games which can be observed from the bar plot below. The Olympic games were held once every 4 years from 1896 - 1992 before staggering into Winter Olympics from 1994 and Summer Olympics from 1996. However, we can notice that there are few gaps in the bar plot for years 1916, 1940, and 1944. All three Olympic games were cancelled due to the World War. There was a Summer Olympics known as the Intercalated Olympic Games held between 1904 and 1908 in the year 1906. The Intercalated Olympic Games were conceived as a series of international athletics competitions that would take place every four years, halfway between the actual Olympics, and would always be hosted in Athens. However, they were only held once, in 1906 and discontinued after that as the athletes did not have enough time to prepare for the Games.\n\n\nCode\n# Grouping the data by 'Year' and summarizing for the count of distinct values for 'Event' in each group\n\nevent_over_year <- olympic_data%>%\n  group_by(Year)%>%\n  summarise(Distinct_Event = n_distinct(Event))\n\n# Bar plot representing the number of distinct events over the years\n\nggplot(event_over_year, aes(x = Year, y = Distinct_Event)) + \n  geom_bar(fill=\"#CD9600\", stat = \"identity\") +\n  ylim(0,350) +\n  labs(title = \"Distribution of Distinct Events over Years\", \n       x = \"Year\", y = \"Count of Distinct Events\") +\n  geom_text(aes(label=Distinct_Event), hjust = -0.3, size = 3, angle = 90)\n\n\n\n\n\nThe distribution of Season over the Year plot below conveys that the ‘Winter’ games originated in 1924. Since the ‘Summer’ games originated way before in 1896 we have more observations for ‘Summer’ games in addition to the reason of more number of events. From 1924 - 1992, both ‘Summer’ and ‘Winter’ Games were held on the same year. The Winter olympics has taken place in 22 distinct years and the Summer Olympics in 29 distinct years.\n\n\nCode\n# Stacked bar graph representing the distribution of Season over the years \n\nggplot(olympic_data, aes(x = Year, fill = Season)) + \n  geom_bar() +\n  labs(title = \"Distribution of Season over Year\", \n       x = \"Year\", y = \"Count\") \n\n\n\n\n\n\n\nCode\n# Displaying the count of distinct years in Winter Olympics\n\nolympic_data%>%\n  filter(Season==\"Winter\")%>%\n  distinct(Year)\n\n\n\n\n  \n\n\n\n\n\nCode\n# Displaying the count of distinct years in Summer Olympics\n\nolympic_data%>%\n  filter(Season==\"Summer\")%>%\n  distinct(Year)\n\n\n\n\n  \n\n\n\nI grouped the data by ‘Year’ and ‘Sex’ variables and summarized the count of unique athletes in order to find the distribution of ‘Sex’ over the years in the Olympic games. I plotted a line plot to represent the distribution of data observations for Sex.\n\n\nCode\n# Grouping by 'Year' and 'Sex' and summarizing the count of unique Athletes\n\nyear_sex <- olympic_data %>% \n  group_by(Year, Sex) %>%\n  summarize(athlete_count = length(unique(ID)))%>%\n  mutate(Year = as.integer(Year))\nyear_sex\n\n\n\n\n  \n\n\n\n\n\nCode\n# Line plot representing the count of athletes over the years grouped by Sex\n\nggplot(year_sex, aes(x = Year, y = athlete_count, group = Sex, color = Sex)) + \n  geom_line() +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Distribution of Athletes over the years grouped by Sex\", \n       x = \"Year\", y = \"Count of Athletes\")\n\n\n\n\n\nThe above line plot can be used to answer the research question: What is the ratio of male to female athletes participating in the Olympic games and has gender equality of athletes participating increased over the past 120 years? The participation of female athletes in Olympics has gradually increased over the past 120 years (which is clear from the trend line). The line plot above indicates that the 1896 Olympics did not have any Female athletes. From 1900 - 1960, the growth of female athletes in the Olympics was slow. After 1960, the Olympics had increased participation of female athletes which is a good sign that more countries are supporting female athletes (may also be a tactic to win more medals in women’s events and increase the medal tally). In the 2016 Rio Olympics, about 40% of the athletes were Female. After 1992, the ratio of male to female athletes has decreased. This can be seen from the gap between the trend lines decreasing after 1992. In future, we can expect the ratio of male to female athletes to be 1:1.\nNext, I grouped the data by ‘region’ variable and summarized the count of distinct ‘Year’ to find the number of times a region/country has participated in the Olympics. For the past 120 years (1896 - 2016), there have been 35 Olympic Games including both Summer and Winter.\n\n\nCode\n# Grouping by 'region' and summarizing the count of distinct 'Year'\n\nregion_participation <- olympic_data %>% \n  group_by(region) %>%\n  summarize(participation_count = length(unique(Year)))\nregion_participation\n\n\n\n\n  \n\n\n\nI plotted a bar plot to represent the participation of region in the Olympics. Since, there are 209 regions, I sorted the region_participation in increasing order of participation_count and took a subset of the data which had participation_count >= 30. This accounted to 21 observations with 7 regions having participated in all the Olympic Games from the beginning. The 7 regions are USA, UK, Switzerland, Italy, Greece, France, and Australia.\n\n\nCode\n# Bar graph representing the participation of region\n\nregion_participation <- region_participation%>%\n  arrange(participation_count)%>%\n  filter(participation_count>=30)\n\nggplot(region_participation, aes(x = reorder(region, participation_count), y = participation_count)) + \n  geom_bar(fill=\"#00BE67\", stat = \"identity\", width = 0.8) +\n  scale_y_continuous(limits = c(0, 35), breaks = seq(0, 35, by = 2)) +\n  labs(title = \"Participation of region in the Olympics\", \n       x = \"Region\", y = \"Number of times participated\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nNext, I grouped the data by ‘City’ variable and summarized the count of distinct ‘Year’ to find the number of times a city has hosted the Olympic Games. The host city is elected by a majority of the votes cast by secret ballot by the members of the IOC. Each active member has one vote.\n\n\nCode\n# Grouping by 'City' and summarizing the count of distinct 'Year'\n\nhost_city <- olympic_data %>% \n  group_by(City) %>%\n  summarize(host_city_count = length(unique(Year)))\nhost_city\n\n\n\n\n  \n\n\n\nI plotted a bar plot to represent the distribution of host city in the Olympics. There are 42 host cities and I sorted the host_city in increasing order of host_city. The cities Athina (alternate name for Athens) and London have hosted the Olympic games thrice and the cities Innsbruck, Paris, Lake Placid, Los Angeles, Sankt Moritz, and Stockholm have hosted the Olympics twice each. The remaining cities have hosted the Olympics only once from 1896 - 2016.\n\n\nCode\n# Bar graph representing the distribution of Host City\n\nhost_city <- host_city%>%\n  arrange(host_city_count)\n\nggplot(host_city, aes(x = City, y = host_city_count)) + \n  geom_bar(fill=\"#8494FF\", stat = \"identity\", width = 0.5) +\n  labs(title = \"Distribution of Host City in the Olympics\", \n       x = \"City\", y = \"Number of times hosted\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1)) +\n  geom_text(aes(label=host_city_count), vjust = -0.5, size = 3)\n\n\n\n\n\nNext, I grouped the data by ‘Sport’ variable and summarized the count of distinct ‘Year’ to find the number of times a Sport has been played in the Olympic Games.\n\n\nCode\n# Grouping by 'Sport' and summarizing the count of distinct 'Year'\n\nsport_year <- olympic_data %>% \n  group_by(Sport) %>%\n  summarize(sport_count = length(unique(Year)))\nsport_year\n\n\n\n\n  \n\n\n\nI plotted a bar plot to represent the distribution of Sport in the Olympics. There are 66 distinct sports and I sorted the sport_year in increasing order of ‘sport_count’. The sports Athletics, Cycling, Fencing, Gymnastics, and Swimming has had events in 29 Olympic Games followed by Rowing and Wrestling in 28 Olympic Games. One reason, for the Sports to have more occurrences in the Olympic Games is that they may have been introduced in the initial Olympic Games. There are few Sports which were introduced as an Olympic sport at a later stage. For example, Taekwondo was introduced as an Olympic Sport in the 2000s.\n\n\nCode\n# Bar graph representing the distribution of Sport\n\nsport_year <- sport_year%>%\n  arrange(sport_count)\n\nggplot(sport_year, aes(x = Sport, y = sport_count)) + \n  geom_bar(fill=\"#E68613\", stat = \"identity\", width = 0.5) +\n  labs(title = \"Distribution of Sport in the Olympics\", \n       x = \"Sport\", y = \"Frequency\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1)) +\n  geom_text(aes(label=sport_count), vjust = -0.5, size = 3)\n\n\n\n\n\nFinally, I found the frequency of values for variable ‘Medal’ using the table() and plotted a bar plot to represent the distribution of data observations for Medal grouped by Sex. From the output of table() and the bar plot we know that the number of bronze, silver and gold medals are approximately 13,000 each and male athletes have won more mdeals compared to female athletes. Out of the 269,731 observations in olympic_data, about 39,772 observations are for medal winning athletes.\n\n\nCode\n# Frequency for variable 'Medal'\n\ntable(olympic_data$Medal)\n\n\n\n  Bronze     Gold No Medal   Silver \n   13295    13369   229959    13108 \n\n\nCode\nprop.table(table(olympic_data$Medal))\n\n\n\n    Bronze       Gold   No Medal     Silver \n0.04928985 0.04956420 0.85254939 0.04859656 \n\n\n\n\nCode\n# Bar graph representing the distribution of data observations for Medal grouped by Sex\n\nggplot(olympic_data, aes(y = Medal)) + \n  geom_bar(fill=\"#7CAE00\", width = 0.5) +\n  facet_grid(~Sex) +\n  labs(title = \"Distribution of data observations for Medal grouped by Sex\", \n       x = \"Count\", y = \"Medal\")\n\n\n\n\n\nI also filtered the olympic_data to remove the observations which had the value ‘No Medal’ and plotted a stacked bar graph to represent the distribution of Medal over Year. This helps us to verify that the ratio of bronze:silver:gold medals over the years is almost 1:1:1. This makes sense as each event will have bronze, silver and gold medals awarded.\n\n\nCode\n# Stacked bar graph representing the distribution of Medal over Year\n\nmedal_year <- olympic_data%>%\n  filter(Medal!=\"No Medal\")\n\nggplot(medal_year, aes(x = Year, fill = Medal)) + \n  geom_bar() +\n  labs(title = \"Distribution of Medal over Year\", \n       x = \"Count\", y = \"Year\")"
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#visualizations",
    "href": "posts/VinithaMaheswaran_FinalProject.html#visualizations",
    "title": "DACSS 601: Final Paper",
    "section": "Visualizations",
    "text": "Visualizations\nThere are few research questions which can be answered using the cleaned ‘olympic_data’ dataset and visualizations.\n\nTop Medal Winning Athletes\nResearch question: Which female athlete and male athlete have won the most number of medals in the Olympic Games held from 1896 - 2016 and analyze their distribution of winning medals? Which female and male athlete have won the most number of medals in a single year of the Olympic Games? What is the distribution of medals won by the top 5 medal winning athletes over the years?\nAs the first step to answer this question, I split the olympic_data into 2 separate tibbles - one containing information about female athletes and another one about male athletes.\n\n\nCode\n# Splitting into female_olympic_data and male_olympic_data\n\nfemale_olympic_data <- olympic_data%>%\n  filter(Sex==\"F\")\nmale_olympic_data <- olympic_data%>%\n  filter(Sex==\"M\")\n\n# Filtering the 'No Medal' observations, grouping by 'Name', performing pivot_wider, summarizing the count of medals won, sorting based on total medals and finding the top 5 medal winning female athletes\n\nmax_medal_female <- female_olympic_data %>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(Name) %>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal))%>%\n  head(5)\nmax_medal_female\n\n\n\n\n  \n\n\n\n\n\nCode\n# Filtering the 'No Medal' observations, grouping by 'Name', performing pivot_wider, summarizing the count of medals won, sorting based on total medals and finding the top 5 medal winning male athletes\n\nmax_medal_male <- male_olympic_data %>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(Name) %>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal))%>%\n  head(5)\nmax_medal_male\n\n\n\n\n  \n\n\n\nIn order to find the top 5 medal winning female athletes, I filtered the ‘No Medal’ observations from female_olympic_data, grouped by ‘Name’, found count of medals for Gold, Silver,and Bronze, performed pivot_wider to get one observation for each athlete, mutated ‘Total_Medal’ variable by adding Gold+Silver+Bronze medals, sorted based on ‘Total_Medal’ in descending order and extracted the top 5 observations. The same steps were repeated for finding the top 5 medal winning male athletes. Larysa Semenivna Latynina (Diriy-) is the female athlete that has won the most number of medals (18) in Olympics in the past 120 years. She represented the Soviet Union team for the Gymnastic Sport and won 9 Gold, 5 Silver and 4 Bronze medals. Michael Fred Phelps, II is the male athlete that has won the most number of medals (28) in Olympics in the past 120 years. He represented team USA for the Swimming Sport and won 23 Gold, 3 Silver and 2 Bronze medals.\n\n\nCode\n# Distribution of Medals won by top 5 medal winning female athletes over the years\n\ntop_medal_female <- female_olympic_data%>%\n  filter(Name %in% max_medal_female$Name)%>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(Name, Year)%>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal))\n\n# Line plot representing the distribution of Medals over the years grouped by Athlete Name\n\nggplot(top_medal_female, aes(x = Year, y = Total_Medal, group = Name, color = Name)) + \n  geom_line() +\n  geom_point() +\n  labs(title = \"Distribution of Medals over the years grouped by Athlete Name (Female)\", \n       x = \"Year\", y = \"Total Medals won\")\n\n\n\n\n\n\n\nCode\n# Distribution of Medals won by top 5 medal winning male athletes over the years\n\ntop_medal_male <- male_olympic_data%>%\n  filter(Name %in% max_medal_male$Name)%>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(Name, Year)%>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal))\n\n# Line plot representing the distribution of Medals over the years grouped by Athlete Name\n\nggplot(top_medal_male, aes(x = Year, y = Total_Medal, group = Name, color = Name)) + \n  geom_line() +\n  geom_point() +\n  labs(title = \"Distribution of Medals over the years grouped by Athlete Name (Male)\", \n       x = \"Year\", y = \"Total Medals won\")\n\n\n\n\n\nFrom the above line plots, we can analyze the distribution of medals won by the top 5 medal winning athletes. Semenivna Latynina (Diriy-) won 6 medals each in years 1956, 1960, and 1964. Michael Fred Phelps, II won 8,8,6, and 6 medals in years 2004, 2008, 2012 and 2016 respectively.\nNext, I have to find the female and male athlete who won the most number of medals in a single year of the Olympic Games.\n\n\nCode\n# Filtering the 'No Medal' observations, grouping by 'Year' and 'Name', performing pivot_wider, summarizing the count of medals won, sorting based on total medals and finding the top 6 medal winning female athletes\n\nmax_medal_female_year <- female_olympic_data %>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(Year,Name) %>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal), desc(Gold))%>%\n  head(6)\nmax_medal_female_year\n\n\n\n\n  \n\n\n\n\n\nCode\n# Filtering the 'No Medal' observations, grouping by 'Year' and 'Name', performing pivot_wider, summarizing the count of medals won, sorting based on total medals and finding the top 6 medal winning male athletes\n\nmax_medal_male_year <- male_olympic_data %>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(Year, Name) %>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal), desc(Gold))%>%\n  head(5)\nmax_medal_male_year\n\n\n\n\n  \n\n\n\nMariya Kindrativna Horokhovska is the female athlete that won the highest number of medals (7) in a single year 1952 of the Olympic Games. She won 2 Gold and 5 Silver medals. Michael Fred Phelps, II is the male athlete that won the highest number of medals (8) in 2 consecutive Olympics held in years 2008 and 2004. He won 8 Gold medals in 2008 and 6 Gold and 2 Bronze medals in 2004. Aleksandr Nikolayevich Dityatin is another male athlete who won 8 medals in the year 1980. He won 3 Gold, 4 Silver and 1 Bronze medals representing team Soviet Union for Sport Gymnastics.\n\n\nTop Medal Winning Region\nResearch Question: **Which region has won the highest number of medals in the Olympic history? What is the distribution of the top 10 medal winning regions?\nFor answering this question, I will be working with the olympic_data.\n\n\nCode\n# Find total number of medals won in Olympics history\n\ntotal_medal <- olympic_data%>%\n  filter(Medal!=\"No Medal\")%>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))\n\n# Filtering the 'No Medal' observations, grouping by 'region', performing pivot_wider, summarizing the count of medals won, sorting based on total medals and finding the top 10 medal winning regions\n\nmax_medal_region <- olympic_data %>%\n  filter(Medal!=\"No Medal\")%>%\n  group_by(region) %>%\n  count(Medal)%>%\n  pivot_wider(names_from = Medal, values_from = n)%>%\n  mutate(Total_Medal = sum(Gold, Silver, Bronze, na.rm=TRUE))%>%\n  arrange(desc(Total_Medal))%>%\n  head(10)%>%\n  mutate(Medal_Won_Percentage = round(Total_Medal/total_medal$Total_Medal,digits = 2)*100)\nmax_medal_region\n\n\n\n\n  \n\n\n\nIn order to find the top 10 medal winning regions, I filtered the ‘No Medal’ observations from olympic_data, grouped by ‘region’, found count of medals for Gold, Silver,and Bronze, performed pivot_wider to get one observation for each region, mutated ‘Total_Medal’ variable by adding Gold+Silver+Bronze medals, sorted based on ‘Total_Medal’ in descending order and extracted the top 10 observations. USA has won the highest number of medals in the Olympics history followed by Russia and Germany. USA has won a total of 5637 medals including 2638 Gold, 1641 Silver, and 1358 Bronze medals. A total of 39,772 medals have been awarded in the Olympics history. I also computed the percentage of medals won out of the total medals by the top 10 medal winning regions. USA has won 14% of the total medals in Olympics.\nNext, I plotted a grouped bar graph to visualize the distribution of medals for the top 10 medal winning regions. Grouped bar chart enables us to compare the different medals (Gold, Silver and Bronze) won by a region within itself and among other regions also. For this purpose, I used the medal_year which I had created earlier while doing descriptive statistics for ‘Medal’ variable.\n\n\nCode\n# Distribution of Medals won by top 10 medal winning regions\n\ntop_medal_region <- medal_year%>%\n  filter(region %in% max_medal_region$region)%>%\n  filter(Medal!=\"No Medal\")\n\n# Grouped bar graph representing the distribution of Medals for the top 10 medal winning regions\n\nggplot(top_medal_region, aes(x = region, fill = Medal)) + \n  geom_bar(position = \"dodge\",width = 0.8) +\n  labs(title = \"Distribution of Medals for top 10 medal winning regions\", \n      x = \"Region\", y = \"Total Medals won\")\n\n\n\n\n\nFrom the grouped bar chart, we can infer that the top 3 medal winning regions have more Gold medals compared to Silver and Bronze.\n\n\nAnalyzing Age/Height/Weight of Athletes over the years\nResearch question: Has there been a significant change in the age/height/weight of athletes participating in the various events over the years?\nI plotted a box plot to represent the Height of Athletes over the years grouped by ‘Sex’ after dropping all the observations with missing values for ‘Height’ and ‘Weight’. After plotting, I noticed that most of the observations for Female athletes has missing ‘Height’ or ‘Weight’ values before the year 1920. Hence, I plotted the box plot starting after 1920. For the purpose of plotting the box plots, I converted the ‘Year’ variable to factor.\n\n\nCode\n# Box plot representing the Height of Athletes over the years grouped by Sex\n\nolympic_data %>% \n  filter(!is.na(Height), !is.na(Weight))%>%\n  filter(Year>1920)%>%\n  ggplot(aes(x=as.factor(Year), y=Height, fill=Sex)) +\n  geom_boxplot() +\n  labs(title = \"Height(cm) of Athletes over the years\", \n       x = \"Year\", y = \"Height(cm)\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nFrom the above box plot, we can observe that for both male and female athletes the height has gradually increased over the years. For each year, we can see few outliers. The Athletes with extremely short height usually participate in Gymnastics and Boxing Sports and the extremely tall athletes usually participate in Sports like Basketball and Volleyball.\nNext, I plotted a box plot to represent the Height of Athletes over the years grouped by ‘Sex’ after dropping all the observations with missing values for ‘Height’ and ‘Weight’. After plotting, I noticed that most of the observations for Female athletes has missing ‘Height’ or ‘Weight’ values before the year 1920. Hence, I plotted the box plot starting after 1920.\n\n\nCode\n# Box plot representing the Weight of Athletes over the years grouped by Sex\n\nolympic_data %>% \n  filter(!is.na(Height), !is.na(Weight))%>%\n  filter(Year>1920)%>%\n  ggplot(aes(x=as.factor(Year), y=Weight, fill=Sex)) +\n  geom_boxplot() +\n  labs(title = \"Weight(kg) of Athletes over the years\", \n       x = \"Year\", y = \"Weight(kg)\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nFrom the above box plot, we can observe that for both male and female athletes the Weight has gradually increased over the years. For each year, we can see few outliers. The Athletes with extremely low weight usually participate in Gymnastics Sport and the extremely heavy weight athletes usually participate in Sports like Judo, Wrestling, and Weightlifting.\n\n\nCode\n# Box plot representing the Age of Athletes over the years grouped by Sex\n\nolympic_data %>% \n  ggplot(aes(x=as.factor(Year), y=Age, fill=Sex)) +\n  geom_boxplot() +\n  labs(title = \"Age of Athletes over the years\", \n       x = \"Year\", y = \"Age\") +\n  theme(axis.text.x=element_text(angle=90, hjust=1))\n\n\n\n\n\nThe age distribution fluctuates in the starting few years and gradually increases after the year 1964. The age distribution for female athletes in the year 1904 is very different from the remaining years. There are few instances where the Age of athletes is above 75 which is quite abnormal for participating in Sports. I looked into the data to analyze the reasons behind these.\n\n\nCode\n# Displaying the data observations of Female athletes for the year 1904\n\nolympic_data%>%\n  filter(Sex==\"F\")%>%\n  filter(Year==1904)\n\n\n\n\n  \n\n\n\nAll the 16 female athletes in the 1904 Olympics, participated in the Archery sport and represent the region USA.\n\n\nCode\n# Displaying the data observations of athletes >= 75 Age\n\nolympic_data%>%\n  filter(Age>=75)\n\n\n\n\n  \n\n\n\nThe 14 athletes with Age >= 75 participated in the ‘Art Competitions’ Sport for Olympic Games which makes sense as Art Competitions does not require mcuh stamina and adrenaline like other sports. The Art Competitions were held at the Olympics from 1912 - 1948."
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#reflection",
    "href": "posts/VinithaMaheswaran_FinalProject.html#reflection",
    "title": "DACSS 601: Final Paper",
    "section": "Reflection",
    "text": "Reflection\n\nProcess\nI started the project by briefly exploring the data and understanding the datatype of each variable. I performed data cleaning on both the datasets and handled missing values for Height, Weight and Age using the mean value grouped by Season, Sex, Year and Event (as it did not make sense to find the mean Age/Height/Weight over the entire data), and for the region using the notes. After cleaning both the datasets, I joined the datasets using NOC code as the key. I used this clean dataset for further analysis. The descriptive statistics gave me insights about the distribution of the variables for Male and Female athletes. Following this, I used visualizations to answer few research questions about the athletes who won the most medals and the distribution of the top 5 medal winning Male and Female athletes. I was also able to find the 3 regions which have won the most medals (accounting to around 35% of the total medals in the Olympic Games). I was able to analyze whether there has been a significant change in the age/height/weight of athletes participating in the various events over the years? The current dataset does not have any information about the coordinates of the regions which makes it impossible to visualize the representation of each region in the Olympic Games for a given year. It would require extra effort to map each region to their corresponding latitude and longitude form the world map.\n\n\nFuture Work\n\nAnalyzing Host City Advantage\nResearch question: Does the host city have any advantage in Olympic games in terms of winning more medals? (i.e does the Team/NOC/region win more medals when the City hosting the Olympic games is in that region)\nAs part of the next steps, I would like to find whether hosting the Olympics gives the region an added advantage in terms of winning more medals. One reason for this may be that, since the host city is constantly in the spotlight during the Olympics it would be bad publicity if the City does not perform well in the Olympics. There is a possibility that the region Government allocates more resources for training their athletes to perform well and win more medals. For finding this out, we would have to map the host city with the region and find the average medals won by the region in the years it did not host the Olympics and compare it with the medals won during the year it hosted the Olympics Games. There is also a possibility, that the hosting cities are the ones with huge wealth and resources to provide training, indicating that they may perform well either way.\n\n\nAnalyzing relationship between Height/Weight and Winning Medals\nResearch question: Does the Height/Weight of the athletes have any correlation with the possibility of winning Medals in the Olympic Games for each Sport and grouped by Gender?\nFor example, we can see that the mean Height and Weight for male athletes playing basketball and winning Gold medals is higher than the non-medal winning male athletes playing basketball. It would be useful to plot a correlation heat map or scatterplot to find the relation between winning medals and height/weight of Athlete grouped by Sport and Sex. We can also try to find whether the athletes can be grouped into clusters based on the Medal Type.\n\n\nCode\n# Summary of Height and Weight for Gold medal winning male athletes\n\nmale_olympic_data%>%\n  filter(Medal==\"Gold\")%>%\n  filter(Sport==\"Basketball\")%>%\n  select(c(Height,Weight))%>%\n  summary()\n\n\n     Height          Weight      \n Min.   :177.0   Min.   : 68.00  \n 1st Qu.:191.0   1st Qu.: 86.00  \n Median :198.0   Median : 95.00  \n Mean   :197.8   Mean   : 95.33  \n 3rd Qu.:205.0   3rd Qu.:104.00  \n Max.   :223.0   Max.   :137.00  \n\n\nCode\n# Summary of Height and Weight for non-medal winning male athletes\n\nmale_olympic_data%>%\n  filter(Medal==\"No Medal\")%>%\n  filter(Sport==\"Basketball\")%>%\n  select(c(Height, Weight))%>%\n  summary()\n\n\n     Height          Weight      \n Min.   :163.0   Min.   : 59.00  \n 1st Qu.:186.0   1st Qu.: 81.90  \n Median :190.0   Median : 85.00  \n Mean   :192.2   Mean   : 88.55  \n 3rd Qu.:199.0   3rd Qu.: 95.00  \n Max.   :226.0   Max.   :156.00"
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#conclusion",
    "href": "posts/VinithaMaheswaran_FinalProject.html#conclusion",
    "title": "DACSS 601: Final Paper",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the above results and visualizations, we can infer that the participation and performance of female athletes in the Olympics is increasing and the ratio gap between male and female athletes is decreasing which is a sigh of gender equality. We have come a long way from not having any female athletes in the 1896 Olympics to 40% of the athletes in the 2016 Rio Olympics being Female. Larysa Semenivna Latynina (Diriy-) is the female athlete that has won the most number of medals (18) and Michael Fred Phelps, II is the male athlete that has won the most number of medals (28) in Olympics in the past 120 years. Larysa represented team Soviet Union (region Russia) and Phelps represents region USA. USA has won the highest number of medals in Olympic history followed by Russia and then Germany. No wonder both the highest medal winning female and male athlete are from those regions. They would have had better resources and more training as these regions give a lot of importance to Sports. Olympics was one place where USA and Russia could compete without fighting war. The mean height/weight of athletes for both male and female athletes have increased gradually over time (other than few outliers) for which we have analyzed the reasons. Other than that, I did not find any significant relationship between Age/Height/Weight of athletes participating in different Sports and winning medals. It would be nice to continue working on the next steps to make further inference."
  },
  {
    "objectID": "posts/VinithaMaheswaran_FinalProject.html#bibliography",
    "href": "posts/VinithaMaheswaran_FinalProject.html#bibliography",
    "title": "DACSS 601: Final Paper",
    "section": "Bibliography",
    "text": "Bibliography\n\nDataset (Sourced from Kaggle) https://www.kaggle.com/datasets/heesoo37/120-years-of-olympic-history-athletes-and-results?datasetId=31029&sortBy=voteCount&language=R&select=noc_regions.csv\nCourse Textbook https://r4ds.had.co.nz/index.html\nData Visualization with R ggplot2 https://rkabacoff.github.io/datavis/\nR programming Language https://www.r-project.org\nOlympic Games https://en.wikipedia.org/wiki/Olympic_Games\nIntercalated Games: the forgotten Athens mid-Olympics of 1906 https://www.greeknewsagenda.gr/topics/culture-society/7516-intercalated-games\nOlympic Games cancelled https://www.historians.org/research-and-publications/perspectives-on-history/summer-2021/the-phantom-olympics-why-japan-forfeited-hosting-the-1940-olympics\nOlympic Art Competitions https://www.olympic-museum.de/art/artcompetition.php\nOlympic events introduced in 2000 https://olympics.com/en/olympic-games/sydney-2000\nOlympics History https://www.2020games.metro.tokyo.lg.jp/eng/taikaijyunbi/olympic/olympic/index.html"
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "",
    "text": "Global Terrorism Dataset"
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html#introduction",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html#introduction",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "Introduction",
    "text": "Introduction\n26/11 (26 Nov) is a very heartbreaking day to remember for a lot of Indians. It was the day when India had one of its deadliest terror attacks. It happened in 2008 and I still remember watching the news for hourly updates on the terror attack. Every year when I come across this day, I always have a lot of questions in my head. How have the trends of Terrorism been over the years? Why are certain cities targeted more than others? What kind of weapons do different extremist groups use? Which is the most deadly extremist group? Is the extremism international or home-grown? and a lot more. The topic for my final project was inspired by this and I decided to do this study on Global Terrorism to help answer my questions. The Global Terrorism Database (GTD) contains more than 200,000 records of terrorist attacks that have taken place around the world from 1970 till 2022. This Database is maintained by the National Consortium for the Study of Terrorism and Responses to Terrorism (START) at the University of Maryland. The database has a codebook wherein detailed explanations for each of the categories are provided. The database has been used by popular news channels to showcase trends in Regional Terror activities. It has all the necessary elements that can help me answer various questions that I hope to find with this study."
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html#data",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html#data",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "Data",
    "text": "Data\n\nReading The Dataset\nThe dataset is in the form of an excel file. It has nearly 209706 rows and 135 columns. Each row corresponds to a terror incident. The various fields include GTD ID, incident date, incident location, incident information, attack information, target/victim information, perpetrator information, perpetrator statistics, claims of responsibility, weapon information, casualty information, consequences, kidnapping/hostage taking information, additional and source information and many more. These fields need to be analysed and only the important ones are to be chosen for the study.\n\n#reading in data set which is in excel format\n# gtd <- read_excel('_data/globalterrorismdb_0522dist.xlsx')\nhead(gtd, n=100)\n\n\n\n  \n\n\n# save.image(\"VishnupriyaVaradharaju.RData\")\n\n\n\nDescribing The Data\nOn having an initial overview of the data, there seems to be a lot of columns with plenty of NA values. Given that there are 135 columns, it will be good to remove those columns that might not add enough value to the analysis. This data has values that are a mix of different types ranging from numerical, character, logical and much more. There is some redundant data as well. Certain fields have categories that are encoded. But along with it the corresponding text variable is also given. For instance, country has codes for which country_txt has the corresponding text. Such fields can be removed to avoid redundancy. Some fields like summary, weapdetail have entirely textual data which is not useful for statistical analysis and can be removed.\n\nstr(gtd)\n\ntibble [209,706 × 135] (S3: tbl_df/tbl/data.frame)\n $ eventid           : num [1:209706] 1.97e+11 1.97e+11 1.97e+11 1.97e+11 1.97e+11 ...\n $ iyear             : num [1:209706] 1970 1970 1970 1970 1970 1970 1970 1970 1970 1970 ...\n $ imonth            : num [1:209706] 7 0 1 1 1 1 1 1 1 1 ...\n $ iday              : num [1:209706] 2 0 0 0 0 1 2 2 2 3 ...\n $ approxdate        : chr [1:209706] NA NA NA NA ...\n $ extended          : num [1:209706] 0 0 0 0 0 0 0 0 0 0 ...\n $ resolution        : POSIXct[1:209706], format: NA NA ...\n $ country           : num [1:209706] 58 130 160 78 101 217 218 217 217 217 ...\n $ country_txt       : chr [1:209706] \"Dominican Republic\" \"Mexico\" \"Philippines\" \"Greece\" ...\n $ region            : num [1:209706] 2 1 5 8 4 1 3 1 1 1 ...\n $ region_txt        : chr [1:209706] \"Central America & Caribbean\" \"North America\" \"Southeast Asia\" \"Western Europe\" ...\n $ provstate         : chr [1:209706] \"National\" \"Federal\" \"Tarlac\" \"Attica\" ...\n $ city              : chr [1:209706] \"Santo Domingo\" \"Mexico city\" \"Unknown\" \"Athens\" ...\n $ latitude          : num [1:209706] 18.5 19.4 15.5 38 33.6 ...\n $ longitude         : num [1:209706] -70 -99.1 120.6 23.8 130.4 ...\n $ specificity       : num [1:209706] 1 1 4 1 1 1 1 1 1 1 ...\n $ vicinity          : num [1:209706] 0 0 0 0 0 0 0 0 0 0 ...\n $ location          : chr [1:209706] NA NA NA NA ...\n $ summary           : chr [1:209706] NA NA NA NA ...\n $ crit1             : num [1:209706] 1 1 1 1 1 1 1 1 1 1 ...\n $ crit2             : num [1:209706] 1 1 1 1 1 1 1 1 1 1 ...\n $ crit3             : num [1:209706] 1 1 1 1 1 1 1 1 1 1 ...\n $ doubtterr         : num [1:209706] 0 0 0 0 -9 0 0 1 0 0 ...\n $ alternative       : num [1:209706] NA NA NA NA NA NA NA 2 NA NA ...\n $ alternative_txt   : chr [1:209706] NA NA NA NA ...\n $ multiple          : num [1:209706] 0 0 0 0 0 0 0 0 0 0 ...\n $ success           : num [1:209706] 1 1 1 1 1 1 0 1 1 1 ...\n $ suicide           : num [1:209706] 0 0 0 0 0 0 0 0 0 0 ...\n $ attacktype1       : num [1:209706] 1 6 1 3 7 2 1 3 7 7 ...\n $ attacktype1_txt   : chr [1:209706] \"Assassination\" \"Hostage Taking (Kidnapping)\" \"Assassination\" \"Bombing/Explosion\" ...\n $ attacktype2       : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ attacktype2_txt   : chr [1:209706] NA NA NA NA ...\n $ attacktype3       : logi [1:209706] NA NA NA NA NA NA ...\n $ attacktype3_txt   : logi [1:209706] NA NA NA NA NA NA ...\n $ targtype1         : num [1:209706] 14 7 10 7 7 3 3 21 4 2 ...\n $ targtype1_txt     : chr [1:209706] \"Private Citizens & Property\" \"Government (Diplomatic)\" \"Journalists & Media\" \"Government (Diplomatic)\" ...\n $ targsubtype1      : num [1:209706] 68 45 54 46 46 22 25 107 28 21 ...\n $ targsubtype1_txt  : chr [1:209706] \"Named Civilian\" \"Diplomatic Personnel (outside of embassy, consulate)\" \"Radio Journalist/Staff/Facility\" \"Embassy/Consulate\" ...\n $ corp1             : chr [1:209706] NA \"Belgian Ambassador Daughter\" \"Voice of America\" NA ...\n $ target1           : chr [1:209706] \"Julio Guzman\" \"Nadine Chaval, daughter\" \"Employee\" \"U.S. Embassy\" ...\n $ natlty1           : num [1:209706] 58 21 217 217 217 217 218 217 217 217 ...\n $ natlty1_txt       : chr [1:209706] \"Dominican Republic\" \"Belgium\" \"United States\" \"United States\" ...\n $ targtype2         : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ targtype2_txt     : chr [1:209706] NA NA NA NA ...\n $ targsubtype2      : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ targsubtype2_txt  : chr [1:209706] NA NA NA NA ...\n $ corp2             : chr [1:209706] NA NA NA NA ...\n $ target2           : chr [1:209706] NA NA NA NA ...\n $ natlty2           : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ natlty2_txt       : chr [1:209706] NA NA NA NA ...\n $ targtype3         : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ targtype3_txt     : chr [1:209706] NA NA NA NA ...\n $ targsubtype3      : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ targsubtype3_txt  : chr [1:209706] NA NA NA NA ...\n $ corp3             : chr [1:209706] NA NA NA NA ...\n $ target3           : chr [1:209706] NA NA NA NA ...\n $ natlty3           : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ natlty3_txt       : chr [1:209706] NA NA NA NA ...\n $ gname             : chr [1:209706] \"MANO-D\" \"23rd of September Communist League\" \"Unknown\" \"Unknown\" ...\n $ gsubname          : chr [1:209706] NA NA NA NA ...\n $ gname2            : chr [1:209706] NA NA NA NA ...\n $ gsubname2         : logi [1:209706] NA NA NA NA NA NA ...\n $ gname3            : logi [1:209706] NA NA NA NA NA NA ...\n $ gsubname3         : logi [1:209706] NA NA NA NA NA NA ...\n $ motive            : chr [1:209706] NA NA NA NA ...\n $ guncertain1       : num [1:209706] 0 0 0 0 0 0 0 0 0 0 ...\n $ guncertain2       : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ guncertain3       : logi [1:209706] NA NA NA NA NA NA ...\n $ individual        : num [1:209706] 0 0 0 0 0 0 0 0 0 0 ...\n $ nperps            : num [1:209706] NA 7 NA NA NA -99 3 -99 1 1 ...\n $ nperpcap          : num [1:209706] NA NA NA NA NA -99 NA -99 1 1 ...\n $ claimed           : num [1:209706] NA NA NA NA NA 0 NA 0 1 0 ...\n $ claimmode         : num [1:209706] NA NA NA NA NA NA NA NA 1 NA ...\n $ claimmode_txt     : chr [1:209706] NA NA NA NA ...\n $ claim2            : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ claimmode2        : logi [1:209706] NA NA NA NA NA NA ...\n $ claimmode2_txt    : logi [1:209706] NA NA NA NA NA NA ...\n $ claim3            : logi [1:209706] NA NA NA NA NA NA ...\n $ claimmode3        : logi [1:209706] NA NA NA NA NA NA ...\n $ claimmode3_txt    : logi [1:209706] NA NA NA NA NA NA ...\n $ compclaim         : logi [1:209706] NA NA NA NA NA NA ...\n $ weaptype1         : num [1:209706] 13 13 13 6 8 5 5 6 8 8 ...\n $ weaptype1_txt     : chr [1:209706] \"Unknown\" \"Unknown\" \"Unknown\" \"Explosives\" ...\n $ weapsubtype1      : num [1:209706] NA NA NA 16 NA 5 2 16 19 20 ...\n $ weapsubtype1_txt  : chr [1:209706] NA NA NA \"Unknown Explosive Type\" ...\n $ weaptype2         : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ weaptype2_txt     : chr [1:209706] NA NA NA NA ...\n $ weapsubtype2      : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ weapsubtype2_txt  : chr [1:209706] NA NA NA NA ...\n $ weaptype3         : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ weaptype3_txt     : chr [1:209706] NA NA NA NA ...\n $ weapsubtype3      : num [1:209706] NA NA NA NA NA NA NA NA NA NA ...\n $ weapsubtype3_txt  : chr [1:209706] NA NA NA NA ...\n $ weaptype4         : logi [1:209706] NA NA NA NA NA NA ...\n $ weaptype4_txt     : logi [1:209706] NA NA NA NA NA NA ...\n $ weapsubtype4      : logi [1:209706] NA NA NA NA NA NA ...\n $ weapsubtype4_txt  : logi [1:209706] NA NA NA NA NA NA ...\n $ weapdetail        : chr [1:209706] NA NA NA \"Explosive\" ...\n $ nkill             : num [1:209706] 1 0 1 NA NA 0 0 0 0 0 ...\n  [list output truncated]\n\n\n\n\nTidying Data\nThere are different aspects to look into while cleaning a data set. Null values can be removed, redundant values can be avoided, re-coding data categories and much more.\n\n1. Removing null values\nGenerally, it is good to have data which has less than 5% of NA values for analysis. But for my study, I am pushing it to 10%. I am dropping all those fields that have more than 10% of Null values. This is to ensure generalization and to avoid incorrect results.\n\n# To find those columns which have more than 10% of null values\ncols <- list()\nfor (col in names(gtd)) {\n  nullVal <- (sum(is.na(gtd[,col]))/nrow(gtd))*100\n  if (nullVal > 10){\n    cols <- append(cols, col)\n  }\n}\nprint(paste(\"Number of columns with NA > 10% is\", length(cols)))\n\n[1] \"Number of columns with NA > 10% is 90\"\n\ngtd_select <- gtd %>% select(-(unlist(cols)))\n\nAfter removing the null values there are 45 fields that are remaining.\n\n2. Data Summary & Cleaning\nThe column eventID 12 digit event ID system where in the first 8 numbers correspond to the date in “yyyymmdd” format. The last 4 numbers are sequential case number for that given day of the format 0001, 0002, etc. This can be removed and new column incDate is created combining the iyear, imonth and iday. This field will be useful for creating time-series plot. Fields with encoded text categories are also removed.\n\ngtd_select <- gtd_select %>% mutate(\n  incDate = ymd(str_c(iyear,imonth,iday, sep=\"/\")),.before = iyear) %>%\n  select(-c(eventid, country,region,specificity,doubtterr,attacktype1,targtype1,targsubtype1,natlty1,weaptype1))\n\nUsing summarytools to get an overview of the data. The min, max, most frequently occurring categories, the percentage of null values, the data types and more information can be obtained from this.\n\ndfSummary(gtd_select)\n\n\n\n  \n\n\n\nFrom the above table we can see that the range of the data is from January 1970 to December 2020, spanning over 50 years. For the columns imonth and iday, there are entries of 0. This is done for those incidents where the date and month are not surely known. There are 891 values like that which can be retained for now and modified when needed for the visualizations.\n\ngtd_select %>% filter(imonth == 0 | iday ==0) %>% count()\n\n\n\n  \n\n\n\nWe can see how the number of incidents has changed over time using a time series plot.\n\nincCount <- gtd_select %>% group_by(iyear) %>% summarise(incidents = n())\nggplot(incCount, aes(x=iyear, y=incidents, color=\"red\")) + ggtitle(\"Global Terror Incidents 1970-2020\") +labs(y=\"Incidents\", x =\"Year\") + theme(axis.text.x=element_text(angle=60, hjust=1)) +\n  geom_line() + geom_point() + theme(legend.position = \"none\")\n\n\n\n\nThe extended column tells if the duration of an incident is more than 24 hours (then 1) or if it’s less than 24 hours (then 0). The country_txt column shows the top countries where the most terror incidents have occurred. We can see that Iraq has the highest number of incidents, then comes Afghanistan, Pakistan, India and Columbia. A lot of middle east and Asian countries have the highest terror incidents. On plotting region_txt, we can see the distribution of the incidents across 12 different regions. The countries that come under each of these regions can be referred from the GTD Codebook. The plot shows that Middle East & North Africa and South Asia have nearly same number of terror incidents and constitute for nearly 53% of global terror incidents.\n\n# top regions with most terror attacks over the years\nregionCnt <- gtd_select %>% group_by(region_txt) %>% summarise(Incidents = n()) %>% arrange(desc(Incidents)) %>% head(n=10)\n\nregionCnt %>% arrange(desc(Incidents)) %>%\n  mutate(name=factor(region_txt, levels=Incidents))  %>%\n  ggplot(aes(x=reorder(region_txt, +Incidents), y=Incidents)) +\n    geom_segment(aes(xend=region_txt, yend=0)) +\n    geom_point(size=4, color=\"red\") +\n    coord_flip() +\n    theme_bw() + ggtitle(\"Region-Wise Terror Incidents 1970-2020\") + labs(y=\"Incidents\", x =\"Regions\")\n\n\n\n\nNext, I want to see those cities in Western Europe (specifically) where highest number of terror incidents have occurred. Plotting the top 10 cities. There is an entry called unknown which can be ignored for this analysis.\n\ncities <- gtd_select %>% filter(region_txt == \"North America\" & city != \"Unknown\") %>% group_by(city, longitude, latitude) %>% summarise(count = n()) %>% arrange(desc(count)) %>% head(n=11)\n\nusa <- map_data(\"usa\")\ncanada <- map_data(\"worldHires\", \"Canada\")\nmexico <- map_data(\"worldHires\", \"Mexico\")\n\nNAmap <- ggplot() + geom_polygon(data = usa,\n                                 aes(x=long, y = lat, group = group),\n                                 fill = \"white\",\n                                 color=\"black\") +\n    geom_polygon(data = canada, aes(x=long, y = lat, group = group),\n                 fill = \"white\", color=\"black\") +\n    geom_polygon(data = mexico, aes(x=long, y = lat, group = group),\n                 fill = \"white\", color=\"black\") +\n    coord_fixed(xlim = c(-140, -55),  ylim = c(10, 85), ratio = 1.2)\n\nNAmap + geom_point(data=cities, aes(x=longitude, y=latitude, color=city, label=city), size=3.0) + ggtitle(\"Cities with highest terror rate in North America 1970-2020\") +\n  theme(line = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n          rect = element_blank()) + labs(x=\"\", y=\"\",color=\"City\")\n\n\n\n\nThe next column vicinity has 1, if the incident happened in immediate vicinity of the city or has 0, if the incident happened in the city itself. The fields crit1, crit2 and crit3 indicate the criteria under which the incident has occurred. * Criterion 1: POLITICAL, ECONOMIC, RELIGIOUS, OR SOCIAL GOAL * Criterion 2: INTENTION TO COERCE, INTIMIDATE OR PUBLICIZE TO LARGER AUDIENCE(S) * Criterion 3: OUTSIDE INTERNATIONAL HUMANITARIAN LAW There are incidents where there is an overlap of all three criterias. These can be filtered for further analysis later on.\nmultiple corresponds to 1, if the incident was part of multiple attacks, else it is 0. The percentage of single incidents is much higher than multiple incidents. success corresponds to 1, if the attack was successful, else it is 0. The attacks can be assassination, armed assault, bombing/explosion, hijacking, hostage taking(barricade or kidnapping), facility/infrastructure attack and unarmed assault. There have been nearly 88% of successful attacks compared to 12% of unsuccessful attacks. suicide corresponds to 1, if it was a suicide attack where the perpetrator did not intend to escape from the attack alive, 0 otherwise. Only 3.5% of the attacks were suicide attacks. This can mean that the perpetrators intended to live to carry out future attacks.\nattacktype1_txt has 9 subcategories of the type of terror attack. One of the field is unknown wherein the attack type could not be determined from the available information. We can see from the plot, that through all years, Bombing/Explosin has been the most common attack type.\n\n# bar plot\nattck <- gtd_select %>% filter(attacktype1_txt != 'Unknown') %>% group_by(iyear, attacktype1_txt) %>% summarise(incidents = n())\nggplot(attck, aes(fill=attacktype1_txt, y=incidents, x=iyear)) +\n    geom_bar(position=\"stack\", stat=\"identity\") + labs(x=\"Year\", y=\"Incidents\", title =\"Terror Attack Types Over The Years\", fill=\"Attack Type\")\n\n\n\n\ntargtype1_txt corresponds to the general type of target/victim. There are 22 categories with the highest category being Private Citizens and Property. Next comes the Military, then the Police, then the Government and then Business. There is an Unknown category here as well. We can ignore targsubtype1_txt for now as it has nearly 112 different subcategories of the main target type. target1 is also too broad a category and is ignored for now.\nnatlty1_txt is the nationality of the Target/Victim. Here, in most cases it is same as that of the country in which the incident took place, but for Hijacking incidents, it is the nationality of the plane and not the passengers. Plotting a graph to see the nationalities of the planes in Hijacking incidents. The most number of such incidents has happened in India followed by Colombia. We can also see a category called International, which might imply that the incident happened while flying over more than one country.\n\nntlt <- gtd_select %>% filter(attacktype1_txt == 'Hijacking') %>% group_by(natlty1_txt) %>% summarise(incidents = n()) %>% arrange(desc(incidents)) %>% head(n=10)\n\nntlt %>% ggplot(aes(x=reorder(natlty1_txt, +incidents), y=incidents)) +\n  geom_bar(stat=\"identity\", fill=\"#f68060\", alpha=.6, width=.4)+\n    coord_flip() +\n    theme_bw() + ggtitle(\"Flight Hijakcing Incidents 1970-2020\") + labs(x=\"Country\", y =\"Incidents\")\n\n\n\n\ngname tells us about the extremist group that was responsible for the terror attack. For nearly 43% of the incidents, the group that is responsible is unknown. From the known groups, the highest is Taliban followed by ISIS, then Shining Path (SL) and then Al-Shabaab.\nguncertain1 corresponds to 1, if the information reported about the attack group is based on speculation or dubious claims of responsibility. It is 0, if the perpetrator for the incident is not suspected. This value is nearly 92.4% for 0.\nindividual is 1, if the perpetrator was not affliated to any known group and was by themselves. It was 0 otherwise. Only 0.4% of the incidents were caused by such individuals.\nweaptype1_txt correspond to the different categories of weapons used. There are 13 different categories including Other (weapons that do not fit into the other categories) and Unknown (weapon type could not be determined). The most used weapon is explosives, followed by Firearms.\n\nweap <- gtd_select %>% group_by(weaptype1_txt) %>% summarise(incidents = n()) %>%\n  mutate(weaptype1_txt =\n    case_when(\n      weaptype1_txt == \"Vehicle (not to include vehicle-borne explosives, i.e., car or truck bombs)\" ~ \"Vehicle\",\n      TRUE ~ as.character(weaptype1_txt)\n    ), fraction = incidents/sum(incidents), ymax = cumsum(fraction), ymin = c(0,head(ymax, n=-1))\n  )\n\n\n# Make the plot\nggplot(weap, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=weaptype1_txt)) +\n     geom_rect() +\n     coord_polar(theta=\"y\") + scale_fill_brewer(palette = \"Paired\") +\n     xlim(c(2, 4)) + guides(fill = guide_legend(title = \"Weapon Type\")) +\n  theme(panel.background = element_rect(fill = \"white\"),\n        panel.grid = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank()) + ggtitle(\"Weapons Used for Terror Incidents 1970-2020\")\n\n\n\n\nnkill and nwound are numerical fields that correspond to the number of people killed and the number of people wounded respectively. From these fields less than 10% of the data are Nulls. We can combine these two into a single field to find the total number of casualties due to a terror incident. The mean casualties is near 5, the median is 1. The min is 0 and max is 12263. On plotting the histogram, it is seen that the plot is severely right skewed (highly positive skewness value).\n\ngtd_select <- gtd_select %>% mutate(\n  ncasual = nkill+nwound, .before = nkill\n)\ncas <- gtd_select %>% filter(is.na(ncasual)==FALSE)\n\nsummary(cas$ncasual)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.000     0.000     1.000     5.234     4.000 12263.000 \n\nskewness(cas$ncasual)\n\n[1] 204.8766\n\nggplot(cas, aes(x=ncasual))+\n  geom_histogram(color=\"darkblue\", fill=\"lightblue\") + ggtitle(\"Casualties Histogram\") +\n  labs(x=\"Casualties\", y=\"Incidents Count\")\n\n\n\n\nproperty is 1 (49.1%), if property was damaged, else it is 0 (37.5%). There is also another entry of ‘-9’ (13.4%) which corresponds to those incidents for which there is not enough data. This can be ignored during visualization.\nishostkid is 1 (8.0%), if the victim was taken hostage or kidnapped, else it is 0 (91.7%). The ‘-9’ (0.3%) category corresponds to unknown entries which can be ignored during visualization.\ndbsource had details about the teams that took efforts to collect and consolidate all this data. Currently, it is being maintained and constantly updated by the START team at University of Maryland. But, there have been other groups that have helped with this data collection through the years. From the summarytable, we can see that nearly 50% of the data was collected by START and nearly 30% of the data was collected by PGIS. But I would like to know which team was responsible for the collection of the data through the years. From the plot, we can see that during the early years, PGIS played a major role in collecting the data, then CETIS for a couple of years, followed by ISVG. Finally START took over to maintain the database entirely. Overlapping of these major teams is not seen from the plot. There are however, other smaller teams which have contributed occasionally to the database.\n\n#Grouping the data and choosing only the top 8 teams (START, PGIS, ISVG, CETIS, CAIN, UMD Schmid 2012, Hewitt Project, UMD Algeria 2010-2012) that have aided with the data collection process.\n\nsources <- c('START', 'PGIS', 'ISVG', 'CETIS', 'CAIN', 'UMD Schmid 2012', 'Hewitt Project', 'UMD Algeria 2010-2012')\ndbsrc <- gtd_select %>% filter(str_detect(dbsource, str_c(sources, collapse = \"|\")), is.na(incDate) == FALSE)\n\nnew <- dbsrc %>% mutate(\n  yrmon = ym(str_c(iyear,imonth, sep=\"/\"))) %>% select(c(yrmon,dbsource)) %>% group_by(yrmon, dbsource) %>%\n  summarise(count= n())\n\nggplot(new, aes(x=yrmon, y=count, fill=dbsource)) +\n    geom_area() + ggtitle(\"Database Collection from 1970 through 2020\") +\n  labs(x=\"Year\", y=\"Incidents\", fill=\"Source\")\n\n\n\n\nINT_LOG, INT_IDEO,INT_MISC, INT_ANY are representations of international attacks. If the value is 1, it implies that the nationality of the attack group/perpetrator is different from that of the victim/target. It is 0 otherwise. There is also ‘-9’ for those incidents that do not have enough information. Here I am ignoring INT_MISC and combining the remaining three columns into a single column. Even if any one of the fields have 1, then it will be considered international. If all have ‘-9’, then those rows are dropped for visualization.\nOn dropping the unknown values, we can see that International Terror incidents were nearly 37% and domestic events were nearly 63%. To explore further, I want to see how many terror attacks were domestic and how many were by international extremists in Western Europe Vs South Asia.\n\ngtd_select <- gtd_select %>% select(-c(INT_MISC)) %>%\n  mutate(intNew =\n    case_when(\n      INT_ANY == -9 & INT_LOG == -9 & INT_IDEO == -9 ~ -9,\n      INT_ANY == 1 | INT_LOG == 1 | INT_IDEO == 1 ~ 1,\n      INT_ANY == 0 | INT_LOG == 0 | INT_IDEO == 0 ~ 0,\n      TRUE ~ as.numeric(-9)\n    )\n  ) %>% select(-c(INT_ANY,INT_LOG,INT_IDEO))\n\n\nint <- gtd_select %>% filter(intNew!=-9) %>%  group_by(intNew, region_txt) %>%\n  summarise(fatal = sum(ncasual, na.rm=TRUE)) %>%\n  mutate(intNew =\n    case_when(\n      intNew == 1 ~ 'International',\n      intNew == 0 ~ 'Domestic',\n      TRUE ~ as.character(\"na\")\n    )\n  )\n\nggplot(int, aes(y=fatal, x=intNew, fill=intNew)) +\n    geom_bar(position=\"stack\", stat=\"identity\") + labs(x=\"\", y=\"Incidents\", title =\"Domestic Vs International Terror Attacks across regions from 1970-2020\", fill=\"Category\" ) + facet_wrap(~region_txt)\n\n\n\n\nFrom the above plot, we can see the variations between different regions. In South Asia, South America, Sub-Saharan Africa we see more domestic attacks than international. While in North America, Western Europe, Middle East & North Africa, we see more international attacks than domestic. More analysis can be done on the origin of the extreme groups and their targets. This can help us understand if there is a direct relation between the main location of the extremist group and the location of their target. It could explain the higher number of domestic threats in South Asia, which could be because of higher number of extremist groups from that region.\nSo far, I have included descriptive statistics of the data. I’ve retained all the necessary fields and values upon which I can perform further analysis to find answers to my research questions.\n\n\n\n\nResearch Questions\n\nHow has terrorism spread throughout the world since 1970 until now? Are there some regions which have constantly faced terror attacks? Are there regions that had previously no terror attacks, but are suddenly under plenty of terror attacks?\nCan correlations be drawn between different numerical fields of this dataset? This can help look at relations between certain factors in the dataset.\nDo certain terror groups have certain ways of attacking? What kind of weapons do they use the most?\nTrends of the most popular terror groups throughout the years. It’ll be good to visualize the rise and fall of different extremist groups over the years. This visualization can help us ponder into why a particular group thrived during certain periods.\nWhat is the average number of people who are killed per terror attack? How does this change region-wise?"
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html#visualization",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html#visualization",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "Visualization",
    "text": "Visualization\nWith the tidied dataset, I would like to answer the research questions that I had posed in the previous section.\n\nViz 1\nFirstly I am splitting the years into four sections - 1970-1984, 1984-1999, 2000-2009, 2010-2020. Plotting the world map for each incident separately along with the distribution of successful and unsuccessful attacks are also shown.\n\n# create data for world coordinates using map_data() function\nworld <- map_data(\"world\")\n\n# converting the success from numerical into categorical data type\n\ngtd_select <- within(gtd_select, {   \n  success.cat <- NA # need to initialize variable\n  success.cat[success == 0] <- \"Unsuccessful\"\n  success.cat[success == 1] <- \"Successful\"\n   } )\n\nplot0 <- gtd_select %>% filter(iyear < 1985)\nplot1 <- gtd_select %>% filter(iyear >= 1985 & iyear < 2000)\nplot2 <- gtd_select %>% filter(iyear >= 2000 & iyear < 2010)\nplot3 <- gtd_select %>% filter(iyear >= 2010)\n\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"lightgray\", size = 0.1\n  ) + geom_point(\n    data = plot0,\n    aes(longitude, latitude,\n        color = success.cat),\n    alpha = 0.5\n  ) +\n  theme_void() + labs(x = NULL, y = NULL, color = \"Attack Status\")+\n  labs(title=\"Global Terror Attacks 1970-1984\")\n\n\n\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"lightgray\", size = 0.1\n  ) + geom_point(\n    data = plot1,\n    aes(longitude, latitude,\n        color = success.cat),\n    alpha = 0.5\n  ) +\n  theme_void() + labs(x = NULL, y = NULL, color = \"Attack Status\")+\n  labs(title=\"Global Terror Attacks 1984-1999\")\n\n\n\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"lightgray\", size = 0.1\n  ) +\n  geom_point(\n    data = plot2,\n    aes(longitude, latitude,\n        color = success.cat),\n    alpha = 0.5\n  ) + labs(x = NULL, y = NULL, color = \"Attack Status\")+\n  theme_void() +\n  labs(title=\"Global Terror Attacks 2000-2009\")\n\n\n\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"lightgray\", size = 0.1\n  ) +\n  geom_point(\n    data = plot3,\n    aes(longitude, latitude,\n        color = success.cat),\n    alpha = 0.5\n  ) +\n  labs(x = NULL, y = NULL, color = \"Attack Status\")+\n  theme_void() +\n  labs(title=\"Global Terror Attacks 2010-2020\")\n\n\n\n\nThere is stark difference between the four plots. We can very clearly see that there has been a rise in the incidents from 1984-1999 and then it has reduced from 2000-2009. Then again during 2010-2020, the incidents have risen globally. Over the years, there has been an increase in the number of incidents in mid African region. In South America, compared to 1984-1999, the incidents of terrorism is lower in 2010-2020. We can also see that the density of the incidents has risen by large proportions in regions of India, Afghanistan, Pakistan, Iran, Iraq and parts of Europe. These plots give a good idea about how terror activites have spread over the years. The difference could also be because of military coups or independence struggles in a lot of countries especially in South America and Africa which would’ve occurred during 1984-1999. The figure also shows that there have been far many successful terror attacks than unsuccessful ones.\n\n\nViz 2\nDrawing a correlation plot can help us determine those numerical variables that are related to other variables. I would like to know such factors which have strong correlation as it can help derive lots of conclusions. Some of the categorical variables like extended, multiple, success are all one-hot encoded and hence can be used as numerical data.\n\ncorFeat <- gtd_select[,c(\"iyear\",\"imonth\",\"iday\",\"extended\",\"vicinity\",\"multiple\",\"success\",\"suicide\",\"guncertain1\",\"individual\",\"nkill\", \"nwound\",\"property\",\"ishostkid\",\"intNew\")]\ncorFeat <- na.omit(corFeat)\ncorrelations <- cor(corFeat)\ncorrplot(correlations, method=\"circle\", order = 'alphabet')\n\n\n\n\nFrom this plot we can see that there is not much correlation between the numerical features of the cleaned data. Only nwound and nkill tend to show high correlation. This is probably because they both are the casualties and terror attack damages can cause death or injure civilians/people. We can also see low correlation between property damage feature and the year. Another low correlation is also seen between intNew and guncertain1. The small correlation between idhostkid and extended can be explained. extend tells if the terror activity is for more than 24 hours or not and idhostkid, tells us if the terror groups have kidnapped hostages or not. Usually, there will be negotiations in such hostage situations which will take time and hence can relate to the terror incident lasting for more than 24 hours. The relations between other factors seem to be pretty weak.\n\n\nViz 3\nHere, I am selecting the top 6 deadly terror groups based on the highest number of casualties. Then on plotting the different attack types used by each group, we can see the most popular method of terror attack that a group follows. Furthermore, on including the weapon type as a sub group, we can see the respective weapons that are being used for carrying out the attacks.\n\ntrgrp <- gtd_select %>% select(iyear, incDate, gname, region_txt) %>% filter(gname != \"Unknown\") %>%\n  group_by(gname) %>% summarise(incidents = n()) %>% arrange(desc(incidents)) %>% head(n=6) \nnames <- c(trgrp$gname)\nint <- gtd_select %>% select(attacktype1_txt, weaptype1_txt, gname) \ngrpattack <- subset(int, gname %in% names) %>% group_by(gname, attacktype1_txt, weaptype1_txt) %>%\n  summarise(incidents = n()) %>% \n  mutate(weaptype1_txt =\n    case_when(\n      weaptype1_txt == \"Vehicle (not to include vehicle-borne explosives, i.e., car or truck bombs)\" ~ \"Vehicle\",\n      TRUE ~ as.character(weaptype1_txt)\n    ))\n\nggplot(grpattack, aes(x=incidents, y=attacktype1_txt , fill=weaptype1_txt)) +\n    geom_bar(position=\"stack\", stat=\"identity\") + labs(y=\"Attack Type\", x=\"Incidents\", title =\"Attack Type Distribution for Top 6 Terror Groups 1970-2020\", fill =\"Weapon Type\") + facet_wrap(~gname) \n\n\n\n\nFrom above, we can see that Taliban has the highest number of attacks with Bombing/Explosion and Armed Assault being nearly equal. There is also an Unknown type of attack that the Taliban follow which is used as much as Bombing/Explosion and Armed Assault. It is good to do research into what this Unknown type might be as it constitutes a major portion of the attack for the most deadliest group. Similarly for other groups like ISIL, Shining Path, we can see that Bombing/Explosion and Armed Assault are the most common attack types. For these attacks, Explosives and Firearms are the weapons that are used. For the Unknown attack type, weapons that are still Unknown are used. Out of the different weapon types, Chemical, Melee, Sabotage Equipment and Vehicles are almost not used or very minimally used. Unarmed Assault, Hostage Taking (Barricade Incident) and Hijacking are also the least popular attack types among these groups. This plot gives an overall idea of the most preferred attack types and weapon types that are used by the deadliest terror groups around the world.\n\n\nViz 4\nI wanted to see how the different extremist groups have lasted through different years. Was there any group that had a downfall? Was there any group that had a sudden uprising? Selecting a few groups only as there are 2000+ such extremist groups.\n\ngrp <- gtd_select %>% select(iyear, incDate, gname, region_txt) %>% filter(gname != \"Unknown\") %>% filter(gname == \"Taliban\" | gname == \"Islamic State of Iraq and the Levant (ISIL)\" | gname == \"Al-Shabaab\" | gname == \"Boko Haram\" | gname == \"Shining Path (SL)\" | gname == \"New People's Army (NPA)\" | gname == \"Farabundo Marti National Liberation Front (FMLN)\" | gname == \"Houthi extremists (Ansar Allah)\" | gname == \"Irish Republican Army (IRA)\" | gname == \"Kurdistan Workers' Party (PKK)\") %>% \n  group_by(iyear, gname) %>% summarise(incidents = n()) %>% arrange(desc(incidents))\n\n\nggplot(grp, aes(x=iyear, y=incidents, color=gname)) + ggtitle(\"Terror Group Wise Attacks 1970-2020\") + labs(y=\"Incidents\", x =\"Year\", color=\"Terror Groups\") + theme(axis.text.x=element_text(angle=60, hjust=1)) +\n  geom_line() + geom_point(size=0.6) \n\n\n\n\nFrom this plot, we can very clearly see that initially there were only about 2-4 groups that were deadly. But in 2020, there are nearly 8 of these deadly terror groups that exist and have carried out terror activities. The Taliban started off in the 2000s and has jumped quickly from 2010 to 2020 to be the most deadly terror group globally. Similarly the ISIL group is also relatively new, having started off in 2013 and peaking during 2016-2017 and has declined in 2020. Furthermore, between the periods of 1996 to 2005, it looks like these terror groups were nascent, waiting to carry out the terror activities over the next decade. This line graph gives an overview of how these different terror groups have grown and declined over the years.\n\n\nViz 5\nI wanted to find the average number of casualties (deaths + wounded) in a terror attack in each region. This can tell the degree of damage that can be expected if a terror attack occurs in a particular place.\n\navgCas <- gtd_select %>% select(ncasual, region_txt) %>% group_by(region_txt) %>% summarise(sumcas= sum(ncasual, na.rm=TRUE), incident= n()) %>% mutate(\n  avgCasReg = sumcas/incident\n) %>% arrange(desc(avgCasReg)) %>% select(region_txt, avgCasReg) %>% rename(Region = region_txt, AverageCasualtyPerAttack = avgCasReg)\navgCas\n\n\n\n  \n\n\n\nEast Asia has the highest average number of casualties per attack. Next is North America. Though South Asia has the second highest number of terror attacks, the average casualty per attack is somewhat in the median of the values. South America has the least average number of casualties per attack."
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html#reflection",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html#reflection",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "Reflection",
    "text": "Reflection\nThis project helped me understand the process of data visualization and analysis. How to convert raw data into information that can be easily conveyed and is useful? For this particular study, I had to make a couple of assumptions in order to simplify the dataset. The codebook had mentioned very nuanced details with respect to the attack type, the duration of the event, if multiple incidents were linked, is the incident idealogically or logistically international and much more. While performing the data analysis I made sure to read through the description and only use that information which will be useful for my work. Given that this dataset is pretty huge I found it hard to figure out how to clean the dataset and reduce the dimensions. Initially I wasn’t sure if I had to drop certain null values, or if I should retain some categories or should I re code Unknown variables and much more. Nearly 80% of my time was spent in cleaning the data to arrive at what was needed for this study. There is so much more that can be done with this dataset. Getting into the granularity of each country and analysing the activities of each terror group can give a lot of insights into local terror groups. This dataset also had a lot of textual data in the form of summary and notes. NLP methods can be applied to these text based fields to group most commonly used words occurring with an event, etc. In my study, I have not imputed the data for a lot of fields, instead I dropped them. But some fields can be imputed based on previous activities. This can help maintain the features of the dataset. Currently, I had dropped more than 50% of the columns as there were plenty of null values."
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html#conclusion",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html#conclusion",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "Conclusion",
    "text": "Conclusion\nThis study has answered a lot of questions that had me pondering upon initially. Incidents of terror attacks have increased during the last 10 years when compared with the years before that. However, these terror incidents are not always constant in a particular region. There are groups that become nascent for sometime before rising up again. Correlation between certain features of the data could not be drawn successfully. But correlated features could be identified if the region or country is narrowed down. The Taliban is the deadliest terror group with the highest number of casualties. Their most common type of attack methods include Bombing & Explosions and Armed Assault. They also do have other types of attack methods which have not been identified currently and need to be explored. Though the number of incidents and casualties are higher in Middle East & North Africa and South Asian regions, the average number of casualties is highest in East Asia followed by North America. There is no limit to the inferences that can be made from analysing this dataset. It’ll be good to know why certain terror groups rose and became frequent with their terror activity and why suddenly they declined. This can help us relate to other global scenarios that have occurred. Some terror activities may have had long term impact on victims. Some victims/targets may not have been killed during the incident, but they could’ve passed away sometime after the incident. Though it is hard to keep track of this, it’ll help provide an accurate number for the number of casualties. The dataset had extensive information that could answer all of my research questions. But it would be beneficial to get into the granularity of the dataset for intricate analysis."
  },
  {
    "objectID": "posts/VishnupriyaVaradharaju_FinalProject.html#bibliography",
    "href": "posts/VishnupriyaVaradharaju_FinalProject.html#bibliography",
    "title": "How has Terrorism Grown Over The Years ? - A Visual Study",
    "section": "Bibliography",
    "text": "Bibliography\n\n[1] https://www.start.umd.edu/gtd/\n[2] https://www.r-project.org\n[3] https://r4ds.had.co.nz/index.html\n[4] https://r-graph-gallery.com/218-basic-barplots-with-ggplot2.html#horiz\n[5] https://rforhr.com/arrange.html\n[6] https://ggplot2.tidyverse.org/index.html\n[7] https://www.kaggle.com/datasets/START-UMD/gtd?datasetId=504&sortBy=voteCount\n[8] https://www.huffingtonpost.co.uk/2015/11/28/islamic-state-terrorism-threat_n_8670458.html\n[9] https://rpubs.com/julianhatwell/gtd\n[10] https://www.datanovia.com/en/blog/how-to-create-a-map-using-ggplot2/"
  }
]