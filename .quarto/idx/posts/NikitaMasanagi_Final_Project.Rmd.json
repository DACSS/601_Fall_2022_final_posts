{"title":"San Francisco Crime Data Exploration","markdown":{"yaml":{"title":"San Francisco Crime Data Exploration","author":"Nikita Masanagi","desription":"Crime Analysis","data":"San Francisco Crime","date":"12/14/2022","format":{"html":{"toc":true,"code-fold":false,"code-copy":true,"code-tools":true}}},"headingText":"Import important libraries needed for functions","containsRefs":false,"markdown":"\n\n```{r}\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(summarytools)\nlibrary(dplyr)\nlibrary(lubridate)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n```\n\n1)  Dataset Description\n\nThe dataset is San Francisco dataset, from 2018 to present. The description of each column is -\n\nIncident Datetime - The date and time when the incident occurred\n\nIncident Date - The date the incident occurred\n\nIncident Time- The time the incident occurred\n\nIncident Year - The year the incident occurred, provided as a convenience for filtering\n\nIncident Day of Week - The day of week the incident occurred\n\nReport Datetime - Distinct from Incident Datetime, Report Datetime is when the report was filed.\n\nRow ID - A unique identifier for each row of data in the dataset\n\nIncident ID - This is the system generated identifier for incident reports. Incident IDs and Incident Numbers both uniquely identify reports, but Incident Numbers are used when referencing cases and report documents.\n\nIncident Number - The number issued on the report, sometimes interchangeably referred to as the Case Number. This number is used to reference cases and report documents.\n\nCAD Number - The Computer Aided Dispatch (CAD) is the system used by the Department of Emergency Management (DEM) to dispatch officers and other public safety personnel. CAD Numbers are assigned by the DEM system and linked to relevant incident reports (Incident Number).\n\nReport Type Code - A system code for report types, these have corresponding descriptions within the dataset.\n\nReport Type Description - The description of the report type, can be one of: Initial; Initial Supplement; Vehicle Initial; Vehicle Supplement; Coplogic Initial; Coplogic Supplement\n\nFiled Online - Non- emergency police reports can be filed online by members of the public using SFPD’s self-service reporting system called Coplogic Values in this field will be “TRUE” if Coplogic was used to file the report.\n\nIncident Code - Incident Codes are the system codes to describe a type of incident. A single incident report can have one or more incident types associated. In those cases you will see multiple rows representing a unique combination of the Incident ID and Incident Code.\n\nIncident Category - A category mapped on to the Incident Code used in statistics and reporting. Mappings provided by the Crime Analysis Unit of the Police Department.\n\nIncident Subcategory - A subcategory mapped to the Incident Code that is used for statistics and reporting. Mappings are provided by the Crime Analysis Unit of the Police Department.\n\nIncident Description - The description of the incident that corresponds with the Incident Code. These are generally self-explanatory.\n\nResolution - The resolution of the incident at the time of the report. Can be one of: • Cite or Arrest Adult • Cite or Arrest Juvenile* • Exceptional Adult • Exceptional Juvenile* • Open or Active • Unfounded \nJuvenile information not maintained in the dataset. \n\nIntersection - The 2 or more street names that intersect closest to the original incident separated by a backward slash.\n\nCNN - The unique identifier of the intersection for reference back to other related basemap datasets.\n\nPolice District - The Police District where the incident occurred. District boundaries can be reviewed in the link below. \n\nAnalysis Neighborhood - This field is used to identify the neighborhood where each incident occurs. Neighborhoods and boundaries are defined by the Department of Public Health and the Mayor's Office of Housing and Community Development. \n\nSupervisor District - There are 11 members elected to the Board of Supervisors in San Francisco, each representing a geographic district. The Board of Supervisors is the legislative body for San Francisco. The districts are numbered 1 through 11. \n\nLatitude - The latitude coordinate in WGS84, spatial reference is EPSG:4326\n\nLongitude - The longitude coordinate in WGS84, spatial reference is EPSG:4326\n\nPoint - Geolocation in OGC WKT format (e.g, POINT(37.4,-122.3)\n\n\n\nAs we can see that this dataset is large, with many fields.To perform some infer analysis and infer from the data, we will need to perform some cleaning, tidying and identify which of these fields can be beneficial to us.\n\nAs we can see that this dataset is large, with many fields.To perform some infer analysis and infer from the data, we will need to perform some cleaning, tidying and identify which of these fields can be beneficial to us.\n\n\n###Dataset\n\nReading in the data\n```{r}\n# Read data from the csv file\ndata <- read.csv(\"_data/SF_Incident_Reports__2018_to_Present.csv\")\n\n```\n\nThe dimensions of the data\n```{r}\ndim(data)\n```\n\nThe datatype of each column\n```{r}\nstr(data)\n```\n\nNow let us check how many null values are present in the dataset.\n\n```{r}\nsum(is.na(data))\n```\n\nLet us check which columns have how many missing values.\n```{r}\nmissing_vals <- colSums(is.na(data))\nmissing_vals[sapply(missing_vals, function(x) any(x > 0))]\n```\nAs we can see, there are some columns with more than 5 lakh nulll values. We can handle that when we are cleaning the data\n\nDataframe summary\n```{r}\nprint(summarytools::dfSummary(data,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n```\n\n\n```{r}\nhead(data)\n```\n### Data Cleaning\n\nThe columns below have maximum of null values and wont be much benefical to us for analysis. Hence we can drop them.\nCivic.Center.Harm.Reduction.Project.Boundary\nHSOC.Zones.as.of.2018.06.05\nInvest.In.Neighborhoods..IIN..Areas\nESNCAG...Boundary.File Central.Market.Tenderloin.Boundary.Polygon...Updated \nCivic.Center.Harm.Reduction.Project.Boundary \n\n```{r}\ncleaned_data <- select(data,-28,-29,-30,-31,-32,-33,-34)\nhead(cleaned_data)\n```\n\n\n###Mutate data\n\nWe can split the column Incident Time column into Minute and hour, so we can manipulate the hour column in further analysis.\n\n```{r}\n\nmutated_data <- cleaned_data %>%\n  separate('Incident.Time',c('Incident.Hour','Incident.Minute'),sep = \":\")\n\n\nmutated_data <- mutated_data %>% \n  mutate(Incident.Month = month(Incident.Datetime, label = T))\n```\n\n\n\n```{r}\ncrime <- mutated_data\n```\n\n###QUESTIONS:\n\nAfter observing the data, certain questions pop up in my mind.We can make observations about crime has increased over the years, how COVID has affected the crime rate, the district wise distribution of crime, what is the most frequent resolution in different districts,what are the top crimes and what time does it occur,what are the crime hotspots.\n\nI will now go over these questions and try to plot interesting graphs so we can get some inference from it.\n\n##Crime over the years: \n\n#a)Distribution of crime over the years from 2018 - Present.\n\nWe can group by the Incident Year and plot the graph.\n```{r}\ncrime_per_year <- crime %>% \n  group_by(Incident.Year) %>% \n  summarise(total = n())\ncolnames(crime_per_year) <- c(\"Year\",\"Total\") \n```\n\n\n```{r}\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\n# Plot\ng <- ggplot(crime_per_year, aes(Year, Total))\ng + geom_bar(stat=\"identity\", width = 0.5, fill=\"blue\") + \n      labs(title=\"Crime in San Francisco\", \n           subtitle=\"2018-2022\", \n           caption=\" Total crime over the years\") +\n      theme(axis.text.x = element_text(angle=65, vjust=0.6))\n\n```\nFrom this graph we can observe that the crime has reduced during COVID years of 2020-2021, which can be an indicator that either the crime had reduced due to lockdown conditions or the reported crime had reduced during those years.\nWe can also observe the crime decreasing slightly from 2018 to Present day.\n\n\n\n#b) Distribution of crime per category\n\nWe can create a dataframe, by grouping by on the basis of Incident Category and also calculate the percentage.\n\n```{r}\ncrime_category <- sort(table(crime$Incident.Category),decreasing = TRUE)\ncrime_category <- data.frame(crime_category[crime_category> 10000])\ncolnames(crime_category) <- c(\"Category\", \"Frequency\")\ncrime_category$Percentage <- crime_category$Frequency / sum(crime_category$Frequency)\n\n```\n\n\n\n```{r}\ncrime_category\n```\n\n\n```{r}\nlibrary(ggplot2)\nlibrary(ggrepel)\nbp<-ggplot(crime_category, aes(x=Category, y=Frequency, fill=Category)) + geom_bar(stat=\"identity\") + \n  theme(axis.text.x=element_blank()) + geom_text_repel(data=crime_category, aes(label=Category))\nbp\n\n```\nFrom the plot we can observe that count of Larceny Theft, is much much greater than the other categories.Disorderly conduct is the least frequent category.\n\n\n#c) Time Series graph for Daily Crimes from 2018-2022\n\n\nWe can mutate the date and group by date, to get the number of crimes each day.\n```{r}\nlibrary(dplyr)\n\ndf_crime_daily <- crime %>%\n  mutate(Date = as.Date(Incident.Date, \"%Y/%m/%d\")) %>%\n  group_by(Date) %>%\n  summarize(count = n()) %>%\n  arrange(Date)\nhead(df_crime_daily)\n\n```\n\n```{r}\nlibrary(ggplot2)\nlibrary(scales)\nplot <- ggplot(df_crime_daily, aes(x = Date, y = count)) +\n  geom_line(color = \"#F2CA27\", size = 0.1) +\n  geom_smooth(color = \"#1A1A1A\") +\n  # fte_theme() +\n  scale_x_date(breaks = date_breaks(\"1 year\"), labels = date_format(\"%Y\")) +\n  labs(x = \"Date of Crime\", y = \"Number of Crimes\", title = \"Daily Crimes in San Francisco from 2018 – 2022\")\nplot\n\n```\nWe can again oberve the dip during the COVID lockdown period, and in general observe around 300-500 crimes per day. With unual low as well as high spikes.\n\n\n\n\n##Crime in District -wise \n\n#a) District wise Crime Distribution\n\nWe can create a dataframe by grouping by the Police district.\n\n\n```{r}\ncrime_per_district <- crime %>% \n  group_by(Police.District) %>% \n  summarise(n = n())\n\ncolnames(crime_per_district) <- c(\"Police.District\", \"Total\")\nhead(crime_per_district )\n\n\n```\n\n```{r}\ng <- ggplot(crime_per_district, aes(Police.District, Total))\ng + geom_bar(stat=\"identity\", width = 0.5, fill=\"pink\") + \n      labs(title=\"District-wise crime in San Francisco\", \n           subtitle=\"2018-2022\", \n           caption=\" Total crime in each district\") +\n      theme(axis.text.x = element_text(angle=65, vjust=0.6))\n\n\n```\nFrom the graph, we can see Central having maximum number of crime and Out of SF is the least.Park can be said to be the safest district as it has the least amount of crime.\n\n\n#b) Resolution rates in each district\n\n```{r}\ng <- ggplot(crime, aes(Police.District))\ng + geom_bar(aes(fill=Resolution), width = 0.5) + \n  theme(axis.text.x = element_text(angle=65, vjust=0.6)) +\n  labs(title=\"Resolution in each District\", \n     )\n\n```\n\nFrom this graph, we can observe that maximum number of cases in all the districts are open or active.Tenderloin has the most arrests and Park has the least.\nThere are some unfounded resolutions in each District.\n\n\n\n##Temporal Trends\n\n#a) Hourly Crime Distribution\n\nWe can create a dataframe by grouping by Incident Hour.\n\n```{r}\ncrime_perhours <- crime %>% \n  group_by(Incident.Hour) %>% \n  summarise(TotalCrime = n())\nhead(crime_perhours)\n\n```\n```{r}\nggplot(crime_perhours, aes(x = Incident.Hour, y = TotalCrime))+\n  geom_col(fill = \"orange\")+\n  theme_minimal()+\n  labs(\n    title = \"Crime per hour, San Francisco 2018 - 2022\",\n    x = \"Hours\",\n    y = \"Total Crime\"\n  )\n\n```\nWe can observe that maximum crimes occur around 12 in the afternoon, then another spike at midnight. It then gradually decreases from 1 am - 7 am in the morning.\n\n#b) Theft time Heatmap\n\n\nOf the above hourly districution, let us focus on Larcent Theft and plot the heatmap.\n```{r}\n\ndf_theft_time <- crime %>%\n  filter(Incident.Category==\"Larceny Theft\")%>%\n  group_by(Incident.Day.of.Week, Incident.Hour) %>%\n  summarize(count = n())\nhead(df_theft_time)\n```\n\n```{r}\nplot <- ggplot(df_theft_time, aes(x = Incident.Hour, y = Incident.Day.of.Week, fill = count)) +\n  geom_tile() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.6), legend.title = element_blank(), legend.position=\"top\", legend.direction=\"horizontal\", legend.key.width=unit(2, \"cm\"), legend.key.height=unit(0.25, \"cm\"), legend.margin=unit(-0.5,\"cm\"), panel.margin=element_blank()) +\n  labs(x = \"Hour of Theft (Local Time)\", y = \"Day of Week of Theft\", title = \"Number of Thefts in San Francisco from 2018 – 2022, by Time of Theft\") +\n  scale_fill_gradient(low = \"white\", high = \"orange\")\nplot\n\n```\nFrom this heatmap, we see a spike in crimes over the weekends in the midnight. We can also seespikes during weekdays around the evenig time from 5pm-8-pm.\n\n\n##Crime hotspots\n\n#a) Map of San Francisco\n\n```{r}\n\nlibrary(ggmap)\n\nsf = get_stamenmap(bbox = c(left = -122.5164, bottom = 37.7066, right = -122.3554, top = 37.8103), \nmaptype = c(\"toner-lite\"), zoom = 13)\n\nmap = ggmap(sf)\nmap\n```\n\n#b) Plotting the first 500 random points on the map\n\n```{r}\nmap + geom_point(data = sample_n(crime, 500), aes(x = Longitude, y = Latitude))\n\n```\nWe can already observe a clustering towards the city center.\n\n\n\n#c) Density Plot\n\n\n```{r}\nmap + \nstat_density2d( data = sample_frac(crime, 0.2), aes(x = Longitude, y = Latitude, fill = ..level.., alpha = ..level..), size = 1, bins = 50, geom = 'polygon') +\nscale_fill_gradient('Crime\\nDensity', low = 'blue', high = 'orange') +\nscale_alpha(range = c(.2, .3), guide = FALSE) +\nguides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))\n\n\n```\nWe can observe certain intersections and having higher density and the maximum density being at the city center area.The parks and grassland regions have no reported crimes.\n\n\n###Conclusion\n\nThus with the help of the SAN FRANCISCO crime dataset, we could plot thee graphs and gain some insight.\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"NikitaMasanagi_Final_Project.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.37","execution":{"freeze":"auto"},"smooth-scroll":true,"page-layout":"full","mainfont":"Open Sans","linkcolor":"#881c1c","theme":{"light":["flatly","../styles.scss"],"dark":["darkly","../styles.scss"]},"comments":{"giscus":{"repo":"DACSS/601_Fall_2022","category":"Announcements"}},"toc-location":"right","search":true,"title":"San Francisco Crime Data Exploration","author":"Nikita Masanagi","desription":"Crime Analysis","data":"San Francisco Crime","date":"12/14/2022","code-copy":true},"extensions":{"book":{"multiFile":true}}}}}